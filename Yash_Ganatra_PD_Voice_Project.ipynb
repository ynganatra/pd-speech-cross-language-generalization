{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "toc_visible": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "# Cross-Language Generalization of Parkinson’s Disease Speech Detection Using Limited Target Data Calibration\n",
    "\n",
    "Yash N. Ganatra, Michael E. DeBakey High School for Health Professions, Houston, TX\n",
    "---"
   ],
   "metadata": {
    "id": "rp4QVv2f0Rd-"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Project Notes\n",
    "\n",
    "This notebook implements a speech-based Parkinson’s disease detection study focused on cross-language generalization under limited target data exposure.\n",
    "\n",
    "The project methodology, preprocessing logic, model configuration, and evaluation pipeline were defined by the author. AI-based coding and debugging support was used during software development to assist with implementation and iterative refinement or debugging. All scientific objectives, dataset selection, and evaluation criteria were determined by the author.\n",
    "\n",
    "## Project context\n",
    "This notebook contains the analysis and experiments supporting the paper:\n",
    "\n",
    "“Cross-Language Generalization of Parkinson’s Disease Speech Detection Using Limited Target Data Calibration”\n",
    "\n",
    "The notebook is intended for research and reproducibility only.\n"
   ],
   "metadata": {
    "id": "jtE0VJ_SwjYg"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Definition of the Datasets\n",
    "\n",
    "---\n",
    "\n",
    "## D1 – NeuroVoz\n",
    "\n",
    "Primary language: Castilian Spanish,\n",
    "Acronym used: ES\n",
    "\n",
    "Description:\n",
    "This dataset contains speech recordings labeled as Healthy Control or Parkinson’s Disease. It includes both sustained phonation (vowel) recordings and read or other speech tasks. Speaker and clip information is provided through CSV metadata files.\n",
    "\n",
    "Counts:\n",
    "\n",
    "Speakers\n",
    "\n",
    "* Total speakers: 107\n",
    "* Male speakers: 61\n",
    "* Female speakers: 46\n",
    "* Parkinson’s disease speakers: 52\n",
    "* Healthy control speakers: 55\n",
    "\n",
    "Audio clips\n",
    "\n",
    "* Total audio clips used: 1692\n",
    "* Vowel or sustained phonation clips: 837\n",
    "* Reading or other speech clips: 855\n",
    "\n",
    "---\n",
    "\n",
    "## D2 – EWA-DB\n",
    "\n",
    "Primary language: Slovak,\n",
    "Acronym used: SK\n",
    "\n",
    "Description:\n",
    "This dataset includes speech recordings labeled as Healthy, Parkinson’s Disease, Alzheimer’s Disease, and Alzheimer–Parkinson’s Disease. Both sustained phonation and read or other speech tasks are available, with metadata stored in TSV files. Only Healthy Control and Parkinson’s Disease recordings were included in this study.\n",
    "\n",
    "Counts:\n",
    "\n",
    "Speakers\n",
    "\n",
    "* Total speakers: 630\n",
    "* Male speakers: 189\n",
    "* Female speakers: 441\n",
    "* Parkinson’s disease speakers: 92\n",
    "* Healthy control speakers: 538\n",
    "\n",
    "Audio clips\n",
    "\n",
    "* Total audio clips used: 5670\n",
    "* Vowel or sustained phonation clips: 630\n",
    "* Reading or other speech clips: 5040\n",
    "\n",
    "---\n",
    "\n",
    "## D3 – UCI PD Speech (NOT USED FOR ANALYSIS)\n",
    "\n",
    "Primary language: Turkish,\n",
    "Acronym used: TK\n",
    "\n",
    "Description:\n",
    "This dataset consists of feature-based Excel files for Healthy Control and Parkinson’s Disease speakers. Raw audio recordings are not available. As a result, this dataset was not used for model training or evaluation.\n",
    "\n",
    "---\n",
    "\n",
    "## D4 – IPVS\n",
    "\n",
    "Primary language: Italian,\n",
    "Acronym used: IT\n",
    "\n",
    "Description:\n",
    "This dataset includes speech recordings from Young Healthy Control, Elderly Healthy Control, and Parkinson’s Disease speakers. Both sustained phonation and read or other speech tasks are available, with metadata provided in Excel files.\n",
    "\n",
    "Counts:\n",
    "\n",
    "Speakers\n",
    "\n",
    "* Total speakers: 65\n",
    "* Male speakers: 44\n",
    "* Female speakers: 21\n",
    "* Parkinson’s disease speakers: 28\n",
    "* Healthy control speakers: 37\n",
    "\n",
    "Audio clips\n",
    "\n",
    "* Total audio clips used: 731\n",
    "* Vowel or sustained phonation clips: 397\n",
    "* Reading or other speech clips: 334\n",
    "\n",
    "---\n",
    "\n",
    "## D5 – MDVR-KCL\n",
    "\n",
    "Primary language: English (UK),\n",
    "Acronym used: EN, ENUK\n",
    "\n",
    "Description:\n",
    "This dataset contains speech recordings labeled as Healthy Control or Parkinson’s Disease. It includes read or other speech tasks only. No external metadata files are provided.\n",
    "\n",
    "Preprocessing variants:\n",
    "\n",
    "* Preprocessed_v1: Original 70/15/15 train, validation, and test speaker split\n",
    "* Preprocessed_v2 (D5v2): Revised 50/20/30 split to improve monolingual evaluation due to the small number of test speakers\n",
    "\n",
    "Counts:\n",
    "\n",
    "Speakers\n",
    "\n",
    "* Total speakers: 37\n",
    "* Male speakers: 13\n",
    "* Female speakers: 24\n",
    "* Parkinson’s disease speakers: 16\n",
    "* Healthy control speakers: 21\n",
    "\n",
    "Audio clips\n",
    "\n",
    "* Total audio clips used: 73\n",
    "* Vowel or sustained phonation clips: 0\n",
    "* Reading or other speech clips: 73\n",
    "\n",
    "---\n",
    "\n",
    "## D6 – “Ah Sound” Dataset (Figshare)\n",
    "\n",
    "Primary language: Not applicable (sustained “ah” sound only; recordings originate from the United States),\n",
    "Acronym used: EN, ENUS_AH\n",
    "\n",
    "Description:\n",
    "This dataset consists only of sustained phonation (“ah” sound) recordings. Clips are labeled as Healthy Control or Parkinson’s Disease, with metadata provided in an Excel file.\n",
    "\n",
    "Counts:\n",
    "\n",
    "Speakers\n",
    "\n",
    "* Total speakers: 81\n",
    "* Male speakers: 37\n",
    "* Female speakers: 44\n",
    "* Parkinson’s disease speakers: 40\n",
    "* Healthy control speakers: 41\n",
    "\n",
    "Audio clips\n",
    "\n",
    "* Total audio clips used: 81\n",
    "* Vowel or sustained phonation clips: 81\n",
    "* Reading or other speech clips: 0\n",
    "\n",
    "---\n",
    "\n",
    "## D7 – Multilingual Combined Dataset\n",
    "\n",
    "Name: D7 Multilingual (combined training pool)\n",
    "\n",
    "Description:\n",
    "This dataset was created by merging D1, D4, D5v2, and D6 into a single training pool. It was designed to support multilingual training and evaluation of cross-language generalization.\n",
    "\n",
    "Primary language: Multilingual by construction\n",
    "\n",
    "Counts:\n",
    "\n",
    "Speakers\n",
    "\n",
    "* Total speakers: 290\n",
    "* Male speakers: 155\n",
    "* Female speakers: 135\n",
    "* Parkinson’s disease speakers: 136\n",
    "* Healthy control speakers: 154\n",
    "\n",
    "Audio clips\n",
    "\n",
    "* Total audio clips used: 2577\n",
    "* Vowel or sustained phonation clips: 1315\n",
    "* Reading or other speech clips: 1262\n",
    "\n",
    "---"
   ],
   "metadata": {
    "id": "dEFXrhX9lkxU"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The following cell resolves a common Google Colab issue where the Google Drive mount directory (/content/drive) already contains leftover files from a previous session, which can cause mounting to fail. The cell first checks whether the mountpoint exists and prints a brief view of its current contents. It then safely unmounts any existing Drive connection using drive.flush_and_unmount(), which has no effect if Drive is not mounted. Next, it recreates the mountpoint folder if needed and removes only the local files and folders inside /content/drive to ensure the directory is empty, without deleting any data from Google Drive itself. After cleaning the mountpoint, it remounts Google Drive at /content/drive using force_remount=True. Finally, it verifies that the expected project directories (such as /content/drive/MyDrive/AI_PD_Project and the Datasets folder) are present and prints the top-level contents of the project folder to confirm that the Drive connection and paths are set up correctly."
   ],
   "metadata": {
    "id": "YlG-mkMupCWP"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Drive remount cleanup: clear a dirty mountpoint and verify project folders\n",
    "# Goal: fix the Colab error where the mount folder already has leftover files.\n",
    "# Inputs: none (checks the local /content/drive mountpoint).\n",
    "# Outputs: a fresh Drive mount at /content/drive and printed checks for key project paths.\n",
    "\n",
    "import os, shutil\n",
    "\n",
    "MOUNT_POINT = \"/content/drive\"\n",
    "\n",
    "# Current mountpoint status (exists and first few items)\n",
    "print(\"Exists?\", os.path.exists(MOUNT_POINT))\n",
    "if os.path.exists(MOUNT_POINT):\n",
    "    print(\"Contents BEFORE:\", os.listdir(MOUNT_POINT)[:50])\n",
    "\n",
    "# Unmount any existing Drive mount (safe even if Drive is not mounted)\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.flush_and_unmount()\n",
    "    print(\"flush_and_unmount() called.\")\n",
    "except Exception as e:\n",
    "    print(\"flush_and_unmount() not available or failed:\", repr(e))\n",
    "\n",
    "# Recreate the mountpoint folder to ensure it exists\n",
    "os.makedirs(MOUNT_POINT, exist_ok=True)\n",
    "\n",
    "# Clear leftover files or folders inside the mountpoint directory only\n",
    "# Note: this deletes only items under /content/drive (the local mount folder), not Drive contents.\n",
    "if os.listdir(MOUNT_POINT):\n",
    "    print(\"Clearing mountpoint folder contents...\")\n",
    "    for name in os.listdir(MOUNT_POINT):\n",
    "        p = os.path.join(MOUNT_POINT, name)\n",
    "        if os.path.isdir(p) and not os.path.islink(p):\n",
    "            shutil.rmtree(p)\n",
    "        else:\n",
    "            os.remove(p)\n",
    "\n",
    "print(\"Contents AFTER clearing:\", os.listdir(MOUNT_POINT))\n",
    "\n",
    "# Mount Google Drive at the cleaned mountpoint\n",
    "from google.colab import drive\n",
    "drive.mount(MOUNT_POINT, force_remount=True)\n",
    "\n",
    "# Quick sanity check: confirm expected project folders exist after mount\n",
    "PROJECT_DIR = \"/content/drive/MyDrive/AI_PD_Project\"\n",
    "DATASETS_DIR = f\"{PROJECT_DIR}/Datasets\"\n",
    "\n",
    "print(\"PROJECT_DIR exists?\", os.path.exists(PROJECT_DIR))\n",
    "print(\"DATASETS_DIR exists?\", os.path.exists(DATASETS_DIR))\n",
    "if os.path.exists(PROJECT_DIR):\n",
    "    print(\"Top-level:\", os.listdir(PROJECT_DIR)[:50])"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jZ--1ak8OTjO"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Preprocessing of D1, D2, D4, D5 and D6 Datasets"
   ],
   "metadata": {
    "id": "ag72q3tMnbvR"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The following cell runs the full **D1 (NeuroVoz, Castilian Spanish) preprocessing** in a single step and creates a standardized `preprocessed_v1` folder that is ready for model training. It mounts Google Drive if needed, installs required libraries (WebRTC VAD and SciPy when missing), and applies fixed preprocessing settings, including a 16 kHz sample rate, loudness normalization rules, voice activity detection settings, a maximum number of clips per speaker, and a speaker level train validation test split of 70 15 15. Output folders are created at the start, and the temporary `_candidates` folder is cleared to prevent mixing files from different runs.\n",
    "\n",
    "The cell then reads metadata from **two CSV files** (`metadata_hc.csv` and `metadata_pd.csv`), combines them into a single table, and checks for required columns (`ID`, `Group`, `Audio`). Age and sex fields are included only if they are present. Each entry is mapped to a speaker ID, a Healthy or Parkinson label, and an audio file path. Any entries with missing or invalid audio paths are skipped.\n",
    "\n",
    "Task type is determined from the audio filename and path. Vowel tasks are identified using codes such as `A1–U3`, spontaneous speech is identified using keyword matches (for example, “spont”), and all remaining files are treated as reading tasks.\n",
    "\n",
    "For each valid source audio file, the cell generates **at most one clip per source file**. Audio is converted to mono, resampled to 16 kHz, and normalized to a consistent loudness using simple RMS based leveling with a peak limit. Speech regions are detected using WebRTC VAD when available, with a fallback to an energy based method. From these regions, the **single longest voiced segment** on the original timeline is selected. Clip extraction follows fixed rules:\n",
    "\n",
    "* **Vowel**: one **2.0 second** clip centered within the longest voiced segment, padded with silence if needed.\n",
    "* **Reading or spontaneous**: one **8.0 second** clip taken from the start of the longest voiced segment, or the full segment if shorter than 8 seconds, without padding as long as it meets the minimum length.\n",
    "\n",
    "If no usable voiced segment is found, the file is skipped and a warning is recorded.\n",
    "\n",
    "Each extracted clip is first saved as a temporary WAV file in `clips/_candidates/`, and a candidate table is built with details such as speaker, task type, label, duration, source path, and clip start and end times. After all candidates are collected, the cell limits the data to at most **8 clips per speaker per task**, removes unused candidate files to save space, and assigns each speaker to exactly one split (train, validation, or test) while keeping class balance across splits. The selected clips are then moved to `clips/train`, `clips/val`, and `clips/test` using standardized filenames.\n",
    "\n",
    "At the end, the cell writes all required outputs: `manifests/manifest_all.csv` and per split manifest files, `logs/preprocess_warnings.csv`, `logs/dataset_summary.json`, and `config/run_config.json`. Final checks confirm that all audio files are mono at 16 kHz and that no speaker appears in more than one split."
   ],
   "metadata": {
    "id": "hgGi05Maqhmi"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# NeuroVoz (D1) preprocessing: 1 clip per source, standardized outputs\n",
    "# Builds train/val/test clips and a single manifest from two metadata tables.\n",
    "# Writes: clips/ (flat per split), manifests/manifest_all.csv, config/run_config.json, logs/*\n",
    "\n",
    "# =========================\n",
    "# D1 Preprocessing v1 (NeuroVoz) — SINGLE COMPLETE CELL (CONSISTENCY-UPDATED)\n",
    "# Summary of behavior:\n",
    "# - For each source audio file, create at most 1 candidate clip and write it immediately\n",
    "# - Select the clip from the longest voiced segment on the source timeline (after resample)\n",
    "# - Vowel tasks: 2.0 s clip (centered if long enough, else zero-padded)\n",
    "# - Reading or spontaneous: 8.0 s clip if possible, else keep true duration (no padding)\n",
    "# - After candidates: cap by (speaker_id, task), split speakers into train/val/test, then move kept clips into clips/<split>/\n",
    "# =========================\n",
    "\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import math\n",
    "import random\n",
    "from typing import List, Tuple, Dict, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import soundfile as sf\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# -------------------------\n",
    "# Drive mount\n",
    "# -------------------------\n",
    "if not os.path.isdir(\"/content/drive/MyDrive\"):\n",
    "    from google.colab import drive  # type: ignore\n",
    "    drive.mount(\"/content/drive\")\n",
    "\n",
    "# -------------------------\n",
    "# Optional packages\n",
    "# - webrtcvad: voice activity detection\n",
    "# - scipy: higher-quality resampling\n",
    "# -------------------------\n",
    "def _d1_try_import_webrtcvad():\n",
    "    try:\n",
    "        import webrtcvad  # type: ignore\n",
    "        return webrtcvad, True\n",
    "    except Exception:\n",
    "        return None, False\n",
    "\n",
    "webrtcvad, HAVE_WEBRTCVAD = _d1_try_import_webrtcvad()\n",
    "if not HAVE_WEBRTCVAD:\n",
    "    !pip -q install webrtcvad\n",
    "    webrtcvad, HAVE_WEBRTCVAD = _d1_try_import_webrtcvad()\n",
    "\n",
    "try:\n",
    "    from scipy import signal  # type: ignore\n",
    "    HAVE_SCIPY = True\n",
    "except Exception:\n",
    "    HAVE_SCIPY = False\n",
    "    !pip -q install scipy\n",
    "    from scipy import signal  # type: ignore\n",
    "    HAVE_SCIPY = True\n",
    "\n",
    "# -------------------------\n",
    "# Paths\n",
    "# Inputs: two metadata CSVs + audio files referenced inside them\n",
    "# Outputs: clips/, manifests/, config/, logs/\n",
    "# -------------------------\n",
    "D1_PROJECT_DIR  = \"/content/drive/MyDrive/AI_PD_Project\"\n",
    "D1_DATASETS_DIR = f\"{D1_PROJECT_DIR}/Datasets\"\n",
    "D1_DIR          = f\"{D1_DATASETS_DIR}/D1-NeuroVoz-Castillan Spanish\"\n",
    "\n",
    "D1_METADATA_DIR = f\"{D1_DIR}/data/metadata\"\n",
    "D1_META_HC      = f\"{D1_METADATA_DIR}/metadata_hc.csv\"\n",
    "D1_META_PD      = f\"{D1_METADATA_DIR}/metadata_pd.csv\"\n",
    "\n",
    "# Outputs\n",
    "D1_OUT_ROOT     = f\"{D1_DIR}/preprocessed_v1\"\n",
    "D1_CLIPS_DIR    = f\"{D1_OUT_ROOT}/clips\"\n",
    "D1_CAND_DIR     = f\"{D1_CLIPS_DIR}/_candidates\"\n",
    "D1_MANIFEST_DIR = f\"{D1_OUT_ROOT}/manifests\"\n",
    "D1_CONFIG_DIR   = f\"{D1_OUT_ROOT}/config\"\n",
    "D1_LOGS_DIR     = f\"{D1_OUT_ROOT}/logs\"\n",
    "\n",
    "for p in [D1_OUT_ROOT, D1_CLIPS_DIR, D1_CAND_DIR, D1_MANIFEST_DIR, D1_CONFIG_DIR, D1_LOGS_DIR]:\n",
    "    os.makedirs(p, exist_ok=True)\n",
    "for sp in [\"train\", \"val\", \"test\"]:\n",
    "    os.makedirs(os.path.join(D1_CLIPS_DIR, sp), exist_ok=True)\n",
    "\n",
    "# Candidate workspace: cleared at start to avoid mixing with older runs\n",
    "if os.path.isdir(D1_CAND_DIR):\n",
    "    try:\n",
    "        import shutil\n",
    "        shutil.rmtree(D1_CAND_DIR)\n",
    "    except Exception:\n",
    "        pass\n",
    "os.makedirs(D1_CAND_DIR, exist_ok=True)\n",
    "\n",
    "# -------------------------\n",
    "# Quick input checks\n",
    "# -------------------------\n",
    "print(\"D1_DIR exists?\", os.path.exists(D1_DIR))\n",
    "print(\"D1_METADATA_DIR exists?\", os.path.exists(D1_METADATA_DIR))\n",
    "print(\"D1_META_HC exists?\", os.path.exists(D1_META_HC))\n",
    "print(\"D1_META_PD exists?\", os.path.exists(D1_META_PD))\n",
    "print(\"webrtcvad available?\", HAVE_WEBRTCVAD)\n",
    "print(\"scipy available?\", HAVE_SCIPY)\n",
    "\n",
    "if not os.path.exists(D1_DIR):\n",
    "    raise FileNotFoundError(f\"D1_DIR not found: {D1_DIR}\")\n",
    "\n",
    "if not os.path.exists(D1_METADATA_DIR):\n",
    "    top = sorted(os.listdir(D1_DIR))[:50]\n",
    "    raise FileNotFoundError(\n",
    "        f\"D1 metadata folder not found: {D1_METADATA_DIR}\\n\"\n",
    "        f\"Top-level contents of D1_DIR (first 50): {top}\"\n",
    "    )\n",
    "\n",
    "if not os.path.exists(D1_META_HC) or not os.path.exists(D1_META_PD):\n",
    "    md = sorted(os.listdir(D1_METADATA_DIR))\n",
    "    raise FileNotFoundError(\n",
    "        \"D1 metadata not found at expected paths:\\n\"\n",
    "        f\"  {D1_META_HC}\\n\"\n",
    "        f\"  {D1_META_PD}\\n\"\n",
    "        f\"Files present in metadata dir: {md}\"\n",
    "    )\n",
    "\n",
    "# -------------------------\n",
    "# Audio + split settings\n",
    "# -------------------------\n",
    "D1_SR = 16000\n",
    "D1_RANDOM_SEED = 1337\n",
    "random.seed(D1_RANDOM_SEED)\n",
    "np.random.seed(D1_RANDOM_SEED)\n",
    "\n",
    "# Loudness: RMS target with a peak limiter (simple and stable)\n",
    "D1_TARGET_RMS_DBFS = -20.0\n",
    "D1_PEAK_LIMIT_DBFS = -1.0\n",
    "D1_MIN_RMS_DBFS    = -60.0\n",
    "D1_MAX_GAIN_DB     = 18.0\n",
    "\n",
    "# Voice activity detection settings (segment finding on source timeline)\n",
    "D1_VAD_MODE       = 2\n",
    "D1_FRAME_MS       = 30\n",
    "D1_PAD_SEC        = 0.25\n",
    "D1_TRAIL_PAD_SEC  = 0.15\n",
    "D1_MAX_GAP_MS     = 200\n",
    "D1_MIN_KEEP_SEC   = 0.30\n",
    "\n",
    "# Clip length rules\n",
    "D1_VOWEL_SEC = 2.0\n",
    "D1_OTHER_SEC = 8.0\n",
    "\n",
    "# Cap per speaker and task (keeps datasets balanced)\n",
    "D1_MAX_CLIPS_PER_SPK_PER_TASK = 8\n",
    "\n",
    "# Speaker-level split (keeps a speaker in only one split)\n",
    "D1_TRAIN_PCT, D1_VAL_PCT, D1_TEST_PCT = 0.70, 0.15, 0.15\n",
    "\n",
    "# -------------------------\n",
    "# Helper functions: naming, reading, resampling, normalization, labels\n",
    "# -------------------------\n",
    "def d1_safe(s: str) -> str:\n",
    "    return re.sub(r\"[^A-Za-z0-9_\\-\\.]+\", \"_\", str(s))\n",
    "\n",
    "def d1_db_to_lin(db: float) -> float:\n",
    "    return 10.0 ** (db / 20.0)\n",
    "\n",
    "def d1_rms_dbfs(y: np.ndarray) -> float:\n",
    "    if y is None or len(y) == 0:\n",
    "        return -120.0\n",
    "    rms = float(np.sqrt(np.mean(y.astype(np.float64) ** 2) + 1e-12))\n",
    "    return 20.0 * math.log10(max(rms, 1e-12))\n",
    "\n",
    "def d1_peak_limit(y: np.ndarray, peak_dbfs: float) -> np.ndarray:\n",
    "    if y is None or len(y) == 0:\n",
    "        return y\n",
    "    peak = float(np.max(np.abs(y)))\n",
    "    lim = d1_db_to_lin(peak_dbfs)\n",
    "    if peak > lim and peak > 0:\n",
    "        y = y * (lim / peak)\n",
    "    return np.clip(y, -1.0, 1.0).astype(np.float32)\n",
    "\n",
    "def d1_norm_rms_then_peak(y: np.ndarray) -> np.ndarray:\n",
    "    if y is None or len(y) == 0:\n",
    "        return y\n",
    "    cur = d1_rms_dbfs(y)\n",
    "    if cur < D1_MIN_RMS_DBFS:\n",
    "        return d1_peak_limit(y.astype(np.float32), D1_PEAK_LIMIT_DBFS)\n",
    "    gain_db = float(D1_TARGET_RMS_DBFS - cur)\n",
    "    gain_db = float(np.clip(gain_db, -60.0, D1_MAX_GAIN_DB))\n",
    "    y2 = (y.astype(np.float32) * d1_db_to_lin(gain_db)).astype(np.float32)\n",
    "    return d1_peak_limit(y2, D1_PEAK_LIMIT_DBFS)\n",
    "\n",
    "def d1_read_mono(path: str) -> Tuple[np.ndarray, int]:\n",
    "    x, sr = sf.read(path, always_2d=True)\n",
    "    x = x.mean(axis=1).astype(np.float32)\n",
    "    if not np.isfinite(x).all():\n",
    "        x = np.nan_to_num(x, nan=0.0, posinf=0.0, neginf=0.0).astype(np.float32)\n",
    "    return x, int(sr)\n",
    "\n",
    "def d1_resample_to_16k(x: np.ndarray, sr_in: int) -> np.ndarray:\n",
    "    if sr_in == D1_SR:\n",
    "        return x.astype(np.float32, copy=False)\n",
    "    g = math.gcd(sr_in, D1_SR)\n",
    "    up = D1_SR // g\n",
    "    down = sr_in // g\n",
    "    y = signal.resample_poly(x.astype(np.float64), up, down).astype(np.float32)\n",
    "    return y.astype(np.float32, copy=False)\n",
    "\n",
    "def d1_float_to_pcm16_bytes(y: np.ndarray) -> bytes:\n",
    "    y = np.clip(y, -1.0, 1.0)\n",
    "    return (y * 32767.0).astype(np.int16).tobytes()\n",
    "\n",
    "# Task inference from filename patterns\n",
    "def d1_task_code(audio_path: str) -> str:\n",
    "    stem = os.path.splitext(os.path.basename(audio_path))[0]\n",
    "    parts = stem.split(\"_\")\n",
    "    if len(parts) >= 3:\n",
    "        return str(parts[1]).upper()\n",
    "    m = re.match(r\"^(HC|PD)_(.+)_(\\d+)$\", stem, flags=re.IGNORECASE)\n",
    "    if m:\n",
    "        return str(m.group(2)).upper()\n",
    "    return \"UNKNOWN\"\n",
    "\n",
    "def d1_is_vowel(code: str) -> bool:\n",
    "    return re.fullmatch(r\"[AEIOU][1-3]\", (code or \"\").upper()) is not None\n",
    "\n",
    "SPONT_KEYS = [\"ESPONT\", \"SPONT\", \"FREE\", \"MONOLOG\", \"MONOLO\", \"LIBRE\", \"DIALOG\", \"CONVERS\"]\n",
    "def d1_is_spont(code: str, path: str) -> bool:\n",
    "    cu = (code or \"\").upper()\n",
    "    pu = (path or \"\").upper()\n",
    "    return any((k in cu) or (k in pu) for k in SPONT_KEYS)\n",
    "\n",
    "def d1_task_type(code: str, path: str) -> str:\n",
    "    if d1_is_vowel(code):\n",
    "        return \"vowel\"\n",
    "    if d1_is_spont(code, path):\n",
    "        return \"spontaneous\"\n",
    "    return \"reading\"\n",
    "\n",
    "def d1_task_short(tt: str) -> str:\n",
    "    tt = (tt or \"\").lower()\n",
    "    if tt == \"vowel\": return \"vowl\"\n",
    "    if tt == \"reading\": return \"read\"\n",
    "    if tt == \"spontaneous\": return \"spont\"\n",
    "    return \"unk\"\n",
    "\n",
    "def d1_label(group_val: str) -> Tuple[str, int]:\n",
    "    g = str(group_val).strip().lower()\n",
    "    if (\"pd\" in g) or (\"parkinson\" in g):\n",
    "        return \"Parkinson\", 1\n",
    "    return \"Healthy\", 0\n",
    "\n",
    "def d1_pd_hc(label_str: str) -> str:\n",
    "    return \"PD\" if str(label_str).lower().startswith(\"parkinson\") else \"HC\"\n",
    "\n",
    "def d1_write_wav(path: str, y: np.ndarray, sr: int):\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "    sf.write(path, np.clip(y, -1.0, 1.0).astype(np.float32), sr, subtype=\"PCM_16\")\n",
    "\n",
    "# -------------------------\n",
    "# Voice segments on SOURCE timeline (after resample)\n",
    "# Returns list of (start_sample, end_sample) segments\n",
    "# -------------------------\n",
    "def d1_vad_segments_source_timeline(y16: np.ndarray, sr: int) -> Optional[List[Tuple[int, int]]]:\n",
    "    if not HAVE_WEBRTCVAD:\n",
    "        return None\n",
    "    if sr != 16000:\n",
    "        return None\n",
    "\n",
    "    frame_len = int(sr * (D1_FRAME_MS / 1000.0))\n",
    "    if frame_len <= 0 or len(y16) < frame_len:\n",
    "        return []\n",
    "\n",
    "    n = len(y16)\n",
    "    n_frames = n // frame_len\n",
    "    if n_frames == 0:\n",
    "        return []\n",
    "\n",
    "    pcm = d1_float_to_pcm16_bytes(y16[:n_frames * frame_len])\n",
    "    vad = webrtcvad.Vad(int(D1_VAD_MODE))\n",
    "\n",
    "    def frame_bytes(i: int) -> bytes:\n",
    "        s = i * frame_len * 2\n",
    "        e = s + frame_len * 2\n",
    "        return pcm[s:e]\n",
    "\n",
    "    flags = [vad.is_speech(frame_bytes(i), 16000) for i in range(n_frames)]\n",
    "\n",
    "    segs = []\n",
    "    i = 0\n",
    "    while i < n_frames:\n",
    "        if not flags[i]:\n",
    "            i += 1\n",
    "            continue\n",
    "        s0 = i\n",
    "        while i < n_frames and flags[i]:\n",
    "            i += 1\n",
    "        e0 = i\n",
    "        segs.append((s0 * frame_len, e0 * frame_len))\n",
    "\n",
    "    max_gap = int(sr * (D1_MAX_GAP_MS / 1000.0))\n",
    "    merged = []\n",
    "    for s, e in segs:\n",
    "        if not merged:\n",
    "            merged.append([s, e])\n",
    "        else:\n",
    "            ps, pe = merged[-1]\n",
    "            if s - pe <= max_gap:\n",
    "                merged[-1][1] = max(pe, e)\n",
    "            else:\n",
    "                merged.append([s, e])\n",
    "\n",
    "    pad = int(round(sr * D1_PAD_SEC))\n",
    "    trail = int(round(sr * D1_TRAIL_PAD_SEC))\n",
    "\n",
    "    out = []\n",
    "    for s, e in merged:\n",
    "        s2 = max(0, s - pad)\n",
    "        e2 = min(n, e + pad + trail)\n",
    "        if e2 > s2:\n",
    "            out.append((int(s2), int(e2)))\n",
    "    return out\n",
    "\n",
    "def d1_energy_segments_source_timeline(y16: np.ndarray, sr: int) -> List[Tuple[int, int]]:\n",
    "    frame = int(sr * 0.02)\n",
    "    hop = frame\n",
    "    if frame <= 0 or len(y16) < frame:\n",
    "        return []\n",
    "\n",
    "    eng = []\n",
    "    idx = []\n",
    "    for i in range(0, len(y16) - frame + 1, hop):\n",
    "        w = y16[i:i+frame]\n",
    "        eng.append(float(np.mean(w*w)))\n",
    "        idx.append(i)\n",
    "    eng = np.array(eng, dtype=np.float32)\n",
    "    thr = float(np.percentile(eng, 25)) * 2.5\n",
    "    thr = max(thr, 1e-8)\n",
    "    keep = eng > thr\n",
    "\n",
    "    segs = []\n",
    "    on = False\n",
    "    s0 = 0\n",
    "    for k, flag in enumerate(keep):\n",
    "        if flag and not on:\n",
    "            on = True\n",
    "            s0 = idx[k]\n",
    "        elif (not flag) and on:\n",
    "            on = False\n",
    "            segs.append((s0, idx[k] + frame))\n",
    "    if on and idx:\n",
    "        segs.append((s0, idx[-1] + frame))\n",
    "\n",
    "    max_gap = int(sr * (D1_MAX_GAP_MS / 1000.0))\n",
    "    merged = []\n",
    "    for s, e in sorted(segs):\n",
    "        if not merged:\n",
    "            merged.append([s, e])\n",
    "        else:\n",
    "            ps, pe = merged[-1]\n",
    "            if s - pe <= max_gap:\n",
    "                merged[-1][1] = max(pe, e)\n",
    "            else:\n",
    "                merged.append([s, e])\n",
    "\n",
    "    pad = int(round(sr * D1_PAD_SEC))\n",
    "    trail = int(round(sr * D1_TRAIL_PAD_SEC))\n",
    "    out = []\n",
    "    for s, e in merged:\n",
    "        s2 = max(0, s - pad)\n",
    "        e2 = min(len(y16), e + pad + trail)\n",
    "        if e2 > s2:\n",
    "            out.append((int(s2), int(e2)))\n",
    "    return out\n",
    "\n",
    "# -------------------------\n",
    "# Single-clip selection per source file\n",
    "# - Picks the longest voiced segment (ties go to earliest)\n",
    "# - Builds exactly one clip based on task type\n",
    "# -------------------------\n",
    "def d1_force_length(x: np.ndarray, n: int) -> np.ndarray:\n",
    "    if x is None:\n",
    "        return np.zeros((n,), dtype=np.float32)\n",
    "    if len(x) == n:\n",
    "        return x.astype(np.float32)\n",
    "    if len(x) > n:\n",
    "        return x[:n].astype(np.float32)\n",
    "    y = np.zeros((n,), dtype=np.float32)\n",
    "    y[:len(x)] = x.astype(np.float32)\n",
    "    return y\n",
    "\n",
    "def d1_pick_longest_segment(segs: List[Tuple[int, int]]) -> Optional[Tuple[int, int]]:\n",
    "    if not segs:\n",
    "        return None\n",
    "    segs2 = sorted(segs, key=lambda t: (-(t[1]-t[0]), t[0]))\n",
    "    return segs2[0]\n",
    "\n",
    "def d1_make_single_clip_from_source_segments(\n",
    "    y16: np.ndarray,\n",
    "    sr: int,\n",
    "    segs: List[Tuple[int, int]],\n",
    "    task_type: str\n",
    ") -> Optional[Dict]:\n",
    "    \"\"\"\n",
    "    Returns a single clip dict:\n",
    "      {audio, clip_start_sec, clip_end_sec, duration_sec}\n",
    "    clip_start/end are SOURCE timeline (post-resample).\n",
    "    \"\"\"\n",
    "    if not segs:\n",
    "        return None\n",
    "\n",
    "    best = d1_pick_longest_segment(segs)\n",
    "    if best is None:\n",
    "        return None\n",
    "    s, e = best\n",
    "    seg = y16[s:e].astype(np.float32)\n",
    "    if len(seg) <= 0:\n",
    "        return None\n",
    "\n",
    "    if task_type == \"vowel\":\n",
    "        L = int(round(sr * D1_VOWEL_SEC))\n",
    "        if len(seg) >= L:\n",
    "            mid = len(seg) // 2\n",
    "            a0 = max(0, mid - L // 2)\n",
    "            a1 = a0 + L\n",
    "            audio = seg[a0:a1].astype(np.float32)\n",
    "            st = float(s + a0) / sr\n",
    "            en = float(s + a1) / sr\n",
    "        else:\n",
    "            audio = d1_force_length(seg, L)\n",
    "            st = float(s) / sr\n",
    "            en = float(s + L) / sr\n",
    "        return {\n",
    "            \"audio\": np.clip(audio, -1.0, 1.0).astype(np.float32),\n",
    "            \"clip_start_sec\": float(st),\n",
    "            \"clip_end_sec\": float(en),\n",
    "            \"duration_sec\": float(len(audio) / sr),\n",
    "        }\n",
    "\n",
    "    L = int(round(sr * D1_OTHER_SEC))\n",
    "    if len(seg) >= L:\n",
    "        audio = seg[:L].astype(np.float32)\n",
    "        st = float(s) / sr\n",
    "        en = float(s + L) / sr\n",
    "        return {\n",
    "            \"audio\": np.clip(audio, -1.0, 1.0).astype(np.float32),\n",
    "            \"clip_start_sec\": float(st),\n",
    "            \"clip_end_sec\": float(en),\n",
    "            \"duration_sec\": float(L / sr),\n",
    "        }\n",
    "\n",
    "    if len(seg) < int(sr * D1_MIN_KEEP_SEC):\n",
    "        return None\n",
    "    st = float(s) / sr\n",
    "    en = float(s + len(seg)) / sr\n",
    "    return {\n",
    "        \"audio\": np.clip(seg, -1.0, 1.0).astype(np.float32),\n",
    "        \"clip_start_sec\": float(st),\n",
    "        \"clip_end_sec\": float(en),\n",
    "        \"duration_sec\": float(len(seg) / sr),\n",
    "    }\n",
    "\n",
    "# -------------------------\n",
    "# Read metadata and build the source table\n",
    "# Inputs: metadata_hc.csv + metadata_pd.csv\n",
    "# Output: one combined table with audio paths, labels, and tasks\n",
    "# -------------------------\n",
    "hc_df = pd.read_csv(D1_META_HC)\n",
    "pd_df = pd.read_csv(D1_META_PD)\n",
    "d1_all = pd.concat([hc_df, pd_df], ignore_index=True).copy()\n",
    "\n",
    "req = [\"ID\", \"Group\", \"Audio\"]\n",
    "miss = [c for c in req if c not in d1_all.columns]\n",
    "if miss:\n",
    "    raise ValueError(f\"D1 metadata missing required columns {miss}. Found: {list(d1_all.columns)}\")\n",
    "\n",
    "# Optional columns: use if present, otherwise leave as NaN\n",
    "col_age = None\n",
    "col_sex = None\n",
    "for c in d1_all.columns:\n",
    "    cl = str(c).strip().lower()\n",
    "    if col_age is None and cl in {\"age\", \"edad\"}:\n",
    "        col_age = c\n",
    "    if col_sex is None and cl in {\"sex\", \"gender\", \"sexo\"}:\n",
    "        col_sex = c\n",
    "\n",
    "use_cols = req + ([col_age] if col_age else []) + ([col_sex] if col_sex else [])\n",
    "d1 = d1_all[use_cols].copy()\n",
    "\n",
    "d1[\"speaker_id\"] = d1[\"ID\"].astype(str)\n",
    "d1[\"audio_path\"] = d1[\"Audio\"].apply(lambda p: os.path.join(D1_DIR, str(p).replace(\"\\\\\", \"/\")))\n",
    "\n",
    "lbl = d1[\"Group\"].apply(d1_label)\n",
    "d1[\"label_str\"] = lbl.apply(lambda t: t[0])\n",
    "d1[\"label_num\"] = lbl.apply(lambda t: t[1]).astype(int)\n",
    "\n",
    "d1[\"age\"] = d1[col_age] if col_age else np.nan\n",
    "d1[\"sex\"] = d1[col_sex] if col_sex else np.nan\n",
    "\n",
    "d1[\"exists\"] = d1[\"audio_path\"].apply(os.path.exists)\n",
    "missing_audio = d1.loc[~d1[\"exists\"], [\"speaker_id\",\"Group\",\"Audio\",\"audio_path\"]].copy()\n",
    "\n",
    "print(\"\\nRows total:\", len(d1))\n",
    "print(\"Rows existing audio:\", int(d1[\"exists\"].sum()))\n",
    "print(\"Rows missing audio:\", int((~d1[\"exists\"]).sum()))\n",
    "\n",
    "d1 = d1.loc[d1[\"exists\"]].copy()\n",
    "\n",
    "# Infer task type from filename or path (vowel vs reading vs spontaneous)\n",
    "d1[\"task_code\"] = d1[\"audio_path\"].apply(d1_task_code)\n",
    "d1[\"task_type\"] = d1.apply(lambda r: d1_task_type(r[\"task_code\"], r[\"audio_path\"]), axis=1)\n",
    "d1[\"task\"] = d1[\"task_type\"].apply(d1_task_short)\n",
    "\n",
    "print(\"\\nTask counts (rows):\")\n",
    "print(d1[\"task\"].value_counts(dropna=False))\n",
    "print(\"\\nLabel counts (rows):\")\n",
    "print(d1[\"label_str\"].value_counts(dropna=False))\n",
    "\n",
    "if len(d1) == 0:\n",
    "    raise RuntimeError(\"No usable D1 rows after existence filtering.\")\n",
    "\n",
    "# -------------------------\n",
    "# Candidate creation: 1 candidate WAV per source (written immediately)\n",
    "# Output: cand_df table + warnings_df log\n",
    "# -------------------------\n",
    "MANIFEST_COLS = [\n",
    "    \"split\",\"dataset\",\"task\",\"speaker_id\",\"sample_id\",\n",
    "    \"label_str\",\"label_num\",\"age\",\"sex\",\n",
    "    \"speaker_key_rel\",\n",
    "    \"clip_path\",\"duration_sec\",\"source_path\",\n",
    "    \"clip_start_sec\",\"clip_end_sec\",\"sr_hz\",\"channels\",\n",
    "    \"clip_is_contiguous\",\n",
    "]\n",
    "\n",
    "cand_rows: List[Dict] = []\n",
    "warn_rows: List[Dict] = []\n",
    "\n",
    "cand_counter = 0\n",
    "\n",
    "pbar = tqdm(d1.itertuples(index=False), total=len(d1), desc=\"D1 preprocess (1 clip per source; write candidates)\", dynamic_ncols=True)\n",
    "\n",
    "for r in pbar:\n",
    "    src = r.audio_path\n",
    "    try:\n",
    "        x, sr0 = d1_read_mono(src)\n",
    "        y = d1_resample_to_16k(x, sr0)\n",
    "        y = d1_norm_rms_then_peak(y)\n",
    "\n",
    "        segs = d1_vad_segments_source_timeline(y, D1_SR)\n",
    "        vad_used = \"webrtcvad\"\n",
    "        if segs is None:\n",
    "            segs = d1_energy_segments_source_timeline(y, D1_SR)\n",
    "            vad_used = \"energy\"\n",
    "\n",
    "        if not segs:\n",
    "            warn_rows.append({\n",
    "                \"dataset\":\"D1\",\"speaker_id\":str(r.speaker_id),\"source_path\":src,\n",
    "                \"warning_type\":\"no_vad_segments\",\"detail\":f\"vad_used={vad_used}\"\n",
    "            })\n",
    "            continue\n",
    "\n",
    "        clip = d1_make_single_clip_from_source_segments(y, D1_SR, segs, r.task_type)\n",
    "        if clip is None:\n",
    "            warn_rows.append({\n",
    "                \"dataset\":\"D1\",\"speaker_id\":str(r.speaker_id),\"source_path\":src,\n",
    "                \"warning_type\":\"no_clip_selected\",\"detail\":f\"vad_used={vad_used}\"\n",
    "            })\n",
    "            continue\n",
    "\n",
    "        task5 = d1_task_short(r.task_type)\n",
    "\n",
    "        cand_counter += 1\n",
    "        cand_name = d1_safe(f\"CAND_{cand_counter:08d}.wav\")\n",
    "        cand_path = os.path.join(D1_CAND_DIR, cand_name)\n",
    "\n",
    "        # Write candidate clip immediately\n",
    "        d1_write_wav(cand_path, clip[\"audio\"], D1_SR)\n",
    "\n",
    "        cand_rows.append({\n",
    "            \"dataset\": \"D1\",\n",
    "            \"task\": task5,\n",
    "            \"speaker_id\": str(r.speaker_id),\n",
    "            \"sample_id\": os.path.basename(src),\n",
    "            \"label_str\": r.label_str,\n",
    "            \"label_num\": int(r.label_num),\n",
    "            \"age\": r.age if pd.notna(r.age) else np.nan,\n",
    "            \"sex\": r.sex if pd.notna(r.sex) else np.nan,\n",
    "            \"speaker_key_rel\": np.nan,      # true null for D1\n",
    "            \"clip_path_cand\": cand_path,    # candidate on disk\n",
    "            \"duration_sec\": float(clip[\"duration_sec\"]),\n",
    "            \"source_path\": src,\n",
    "            \"clip_start_sec\": float(clip[\"clip_start_sec\"]),\n",
    "            \"clip_end_sec\": float(clip[\"clip_end_sec\"]),\n",
    "            \"sr_hz\": int(D1_SR),\n",
    "            \"channels\": 1,\n",
    "            \"clip_is_contiguous\": True,\n",
    "            \"__vad_used__\": vad_used,\n",
    "        })\n",
    "\n",
    "    except Exception as e:\n",
    "        warn_rows.append({\n",
    "            \"dataset\":\"D1\",\"speaker_id\":str(getattr(r,\"speaker_id\",\"\")),\"source_path\":src,\n",
    "            \"warning_type\":\"preprocess_error\",\"detail\":repr(e)\n",
    "        })\n",
    "\n",
    "cand_df = pd.DataFrame(cand_rows)\n",
    "warnings_df = pd.DataFrame(warn_rows)\n",
    "\n",
    "print(\"\\nD1 candidates written:\", int(len(cand_df)))\n",
    "if len(cand_df) == 0:\n",
    "    raise RuntimeError(\"No D1 clips produced. Check VAD settings and audio paths.\")\n",
    "\n",
    "# -------------------------\n",
    "# Cap: limit clips per (speaker_id, task)\n",
    "# Unkept candidates are deleted to save space\n",
    "# -------------------------\n",
    "def d1_cap_manifest_keep_set(df: pd.DataFrame, max_k: int, seed: int) -> Tuple[pd.DataFrame, set]:\n",
    "    rng = np.random.default_rng(seed)\n",
    "    kept_idx = []\n",
    "    for (spk, task), g in df.groupby([\"speaker_id\", \"task\"], sort=False):\n",
    "        idxs = g.index.to_numpy()\n",
    "        if len(idxs) <= max_k:\n",
    "            kept_idx.extend(idxs.tolist())\n",
    "        else:\n",
    "            chosen = rng.choice(idxs, size=max_k, replace=False)\n",
    "            kept_idx.extend(chosen.tolist())\n",
    "    kept_idx = sorted(set(kept_idx))\n",
    "    return df.loc[kept_idx].reset_index(drop=True), set(kept_idx)\n",
    "\n",
    "cand_df_capped, keep_set = d1_cap_manifest_keep_set(cand_df, D1_MAX_CLIPS_PER_SPK_PER_TASK, D1_RANDOM_SEED)\n",
    "print(\"D1 clips after cap:\", int(len(cand_df_capped)))\n",
    "\n",
    "to_delete = cand_df.loc[~cand_df.index.isin(list(keep_set)), \"clip_path_cand\"].tolist()\n",
    "deleted = 0\n",
    "for p in to_delete:\n",
    "    try:\n",
    "        if os.path.exists(p):\n",
    "            os.remove(p)\n",
    "            deleted += 1\n",
    "    except Exception as e:\n",
    "        warnings_df = pd.concat([warnings_df, pd.DataFrame([{\n",
    "            \"dataset\": \"D1\",\n",
    "            \"speaker_id\": \"\",\n",
    "            \"source_path\": \"\",\n",
    "            \"warning_type\": \"candidate_delete_failed\",\n",
    "            \"detail\": f\"{p} :: {repr(e)}\"\n",
    "        }])], ignore_index=True)\n",
    "\n",
    "print(\"Deleted unkept candidates:\", deleted)\n",
    "cand_df = cand_df_capped\n",
    "\n",
    "# -------------------------\n",
    "# Speaker-level split (label-stratified)\n",
    "# Output: a split assignment per speaker, merged onto cand_df\n",
    "# -------------------------\n",
    "def d1_split_speakers(df: pd.DataFrame, fracs=(0.70, 0.15, 0.15), seed=1337) -> pd.DataFrame:\n",
    "    rng = np.random.default_rng(seed)\n",
    "    spk_df = df[[\"speaker_id\", \"label_num\", \"label_str\"]].drop_duplicates().copy()\n",
    "\n",
    "    rows = []\n",
    "    for lbl in [0, 1]:\n",
    "        spks = spk_df[spk_df[\"label_num\"] == lbl][\"speaker_id\"].tolist()\n",
    "        rng.shuffle(spks)\n",
    "        n = len(spks)\n",
    "        n_train = int(round(fracs[0] * n))\n",
    "        n_val = int(round(fracs[1] * n))\n",
    "\n",
    "        train_spks = spks[:n_train]\n",
    "        val_spks = spks[n_train:n_train + n_val]\n",
    "        test_spks = spks[n_train + n_val:]\n",
    "\n",
    "        rows += [{\"speaker_id\": s, \"split\": \"train\"} for s in train_spks]\n",
    "        rows += [{\"speaker_id\": s, \"split\": \"val\"} for s in val_spks]\n",
    "        rows += [{\"speaker_id\": s, \"split\": \"test\"} for s in test_spks]\n",
    "\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "spk_split = d1_split_speakers(cand_df, (D1_TRAIN_PCT, D1_VAL_PCT, D1_TEST_PCT), D1_RANDOM_SEED)\n",
    "\n",
    "cand_df = cand_df.drop(columns=[\"split\"], errors=\"ignore\")\n",
    "cand_df = cand_df.merge(spk_split, on=\"speaker_id\", how=\"left\", validate=\"many_to_one\")\n",
    "\n",
    "if cand_df[\"split\"].isna().any():\n",
    "    ex = cand_df.loc[cand_df[\"split\"].isna(),\"speaker_id\"].drop_duplicates().head(10).tolist()\n",
    "    raise RuntimeError(f\"Some speakers did not get split. Example: {ex}\")\n",
    "\n",
    "# -------------------------\n",
    "# Finalize: move kept candidates into clips/<split>/ and build manifest_all\n",
    "# -------------------------\n",
    "import shutil\n",
    "\n",
    "global_counter = 0\n",
    "final_rows: List[Dict] = []\n",
    "\n",
    "pbar2 = tqdm(cand_df.itertuples(index=False), total=len(cand_df), desc=\"D1 finalize (move kept)\", dynamic_ncols=True)\n",
    "\n",
    "for r in pbar2:\n",
    "    global_counter += 1\n",
    "\n",
    "    label_tag = d1_pd_hc(r.label_str)\n",
    "    task5 = str(r.task)\n",
    "    spk = str(r.speaker_id)\n",
    "\n",
    "    out_name = d1_safe(f\"D1_{label_tag}_{spk}_{task5}_{global_counter:06d}.wav\")\n",
    "    out_path = os.path.join(D1_CLIPS_DIR, r.split, out_name)\n",
    "\n",
    "    cand_path = getattr(r, \"clip_path_cand\")\n",
    "    if not os.path.exists(cand_path):\n",
    "        warnings_df = pd.concat([warnings_df, pd.DataFrame([{\n",
    "            \"dataset\":\"D1\",\"speaker_id\":spk,\"source_path\":r.source_path,\n",
    "            \"warning_type\":\"missing_candidate_file\",\"detail\":cand_path\n",
    "        }])], ignore_index=True)\n",
    "        continue\n",
    "\n",
    "    shutil.move(cand_path, out_path)\n",
    "\n",
    "    final_rows.append({\n",
    "        \"split\": r.split,\n",
    "        \"dataset\": \"D1\",\n",
    "        \"task\": task5,\n",
    "        \"speaker_id\": spk,\n",
    "        \"sample_id\": r.sample_id,\n",
    "        \"label_str\": r.label_str,\n",
    "        \"label_num\": int(r.label_num),\n",
    "        \"age\": r.age if pd.notna(r.age) else np.nan,\n",
    "        \"sex\": r.sex if pd.notna(r.sex) else np.nan,\n",
    "        \"speaker_key_rel\": np.nan,\n",
    "        \"clip_path\": out_path,\n",
    "        \"duration_sec\": float(r.duration_sec),\n",
    "        \"source_path\": r.source_path,\n",
    "        \"clip_start_sec\": float(r.clip_start_sec),\n",
    "        \"clip_end_sec\": float(r.clip_end_sec),\n",
    "        \"sr_hz\": int(r.sr_hz),\n",
    "        \"channels\": 1,\n",
    "        \"clip_is_contiguous\": True,\n",
    "    })\n",
    "\n",
    "# Cleanup: remove any leftover candidate files (best effort)\n",
    "try:\n",
    "    if os.path.isdir(D1_CAND_DIR):\n",
    "        leftovers = list(os.scandir(D1_CAND_DIR))\n",
    "        for ent in leftovers:\n",
    "            try:\n",
    "                os.remove(ent.path)\n",
    "            except Exception:\n",
    "                pass\n",
    "        try:\n",
    "            os.rmdir(D1_CAND_DIR)\n",
    "        except Exception:\n",
    "            pass\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "manifest_df = pd.DataFrame(final_rows)\n",
    "\n",
    "# Ensure standardized schema + column order\n",
    "for c in MANIFEST_COLS:\n",
    "    if c not in manifest_df.columns:\n",
    "        manifest_df[c] = np.nan\n",
    "manifest_df = manifest_df[MANIFEST_COLS].copy()\n",
    "\n",
    "# -------------------------\n",
    "# Save outputs\n",
    "# - manifests/manifest_all.csv (+ per-split manifests)\n",
    "# - logs/preprocess_warnings.csv and logs/dataset_summary.json\n",
    "# - config/run_config.json\n",
    "# -------------------------\n",
    "manifest_all_path = os.path.join(D1_MANIFEST_DIR, \"manifest_all.csv\")\n",
    "manifest_df.to_csv(manifest_all_path, index=False)\n",
    "\n",
    "for sp in [\"train\",\"val\",\"test\"]:\n",
    "    p = os.path.join(D1_MANIFEST_DIR, f\"manifest_{sp}.csv\")\n",
    "    manifest_df.loc[manifest_df[\"split\"] == sp].to_csv(p, index=False)\n",
    "\n",
    "warnings_path = os.path.join(D1_LOGS_DIR, \"preprocess_warnings.csv\")\n",
    "warnings_df.to_csv(warnings_path, index=False)\n",
    "\n",
    "summary = {\n",
    "    \"dataset\": \"D1\",\n",
    "    \"source_root\": D1_DIR,\n",
    "    \"metadata_dir\": D1_METADATA_DIR,\n",
    "    \"metadata_hc\": D1_META_HC,\n",
    "    \"metadata_pd\": D1_META_PD,\n",
    "    \"sr_hz\": int(D1_SR),\n",
    "    \"webrtcvad_available\": bool(HAVE_WEBRTCVAD),\n",
    "    \"scipy_available\": bool(HAVE_SCIPY),\n",
    "    \"n_source_rows_used\": int(len(d1)),\n",
    "    \"n_unique_speakers_source\": int(d1[\"speaker_id\"].nunique()) if len(d1) else 0,\n",
    "    \"label_counts_source_rows\": d1[\"label_str\"].value_counts(dropna=False).to_dict() if len(d1) else {},\n",
    "    \"n_clips_total\": int(len(manifest_df)),\n",
    "    \"label_counts_clips\": manifest_df[\"label_str\"].value_counts(dropna=False).to_dict() if len(manifest_df) else {},\n",
    "    \"split_counts_clips\": manifest_df[\"split\"].value_counts(dropna=False).to_dict() if len(manifest_df) else {},\n",
    "    \"task_counts_clips\": manifest_df[\"task\"].value_counts(dropna=False).to_dict() if len(manifest_df) else {},\n",
    "    \"n_warnings\": int(len(warnings_df)),\n",
    "    \"n_missing_audio_rows_in_metadata\": int(len(missing_audio)),\n",
    "    \"notes\": (\n",
    "        \"At most 1 clip per source file. Candidate written immediately to clips/_candidates, \"\n",
    "        \"then cap/split and moved to clips/<split>. \"\n",
    "        \"Selection uses the longest voiced segment on source timeline.\"\n",
    "    ),\n",
    "}\n",
    "summary_path = os.path.join(D1_LOGS_DIR, \"dataset_summary.json\")\n",
    "with open(summary_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "run_cfg = {\n",
    "    \"dataset\": \"D1\",\n",
    "    \"paths\": {\n",
    "        \"dataset_dir\": D1_DIR,\n",
    "        \"metadata_dir\": D1_METADATA_DIR,\n",
    "        \"metadata_hc\": D1_META_HC,\n",
    "        \"metadata_pd\": D1_META_PD,\n",
    "        \"out_root\": D1_OUT_ROOT,\n",
    "        \"clips_dir\": D1_CLIPS_DIR,\n",
    "        \"candidate_dir\": D1_CAND_DIR,\n",
    "        \"manifest_all\": manifest_all_path,\n",
    "        \"warnings_csv\": warnings_path,\n",
    "        \"summary_json\": summary_path,\n",
    "    },\n",
    "    \"folder_structure\": \"clips/<split>/ (flat) with temporary clips/_candidates during run\",\n",
    "    \"filename_format\": \"D1_{HC|PD}_{speaker_id}_{task<=5}_{global_index:06d}.wav\",\n",
    "    \"manifest_schema_order\": MANIFEST_COLS,\n",
    "    \"cap_policy\": {\"groupby\": [\"speaker_id\", \"task\"], \"max_per_group\": int(D1_MAX_CLIPS_PER_SPK_PER_TASK)},\n",
    "    \"clip_boundary_policy\": \"clip_start_sec/clip_end_sec are source-timeline (post-resample). clip_is_contiguous=True.\",\n",
    "    \"one_clip_per_source_file\": True,\n",
    "    \"one_clip_selection_rule\": \"Pick longest voiced segment; vowel=2s centered/padded, others=8s from start or true duration if shorter.\",\n",
    "    \"notes\": \"Manual RMS gain + peak limiting; no pyloudnorm.\",\n",
    "}\n",
    "run_cfg_path = os.path.join(D1_CONFIG_DIR, \"run_config.json\")\n",
    "with open(run_cfg_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(run_cfg, f, indent=2)\n",
    "\n",
    "print(\"\\nDONE: D1 preprocessing\")\n",
    "print(\"Manifest:\", manifest_all_path)\n",
    "print(\"Warnings:\", warnings_path)\n",
    "print(\"Summary:\", summary_path)\n",
    "print(\"Run config:\", run_cfg_path)\n",
    "print(\"Clips written:\", int(len(manifest_df)))\n",
    "print(\"Missing-audio metadata rows (not processed):\", int(len(missing_audio)))\n",
    "\n",
    "print(\"\\nSanity checks:\")\n",
    "print(\"Unique SR:\", sorted(manifest_df[\"sr_hz\"].unique().tolist()) if len(manifest_df) else [])\n",
    "print(\"Unique channels:\", sorted(manifest_df[\"channels\"].unique().tolist()) if len(manifest_df) else [])\n",
    "\n",
    "# Speaker split uniqueness check (speaker must appear in only one split)\n",
    "spk_split_chk = manifest_df[[\"speaker_id\", \"split\"]].drop_duplicates()\n",
    "dup = spk_split_chk.groupby(\"speaker_id\")[\"split\"].nunique()\n",
    "bad = dup[dup > 1]\n",
    "print(\"Speakers appearing in multiple splits:\", int(len(bad)))\n",
    "if len(bad) > 0:\n",
    "    print(\"Example bad speaker ids:\", bad.index.tolist()[:10])\n",
    "    raise RuntimeError(\"Speaker appears in more than one split.\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 758,
     "referenced_widgets": [
      "76d69151f314404887db28dddfe9d3a0",
      "5f278e8c4011439bae3cff733515e31a",
      "1fefb7c24a7749f58830512afb8b7073",
      "3c6a6b43a33142c4a74b320cdfa4f0c4",
      "0f54c83a417e40bab7783a6159e4048b",
      "06314438930045b3bbe6ef68f53d1234",
      "60a38a4e87a6498b9a01bcc8076161f6",
      "c1108a85b91b416398ce901a1b2055d4",
      "e309dd511e82426eae1e1d1d17abada8",
      "c0319c2ad5404a219cf8678b60b47277",
      "9dcfc7ef9e1d43cd95fc0b1dd613ed23",
      "088e1d69e75d4d3593dfd5c21f05998c",
      "c8a5902bb57844ec8dfcc7b3af5b070e",
      "70865aee196b477895ca51d48dbbdd0d",
      "a01513b044e244648adccaf294522d90",
      "dd53fbdfbe3f487ab1d447939099d3ff",
      "598d56f8a9db46d39eff132c742f5f7e",
      "00ed2dbcc08f434fa553afdefa633784",
      "1ed6c34d3b1540f7952cb60496b21193",
      "73ef7aabdf224819ba47165dfc33dea9",
      "b295d941b62d4aa8951f7bdfe773b911",
      "58965ba3e683465aab36bc357ffd062e"
     ]
    },
    "id": "nbI8JwdXC-0o"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "The following cell runs the full D2 (EWA-DB, Slovak) preprocessing in one place and is written so it can be safely rerun if Google Drive disconnects during execution. It reads the dataset metadata tables (`FILES.TSV` and `SPEAKERS.TSV`), filters the records to include only usable audio (Healthy or Parkinson diagnosis, inclusion criteria met, publish agreement true when present, and an available audio file), and then checks that each referenced audio file actually exists. Any missing files are recorded in `missing_paths.csv`.\n",
    "\n",
    "For each remaining source audio file, the cell creates **exactly one** processed clip. The audio is converted to mono, resampled to 16 kHz, and normalized to a consistent loudness using simple RMS based leveling with a peak limit. Speech regions are detected using voice activity detection (WebRTC VAD when available, otherwise an energy based fallback), and the detected speech segments are stitched together into a single voiced signal. For reading style clips, a light trim is applied to reduce leading silence. Clip selection follows the dataset rules: if `PICTURE == vokal`, a **2.0 second vowel clip** is produced and padded with silence if needed; otherwise, a **reading style clip** of up to **8.0 seconds** is created. If the voiced signal is longer than 8 seconds, the best 8 second window is chosen to reduce early silence; if it is shorter, the true duration is kept with no padding. The start and end times written to the manifest refer to the **voiced, stitched timeline**, not the original raw audio.\n",
    "\n",
    "To support restarts, candidate clips are written immediately to `clips/_candidates/` using deterministic filenames that remain the same across reruns. If a candidate file already exists, that source is skipped instead of being regenerated. A local temporary cache is used by default, where files are first written under `/content/…` and then copied to Google Drive with retries, which helps if Drive becomes unstable. Progress is saved periodically to `logs/candidates_checkpoint.csv`, and any issues such as read errors, missing speech segments, Drive interruptions, or trimming events are logged in `logs/preprocess_warnings.csv`.\n",
    "\n",
    "After all candidate clips are created, the cell enforces a limit of **8 clips per (speaker, task)** and removes any extra candidate files. Speakers are then assigned to train, validation, and test splits at the **speaker level** using a 70/15/15 ratio, stratified by label so that no speaker appears in more than one split. The selected clips are moved into `clips/train`, `clips/val`, and `clips/test` with standardized filenames, and the final outputs are written to the run folder, including `manifests/manifest_all.csv`, `config/run_config.json`, and `logs/dataset_summary.json`, along with the warning and checkpoint logs."
   ],
   "metadata": {
    "id": "QeeVzNSvrh9P"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# D2 (EWA-DB) Preprocessing v1 — Resume-Safe, 1 Clip per Source, Stream Write\n",
    "# Inputs: FILES.TSV + SPEAKERS.TSV + referenced audio files under the dataset folder\n",
    "# Outputs: clips/<split>/ WAVs, manifests/manifest_all.csv, logs/preprocess_warnings.csv + dataset_summary.json (+ checkpoints)\n",
    "\n",
    "# =========================\n",
    "# D2 Preprocessing v1 (EWA-DB) — SINGLE COMPLETE CELL\n",
    "# RESUME-SAFE + STREAM-WRITE + 1CLIP/SRC + DIAGNOSTICS FOR DRIVE DISCONNECT\n",
    "# =========================\n",
    "# What this cell does:\n",
    "# - Reads metadata tables (FILES.TSV, SPEAKERS.TSV), filters usable rows, verifies audio paths\n",
    "# - Creates exactly 1 processed candidate clip per source audio (resume-safe across restarts)\n",
    "# - Caps clips per (speaker, task), splits speakers into train/val/test\n",
    "# - Moves kept clips into final folders and writes manifests, logs, and config\n",
    "\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import math\n",
    "import random\n",
    "import shutil\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple, Dict, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import soundfile as sf\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# -------------------------\n",
    "# Drive mount and environment\n",
    "# Inputs: Colab runtime + Google Drive\n",
    "# Outputs: Drive mounted at /content/drive\n",
    "# -------------------------\n",
    "if not os.path.isdir(\"/content/drive/MyDrive\"):\n",
    "    from google.colab import drive  # type: ignore\n",
    "    drive.mount(\"/content/drive\")\n",
    "\n",
    "# -------------------------\n",
    "# Package checks (install if missing)\n",
    "# Inputs: runtime environment\n",
    "# Outputs: webrtcvad and (optional) scipy resampler available\n",
    "# -------------------------\n",
    "def _d2_try_import_webrtcvad():\n",
    "    try:\n",
    "        import webrtcvad  # type: ignore\n",
    "        return webrtcvad, True\n",
    "    except Exception:\n",
    "        return None, False\n",
    "\n",
    "webrtcvad, D2_HAVE_WEBRTCVAD = _d2_try_import_webrtcvad()\n",
    "if not D2_HAVE_WEBRTCVAD:\n",
    "    !pip -q install webrtcvad\n",
    "    webrtcvad, D2_HAVE_WEBRTCVAD = _d2_try_import_webrtcvad()\n",
    "\n",
    "try:\n",
    "    from scipy.signal import resample_poly  # type: ignore\n",
    "    D2_HAVE_SCIPY = True\n",
    "except Exception:\n",
    "    D2_HAVE_SCIPY = False\n",
    "\n",
    "# -------------------------\n",
    "# Dataset and output paths\n",
    "# Inputs: dataset folder + TSV filenames\n",
    "# Outputs: run folder with clips/, manifests/, config/, logs/ created\n",
    "# -------------------------\n",
    "D2_PROJECT_DIR  = \"/content/drive/MyDrive/AI_PD_Project\"\n",
    "D2_DATASETS_DIR = f\"{D2_PROJECT_DIR}/Datasets\"\n",
    "D2_EWA_DIR      = f\"{D2_DATASETS_DIR}/D2-Slovak (EWA-DB)/EWA-DB\"\n",
    "\n",
    "D2_FILES_TSV    = f\"{D2_EWA_DIR}/FILES.TSV\"\n",
    "D2_SPEAKERS_TSV = f\"{D2_EWA_DIR}/SPEAKERS.TSV\"\n",
    "D2_README_TXT   = f\"{D2_EWA_DIR}/README.TXT\"\n",
    "\n",
    "# Output root (Drive)\n",
    "D2_OUT_ROOT     = f\"{D2_EWA_DIR}/preprocessed_v1\"\n",
    "\n",
    "# Run isolation\n",
    "# - Keep the same RUN_NAME to resume the same run after a disconnect\n",
    "# - Change RUN_NAME to start a clean, separate run folder\n",
    "D2_RUN_NAME     = \"run_v1\"  # change to \"run_v2\" if one want a totally fresh run folder\n",
    "\n",
    "D2_RUN_ROOT     = f\"{D2_OUT_ROOT}/runs/{D2_RUN_NAME}\"\n",
    "D2_CLIPS_DIR    = f\"{D2_RUN_ROOT}/clips\"\n",
    "D2_MANIFEST_DIR = f\"{D2_RUN_ROOT}/manifests\"\n",
    "D2_CONFIG_DIR   = f\"{D2_RUN_ROOT}/config\"\n",
    "D2_LOGS_DIR     = f\"{D2_RUN_ROOT}/logs\"\n",
    "\n",
    "# Candidate clips (temporary stage)\n",
    "D2_CAND_DIR_DRIVE = f\"{D2_CLIPS_DIR}/_candidates\"\n",
    "\n",
    "# Optional local cache to reduce Drive write failures\n",
    "# - Writes candidate WAVs locally first, then copies to Drive\n",
    "D2_USE_LOCAL_CAND_CACHE = True\n",
    "D2_CAND_DIR_LOCAL = f\"/content/d2_candidates_cache/{D2_RUN_NAME}\"\n",
    "\n",
    "# Reset is destructive: deletes the whole run folder\n",
    "D2_RESET_RUN = False\n",
    "\n",
    "# Create required folders\n",
    "for p in [D2_RUN_ROOT, D2_CLIPS_DIR, D2_MANIFEST_DIR, D2_CONFIG_DIR, D2_LOGS_DIR]:\n",
    "    os.makedirs(p, exist_ok=True)\n",
    "for sp in [\"train\", \"val\", \"test\"]:\n",
    "    os.makedirs(os.path.join(D2_CLIPS_DIR, sp), exist_ok=True)\n",
    "\n",
    "os.makedirs(D2_CAND_DIR_DRIVE, exist_ok=True)\n",
    "if D2_USE_LOCAL_CAND_CACHE:\n",
    "    os.makedirs(D2_CAND_DIR_LOCAL, exist_ok=True)\n",
    "\n",
    "# Reset behavior (explicit only)\n",
    "if D2_RESET_RUN:\n",
    "    print(\"D2_RESET_RUN=True -> deleting run folder:\", D2_RUN_ROOT)\n",
    "    shutil.rmtree(D2_RUN_ROOT, ignore_errors=True)\n",
    "    for p in [D2_RUN_ROOT, D2_CLIPS_DIR, D2_MANIFEST_DIR, D2_CONFIG_DIR, D2_LOGS_DIR]:\n",
    "        os.makedirs(p, exist_ok=True)\n",
    "    for sp in [\"train\", \"val\", \"test\"]:\n",
    "        os.makedirs(os.path.join(D2_CLIPS_DIR, sp), exist_ok=True)\n",
    "    os.makedirs(D2_CAND_DIR_DRIVE, exist_ok=True)\n",
    "    if D2_USE_LOCAL_CAND_CACHE:\n",
    "        shutil.rmtree(D2_CAND_DIR_LOCAL, ignore_errors=True)\n",
    "        os.makedirs(D2_CAND_DIR_LOCAL, exist_ok=True)\n",
    "\n",
    "# -------------------------\n",
    "# Quick checks (fail early if dataset is missing)\n",
    "# Inputs: expected dataset files and folders\n",
    "# Outputs: printed status; raises if critical files are missing\n",
    "# -------------------------\n",
    "print(\"D2_EWA_DIR exists?\", os.path.exists(D2_EWA_DIR))\n",
    "print(\"FILES.TSV exists?\", os.path.exists(D2_FILES_TSV))\n",
    "print(\"SPEAKERS.TSV exists?\", os.path.exists(D2_SPEAKERS_TSV))\n",
    "print(\"README.TXT exists?\", os.path.exists(D2_README_TXT))\n",
    "print(\"webrtcvad available?\", D2_HAVE_WEBRTCVAD)\n",
    "print(\"scipy available?\", D2_HAVE_SCIPY)\n",
    "print(\"Run root:\", D2_RUN_ROOT)\n",
    "print(\"Candidate dir (Drive):\", D2_CAND_DIR_DRIVE)\n",
    "print(\"Using local candidate cache?\", D2_USE_LOCAL_CAND_CACHE)\n",
    "if D2_USE_LOCAL_CAND_CACHE:\n",
    "    print(\"Candidate dir (Local):\", D2_CAND_DIR_LOCAL)\n",
    "\n",
    "if not os.path.exists(D2_EWA_DIR):\n",
    "    raise FileNotFoundError(f\"D2_EWA_DIR not found: {D2_EWA_DIR}\")\n",
    "if not os.path.exists(D2_FILES_TSV) or not os.path.exists(D2_SPEAKERS_TSV):\n",
    "    raise FileNotFoundError(f\"Missing FILES.TSV or SPEAKERS.TSV under: {D2_EWA_DIR}\")\n",
    "\n",
    "# -------------------------\n",
    "# Processing settings\n",
    "# Inputs: constants (sampling rate, clip lengths, split fractions)\n",
    "# Outputs: consistent preprocessing behavior across runs\n",
    "# -------------------------\n",
    "D2_SR = 16000\n",
    "D2_RANDOM_SEED = 1337\n",
    "random.seed(D2_RANDOM_SEED)\n",
    "np.random.seed(D2_RANDOM_SEED)\n",
    "\n",
    "# Loudness leveling (simple RMS target + peak limiting)\n",
    "D2_TARGET_RMS_DBFS = -20.0\n",
    "D2_PEAK_LIMIT_DBFS = -1.0\n",
    "D2_MIN_RMS_DBFS    = -60.0\n",
    "D2_MAX_GAIN_DB     = 18.0\n",
    "\n",
    "# Voice activity detection (used only to build a voiced-only signal)\n",
    "D2_VAD_MODE       = 2\n",
    "D2_FRAME_MS       = 30\n",
    "D2_MIN_SPEECH_MS  = 200\n",
    "D2_MERGE_GAP_MS   = 200\n",
    "D2_PAD_SEC        = 0.25\n",
    "D2_MIN_KEEP_SEC   = 0.30\n",
    "\n",
    "# Clip rules (fixed)\n",
    "# - vowel: 2.0s, pad if short\n",
    "# - reading: 8.0s if long enough else keep true duration\n",
    "D2_VOWEL_SEC = 2.0\n",
    "D2_READ_SEC  = 8.0\n",
    "\n",
    "# Cap and split\n",
    "D2_MAX_CLIPS_PER_SPK_TASK = 8\n",
    "D2_TRAIN_PCT, D2_VAL_PCT, D2_TEST_PCT = 0.70, 0.15, 0.15\n",
    "\n",
    "# Audio existence checking\n",
    "D2_DO_EXISTENCE_CHECK = True\n",
    "D2_EXIST_CHECK_UNIQUE_ONLY = True\n",
    "\n",
    "# Robust writing (retries help with transient Drive errors)\n",
    "D2_WRITE_RETRIES = 4\n",
    "D2_WRITE_SLEEP   = 0.75\n",
    "\n",
    "# Progress checkpoint (diagnostic)\n",
    "D2_CHECKPOINT_EVERY = 500  # rows\n",
    "D2_CAND_CHECKPOINT_CSV = os.path.join(D2_LOGS_DIR, \"candidates_checkpoint.csv\")\n",
    "\n",
    "# -------------------------\n",
    "# Manifest schema (fixed order)\n",
    "# Inputs: processed clips + metadata\n",
    "# Outputs: manifest_all.csv and per-split manifests in the same column order\n",
    "# -------------------------\n",
    "MANIFEST_COLS = [\n",
    "    \"split\",\"dataset\",\"task\",\"speaker_id\",\"sample_id\",\n",
    "    \"label_str\",\"label_num\",\"age\",\"sex\",\"speaker_key_rel\",\n",
    "    \"clip_path\",\"duration_sec\",\"source_path\",\n",
    "    \"clip_start_sec\",\"clip_end_sec\",\"sr_hz\",\"channels\",\n",
    "    \"clip_is_contiguous\",\n",
    "]\n",
    "\n",
    "# -------------------------\n",
    "# Utility functions\n",
    "# Inputs: strings/audio arrays\n",
    "# Outputs: cleaned names, normalized audio, VAD segments, final clip selection\n",
    "# -------------------------\n",
    "def d2_safe(s: str) -> str:\n",
    "    return re.sub(r\"[^A-Za-z0-9_\\-\\.]+\", \"_\", str(s))\n",
    "\n",
    "def d2_db_to_lin(db: float) -> float:\n",
    "    return 10.0 ** (db / 20.0)\n",
    "\n",
    "def d2_rms_dbfs(y: np.ndarray) -> float:\n",
    "    if y is None or len(y) == 0:\n",
    "        return -120.0\n",
    "    rms = float(np.sqrt(np.mean(y.astype(np.float64) ** 2) + 1e-12))\n",
    "    return 20.0 * math.log10(max(rms, 1e-12))\n",
    "\n",
    "def d2_peak_limit(y: np.ndarray, peak_dbfs: float) -> np.ndarray:\n",
    "    if y is None or len(y) == 0:\n",
    "        return y\n",
    "    peak = float(np.max(np.abs(y)))\n",
    "    lim = d2_db_to_lin(peak_dbfs)\n",
    "    if peak > lim and peak > 0:\n",
    "        y = y * (lim / peak)\n",
    "    return np.clip(y, -1.0, 1.0).astype(np.float32)\n",
    "\n",
    "def d2_norm_rms_then_peak(y: np.ndarray) -> np.ndarray:\n",
    "    if y is None or len(y) == 0:\n",
    "        return y\n",
    "    cur = d2_rms_dbfs(y)\n",
    "    if cur < D2_MIN_RMS_DBFS:\n",
    "        return d2_peak_limit(y.astype(np.float32), D2_PEAK_LIMIT_DBFS)\n",
    "    gain_db = float(D2_TARGET_RMS_DBFS - cur)\n",
    "    gain_db = float(np.clip(gain_db, -60.0, D2_MAX_GAIN_DB))\n",
    "    y2 = (y.astype(np.float32) * d2_db_to_lin(gain_db)).astype(np.float32)\n",
    "    return d2_peak_limit(y2, D2_PEAK_LIMIT_DBFS)\n",
    "\n",
    "def d2_read_mono(path: str) -> Tuple[np.ndarray, int]:\n",
    "    x, sr = sf.read(path, always_2d=True)\n",
    "    x = x.mean(axis=1).astype(np.float32)\n",
    "    if not np.isfinite(x).all():\n",
    "        x = np.nan_to_num(x, nan=0.0, posinf=0.0, neginf=0.0).astype(np.float32)\n",
    "    return x, int(sr)\n",
    "\n",
    "def d2_resample(y: np.ndarray, sr_in: int, sr_out: int) -> np.ndarray:\n",
    "    if sr_in == sr_out:\n",
    "        return y.astype(np.float32, copy=False)\n",
    "    if D2_HAVE_SCIPY:\n",
    "        g = math.gcd(sr_in, sr_out)\n",
    "        up = sr_out // g\n",
    "        down = sr_in // g\n",
    "        return resample_poly(y.astype(np.float64), up, down).astype(np.float32, copy=False)\n",
    "    n_new = int(round(len(y) * (sr_out / sr_in)))\n",
    "    if n_new <= 1:\n",
    "        return y[:1].astype(np.float32, copy=False)\n",
    "    x_old = np.linspace(0.0, 1.0, num=len(y), endpoint=False)\n",
    "    x_new = np.linspace(0.0, 1.0, num=n_new, endpoint=False)\n",
    "    return np.interp(x_new, x_old, y).astype(np.float32, copy=False)\n",
    "\n",
    "def d2_pcm16_bytes(y: np.ndarray) -> bytes:\n",
    "    y = np.clip(y, -1.0, 1.0)\n",
    "    return (y * 32767.0).astype(np.int16).tobytes()\n",
    "\n",
    "def d2_task_type_from_picture(picture: str) -> str:\n",
    "    return \"vowel\" if str(picture).strip().lower() == \"vokal\" else \"reading\"\n",
    "\n",
    "def d2_task_short(tt: str) -> str:\n",
    "    tt = (tt or \"\").lower()\n",
    "    if tt == \"vowel\":\n",
    "        return \"vowl\"\n",
    "    if tt == \"reading\":\n",
    "        return \"read\"\n",
    "    return \"unk\"\n",
    "\n",
    "def d2_label(diag: str) -> Tuple[str, int, str]:\n",
    "    d = str(diag).strip().lower()\n",
    "    if \"parkinson\" in d:\n",
    "        return \"Parkinson\", 1, \"PD\"\n",
    "    return \"Healthy\", 0, \"HC\"\n",
    "\n",
    "def d2_safe_write_wav(path: str, y: np.ndarray, sr: int) -> None:\n",
    "    # Writes WAV with retries to handle transient filesystem issues\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "    last_err = None\n",
    "    for attempt in range(1, D2_WRITE_RETRIES + 1):\n",
    "        try:\n",
    "            sf.write(path, np.clip(y, -1.0, 1.0).astype(np.float32), sr, subtype=\"PCM_16\")\n",
    "            return\n",
    "        except Exception as e:\n",
    "            last_err = e\n",
    "            time.sleep(D2_WRITE_SLEEP * attempt)\n",
    "    raise RuntimeError(f\"Failed to write WAV: {path}. Last error: {repr(last_err)}\")\n",
    "\n",
    "def d2_to_boolish(x) -> bool:\n",
    "    # Parses common “true-ish” values from TSV text\n",
    "    if isinstance(x, bool):\n",
    "        return x\n",
    "    s = str(x).strip().lower()\n",
    "    return s in {\"true\",\"1\",\"yes\",\"y\",\"t\"}\n",
    "\n",
    "# --- Voiced-segment utilities (builds a voiced-only concatenated signal) ---\n",
    "def d2_energy_segments(y: np.ndarray, sr: int) -> List[Tuple[int, int]]:\n",
    "    frame = int(sr * 0.02)\n",
    "    hop = frame\n",
    "    if frame <= 0 or len(y) < frame:\n",
    "        return []\n",
    "    eng = []\n",
    "    idx = []\n",
    "    for i in range(0, len(y) - frame + 1, hop):\n",
    "        w = y[i:i+frame]\n",
    "        eng.append(float(np.mean(w*w)))\n",
    "        idx.append(i)\n",
    "    eng = np.array(eng, dtype=np.float32)\n",
    "    thr = float(np.percentile(eng, 25)) * 2.5\n",
    "    thr = max(thr, 1e-8)\n",
    "    keep = eng > thr\n",
    "\n",
    "    segs = []\n",
    "    on = False\n",
    "    s0 = 0\n",
    "    for k, flag in enumerate(keep):\n",
    "        if flag and not on:\n",
    "            on = True\n",
    "            s0 = idx[k]\n",
    "        elif (not flag) and on:\n",
    "            on = False\n",
    "            segs.append((s0, idx[k] + frame))\n",
    "    if on and idx:\n",
    "        segs.append((s0, idx[-1] + frame))\n",
    "    return segs\n",
    "\n",
    "def d2_merge(segs: List[Tuple[int,int]], sr: int) -> List[Tuple[int,int]]:\n",
    "    if not segs:\n",
    "        return []\n",
    "    gap = int(sr * (D2_MERGE_GAP_MS/1000.0))\n",
    "    min_len = int(sr * (D2_MIN_SPEECH_MS/1000.0))\n",
    "    segs = sorted(segs)\n",
    "    merged = [list(segs[0])]\n",
    "    for s,e in segs[1:]:\n",
    "        if s - merged[-1][1] <= gap:\n",
    "            merged[-1][1] = max(merged[-1][1], e)\n",
    "        else:\n",
    "            merged.append([s,e])\n",
    "    out = []\n",
    "    for s,e in merged:\n",
    "        if (e-s) >= min_len:\n",
    "            out.append((int(s), int(e)))\n",
    "    return out\n",
    "\n",
    "def d2_pad(segs: List[Tuple[int,int]], sr: int, n: int) -> List[Tuple[int,int]]:\n",
    "    pad = int(sr * D2_PAD_SEC)\n",
    "    out = []\n",
    "    for s,e in segs:\n",
    "        s2 = max(0, s-pad)\n",
    "        e2 = min(n, e+pad)\n",
    "        if e2 > s2:\n",
    "            out.append((s2,e2))\n",
    "    return out\n",
    "\n",
    "def d2_webrtc_segments(y: np.ndarray, sr: int) -> Optional[List[Tuple[int,int]]]:\n",
    "    # Uses webrtcvad only at 16 kHz; returns source segments used to build voiced-only signal\n",
    "    if not D2_HAVE_WEBRTCVAD or sr != 16000:\n",
    "        return None\n",
    "    frame_len = int(sr * (D2_FRAME_MS/1000.0))\n",
    "    if frame_len <= 0 or len(y) < frame_len:\n",
    "        return []\n",
    "\n",
    "    n_frames = int(math.ceil(len(y)/frame_len))\n",
    "    pad_samp = n_frames*frame_len - len(y)\n",
    "    if pad_samp > 0:\n",
    "        y = np.concatenate([y, np.zeros(pad_samp, dtype=np.float32)], axis=0)\n",
    "    pcm = d2_pcm16_bytes(y)\n",
    "\n",
    "    vad = webrtcvad.Vad(int(D2_VAD_MODE))\n",
    "    flags = []\n",
    "    for i in range(n_frames):\n",
    "        s = i*frame_len*2\n",
    "        e = s + frame_len*2\n",
    "        flags.append(vad.is_speech(pcm[s:e], 16000))\n",
    "\n",
    "    segs = []\n",
    "    on = False\n",
    "    s0 = 0\n",
    "    for i,f in enumerate(flags):\n",
    "        if f and not on:\n",
    "            on = True\n",
    "            s0 = i\n",
    "        elif (not f) and on:\n",
    "            on = False\n",
    "            segs.append((s0*frame_len, i*frame_len))\n",
    "    if on:\n",
    "        segs.append((s0*frame_len, n_frames*frame_len))\n",
    "\n",
    "    n0 = len(y) - pad_samp if pad_samp > 0 else len(y)\n",
    "    return [(max(0,s), min(n0,e)) for s,e in segs]\n",
    "\n",
    "def d2_voiced_concat(y: np.ndarray, sr: int) -> Tuple[np.ndarray, str]:\n",
    "    # Creates a voiced-only signal by concatenating voiced segments\n",
    "    segs = d2_webrtc_segments(y, sr)\n",
    "    if segs is None:\n",
    "        segs = d2_energy_segments(y, sr)\n",
    "        used = \"energy\"\n",
    "    else:\n",
    "        used = \"webrtcvad\"\n",
    "    segs = d2_merge(segs, sr)\n",
    "    segs = d2_pad(segs, sr, len(y))\n",
    "    if not segs:\n",
    "        return y.astype(np.float32, copy=False), used\n",
    "    return np.concatenate([y[s:e] for s,e in segs], axis=0).astype(np.float32, copy=False), used\n",
    "\n",
    "def d2_trim_leading_silence(y: np.ndarray, sr: int, max_trim_sec: float = 1.5, thr_dbfs: float = -45.0) -> Tuple[np.ndarray, int]:\n",
    "    # Conservative leading trim to reduce silent start in reading clips\n",
    "    if y is None or len(y) == 0:\n",
    "        return y, 0\n",
    "    max_trim = int(sr * max_trim_sec)\n",
    "    n = min(len(y), max_trim)\n",
    "    if n <= 0:\n",
    "        return y, 0\n",
    "    thr = d2_db_to_lin(thr_dbfs)\n",
    "    idx = 0\n",
    "    while idx < n and float(abs(y[idx])) < thr:\n",
    "        idx += 1\n",
    "    if idx >= n:\n",
    "        return y, 0\n",
    "    back = int(0.02 * sr)\n",
    "    idx2 = max(0, idx - back)\n",
    "    return y[idx2:], idx2\n",
    "\n",
    "def d2_best_reading_window(voiced: np.ndarray, sr: int, target_sec: float = 8.0) -> Tuple[np.ndarray, int, int]:\n",
    "    # Picks an 8s crop that tends to start with speech sooner\n",
    "    target_len = int(round(sr * target_sec))\n",
    "    n = len(voiced)\n",
    "    if n <= target_len:\n",
    "        return voiced.astype(np.float32, copy=False), 0, n\n",
    "    offsets = sorted(set([0, int(0.25*(n-target_len)), int(0.5*(n-target_len)), int(0.75*(n-target_len)), n-target_len]))\n",
    "    best = None\n",
    "    best_score = -1.0\n",
    "    for st in offsets:\n",
    "        en = st + target_len\n",
    "        w = voiced[st:en]\n",
    "        k = int(0.8 * sr)\n",
    "        k = min(k, len(w))\n",
    "        if k <= 0:\n",
    "            continue\n",
    "        score = float(np.mean(w[:k].astype(np.float64)**2))\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best = (st, en)\n",
    "    if best is None:\n",
    "        st = (n - target_len)//2\n",
    "        en = st + target_len\n",
    "        return voiced[st:en].astype(np.float32, copy=False), st, en\n",
    "    st, en = best\n",
    "    return voiced[st:en].astype(np.float32, copy=False), st, en\n",
    "\n",
    "def d2_choose_one_clip(voiced: np.ndarray, sr: int, is_vowel: bool) -> Tuple[np.ndarray, float, float]:\n",
    "    \"\"\"\n",
    "    Exactly one clip per source:\n",
    "      - Vowel: 2.0s center crop if longer; zero-pad if shorter\n",
    "      - Reading: best 8.0s window if long enough; else keep true duration\n",
    "    clip_start_sec/clip_end_sec are on the voiced-concat timeline.\n",
    "    \"\"\"\n",
    "    if voiced is None or len(voiced) == 0:\n",
    "        return np.zeros((0,), dtype=np.float32), 0.0, 0.0\n",
    "\n",
    "    if is_vowel:\n",
    "        target_len = int(round(sr * D2_VOWEL_SEC))\n",
    "        n = len(voiced)\n",
    "        if n >= target_len:\n",
    "            start = (n - target_len)//2\n",
    "            end = start + target_len\n",
    "            clip = voiced[start:end].astype(np.float32, copy=False)\n",
    "            return clip, float(start)/sr, float(end)/sr\n",
    "        tmp = np.zeros((target_len,), dtype=np.float32)\n",
    "        tmp[:n] = voiced.astype(np.float32, copy=False)\n",
    "        return tmp, 0.0, float(target_len)/sr\n",
    "\n",
    "    n = len(voiced)\n",
    "    target_len = int(round(sr * D2_READ_SEC))\n",
    "    if n >= target_len:\n",
    "        clip, st, en = d2_best_reading_window(voiced, sr, D2_READ_SEC)\n",
    "        return clip, float(st)/sr, float(en)/sr\n",
    "    return voiced.astype(np.float32, copy=False), 0.0, float(n)/sr\n",
    "\n",
    "def d2_candidate_filename(row) -> str:\n",
    "    # Deterministic name so the same row produces the same candidate file after restart\n",
    "    spk = d2_safe(str(row.speaker_id))\n",
    "    task = d2_safe(str(row.task)[:5])\n",
    "    base = d2_safe(Path(str(row.audio_path)).name)\n",
    "    tag = \"vowl\" if str(row.task_type).lower() == \"vowel\" else \"read\"\n",
    "    return f\"D2CAND_{spk}_{tag}_{task}_{base}.wav\"\n",
    "\n",
    "def d2_drive_ok() -> bool:\n",
    "    # Lightweight check used during long runs\n",
    "    return os.path.isdir(\"/content/drive/MyDrive\") and os.path.isdir(D2_EWA_DIR)\n",
    "\n",
    "def d2_sync_one_local_to_drive(local_path: str, drive_path: str) -> None:\n",
    "    # Copies local candidate to Drive with retries\n",
    "    os.makedirs(os.path.dirname(drive_path), exist_ok=True)\n",
    "    last_err = None\n",
    "    for attempt in range(1, D2_WRITE_RETRIES + 1):\n",
    "        try:\n",
    "            shutil.copy2(local_path, drive_path)\n",
    "            return\n",
    "        except Exception as e:\n",
    "            last_err = e\n",
    "            time.sleep(D2_WRITE_SLEEP * attempt)\n",
    "    raise RuntimeError(f\"Failed to sync local->drive: {local_path} -> {drive_path}. Last error: {repr(last_err)}\")\n",
    "\n",
    "# -------------------------\n",
    "# Load metadata + filter usable rows\n",
    "# Inputs: FILES.TSV, SPEAKERS.TSV\n",
    "# Outputs: filtered table with audio_path, labels, task type, speaker metadata\n",
    "# -------------------------\n",
    "print(\"\\nReading TSV/README...\")\n",
    "\n",
    "files_df = pd.read_csv(D2_FILES_TSV, sep=\"\\t\", dtype=str, keep_default_na=False, na_values=[\"\"])\n",
    "speakers_df = pd.read_csv(D2_SPEAKERS_TSV, sep=\"\\t\", dtype=str, keep_default_na=False, na_values=[\"\"])\n",
    "_ = Path(D2_README_TXT).read_text(errors=\"ignore\") if os.path.exists(D2_README_TXT) else \"\"\n",
    "\n",
    "print(\"FILES.TSV rows:\", len(files_df))\n",
    "print(\"SPEAKERS.TSV rows:\", len(speakers_df))\n",
    "\n",
    "req_files = {\"SPEAKER_CODE\",\"DIAGNOSIS\",\"INCLUSIVE_CRITERIA\",\"AUDIOFILE\",\"PICTURE\"}\n",
    "miss_f = sorted(list(req_files - set(files_df.columns)))\n",
    "if miss_f:\n",
    "    raise ValueError(f\"FILES.TSV missing columns: {miss_f}\")\n",
    "if \"SPEAKER_CODE\" not in speakers_df.columns:\n",
    "    raise ValueError(\"SPEAKERS.TSV missing column: SPEAKER_CODE\")\n",
    "\n",
    "d2 = files_df.copy()\n",
    "\n",
    "# Canonical filters: diagnosis, inclusion, publish agreement (if present), audio availability\n",
    "d2[\"diag_norm\"] = d2[\"DIAGNOSIS\"].astype(str).str.strip()\n",
    "d2 = d2[d2[\"diag_norm\"].isin([\"Healthy\",\"Parkinson\"])].copy()\n",
    "\n",
    "d2[\"inclusive_ok\"] = d2[\"INCLUSIVE_CRITERIA\"].apply(d2_to_boolish)\n",
    "d2 = d2[d2[\"inclusive_ok\"]].copy()\n",
    "\n",
    "if \"PUBLISH_AGREEMENT\" in d2.columns:\n",
    "    d2[\"publish_ok\"] = d2[\"PUBLISH_AGREEMENT\"].apply(d2_to_boolish)\n",
    "    d2 = d2[d2[\"publish_ok\"]].copy()\n",
    "\n",
    "d2 = d2[d2[\"AUDIOFILE\"].astype(str).str.strip() != \"<not-available>\"].copy()\n",
    "\n",
    "print(\"\\nRows after diag + inclusive + publish(if exists) + audio:\", len(d2))\n",
    "print(\"Counts by diag_norm:\")\n",
    "print(d2[\"diag_norm\"].value_counts())\n",
    "\n",
    "def d2_make_audio_path(p: str) -> str:\n",
    "    p = str(p).strip().lstrip(\"./\").replace(\"\\\\\", \"/\")\n",
    "    return os.path.join(D2_EWA_DIR, p)\n",
    "\n",
    "d2[\"audio_path\"] = d2[\"AUDIOFILE\"].apply(d2_make_audio_path)\n",
    "d2[\"speaker_id\"] = d2[\"SPEAKER_CODE\"].astype(str)\n",
    "\n",
    "# Task mapping: vokal -> vowel clip rules, everything else -> reading clip rules\n",
    "d2[\"task_type\"] = d2[\"PICTURE\"].apply(d2_task_type_from_picture)\n",
    "d2[\"task\"] = d2[\"task_type\"].apply(d2_task_short)\n",
    "\n",
    "# Labels\n",
    "lab = d2[\"diag_norm\"].apply(d2_label)\n",
    "d2[\"label_str\"] = lab.apply(lambda t: t[0])\n",
    "d2[\"label_num\"] = lab.apply(lambda t: t[1]).astype(int)\n",
    "d2[\"hc_pd_tag\"] = lab.apply(lambda t: t[2])\n",
    "\n",
    "# Speaker metadata (optional columns only)\n",
    "col_age = \"AGE\" if \"AGE\" in speakers_df.columns else None\n",
    "col_sex = \"SEX\" if \"SEX\" in speakers_df.columns else None\n",
    "\n",
    "speaker_key_candidates = {\"speaker_key_rel\",\"speaker_key\",\"speakerkeyrel\",\"speakerkey\",\"speaker_key_rel \"}\n",
    "d2_spk_key_col = None\n",
    "for c in speakers_df.columns:\n",
    "    if str(c).strip().lower() in speaker_key_candidates:\n",
    "        d2_spk_key_col = c\n",
    "        break\n",
    "\n",
    "spk_cols = [\"SPEAKER_CODE\"]\n",
    "if col_age: spk_cols.append(col_age)\n",
    "if col_sex: spk_cols.append(col_sex)\n",
    "if d2_spk_key_col and d2_spk_key_col not in spk_cols:\n",
    "    spk_cols.append(d2_spk_key_col)\n",
    "\n",
    "d2 = d2.merge(speakers_df[spk_cols], on=\"SPEAKER_CODE\", how=\"left\", validate=\"many_to_one\")\n",
    "d2[\"age\"] = d2[col_age] if col_age else np.nan\n",
    "d2[\"sex\"] = d2[col_sex] if col_sex else np.nan\n",
    "d2[\"speaker_key_rel\"] = d2[d2_spk_key_col] if d2_spk_key_col else np.nan\n",
    "\n",
    "# -------------------------\n",
    "# Audio existence check\n",
    "# Inputs: filtered rows with audio_path\n",
    "# Outputs: missing_paths.csv + filtered rows with existing audio only\n",
    "# -------------------------\n",
    "missing_audio_rows = pd.DataFrame(columns=[\"speaker_id\",\"audio_path\",\"AUDIOFILE\",\"diag_norm\",\"PICTURE\"])\n",
    "\n",
    "if D2_DO_EXISTENCE_CHECK:\n",
    "    if D2_EXIST_CHECK_UNIQUE_ONLY:\n",
    "        uniq = d2[\"audio_path\"].dropna().astype(str).unique().tolist()\n",
    "        exists_map = {}\n",
    "        for p in tqdm(uniq, desc=\"D2 existence check (unique paths)\", dynamic_ncols=True):\n",
    "            exists_map[p] = os.path.exists(p)\n",
    "        d2[\"exists\"] = d2[\"audio_path\"].map(exists_map).fillna(False)\n",
    "    else:\n",
    "        d2[\"exists\"] = [os.path.exists(p) for p in tqdm(d2[\"audio_path\"].astype(str).tolist(),\n",
    "                                                        desc=\"D2 existence check\", dynamic_ncols=True)]\n",
    "else:\n",
    "    d2[\"exists\"] = True\n",
    "\n",
    "missing_audio_rows = d2.loc[~d2[\"exists\"], [\"speaker_id\",\"audio_path\",\"AUDIOFILE\",\"diag_norm\",\"PICTURE\"]].copy()\n",
    "print(\"\\nAudio exists:\", int(d2[\"exists\"].sum()), \"missing:\", int((~d2[\"exists\"]).sum()))\n",
    "\n",
    "d2_missing_csv = os.path.join(D2_MANIFEST_DIR, \"missing_paths.csv\")\n",
    "os.makedirs(os.path.dirname(d2_missing_csv), exist_ok=True)\n",
    "missing_audio_rows.to_csv(d2_missing_csv, index=False)\n",
    "print(\"Saved missing list:\", d2_missing_csv)\n",
    "\n",
    "d2 = d2.loc[d2[\"exists\"]].copy()\n",
    "if len(d2) == 0:\n",
    "    raise RuntimeError(\"No usable D2 rows after filtering + existence checks.\")\n",
    "\n",
    "print(\"\\nLabel counts (rows):\")\n",
    "print(d2[\"label_str\"].value_counts(dropna=False))\n",
    "print(\"\\nTask counts (rows):\")\n",
    "print(d2[\"task\"].value_counts(dropna=False))\n",
    "\n",
    "# -------------------------\n",
    "# Resume snapshot (before heavy work)\n",
    "# Inputs: candidate folders\n",
    "# Outputs: printed counts and an estimated skip rate\n",
    "# -------------------------\n",
    "d2_proc = d2.sort_values([\"speaker_id\",\"audio_path\"]).reset_index(drop=True)\n",
    "\n",
    "existing_drive = len(list(Path(D2_CAND_DIR_DRIVE).glob(\"D2CAND_*.wav\")))\n",
    "existing_local = len(list(Path(D2_CAND_DIR_LOCAL).glob(\"D2CAND_*.wav\"))) if D2_USE_LOCAL_CAND_CACHE else 0\n",
    "\n",
    "print(\"\\n=== Resume diagnostics ===\")\n",
    "print(\"Rows to process (after filters + exists):\", len(d2_proc))\n",
    "print(\"Existing candidates on Drive:\", existing_drive)\n",
    "if D2_USE_LOCAL_CAND_CACHE:\n",
    "    print(\"Existing candidates in Local cache:\", existing_local)\n",
    "\n",
    "sample_n = min(2000, len(d2_proc))\n",
    "already = 0\n",
    "for rr in d2_proc.head(sample_n).itertuples(index=False):\n",
    "    fn = d2_candidate_filename(rr)\n",
    "    drive_path = os.path.join(D2_CAND_DIR_DRIVE, fn)\n",
    "    local_path = os.path.join(D2_CAND_DIR_LOCAL, fn) if D2_USE_LOCAL_CAND_CACHE else \"\"\n",
    "    if os.path.exists(drive_path) or (D2_USE_LOCAL_CAND_CACHE and os.path.exists(local_path)):\n",
    "        already += 1\n",
    "print(f\"Sample skip rate (first {sample_n} rows): {already}/{sample_n} already have candidate\")\n",
    "\n",
    "# -------------------------\n",
    "# Candidate creation (resume-safe, 1 clip per source)\n",
    "# Inputs: d2_proc rows + source audio files\n",
    "# Outputs: clips/_candidates/*.wav (and checkpoint CSV)\n",
    "# -------------------------\n",
    "cand_rows: List[Dict] = []\n",
    "warn_rows: List[Dict] = []\n",
    "\n",
    "pbar = tqdm(d2_proc.itertuples(index=False), total=len(d2_proc),\n",
    "            desc=\"D2 preprocess (resume-safe; 1 clip per source)\", dynamic_ncols=True)\n",
    "\n",
    "written_now = 0\n",
    "skipped_existing = 0\n",
    "bad_reads = 0\n",
    "\n",
    "def d2_checkpoint_save(rows: List[Dict]) -> None:\n",
    "    # Writes a current snapshot of candidate records (diagnostic only)\n",
    "    if not rows:\n",
    "        return\n",
    "    try:\n",
    "        ck = pd.DataFrame(rows)\n",
    "        ck.to_csv(D2_CAND_CHECKPOINT_CSV, index=False)\n",
    "    except Exception as e:\n",
    "        warn_rows.append({\n",
    "            \"dataset\":\"D2\",\"speaker_id\":\"\",\"source_path\":\"\",\n",
    "            \"warning_type\":\"checkpoint_write_failed\",\"detail\":repr(e)\n",
    "        })\n",
    "\n",
    "for i, r in enumerate(pbar, start=1):\n",
    "    src = str(r.audio_path)\n",
    "\n",
    "    # Deterministic candidate filenames enable safe restarts\n",
    "    cand_fn = d2_candidate_filename(r)\n",
    "    cand_drive_path = os.path.join(D2_CAND_DIR_DRIVE, cand_fn)\n",
    "    cand_local_path = os.path.join(D2_CAND_DIR_LOCAL, cand_fn) if D2_USE_LOCAL_CAND_CACHE else None\n",
    "\n",
    "    # Resume-safe: skip processing if candidate already exists\n",
    "    if os.path.exists(cand_drive_path) or (cand_local_path and os.path.exists(cand_local_path)):\n",
    "        skipped_existing += 1\n",
    "        # Still record a row so downstream cap/split can proceed\n",
    "        cand_rows.append({\n",
    "            \"dataset\": \"D2\",\n",
    "            \"task\": str(r.task)[:5],\n",
    "            \"speaker_id\": str(r.speaker_id),\n",
    "            \"sample_id\": os.path.basename(src),\n",
    "            \"label_str\": r.label_str,\n",
    "            \"label_num\": int(r.label_num),\n",
    "            \"age\": r.age if pd.notna(r.age) else np.nan,\n",
    "            \"sex\": r.sex if pd.notna(r.sex) else np.nan,\n",
    "            \"speaker_key_rel\": r.speaker_key_rel if pd.notna(r.speaker_key_rel) else np.nan,\n",
    "            \"clip_path_cand\": cand_drive_path,\n",
    "            \"duration_sec\": np.nan,\n",
    "            \"source_path\": src,\n",
    "            \"clip_start_sec\": np.nan,\n",
    "            \"clip_end_sec\": np.nan,\n",
    "            \"sr_hz\": int(D2_SR),\n",
    "            \"channels\": 1,\n",
    "            \"clip_is_contiguous\": True,\n",
    "            \"hc_pd_tag\": str(r.hc_pd_tag).strip().upper(),\n",
    "        })\n",
    "        continue\n",
    "\n",
    "    # Quick Drive availability note (does not stop the run)\n",
    "    if not d2_drive_ok():\n",
    "        warn_rows.append({\n",
    "            \"dataset\":\"D2\",\"speaker_id\":str(r.speaker_id),\"source_path\":src,\n",
    "            \"warning_type\":\"drive_not_available\",\"detail\":\"Drive path not available during processing\"\n",
    "        })\n",
    "\n",
    "    try:\n",
    "        # Load -> resample -> normalize loudness\n",
    "        y, sr0 = d2_read_mono(src)\n",
    "        y = d2_resample(y, sr0, D2_SR)\n",
    "        y = d2_norm_rms_then_peak(y)\n",
    "\n",
    "        # Build a voiced-only signal (concatenated segments)\n",
    "        voiced, vad_used = d2_voiced_concat(y, D2_SR)\n",
    "        if len(voiced) < int(D2_SR * D2_MIN_KEEP_SEC):\n",
    "            voiced = y\n",
    "            warn_rows.append({\n",
    "                \"dataset\":\"D2\",\"speaker_id\":str(r.speaker_id),\"source_path\":src,\n",
    "                \"warning_type\":\"vad_too_short_fallback_original\",\n",
    "                \"detail\": f\"vad_used={vad_used}\"\n",
    "            })\n",
    "\n",
    "        is_vowel = (str(r.task_type).lower() == \"vowel\")\n",
    "\n",
    "        # Reading: trim some leading silence before choosing the final window\n",
    "        if not is_vowel:\n",
    "            voiced, trimmed = d2_trim_leading_silence(voiced, D2_SR, max_trim_sec=1.5, thr_dbfs=-45.0)\n",
    "            if trimmed > 0:\n",
    "                warn_rows.append({\n",
    "                    \"dataset\":\"D2\",\"speaker_id\":str(r.speaker_id),\"source_path\":src,\n",
    "                    \"warning_type\":\"leading_silence_trim_applied\",\n",
    "                    \"detail\": f\"trimmed_samples={trimmed}\"\n",
    "                })\n",
    "\n",
    "        # Select exactly one final clip from the voiced-only signal\n",
    "        clip_audio, st, en = d2_choose_one_clip(voiced, D2_SR, is_vowel)\n",
    "\n",
    "        if clip_audio is None or len(clip_audio) == 0:\n",
    "            warn_rows.append({\n",
    "                \"dataset\":\"D2\",\"speaker_id\":str(r.speaker_id),\"source_path\":src,\n",
    "                \"warning_type\":\"empty_clip_after_processing\",\n",
    "                \"detail\": f\"is_vowel={is_vowel}, vad_used={vad_used}\"\n",
    "            })\n",
    "            continue\n",
    "\n",
    "        # Write candidate to local cache then sync to Drive (or write directly to Drive)\n",
    "        if D2_USE_LOCAL_CAND_CACHE and cand_local_path:\n",
    "            d2_safe_write_wav(cand_local_path, clip_audio, D2_SR)\n",
    "            d2_sync_one_local_to_drive(cand_local_path, cand_drive_path)\n",
    "        else:\n",
    "            d2_safe_write_wav(cand_drive_path, clip_audio, D2_SR)\n",
    "\n",
    "        written_now += 1\n",
    "\n",
    "        # Record candidate metadata (used later for cap/split/final move)\n",
    "        cand_rows.append({\n",
    "            \"dataset\": \"D2\",\n",
    "            \"task\": str(r.task)[:5],\n",
    "            \"speaker_id\": str(r.speaker_id),\n",
    "            \"sample_id\": os.path.basename(src),\n",
    "            \"label_str\": r.label_str,\n",
    "            \"label_num\": int(r.label_num),\n",
    "            \"age\": r.age if pd.notna(r.age) else np.nan,\n",
    "            \"sex\": r.sex if pd.notna(r.sex) else np.nan,\n",
    "            \"speaker_key_rel\": r.speaker_key_rel if pd.notna(r.speaker_key_rel) else np.nan,\n",
    "            \"clip_path_cand\": cand_drive_path,\n",
    "            \"duration_sec\": float(len(clip_audio) / D2_SR),\n",
    "            \"source_path\": src,\n",
    "            \"clip_start_sec\": float(st),\n",
    "            \"clip_end_sec\": float(en),\n",
    "            \"sr_hz\": int(D2_SR),\n",
    "            \"channels\": 1,\n",
    "            \"clip_is_contiguous\": True,\n",
    "            \"hc_pd_tag\": str(r.hc_pd_tag).strip().upper(),\n",
    "        })\n",
    "\n",
    "    except Exception as e:\n",
    "        bad_reads += 1\n",
    "        warn_rows.append({\n",
    "            \"dataset\":\"D2\",\n",
    "            \"speaker_id\": str(getattr(r, \"speaker_id\", \"\")),\n",
    "            \"source_path\": src,\n",
    "            \"warning_type\":\"preprocess_error\",\n",
    "            \"detail\": repr(e)\n",
    "        })\n",
    "\n",
    "    # Periodic checkpoint to confirm progress and aid debugging\n",
    "    if (i % D2_CHECKPOINT_EVERY) == 0:\n",
    "        d2_checkpoint_save(cand_rows)\n",
    "        pbar.set_postfix({\n",
    "            \"written_now\": written_now,\n",
    "            \"skipped_exist\": skipped_existing,\n",
    "            \"bad_reads\": bad_reads\n",
    "        })\n",
    "\n",
    "# Final checkpoint (latest snapshot)\n",
    "d2_checkpoint_save(cand_rows)\n",
    "\n",
    "cand_df = pd.DataFrame(cand_rows)\n",
    "warnings_df = pd.DataFrame(warn_rows)\n",
    "\n",
    "print(\"\\n=== Candidate stage summary ===\")\n",
    "print(\"Rows iterated:\", len(d2_proc))\n",
    "print(\"Candidates recorded in table:\", len(cand_df))\n",
    "print(\"Written now (this run):\", written_now)\n",
    "print(\"Skipped because candidate existed:\", skipped_existing)\n",
    "print(\"Bad reads:\", bad_reads)\n",
    "print(\"Checkpoint CSV (latest):\", D2_CAND_CHECKPOINT_CSV)\n",
    "\n",
    "if len(cand_df) == 0:\n",
    "    if len(warnings_df):\n",
    "        print(\"\\nSample warnings (first 10):\")\n",
    "        print(warnings_df.head(10).to_string(index=False))\n",
    "    raise RuntimeError(\"No D2 clip candidates produced. Check audio paths/VAD settings.\")\n",
    "\n",
    "# Keep only candidates that exist on Drive (required for downstream move)\n",
    "cand_df[\"cand_exists\"] = cand_df[\"clip_path_cand\"].astype(str).apply(os.path.exists)\n",
    "missing_cands = cand_df.loc[~cand_df[\"cand_exists\"]].copy()\n",
    "if len(missing_cands) > 0:\n",
    "    missing_cands_csv = os.path.join(D2_MANIFEST_DIR, \"missing_candidates.csv\")\n",
    "    missing_cands.to_csv(missing_cands_csv, index=False)\n",
    "    print(\"\\nMissing candidate files on Drive:\", len(missing_cands), \"saved:\", missing_cands_csv)\n",
    "cand_df = cand_df.loc[cand_df[\"cand_exists\"]].drop(columns=[\"cand_exists\"]).reset_index(drop=True)\n",
    "\n",
    "# -------------------------\n",
    "# Cap per speaker and task\n",
    "# Inputs: cand_df (already-written candidates)\n",
    "# Outputs: reduced cand_df; deletes unkept candidate WAV files\n",
    "# -------------------------\n",
    "def d2_cap_candidates(df: pd.DataFrame, max_k: int, seed: int) -> Tuple[pd.DataFrame, int]:\n",
    "    if df.empty:\n",
    "        return df.copy(), 0\n",
    "    rng = np.random.default_rng(seed)\n",
    "    keep_idx = []\n",
    "    for (spk, task), g in df.groupby([\"speaker_id\", \"task\"], sort=False):\n",
    "        idxs = g.index.to_numpy()\n",
    "        if len(idxs) <= max_k:\n",
    "            keep_idx.extend(idxs.tolist())\n",
    "        else:\n",
    "            chosen = rng.choice(idxs, size=max_k, replace=False)\n",
    "            keep_idx.extend(chosen.tolist())\n",
    "\n",
    "    keep_idx = sorted(set(keep_idx))\n",
    "    keep_set = set(keep_idx)\n",
    "\n",
    "    deleted = 0\n",
    "    unkept_paths = df.loc[~df.index.isin(list(keep_set)), \"clip_path_cand\"].tolist()\n",
    "    for p in unkept_paths:\n",
    "        try:\n",
    "            if os.path.exists(p):\n",
    "                os.remove(p)\n",
    "                deleted += 1\n",
    "        except Exception as e:\n",
    "            warnings_df_local = pd.DataFrame([{\n",
    "                \"dataset\":\"D2\",\"speaker_id\":\"\",\"source_path\":\"\",\n",
    "                \"warning_type\":\"candidate_delete_failed\",\"detail\":f\"{p} :: {repr(e)}\"\n",
    "            }])\n",
    "            nonlocal_warnings.append(warnings_df_local)\n",
    "\n",
    "    return df.loc[keep_idx].reset_index(drop=True), deleted\n",
    "\n",
    "nonlocal_warnings = []\n",
    "cand_df, deleted = d2_cap_candidates(cand_df, D2_MAX_CLIPS_PER_SPK_TASK, D2_RANDOM_SEED)\n",
    "if nonlocal_warnings:\n",
    "    warnings_df = pd.concat([warnings_df] + nonlocal_warnings, ignore_index=True)\n",
    "\n",
    "print(\"\\nD2 candidates after cap:\", int(len(cand_df)))\n",
    "print(\"Deleted unkept candidates:\", int(deleted))\n",
    "\n",
    "# -------------------------\n",
    "# Speaker-level split (stratified by label)\n",
    "# Inputs: capped candidates\n",
    "# Outputs: split assignment per speaker; merged into cand_df\n",
    "# -------------------------\n",
    "def d2_split_speakers(df: pd.DataFrame, fracs=(0.70, 0.15, 0.15), seed=1337) -> pd.DataFrame:\n",
    "    rng = np.random.default_rng(seed)\n",
    "    spk_df = df[[\"speaker_id\", \"label_num\"]].drop_duplicates().copy()\n",
    "\n",
    "    rows = []\n",
    "    for lbl in [0, 1]:\n",
    "        spks = spk_df[spk_df[\"label_num\"] == lbl][\"speaker_id\"].tolist()\n",
    "        rng.shuffle(spks)\n",
    "        n = len(spks)\n",
    "        n_train = int(round(fracs[0] * n))\n",
    "        n_val   = int(round(fracs[1] * n))\n",
    "\n",
    "        train_spks = spks[:n_train]\n",
    "        val_spks   = spks[n_train:n_train + n_val]\n",
    "        test_spks  = spks[n_train + n_val:]\n",
    "\n",
    "        rows += [{\"speaker_id\": s, \"split\": \"train\"} for s in train_spks]\n",
    "        rows += [{\"speaker_id\": s, \"split\": \"val\"} for s in val_spks]\n",
    "        rows += [{\"speaker_id\": s, \"split\": \"test\"} for s in test_spks]\n",
    "\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "spk_split = d2_split_speakers(cand_df, (D2_TRAIN_PCT, D2_VAL_PCT, D2_TEST_PCT), D2_RANDOM_SEED)\n",
    "cand_df = cand_df.merge(spk_split, on=\"speaker_id\", how=\"left\", validate=\"many_to_one\")\n",
    "\n",
    "if cand_df[\"split\"].isna().any():\n",
    "    ex = cand_df.loc[cand_df[\"split\"].isna(),\"speaker_id\"].drop_duplicates().head(10).tolist()\n",
    "    raise RuntimeError(f\"Some speakers did not get split. Example: {ex}\")\n",
    "\n",
    "print(\"\\nSplit counts (clips):\")\n",
    "print(cand_df[\"split\"].value_counts(dropna=False))\n",
    "print(\"Label by split (clips):\")\n",
    "print(pd.crosstab(cand_df[\"split\"], cand_df[\"label_str\"]))\n",
    "\n",
    "# -------------------------\n",
    "# Finalize: move clips into clips/<split>/ and build manifest\n",
    "# Inputs: kept candidates + split assignment\n",
    "# Outputs: final WAVs + manifest rows\n",
    "# -------------------------\n",
    "global_clip_idx = 0\n",
    "final_rows: List[Dict] = []\n",
    "\n",
    "pbar2 = tqdm(cand_df.itertuples(index=False), total=len(cand_df),\n",
    "             desc=\"D2 finalize (move kept)\", dynamic_ncols=True)\n",
    "\n",
    "for r in pbar2:\n",
    "    global_clip_idx += 1\n",
    "\n",
    "    tag = str(r.hc_pd_tag).strip().upper()\n",
    "    if tag not in {\"HC\",\"PD\"}:\n",
    "        tag = \"HC\" if str(r.label_str).lower().startswith(\"healthy\") else \"PD\"\n",
    "\n",
    "    task5 = str(r.task)[:5]\n",
    "    spk = str(r.speaker_id)\n",
    "\n",
    "    out_name = d2_safe(f\"D2_{tag}_{spk}_{task5}_{global_clip_idx:06d}.wav\")\n",
    "    out_path = os.path.join(D2_CLIPS_DIR, str(r.split), out_name)\n",
    "\n",
    "    cand_path = str(r.clip_path_cand)\n",
    "    if not os.path.exists(cand_path):\n",
    "        warnings_df = pd.concat([warnings_df, pd.DataFrame([{\n",
    "            \"dataset\":\"D2\",\"speaker_id\":spk,\"source_path\":str(r.source_path),\n",
    "            \"warning_type\":\"missing_candidate_file\",\"detail\":cand_path\n",
    "        }])], ignore_index=True)\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        os.makedirs(os.path.dirname(out_path), exist_ok=True)\n",
    "        shutil.move(cand_path, out_path)\n",
    "    except Exception as e:\n",
    "        warnings_df = pd.concat([warnings_df, pd.DataFrame([{\n",
    "            \"dataset\":\"D2\",\"speaker_id\":spk,\"source_path\":str(r.source_path),\n",
    "            \"warning_type\":\"candidate_move_failed\",\n",
    "            \"detail\": f\"{cand_path} -> {out_path} :: {repr(e)}\"\n",
    "        }])], ignore_index=True)\n",
    "        continue\n",
    "\n",
    "    final_rows.append({\n",
    "        \"split\": str(r.split),\n",
    "        \"dataset\": \"D2\",\n",
    "        \"task\": task5,\n",
    "        \"speaker_id\": spk,\n",
    "        \"sample_id\": str(r.sample_id),\n",
    "        \"label_str\": str(r.label_str),\n",
    "        \"label_num\": int(r.label_num),\n",
    "        \"age\": r.age if pd.notna(r.age) else np.nan,\n",
    "        \"sex\": r.sex if pd.notna(r.sex) else np.nan,\n",
    "        \"speaker_key_rel\": r.speaker_key_rel if pd.notna(r.speaker_key_rel) else np.nan,\n",
    "        \"clip_path\": out_path,\n",
    "        \"duration_sec\": float(r.duration_sec) if pd.notna(r.duration_sec) else np.nan,\n",
    "        \"source_path\": str(r.source_path),\n",
    "        \"clip_start_sec\": float(r.clip_start_sec) if pd.notna(r.clip_start_sec) else np.nan,\n",
    "        \"clip_end_sec\": float(r.clip_end_sec) if pd.notna(r.clip_end_sec) else np.nan,\n",
    "        \"sr_hz\": int(r.sr_hz),\n",
    "        \"channels\": 1,\n",
    "        \"clip_is_contiguous\": True,\n",
    "    })\n",
    "\n",
    "# Candidate folder is kept to support future resume behavior\n",
    "try:\n",
    "    if os.path.isdir(D2_CAND_DIR_DRIVE):\n",
    "        leftovers = list(Path(D2_CAND_DIR_DRIVE).glob(\"*.wav\"))\n",
    "        if len(leftovers) == 0:\n",
    "            pass\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "manifest_df = pd.DataFrame(final_rows)\n",
    "\n",
    "# Ensure schema and column order\n",
    "for c in MANIFEST_COLS:\n",
    "    if c not in manifest_df.columns:\n",
    "        manifest_df[c] = np.nan\n",
    "manifest_df = manifest_df[MANIFEST_COLS].copy()\n",
    "\n",
    "# -------------------------\n",
    "# Save outputs: manifests, logs, config\n",
    "# Inputs: manifest_df + warnings_df + run settings\n",
    "# Outputs: manifest_all.csv, preprocess_warnings.csv, dataset_summary.json, run_config.json\n",
    "# -------------------------\n",
    "manifest_all_path = os.path.join(D2_MANIFEST_DIR, \"manifest_all.csv\")\n",
    "warnings_path     = os.path.join(D2_LOGS_DIR, \"preprocess_warnings.csv\")\n",
    "summary_path      = os.path.join(D2_LOGS_DIR, \"dataset_summary.json\")\n",
    "run_cfg_path      = os.path.join(D2_CONFIG_DIR, \"run_config.json\")\n",
    "\n",
    "os.makedirs(os.path.dirname(manifest_all_path), exist_ok=True)\n",
    "os.makedirs(os.path.dirname(warnings_path), exist_ok=True)\n",
    "os.makedirs(os.path.dirname(summary_path), exist_ok=True)\n",
    "os.makedirs(os.path.dirname(run_cfg_path), exist_ok=True)\n",
    "\n",
    "manifest_df.to_csv(manifest_all_path, index=False)\n",
    "warnings_df.to_csv(warnings_path, index=False)\n",
    "\n",
    "summary = {\n",
    "    \"dataset\": \"D2\",\n",
    "    \"run_name\": D2_RUN_NAME,\n",
    "    \"source_root\": D2_EWA_DIR,\n",
    "    \"files_tsv\": D2_FILES_TSV,\n",
    "    \"speakers_tsv\": D2_SPEAKERS_TSV,\n",
    "    \"sr_hz\": int(D2_SR),\n",
    "    \"webrtcvad_available\": bool(D2_HAVE_WEBRTCVAD),\n",
    "    \"scipy_available\": bool(D2_HAVE_SCIPY),\n",
    "    \"one_clip_per_source_file\": True,\n",
    "    \"resume_safe_candidates\": True,\n",
    "    \"local_candidate_cache_enabled\": bool(D2_USE_LOCAL_CAND_CACHE),\n",
    "    \"paths\": {\n",
    "        \"run_root\": D2_RUN_ROOT,\n",
    "        \"candidate_dir_drive\": D2_CAND_DIR_DRIVE,\n",
    "        \"candidate_dir_local\": D2_CAND_DIR_LOCAL if D2_USE_LOCAL_CAND_CACHE else None,\n",
    "        \"manifest_all\": manifest_all_path,\n",
    "        \"warnings_csv\": warnings_path,\n",
    "        \"missing_paths_csv\": d2_missing_csv,\n",
    "        \"candidates_checkpoint_csv\": D2_CAND_CHECKPOINT_CSV,\n",
    "    },\n",
    "    \"filtering\": {\n",
    "        \"diagnosis_in\": [\"Healthy\",\"Parkinson\"],\n",
    "        \"inclusive_criteria_required\": True,\n",
    "        \"publish_agreement_required_if_column_exists\": (\"PUBLISH_AGREEMENT\" in files_df.columns),\n",
    "        \"audiofile_not_available_excluded\": True,\n",
    "        \"picture_mapping_rule\": {\"vokal\": \"vowel\", \"else\": \"reading\"},\n",
    "        \"existence_check_enabled\": bool(D2_DO_EXISTENCE_CHECK),\n",
    "        \"existence_check_unique_only\": bool(D2_EXIST_CHECK_UNIQUE_ONLY),\n",
    "    },\n",
    "    \"counts\": {\n",
    "        \"n_source_rows_after_filters\": int(len(d2_proc)),\n",
    "        \"n_missing_audio_rows\": int(len(missing_audio_rows)),\n",
    "        \"n_candidates_recorded\": int(len(cand_df)),\n",
    "        \"n_candidates_written_this_run\": int(written_now),\n",
    "        \"n_candidates_skipped_existing\": int(skipped_existing),\n",
    "        \"n_candidates_deleted_by_cap\": int(deleted),\n",
    "        \"n_clips_written\": int(len(manifest_df)),\n",
    "        \"label_counts_rows\": d2_proc[\"label_str\"].value_counts(dropna=False).to_dict(),\n",
    "        \"task_counts_rows\": d2_proc[\"task\"].value_counts(dropna=False).to_dict(),\n",
    "        \"label_counts_clips\": manifest_df[\"label_str\"].value_counts(dropna=False).to_dict() if len(manifest_df) else {},\n",
    "        \"split_counts_clips\": manifest_df[\"split\"].value_counts(dropna=False).to_dict() if len(manifest_df) else {},\n",
    "        \"task_counts_clips\": manifest_df[\"task\"].value_counts(dropna=False).to_dict() if len(manifest_df) else {},\n",
    "        \"n_warnings\": int(len(warnings_df)),\n",
    "    },\n",
    "    \"notes\": (\n",
    "        \"Exactly 1 candidate WAV is written per source file to clips/_candidates (resume-safe), then capped/split and moved to clips/<split>. \"\n",
    "        \"clip_start_sec/clip_end_sec are on concatenated voiced timeline (not source-timeline). \"\n",
    "        \"Vowel clips are 2.0s padded if short; reading clips are 8.0s if long enough else true duration without padding. \"\n",
    "        \"Reading clips apply conservative leading-silence trim and best-window selection to reduce initial silence.\"\n",
    "    ),\n",
    "}\n",
    "with open(summary_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "run_cfg = {\n",
    "    \"dataset\": \"D2\",\n",
    "    \"run_name\": D2_RUN_NAME,\n",
    "    \"paths\": {\n",
    "        \"dataset_dir\": D2_EWA_DIR,\n",
    "        \"files_tsv\": D2_FILES_TSV,\n",
    "        \"speakers_tsv\": D2_SPEAKERS_TSV,\n",
    "        \"run_root\": D2_RUN_ROOT,\n",
    "        \"clips_dir\": D2_CLIPS_DIR,\n",
    "        \"candidate_dir_drive\": D2_CAND_DIR_DRIVE,\n",
    "        \"candidate_dir_local\": D2_CAND_DIR_LOCAL if D2_USE_LOCAL_CAND_CACHE else None,\n",
    "        \"manifest_all\": manifest_all_path,\n",
    "        \"warnings_csv\": warnings_path,\n",
    "        \"missing_paths_csv\": d2_missing_csv,\n",
    "        \"candidates_checkpoint_csv\": D2_CAND_CHECKPOINT_CSV,\n",
    "        \"summary_json\": summary_path,\n",
    "    },\n",
    "    \"standard_structure\": {\n",
    "        \"clips\": \"clips/<split>/ (flat), with temporary clips/_candidates during run\",\n",
    "        \"manifests\": \"manifests/manifest_all.csv\",\n",
    "        \"config\": \"config/run_config.json\",\n",
    "        \"logs\": [\"logs/preprocess_warnings.csv\", \"logs/dataset_summary.json\", \"logs/candidates_checkpoint.csv\"],\n",
    "    },\n",
    "    \"manifest_schema_order\": MANIFEST_COLS,\n",
    "    \"filename_format\": \"D2_{HC|PD}_{speaker_id}_{task<=5}_{global_index:06d}.wav\",\n",
    "    \"task_mapping\": {\"PICTURE == 'vokal'\": \"vowl\", \"else\": \"read\"},\n",
    "    \"cap_policy\": {\"groupby\": [\"speaker_id\", \"task\"], \"max_per_group\": int(D2_MAX_CLIPS_PER_SPK_TASK)},\n",
    "    \"normalization\": {\n",
    "        \"method\": \"manual RMS gain + peak limiting\",\n",
    "        \"target_rms_dbfs\": float(D2_TARGET_RMS_DBFS),\n",
    "        \"peak_limit_dbfs\": float(D2_PEAK_LIMIT_DBFS),\n",
    "        \"min_rms_dbfs\": float(D2_MIN_RMS_DBFS),\n",
    "        \"max_gain_db\": float(D2_MAX_GAIN_DB),\n",
    "        \"pyloudnorm\": False,\n",
    "    },\n",
    "    \"vad\": {\n",
    "        \"method\": \"webrtcvad if available at 16 kHz else energy fallback; pad ±0.25 s\",\n",
    "        \"mode\": int(D2_VAD_MODE),\n",
    "        \"frame_ms\": int(D2_FRAME_MS),\n",
    "        \"min_speech_ms\": int(D2_MIN_SPEECH_MS),\n",
    "        \"merge_gap_ms\": int(D2_MERGE_GAP_MS),\n",
    "        \"pad_sec\": float(D2_PAD_SEC),\n",
    "        \"min_keep_sec\": float(D2_MIN_KEEP_SEC),\n",
    "    },\n",
    "    \"resume\": {\n",
    "        \"resume_safe_candidates\": True,\n",
    "        \"deterministic_candidate_naming\": True,\n",
    "        \"skip_if_candidate_exists\": True,\n",
    "        \"checkpoint_every_rows\": int(D2_CHECKPOINT_EVERY),\n",
    "        \"local_candidate_cache\": bool(D2_USE_LOCAL_CAND_CACHE),\n",
    "        \"reset_run\": bool(D2_RESET_RUN),\n",
    "    },\n",
    "    \"clip_rules\": {\n",
    "        \"vowel\": {\"sec\": float(D2_VOWEL_SEC), \"pad_if_short\": True, \"selection\": \"center-crop if longer\"},\n",
    "        \"reading\": {\"sec\": float(D2_READ_SEC), \"pad_if_short\": False, \"selection\": \"best-of-multiple windows if longer else keep full\",\n",
    "                    \"leading_silence_trim\": {\"enabled\": True, \"max_trim_sec\": 1.5, \"thr_dbfs\": -45.0}},\n",
    "    },\n",
    "    \"seed\": int(D2_RANDOM_SEED),\n",
    "}\n",
    "with open(run_cfg_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(run_cfg, f, indent=2)\n",
    "\n",
    "print(\"\\nDONE: D2 preprocessing\")\n",
    "print(\"Run root:\", D2_RUN_ROOT)\n",
    "print(\"Manifest:\", manifest_all_path)\n",
    "print(\"Warnings:\", warnings_path)\n",
    "print(\"Summary:\", summary_path)\n",
    "print(\"Run config:\", run_cfg_path)\n",
    "print(\"Clips written:\", int(len(manifest_df)))\n",
    "print(\"Missing-audio rows (not processed):\", int(len(missing_audio_rows)))\n",
    "\n",
    "print(\"\\nSanity checks:\")\n",
    "print(\"Unique SR:\", sorted(manifest_df[\"sr_hz\"].dropna().unique().tolist()) if len(manifest_df) else [])\n",
    "print(\"Unique channels:\", sorted(manifest_df[\"channels\"].dropna().unique().tolist()) if len(manifest_df) else [])\n",
    "\n",
    "# Speaker split uniqueness check\n",
    "spk_split_chk = manifest_df[[\"speaker_id\", \"split\"]].drop_duplicates()\n",
    "dup = spk_split_chk.groupby(\"speaker_id\")[\"split\"].nunique()\n",
    "bad = dup[dup > 1]\n",
    "print(\"Speakers appearing in multiple splits:\", int(len(bad)))\n",
    "if len(bad) > 0:\n",
    "    print(\"Example bad speaker ids:\", bad.index.tolist()[:10])\n",
    "    raise RuntimeError(\"Speaker appears in more than one split.\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "d0750bfc781a41f992de3a68b81226f3",
      "c5378996bb724eeeb668f04d07c06cc8",
      "f0d93d99e79846468967891643e42a0c",
      "007a1af3f0c7439fa5a2a786ab6ac36f",
      "26819cbddb74406ea8d76023beab0968",
      "84ff1b062d7048ec9e001a643962084b",
      "a5c1933fd33d4b9bb898f817813a76fb",
      "783ad4c99a764c67b2766b509cf8cdc2",
      "fee564a431b445fdabbfa693ca964961",
      "20a5ddac01f1451ea4b4d7a12b34698e",
      "66b35e2376b345bdadb9913af380b78e",
      "e1227454339f465dbf212d5ce732b42c",
      "e4bab049401b4ece8c40e4c080a8e58d",
      "6bbe2fc7ea524066bcf2bf7d96a15c21",
      "0850b08340db47129a8b27d3b4184396",
      "c43dde9c4e8a49f186d3440b59bee3a4",
      "aa587ee55b90477fb8547ab3e055baf2",
      "4048310a519c4152ae4372cca6a16825",
      "6ec25dd736fc4a4eb68ae941b446a0b1",
      "8bed5490081e4f9faf610c82aa160a60",
      "212ea975623147b89dc01a1771f36d8f",
      "8fa827e18fef4cf2ae9fcac856cb9b42",
      "18ad811d22b24917b202b37555b1fe36",
      "2aa59a3cc8bf4f67a1421d6d13702f6a",
      "0465f3265fb2427fa90c282c32603476",
      "3559b2c45d224dd28d917b8e07fa23c3",
      "962557bb18dd4a87b96023c45e7e2a51",
      "9b2619e793094dab8d93bd19f9d3009e",
      "99d64590baac4c998e479a1eb36ef7da",
      "fcd6bf8179324ff78ffc86e6e7540c7c",
      "90bfb29854ec48bb80c8c8e60e9715c5",
      "6e89c8c568c444b7842a3bb47aca3593",
      "4f3930d3daaf471ca4c1274a0e974ed4"
     ]
    },
    "id": "1BrLpnUIKUa4"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "The following cell copies the full contents of the D2 preprocessing run folder (`runs/run_v1/`) into the main D2 `preprocessed_v1/` directory using `rsync`, a reliable file copy tool. The entire folder structure and all files (including `clips/`, `manifests/`, `config/`, and `logs/`) are kept intact, and the command prints each file as it is copied. The `-a` option copies directories recursively while preserving timestamps and permissions, and the `-v` option enables detailed output. This step is typically run after preprocessing finishes in a temporary run folder and moves the finalized outputs into the standard `preprocessed_v1` location for use in training and evaluation."
   ],
   "metadata": {
    "id": "2l9ihUnT4Qf0"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!rsync -av \\\n",
    "  \"/content/drive/MyDrive/AI_PD_Project/Datasets/D2-Slovak (EWA-DB)/EWA-DB/preprocessed_v1/runs/run_v1/\" \\\n",
    "  \"/content/drive/MyDrive/AI_PD_Project/Datasets/D2-Slovak (EWA-DB)/EWA-DB/preprocessed_v1/\""
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RAhh7bNFywHV"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "The following cell preprocesses Dataset D4 (IPVS Italian) into the project’s standard `preprocessed_v1` format so it matches the structure used for the other datasets. It mounts Google Drive, installs required audio tools if they are missing (WebRTC VAD and SciPy), defines the input folders for young healthy, elderly healthy, and Parkinson speakers, and creates the output folders for clips, manifests, logs, and configuration files. At the start, it clears the temporary `_candidates` folder to avoid mixing files from earlier runs and performs basic checks to confirm that all dataset folders and dependencies are available.\n",
    "\n",
    "The cell then builds a clean list of usable source WAV files. Each filename is parsed using the IPVS naming convention to extract the task code, repetition number, and basic metadata such as sex and recording date. Only files that match known vowel or reading tasks are kept. Speakers are identified using the original folder structure (`speaker_key_rel`) and converted into a stable, unique `speaker_id` by hashing this value, which avoids conflicts when folder names are similar. This results in a table where each row represents one source file with its speaker, task type, and label (Healthy or Parkinson).\n",
    "\n",
    "For each indexed source file, the cell cleans the audio and selects **at most one clip per source file**. Audio is loaded as mono and resampled to 16 kHz, then cleaned using DC offset removal and optional hum reduction with high pass and notch filters at 50 Hz and its harmonics. Loudness is normalized using an RMS based method with gain and peak limiting, without using pyloudnorm. Speech regions are detected using voice activity detection, and the single best region is chosen using a clear rule: **the longest voiced segment is selected**, with the earliest segment chosen if there is a tie. From this region, the final clip is created as follows: for vowel tasks, a 2 second clip is taken from the center of the segment and padded if needed; for reading tasks, up to the first 8 seconds are used, or the full duration if it is shorter but not extremely brief. Optional silence trimming is applied to reduce leading and trailing quiet sections. Each selected clip is immediately written as a temporary WAV file in `clips/_candidates/`, and a record is stored with timing information (`clip_start_sec`, `clip_end_sec`), duration, and metadata.\n",
    "\n",
    "Once all candidate clips are created, the cell applies a balance limit by keeping no more than 8 clips per `(speaker_id, task)` group. Any extra candidate WAV files are deleted at this stage. The remaining clips are then split into training, validation, and test sets at the **speaker level** (70/15/15), while maintaining a similar balance between Healthy and Parkinson speakers. The kept clips are moved into the final flat folder structure `clips/<split>/` using standardized filenames, and a single `manifest_all.csv` is written with a fixed column order used across all datasets. The cell also creates split specific manifest files, a warnings log for skipped or problematic files, a dataset summary JSON with counts and key settings, and a run configuration JSON that records all preprocessing paths and rules. Final checks confirm that every clip listed in the manifests exists on disk and that no speaker appears in more than one split."
   ],
   "metadata": {
    "id": "I07emi6WtTDR"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# ============================================================\n",
    "# D4 (IPVS) Preprocessing v1 — One Clip Per Source File\n",
    "# Inputs: raw IPVS WAV files across YHC, HEC, PD folders\n",
    "# Outputs: clips/<split>/ WAVs (flat), manifests/manifest_all.csv (+ per-split),\n",
    "#          config/run_config.json, logs/preprocess_warnings.csv, logs/dataset_summary.json\n",
    "# Notes: Writes temporary candidates to clips/_candidates/ during processing, then moves kept clips.\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import math\n",
    "import random\n",
    "import shutil\n",
    "import hashlib\n",
    "from typing import List, Tuple, Dict, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import soundfile as sf\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# -------------------------\n",
    "# Drive mount check\n",
    "# Purpose: ensure dataset and output folders are reachable in Colab\n",
    "# -------------------------\n",
    "if not os.path.isdir(\"/content/drive/MyDrive\"):\n",
    "    from google.colab import drive  # type: ignore\n",
    "    drive.mount(\"/content/drive\")\n",
    "\n",
    "# -------------------------\n",
    "# Optional dependencies\n",
    "# Purpose: enable VAD and signal processing (install if missing)\n",
    "# Inputs: none\n",
    "# Outputs: flags showing which optional libraries are available\n",
    "# -------------------------\n",
    "def _d4_try_import_webrtcvad():\n",
    "    try:\n",
    "        import webrtcvad  # type: ignore\n",
    "        return webrtcvad, True\n",
    "    except Exception:\n",
    "        return None, False\n",
    "\n",
    "webrtcvad, HAVE_WEBRTCVAD = _d4_try_import_webrtcvad()\n",
    "if not HAVE_WEBRTCVAD:\n",
    "    !pip -q install webrtcvad\n",
    "    webrtcvad, HAVE_WEBRTCVAD = _d4_try_import_webrtcvad()\n",
    "\n",
    "try:\n",
    "    from scipy import signal  # type: ignore\n",
    "    HAVE_SCIPY = True\n",
    "except Exception:\n",
    "    HAVE_SCIPY = False\n",
    "    !pip -q install scipy\n",
    "    from scipy import signal  # type: ignore\n",
    "    HAVE_SCIPY = True\n",
    "\n",
    "# -------------------------\n",
    "# Paths and standardized outputs\n",
    "# Inputs: dataset root folder\n",
    "# Outputs: preprocessed_v1 folder tree (clips/manifests/config/logs)\n",
    "# -------------------------\n",
    "D4_PROJECT_DIR = \"/content/drive/MyDrive/AI_PD_Project\"\n",
    "D4_DATASETS_DIR = f\"{D4_PROJECT_DIR}/Datasets\"\n",
    "D4_IPVS_DIR = f\"{D4_DATASETS_DIR}/D4-Italian (IPVS)\"\n",
    "\n",
    "D4_GROUP_DIRS = {\n",
    "    \"YHC\": os.path.join(D4_IPVS_DIR, \"15 Young Healthy Control\"),\n",
    "    \"HEC\": os.path.join(D4_IPVS_DIR, \"22 Elderly Healthy Control\"),\n",
    "    \"PD\":  os.path.join(D4_IPVS_DIR, \"28 People with Parkinson's disease\"),\n",
    "}\n",
    "\n",
    "D4_OUT_ROOT = os.path.join(D4_IPVS_DIR, \"preprocessed_v1\")\n",
    "D4_CLIPS_DIR = os.path.join(D4_OUT_ROOT, \"clips\")\n",
    "D4_CAND_DIR  = os.path.join(D4_CLIPS_DIR, \"_candidates\")   # temporary staging during this run\n",
    "D4_MANIFESTS_DIR = os.path.join(D4_OUT_ROOT, \"manifests\")\n",
    "D4_CONFIG_DIR = os.path.join(D4_OUT_ROOT, \"config\")\n",
    "D4_LOGS_DIR = os.path.join(D4_OUT_ROOT, \"logs\")\n",
    "\n",
    "for p in [D4_OUT_ROOT, D4_CLIPS_DIR, D4_CAND_DIR, D4_MANIFESTS_DIR, D4_CONFIG_DIR, D4_LOGS_DIR]:\n",
    "    os.makedirs(p, exist_ok=True)\n",
    "for sp in [\"train\", \"val\", \"test\"]:\n",
    "    os.makedirs(os.path.join(D4_CLIPS_DIR, sp), exist_ok=True)\n",
    "\n",
    "# Purpose: avoid mixing candidate files from earlier runs\n",
    "if os.path.isdir(D4_CAND_DIR):\n",
    "    try:\n",
    "        shutil.rmtree(D4_CAND_DIR)\n",
    "    except Exception:\n",
    "        pass\n",
    "os.makedirs(D4_CAND_DIR, exist_ok=True)\n",
    "\n",
    "# -------------------------\n",
    "# Fail-fast checks\n",
    "# Purpose: confirm input folders and optional tools are available\n",
    "# -------------------------\n",
    "print(\"D4_IPVS_DIR exists?\", os.path.isdir(D4_IPVS_DIR))\n",
    "for grp, pth in D4_GROUP_DIRS.items():\n",
    "    print(f\"{grp} folder exists? {os.path.isdir(pth)} -> {pth}\")\n",
    "print(\"webrtcvad available?\", HAVE_WEBRTCVAD)\n",
    "print(\"scipy available?\", HAVE_SCIPY)\n",
    "\n",
    "if not os.path.isdir(D4_IPVS_DIR):\n",
    "    raise FileNotFoundError(f\"D4_IPVS_DIR not found: {D4_IPVS_DIR}\")\n",
    "\n",
    "# -------------------------\n",
    "# Preprocessing constants\n",
    "# Purpose: unify audio format and enforce the one-clip-per-source rule\n",
    "# -------------------------\n",
    "D4_TARGET_SR = 16000\n",
    "\n",
    "# Purpose: stable RMS-based leveling + peak limiting (no external loudness library)\n",
    "D4_TARGET_LUFS_LIKE = -23.0   # implemented as RMS-based approximation\n",
    "D4_PEAK_CEIL = 0.95\n",
    "D4_MAX_GAIN_DB = 18.0\n",
    "\n",
    "# Purpose: VAD settings for voiced-region detection\n",
    "D4_VAD_AGGRESSIVENESS = 2\n",
    "D4_VAD_FRAME_MS = 30\n",
    "D4_VAD_PAD_SEC = 0.25\n",
    "D4_POST_VAD_TRAIL_PAD_SEC = 0.15\n",
    "D4_MIN_KEEP_SEC = 0.30\n",
    "\n",
    "# Purpose: task-dependent clip length\n",
    "D4_VOWEL_SEC = 2.0\n",
    "D4_READ_SEC = 8.0\n",
    "D4_OPTION_A = True  # reading < 8s: keep true duration (no padding)\n",
    "\n",
    "# Purpose: cap clips per (speaker, task) after candidates are created\n",
    "D4_MAX_CLIPS_PER_SPK_PER_TASK = 8\n",
    "\n",
    "# Purpose: speaker-level stratified split (label-balanced)\n",
    "D4_RANDOM_SEED = 1337\n",
    "D4_SPLIT_FRACS = (0.70, 0.15, 0.15)  # train/val/test\n",
    "\n",
    "random.seed(D4_RANDOM_SEED)\n",
    "np.random.seed(D4_RANDOM_SEED)\n",
    "\n",
    "# Purpose: keep D4-specific hum reduction behavior\n",
    "D4_HUM_REMOVE = True\n",
    "D4_HUM_BASE_HZ = 50.0\n",
    "D4_HUM_N_HARMONICS = 6\n",
    "D4_HUM_Q = 35.0\n",
    "D4_HP_CUTOFF_HZ = 70.0\n",
    "\n",
    "# Purpose: optional trimming steps used inside segments/clips\n",
    "D4_TRIM_SEGMENTS = True\n",
    "D4_TRIM_CLIPS = True\n",
    "\n",
    "# -------------------------\n",
    "# Small helpers: safe names, labels, and task tags\n",
    "# -------------------------\n",
    "def d4_safe(s: str) -> str:\n",
    "    # Purpose: make filenames filesystem-safe\n",
    "    return re.sub(r\"[^A-Za-z0-9_\\-\\.]+\", \"_\", str(s))\n",
    "\n",
    "def d4_pd_hc(label_str: str) -> str:\n",
    "    # Purpose: short label tag for filenames\n",
    "    return \"PD\" if str(label_str).lower().startswith(\"parkinson\") else \"HC\"\n",
    "\n",
    "def d4_task_short(task_type: str) -> str:\n",
    "    # Purpose: map task types into short tokens used in filenames/manifest\n",
    "    tt = (task_type or \"\").lower()\n",
    "    if tt == \"vowel\":\n",
    "        return \"vowl\"\n",
    "    if tt == \"reading\":\n",
    "        return \"read\"\n",
    "    if tt == \"spontaneous\":\n",
    "        return \"spont\"\n",
    "    return \"unk\"\n",
    "\n",
    "# -------------------------\n",
    "# IPVS filename parsing + task mapping\n",
    "# Inputs: filename stem tokens\n",
    "# Outputs: parsed fields (task, rep, subject token, sex, recording date token)\n",
    "# -------------------------\n",
    "def d4_normalize_stem(stem: str) -> str:\n",
    "    # Purpose: remove spaces and trailing dots for consistent parsing\n",
    "    return re.sub(r\"\\s+\", \"\", str(stem)).rstrip(\".\")\n",
    "\n",
    "d4_ipvs_pattern = re.compile(\n",
    "    r\"\"\"\n",
    "    ^(?P<task>[A-Za-z]{1,3})\n",
    "    (?P<rep>\\d+)\n",
    "    (?P<subject>.+?)\n",
    "    (?P<birth_year>\\d{2})\n",
    "    (?P<sex>[MF])\n",
    "    (?P<recdate>\\d{10}|\\d{12})$\n",
    "    \"\"\",\n",
    "    re.VERBOSE\n",
    ")\n",
    "\n",
    "def d4_parse_ipvs_basename(basename_no_ext: str) -> Optional[Dict[str, str]]:\n",
    "    # Purpose: parse metadata encoded in the filename\n",
    "    s = d4_normalize_stem(basename_no_ext)\n",
    "    m = d4_ipvs_pattern.match(s)\n",
    "    if not m:\n",
    "        return None\n",
    "    d = m.groupdict()\n",
    "    d[\"task\"] = str(d[\"task\"]).upper()\n",
    "    d[\"subject\"] = str(d[\"subject\"]).upper()\n",
    "    d[\"recdate_fmt\"] = \"DDMMYYYYHHMM\" if len(d[\"recdate\"]) == 12 else \"DDMMYYHHMM\"\n",
    "    return d\n",
    "\n",
    "D4_VOWEL_TASKS = {\"VA\", \"VE\", \"VI\", \"VO\", \"VU\"}\n",
    "D4_READ_TASKS  = {\"PR\", \"FB\", \"D\", \"B\"}\n",
    "\n",
    "def d4_task_type(task: str) -> Optional[str]:\n",
    "    # Purpose: collapse many IPVS tasks into the two modeled groups\n",
    "    t = str(task).upper()\n",
    "    if t in D4_VOWEL_TASKS:\n",
    "        return \"vowel\"\n",
    "    if t in D4_READ_TASKS:\n",
    "        return \"reading\"\n",
    "    return None\n",
    "\n",
    "# -------------------------\n",
    "# File indexing and speaker_id creation (keep dataset-specific logic)\n",
    "# Inputs: group folders with nested subfolders\n",
    "# Outputs: speaker_key_rel and stable speaker_id for splitting and tracking\n",
    "# -------------------------\n",
    "def d4_walk_wavs_only_raw(group_root: str) -> List[str]:\n",
    "    # Purpose: list raw WAVs while skipping any generated folders\n",
    "    wavs = []\n",
    "    for root, dirs, files in os.walk(group_root):\n",
    "        dirs[:] = [d for d in dirs if d not in {\"preprocessed_v1\", \".ipynb_checkpoints\"}]\n",
    "        for f in files:\n",
    "            if f.lower().endswith(\".wav\"):\n",
    "                wavs.append(os.path.join(root, f))\n",
    "    return wavs\n",
    "\n",
    "def d4_speaker_key_rel(group_root: str, wav_path: str, group: str) -> str:\n",
    "    # Purpose: build a relative speaker key based on folder structure\n",
    "    rel_dir = os.path.relpath(os.path.dirname(wav_path), group_root)\n",
    "    parts = [p for p in rel_dir.split(os.sep) if p not in {\"\", \".\"}]\n",
    "\n",
    "    if group in {\"YHC\", \"HEC\"}:\n",
    "        if len(parts) >= 1:\n",
    "            return f\"{group}/{parts[0]}\"\n",
    "        return f\"{group}/{os.path.basename(os.path.dirname(wav_path))}\"\n",
    "\n",
    "    if len(parts) >= 2:\n",
    "        return f\"{group}/{parts[0]}/{parts[1]}\"\n",
    "    if len(parts) == 1:\n",
    "        return f\"{group}/{parts[0]}\"\n",
    "    return f\"{group}/{os.path.basename(os.path.dirname(wav_path))}\"\n",
    "\n",
    "def d4_speaker_name_from_key(speaker_key_rel: str) -> str:\n",
    "    return speaker_key_rel.split(\"/\")[-1]\n",
    "\n",
    "def d4_speaker_id_from_key(speaker_key_rel: str) -> str:\n",
    "    # Purpose: stable ID that does not depend on absolute paths\n",
    "    h = hashlib.md5(speaker_key_rel.encode(\"utf-8\")).hexdigest()[:8]\n",
    "    speaker_name = d4_speaker_name_from_key(speaker_key_rel)\n",
    "    group = speaker_key_rel.split(\"/\")[0]\n",
    "    return f\"D4_{group}__{speaker_name}__{h}\"\n",
    "\n",
    "def d4_build_index() -> Tuple[pd.DataFrame, List[str], int]:\n",
    "    # Inputs: group roots\n",
    "    # Outputs: table of usable source files (one row per source WAV)\n",
    "    wav_files = []\n",
    "    wav_files_with_roots = []\n",
    "\n",
    "    for grp, grp_root in D4_GROUP_DIRS.items():\n",
    "        if not os.path.isdir(grp_root):\n",
    "            print(\"WARNING missing group folder:\", grp_root)\n",
    "            continue\n",
    "        grp_wavs = d4_walk_wavs_only_raw(grp_root)\n",
    "        wav_files.extend(grp_wavs)\n",
    "        wav_files_with_roots.extend([(w, grp, grp_root) for w in grp_wavs])\n",
    "\n",
    "    rows = []\n",
    "    unmatched = []\n",
    "\n",
    "    for wav_path, grp, grp_root in wav_files_with_roots:\n",
    "        base = os.path.basename(wav_path)\n",
    "        stem = d4_normalize_stem(base[:-4])\n",
    "\n",
    "        parsed = d4_parse_ipvs_basename(stem)\n",
    "        if parsed is None:\n",
    "            unmatched.append(stem)\n",
    "            continue\n",
    "\n",
    "        task = parsed[\"task\"]\n",
    "        ttype = d4_task_type(task)\n",
    "        if ttype is None:\n",
    "            continue\n",
    "\n",
    "        speaker_key = d4_speaker_key_rel(grp_root, wav_path, grp)\n",
    "        speaker_id = d4_speaker_id_from_key(speaker_key)\n",
    "\n",
    "        label_str = \"Parkinson\" if grp == \"PD\" else \"Healthy\"\n",
    "        label_num = 1 if grp == \"PD\" else 0\n",
    "\n",
    "        rows.append({\n",
    "            \"audio_path\": wav_path,\n",
    "            \"group\": grp,\n",
    "            \"speaker_key_rel\": speaker_key,\n",
    "            \"speaker_id\": speaker_id,\n",
    "            \"task\": task,\n",
    "            \"rep\": int(parsed[\"rep\"]),\n",
    "            \"subject_token\": parsed[\"subject\"],\n",
    "            \"birth_year_2d\": parsed[\"birth_year\"],\n",
    "            \"sex\": parsed[\"sex\"],\n",
    "            \"recdate_raw\": parsed[\"recdate\"],\n",
    "            \"recdate_fmt\": parsed[\"recdate_fmt\"],\n",
    "            \"task_type\": ttype,\n",
    "            \"label_str\": label_str,\n",
    "            \"label_num\": label_num,\n",
    "            \"dataset\": \"D4\",\n",
    "        })\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    total_wavs = len(wav_files)\n",
    "\n",
    "    print(\"\\nD4 total wavs found:\", total_wavs)\n",
    "    print(\"D4 matched+used (after task filter):\", len(df), \"unmatched stems:\", len(unmatched))\n",
    "    if unmatched:\n",
    "        print(\"First 15 unmatched stems:\", unmatched[:15])\n",
    "\n",
    "    return df, unmatched, total_wavs\n",
    "\n",
    "# -------------------------\n",
    "# Audio utilities (manual leveling, trimming, hum reduction, resampling)\n",
    "# Purpose: produce clean mono 16 kHz audio for VAD and clip extraction\n",
    "# -------------------------\n",
    "def d4_peak_limit(x: np.ndarray, peak_ceiling: float = D4_PEAK_CEIL) -> np.ndarray:\n",
    "    # Purpose: ensure headroom to avoid clipping\n",
    "    if x is None or len(x) == 0:\n",
    "        return x\n",
    "    peak = float(np.max(np.abs(x)))\n",
    "    if peak <= 0:\n",
    "        return x.astype(np.float32)\n",
    "    if peak > peak_ceiling:\n",
    "        x = x * (peak_ceiling / peak)\n",
    "    return x.astype(np.float32)\n",
    "\n",
    "def d4_integrated_rms_db(x: np.ndarray) -> Optional[float]:\n",
    "    # Purpose: RMS in dB for gain calculation\n",
    "    if x is None or len(x) == 0:\n",
    "        return None\n",
    "    rms = float(np.sqrt(np.mean(x.astype(np.float64) ** 2)))\n",
    "    if rms <= 1e-12:\n",
    "        return None\n",
    "    return float(20.0 * np.log10(rms))\n",
    "\n",
    "def d4_remove_dc(x: np.ndarray) -> np.ndarray:\n",
    "    # Purpose: remove constant offset that can confuse filters and VAD\n",
    "    if x is None or len(x) == 0:\n",
    "        return x.astype(np.float32)\n",
    "    y = x.astype(np.float32)\n",
    "    y = y - float(np.mean(y))\n",
    "    return y.astype(np.float32)\n",
    "\n",
    "def d4_soft_clip(x: np.ndarray) -> np.ndarray:\n",
    "    # Purpose: gentle limiting when a large gain is applied\n",
    "    if x is None or len(x) == 0:\n",
    "        return x.astype(np.float32)\n",
    "    y = np.tanh(1.25 * x.astype(np.float32)) / np.tanh(1.25)\n",
    "    return y.astype(np.float32)\n",
    "\n",
    "def d4_apply_level_target_manual(x: np.ndarray, sr: int) -> Tuple[np.ndarray, Dict[str, float]]:\n",
    "    # Inputs: float32 audio\n",
    "    # Outputs: leveled audio + small stats for debugging/logging\n",
    "    info = {\"lvl_in_db\": float(\"nan\"), \"lvl_out_db\": float(\"nan\"), \"gain_db\": 0.0, \"peak_in\": 0.0, \"peak_out\": 0.0}\n",
    "    if x is None or len(x) == 0:\n",
    "        return x.astype(np.float32), info\n",
    "\n",
    "    info[\"peak_in\"] = float(np.max(np.abs(x)))\n",
    "    in_db = d4_integrated_rms_db(x)\n",
    "    if in_db is None:\n",
    "        y = d4_peak_limit(x, D4_PEAK_CEIL)\n",
    "        info[\"peak_out\"] = float(np.max(np.abs(y))) if len(y) else 0.0\n",
    "        return y.astype(np.float32), info\n",
    "\n",
    "    info[\"lvl_in_db\"] = float(in_db)\n",
    "\n",
    "    # Purpose: RMS-like target (shifted from the LUFS-like constant)\n",
    "    target_rms_db = float(D4_TARGET_LUFS_LIKE + 7.0)\n",
    "    gain_db = float(target_rms_db - in_db)\n",
    "    gain_db = float(np.clip(gain_db, -60.0, D4_MAX_GAIN_DB))\n",
    "    info[\"gain_db\"] = float(gain_db)\n",
    "\n",
    "    gain = float(10.0 ** (gain_db / 20.0))\n",
    "    y = (x.astype(np.float32) * gain).astype(np.float32)\n",
    "\n",
    "    if gain_db > 12.0:\n",
    "        y = d4_soft_clip(y)\n",
    "\n",
    "    y = d4_peak_limit(y, D4_PEAK_CEIL)\n",
    "    y = np.clip(y, -1.0, 1.0).astype(np.float32)\n",
    "\n",
    "    info[\"peak_out\"] = float(np.max(np.abs(y))) if len(y) else 0.0\n",
    "    out_db = d4_integrated_rms_db(y)\n",
    "    if out_db is not None:\n",
    "        info[\"lvl_out_db\"] = float(out_db)\n",
    "\n",
    "    return y.astype(np.float32), info\n",
    "\n",
    "def d4_trim_silence_energy(\n",
    "    x: np.ndarray,\n",
    "    sr: int,\n",
    "    frame_ms: int = 20,\n",
    "    hop_ms: int = 10,\n",
    "    db_margin: float = 25.0,\n",
    "    min_keep_ms: int = 150\n",
    ") -> np.ndarray:\n",
    "    # Purpose: drop leading/trailing low-energy regions while keeping a minimum duration\n",
    "    if x is None or len(x) == 0:\n",
    "        return x.astype(np.float32)\n",
    "\n",
    "    frame = int(sr * frame_ms / 1000.0)\n",
    "    hop = int(sr * hop_ms / 1000.0)\n",
    "    if frame <= 0 or hop <= 0 or len(x) < frame:\n",
    "        return x.astype(np.float32)\n",
    "\n",
    "    rms_db = []\n",
    "    idx = 0\n",
    "    while idx + frame <= len(x):\n",
    "        w = x[idx:idx+frame]\n",
    "        rms = float(np.sqrt(np.mean(w.astype(np.float64) ** 2)))\n",
    "        db = -120.0 if rms <= 1e-12 else (20.0 * np.log10(rms))\n",
    "        rms_db.append(db)\n",
    "        idx += hop\n",
    "\n",
    "    rms_db = np.array(rms_db, dtype=np.float32)\n",
    "    if len(rms_db) == 0:\n",
    "        return x.astype(np.float32)\n",
    "\n",
    "    mx = float(np.max(rms_db))\n",
    "    thr = mx - float(db_margin)\n",
    "    keep = np.where(rms_db >= thr)[0]\n",
    "    if len(keep) == 0:\n",
    "        return x.astype(np.float32)\n",
    "\n",
    "    start_f = int(keep[0])\n",
    "    end_f = int(keep[-1])\n",
    "\n",
    "    start = max(0, start_f * hop)\n",
    "    end = min(len(x), end_f * hop + frame)\n",
    "\n",
    "    min_keep = int(sr * (min_keep_ms / 1000.0))\n",
    "    if end - start < min_keep:\n",
    "        mid = (start + end) // 2\n",
    "        start = max(0, mid - min_keep // 2)\n",
    "        end = min(len(x), start + min_keep)\n",
    "\n",
    "    return x[start:end].astype(np.float32)\n",
    "\n",
    "def d4_force_length(x: np.ndarray, n: int) -> np.ndarray:\n",
    "    # Purpose: pad or trim to an exact length (used for vowel clips)\n",
    "    if x is None:\n",
    "        return np.zeros((n,), dtype=np.float32)\n",
    "    if len(x) == n:\n",
    "        return x.astype(np.float32)\n",
    "    if len(x) > n:\n",
    "        return x[:n].astype(np.float32)\n",
    "    y = np.zeros((n,), dtype=np.float32)\n",
    "    y[:len(x)] = x.astype(np.float32)\n",
    "    return y\n",
    "\n",
    "def d4_float_to_pcm16(x: np.ndarray) -> bytes:\n",
    "    # Purpose: VAD requires 16-bit PCM bytes\n",
    "    x = np.clip(x, -1.0, 1.0)\n",
    "    return (x * 32767.0).astype(np.int16).tobytes()\n",
    "\n",
    "def d4_notch_filter(x: np.ndarray, sr: int, f0: float, q: float) -> np.ndarray:\n",
    "    # Purpose: remove a narrowband tone (hum harmonic)\n",
    "    if f0 <= 0 or f0 >= (sr / 2.0):\n",
    "        return x.astype(np.float32)\n",
    "    b, a = signal.iirnotch(w0=f0, Q=q, fs=sr)\n",
    "    y = signal.filtfilt(b, a, x.astype(np.float64)).astype(np.float32)\n",
    "    return y\n",
    "\n",
    "def d4_highpass_filter(x: np.ndarray, sr: int, cutoff_hz: float) -> np.ndarray:\n",
    "    # Purpose: reduce very low-frequency rumble\n",
    "    if cutoff_hz <= 0:\n",
    "        return x.astype(np.float32)\n",
    "    cutoff_hz = min(cutoff_hz, sr / 2.0 - 1.0)\n",
    "    b, a = signal.butter(2, cutoff_hz, btype=\"highpass\", fs=sr)\n",
    "    y = signal.filtfilt(b, a, x.astype(np.float64)).astype(np.float32)\n",
    "    return y\n",
    "\n",
    "def d4_hum_reduce(x: np.ndarray, sr: int) -> np.ndarray:\n",
    "    # Purpose: remove DC, apply high-pass, then notch out mains hum harmonics\n",
    "    if x is None or len(x) == 0:\n",
    "        return x.astype(np.float32)\n",
    "    y = x.astype(np.float32)\n",
    "\n",
    "    y = d4_remove_dc(y)\n",
    "    y = d4_highpass_filter(y, sr, D4_HP_CUTOFF_HZ)\n",
    "\n",
    "    for k in range(1, D4_HUM_N_HARMONICS + 1):\n",
    "        f0 = D4_HUM_BASE_HZ * k\n",
    "        if f0 >= sr / 2.0:\n",
    "            break\n",
    "        y = d4_notch_filter(y, sr, f0, D4_HUM_Q)\n",
    "\n",
    "    y = d4_peak_limit(y, D4_PEAK_CEIL)\n",
    "    y = np.clip(y, -1.0, 1.0).astype(np.float32)\n",
    "    return y\n",
    "\n",
    "def d4_load_audio_mono_16k(path: str, target_sr: int = D4_TARGET_SR) -> Tuple[np.ndarray, int]:\n",
    "    # Inputs: source WAV path\n",
    "    # Outputs: mono float32 audio at 16 kHz\n",
    "    x, sr = sf.read(path, always_2d=True)\n",
    "    x = x.mean(axis=1).astype(np.float32)\n",
    "\n",
    "    if not np.isfinite(x).all():\n",
    "        x = np.nan_to_num(x, nan=0.0, posinf=0.0, neginf=0.0).astype(np.float32)\n",
    "\n",
    "    if sr != target_sr:\n",
    "        g = np.gcd(sr, target_sr)\n",
    "        up = target_sr // g\n",
    "        down = sr // g\n",
    "        x = signal.resample_poly(x.astype(np.float64), up, down).astype(np.float32)\n",
    "        sr = target_sr\n",
    "\n",
    "    x = np.clip(x, -1.0, 1.0).astype(np.float32)\n",
    "    return x.astype(np.float32), sr\n",
    "\n",
    "# -------------------------\n",
    "# VAD segments (source timeline)\n",
    "# Inputs: leveled audio at 16 kHz\n",
    "# Outputs: list of (start_sample, end_sample) in the source timeline\n",
    "# -------------------------\n",
    "def d4_vad_segments_source_timeline(\n",
    "    x: np.ndarray,\n",
    "    sr: int,\n",
    "    vad_aggr: int = D4_VAD_AGGRESSIVENESS,\n",
    "    frame_ms: int = D4_VAD_FRAME_MS\n",
    ") -> List[Tuple[int, int]]:\n",
    "    if not HAVE_WEBRTCVAD:\n",
    "        return []\n",
    "\n",
    "    assert sr == 16000, \"VAD expects 16k audio.\"\n",
    "    vad = webrtcvad.Vad(int(vad_aggr))\n",
    "\n",
    "    frame_len = int(sr * (frame_ms / 1000.0))\n",
    "    if frame_len <= 0:\n",
    "        return []\n",
    "\n",
    "    n = len(x)\n",
    "    n_frames = n // frame_len\n",
    "    if n_frames == 0:\n",
    "        return []\n",
    "\n",
    "    pcm = d4_float_to_pcm16(x[:n_frames * frame_len])\n",
    "\n",
    "    def frame_bytes(i: int) -> bytes:\n",
    "        start = i * frame_len * 2\n",
    "        end = start + frame_len * 2\n",
    "        return pcm[start:end]\n",
    "\n",
    "    voiced = [vad.is_speech(frame_bytes(i), sr) for i in range(n_frames)]\n",
    "\n",
    "    segs = []\n",
    "    i = 0\n",
    "    while i < n_frames:\n",
    "        if not voiced[i]:\n",
    "            i += 1\n",
    "            continue\n",
    "        s0 = i\n",
    "        while i < n_frames and voiced[i]:\n",
    "            i += 1\n",
    "        e0 = i\n",
    "        segs.append((s0 * frame_len, e0 * frame_len))\n",
    "\n",
    "    # Purpose: merge short gaps between voiced segments\n",
    "    max_gap = int(0.20 * sr)\n",
    "    merged = []\n",
    "    for s, e in segs:\n",
    "        if not merged:\n",
    "            merged.append([s, e])\n",
    "        else:\n",
    "            ps, pe = merged[-1]\n",
    "            if s - pe <= max_gap:\n",
    "                merged[-1][1] = e\n",
    "            else:\n",
    "                merged.append([s, e])\n",
    "\n",
    "    # Purpose: pad segments slightly to avoid hard cuts\n",
    "    pad = int(round(D4_VAD_PAD_SEC * sr))\n",
    "    tail = int(round(D4_POST_VAD_TRAIL_PAD_SEC * sr))\n",
    "\n",
    "    out = []\n",
    "    for s, e in merged:\n",
    "        s2 = max(0, s - pad)\n",
    "        e2 = min(n, e + pad + tail)\n",
    "        if e2 > s2:\n",
    "            out.append((int(s2), int(e2)))\n",
    "\n",
    "    return out\n",
    "\n",
    "# -------------------------\n",
    "# Single-clip selection per source file\n",
    "# Rule: choose the longest voiced segment (tie-breaker: earliest)\n",
    "# Output: one clip dict or None\n",
    "# -------------------------\n",
    "def d4_pick_longest_segment(segs: List[Tuple[int, int]]) -> Optional[Tuple[int, int]]:\n",
    "    if not segs:\n",
    "        return None\n",
    "    segs2 = sorted(segs, key=lambda t: (-(t[1]-t[0]), t[0]))\n",
    "    return segs2[0]\n",
    "\n",
    "def d4_make_single_clip_from_source_segments(\n",
    "    y: np.ndarray,\n",
    "    sr: int,\n",
    "    segs: List[Tuple[int, int]],\n",
    "    task_type: str\n",
    ") -> Optional[Dict]:\n",
    "    # Inputs: full audio + VAD segments + task type\n",
    "    # Outputs: {audio, start, end, duration} or None\n",
    "    if not segs:\n",
    "        return None\n",
    "\n",
    "    best = d4_pick_longest_segment(segs)\n",
    "    if best is None:\n",
    "        return None\n",
    "    s, e = best\n",
    "\n",
    "    seg = y[s:e].astype(np.float32)\n",
    "    if D4_TRIM_SEGMENTS:\n",
    "        seg = d4_trim_silence_energy(seg, sr, db_margin=28.0)\n",
    "    if seg is None or len(seg) <= 0:\n",
    "        return None\n",
    "\n",
    "    if task_type == \"vowel\":\n",
    "        # Purpose: fixed 2.0 s, centered in the chosen segment\n",
    "        L = int(round(D4_VOWEL_SEC * sr))\n",
    "        if len(seg) >= L:\n",
    "            mid = len(seg) // 2\n",
    "            a0 = max(0, mid - L // 2)\n",
    "            a1 = a0 + L\n",
    "            audio = seg[a0:a1].astype(np.float32)\n",
    "            src_start = float(s + a0) / sr\n",
    "            src_end = float(s + a1) / sr\n",
    "        else:\n",
    "            # Purpose: allow short vowel by padding to 2.0 s\n",
    "            audio = d4_force_length(seg, L).astype(np.float32)\n",
    "            src_start = float(s) / sr\n",
    "            src_end = float(s + L) / sr  # padded end\n",
    "\n",
    "        if D4_TRIM_CLIPS:\n",
    "            t = d4_trim_silence_energy(audio, sr, db_margin=30.0)\n",
    "            audio = d4_force_length(t, L)\n",
    "\n",
    "        audio = d4_peak_limit(audio, D4_PEAK_CEIL)\n",
    "        audio = np.clip(audio, -1.0, 1.0).astype(np.float32)\n",
    "        return {\"audio\": audio, \"start\": float(src_start), \"end\": float(src_end), \"duration\": float(len(audio))/sr}\n",
    "\n",
    "    # reading\n",
    "    L = int(round(D4_READ_SEC * sr))\n",
    "    if len(seg) >= L:\n",
    "        # Purpose: enforce one-window rule (first 8 seconds only)\n",
    "        audio = seg[:L].astype(np.float32)\n",
    "        src_start = float(s) / sr\n",
    "        src_end = float(s + L) / sr\n",
    "        audio = d4_peak_limit(audio, D4_PEAK_CEIL)\n",
    "        audio = np.clip(audio, -1.0, 1.0).astype(np.float32)\n",
    "        return {\"audio\": audio, \"start\": float(src_start), \"end\": float(src_end), \"duration\": float(len(audio))/sr}\n",
    "\n",
    "    # Purpose: keep shorter reading clips only if not too short\n",
    "    if len(seg) < int(sr * D4_MIN_KEEP_SEC):\n",
    "        return None\n",
    "\n",
    "    audio = seg.astype(np.float32)\n",
    "    audio = d4_peak_limit(audio, D4_PEAK_CEIL)\n",
    "    audio = np.clip(audio, -1.0, 1.0).astype(np.float32)\n",
    "    src_start = float(s) / sr\n",
    "    src_end = float(s + len(audio)) / sr\n",
    "    return {\"audio\": audio, \"start\": float(src_start), \"end\": float(src_end), \"duration\": float(len(audio))/sr}\n",
    "\n",
    "# -------------------------\n",
    "# WAV writer\n",
    "# Inputs: clip audio\n",
    "# Outputs: PCM_16 WAV file on Drive\n",
    "# -------------------------\n",
    "def d4_write_wav(path: str, x: np.ndarray, sr: int):\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "    x = np.clip(x, -1.0, 1.0).astype(np.float32)\n",
    "    sf.write(path, x, sr, subtype=\"PCM_16\")\n",
    "\n",
    "# -------------------------\n",
    "# Build source index (one row per source WAV)\n",
    "# Outputs: d4_index used for preprocessing loop\n",
    "# -------------------------\n",
    "d4_index, d4_unmatched, d4_total_wavs = d4_build_index()\n",
    "\n",
    "if len(d4_index) == 0:\n",
    "    raise RuntimeError(\"No usable D4 rows after indexing/parsing/task mapping.\")\n",
    "\n",
    "print(\"\\nD4 index shape:\", d4_index.shape)\n",
    "print(\"Group counts:\\n\", d4_index[\"group\"].value_counts(dropna=False), \"\\n\")\n",
    "print(\"Task_type counts:\\n\", d4_index[\"task_type\"].value_counts(dropna=False), \"\\n\")\n",
    "print(\"Unique speakers:\", int(d4_index[\"speaker_id\"].nunique()))\n",
    "\n",
    "# -------------------------\n",
    "# Preprocess each source → write one candidate clip immediately\n",
    "# Inputs: source WAV paths\n",
    "# Outputs: candidate WAVs + candidate metadata table\n",
    "# -------------------------\n",
    "MANIFEST_COLS = [\n",
    "    \"split\", \"dataset\", \"task\", \"speaker_id\", \"sample_id\",\n",
    "    \"label_str\", \"label_num\", \"age\", \"sex\",\n",
    "    \"speaker_key_rel\",\n",
    "    \"clip_path\", \"duration_sec\", \"source_path\",\n",
    "    \"clip_start_sec\", \"clip_end_sec\",\n",
    "    \"sr_hz\", \"channels\",\n",
    "    \"clip_is_contiguous\",\n",
    "]\n",
    "\n",
    "cand_rows: List[Dict] = []\n",
    "warn_rows: List[Dict] = []\n",
    "cand_counter = 0\n",
    "\n",
    "pbar = tqdm(d4_index.itertuples(index=False), total=len(d4_index),\n",
    "            desc=\"D4 preprocess (1 clip per source; write candidates)\", dynamic_ncols=True)\n",
    "\n",
    "for r in pbar:\n",
    "    src = r.audio_path\n",
    "    try:\n",
    "        # Pipeline: load/resample → DC removal → hum reduction → leveling\n",
    "        y, sr = d4_load_audio_mono_16k(src, D4_TARGET_SR)\n",
    "        if sr != D4_TARGET_SR:\n",
    "            raise RuntimeError(f\"Unexpected SR after load: {sr}\")\n",
    "\n",
    "        y = d4_remove_dc(y)\n",
    "        if D4_HUM_REMOVE:\n",
    "            y = d4_hum_reduce(y, sr)\n",
    "        y, _linfo = d4_apply_level_target_manual(y, sr)\n",
    "\n",
    "        # Purpose: find voiced regions on the processed signal\n",
    "        segs = d4_vad_segments_source_timeline(y, sr)\n",
    "        if not segs:\n",
    "            warn_rows.append({\"dataset\": \"D4\", \"speaker_id\": r.speaker_id, \"source_path\": src,\n",
    "                              \"warning_type\": \"no_vad_segments\", \"detail\": \"\"})\n",
    "            continue\n",
    "\n",
    "        # Purpose: choose exactly one clip from the voiced regions\n",
    "        clip = d4_make_single_clip_from_source_segments(y, sr, segs, r.task_type)\n",
    "        if clip is None:\n",
    "            warn_rows.append({\"dataset\": \"D4\", \"speaker_id\": r.speaker_id, \"source_path\": src,\n",
    "                              \"warning_type\": \"no_clip_selected\", \"detail\": \"\"})\n",
    "            continue\n",
    "\n",
    "        task5 = d4_task_short(r.task_type)\n",
    "\n",
    "        cand_counter += 1\n",
    "        cand_name = d4_safe(f\"CAND_{cand_counter:08d}.wav\")\n",
    "        cand_path = os.path.join(D4_CAND_DIR, cand_name)\n",
    "\n",
    "        # Write candidate now (staging file)\n",
    "        d4_write_wav(cand_path, clip[\"audio\"], sr)\n",
    "\n",
    "        cand_rows.append({\n",
    "            \"dataset\": \"D4\",\n",
    "            \"task\": task5,\n",
    "            \"speaker_id\": r.speaker_id,\n",
    "            \"sample_id\": os.path.basename(src),\n",
    "            \"label_str\": r.label_str,\n",
    "            \"label_num\": int(r.label_num),\n",
    "            \"age\": np.nan,\n",
    "            \"sex\": r.sex if pd.notna(r.sex) else np.nan,\n",
    "            \"speaker_key_rel\": r.speaker_key_rel,\n",
    "            \"clip_path_cand\": cand_path,\n",
    "            \"duration_sec\": float(clip[\"duration\"]),\n",
    "            \"source_path\": src,\n",
    "            \"clip_start_sec\": float(clip[\"start\"]),\n",
    "            \"clip_end_sec\": float(clip[\"end\"]),\n",
    "            \"sr_hz\": int(sr),\n",
    "            \"channels\": 1,\n",
    "            \"clip_is_contiguous\": True,\n",
    "        })\n",
    "\n",
    "    except Exception as e:\n",
    "        warn_rows.append({\"dataset\": \"D4\", \"speaker_id\": getattr(r, \"speaker_id\", \"\"),\n",
    "                          \"source_path\": src, \"warning_type\": \"preprocess_error\", \"detail\": repr(e)})\n",
    "\n",
    "cand_df = pd.DataFrame(cand_rows)\n",
    "warnings_df = pd.DataFrame(warn_rows)\n",
    "\n",
    "if len(cand_df) == 0:\n",
    "    raise RuntimeError(\"No D4 clips produced. Check indexing/parsing/VAD settings.\")\n",
    "\n",
    "print(\"\\nD4 candidates written:\", int(len(cand_df)))\n",
    "print(\"Candidate dir:\", D4_CAND_DIR)\n",
    "\n",
    "# -------------------------\n",
    "# Cap candidates per (speaker_id, task), then delete unkept candidate WAVs\n",
    "# Inputs: candidate table + candidate files\n",
    "# Outputs: capped candidate table, staging cleaned\n",
    "# -------------------------\n",
    "def d4_cap_manifest(df: pd.DataFrame, max_k: int, seed: int) -> Tuple[pd.DataFrame, set]:\n",
    "    rng = np.random.default_rng(seed)\n",
    "    kept_idx = []\n",
    "    for (spk, tt), g in df.groupby([\"speaker_id\", \"task\"], sort=False):\n",
    "        idxs = g.index.to_numpy()\n",
    "        if len(idxs) <= max_k:\n",
    "            kept_idx.extend(idxs.tolist())\n",
    "        else:\n",
    "            chosen = rng.choice(idxs, size=max_k, replace=False)\n",
    "            kept_idx.extend(chosen.tolist())\n",
    "    kept_idx = sorted(set(kept_idx))\n",
    "    kept_set = set(kept_idx)\n",
    "    return df.loc[kept_idx].reset_index(drop=True), kept_set\n",
    "\n",
    "cand_df_capped, keep_set = d4_cap_manifest(cand_df, D4_MAX_CLIPS_PER_SPK_PER_TASK, D4_RANDOM_SEED)\n",
    "print(\"D4 clips after cap:\", int(len(cand_df_capped)))\n",
    "\n",
    "to_delete = cand_df.loc[~cand_df.index.isin(list(keep_set)), \"clip_path_cand\"].tolist()\n",
    "deleted = 0\n",
    "for p in to_delete:\n",
    "    try:\n",
    "        if os.path.exists(p):\n",
    "            os.remove(p)\n",
    "            deleted += 1\n",
    "    except Exception as e:\n",
    "        warnings_df = pd.concat([warnings_df, pd.DataFrame([{\n",
    "            \"dataset\": \"D4\",\n",
    "            \"speaker_id\": \"\",\n",
    "            \"source_path\": \"\",\n",
    "            \"warning_type\": \"candidate_delete_failed\",\n",
    "            \"detail\": f\"{p} :: {repr(e)}\"\n",
    "        }])], ignore_index=True)\n",
    "\n",
    "print(\"Deleted unkept candidates:\", deleted)\n",
    "cand_df = cand_df_capped\n",
    "\n",
    "# -------------------------\n",
    "# Speaker-level split (stratified by label_num)\n",
    "# Inputs: unique speakers from kept candidates\n",
    "# Outputs: split assignment merged back onto cand_df\n",
    "# -------------------------\n",
    "def d4_split_speakers(df: pd.DataFrame, fracs=(0.70, 0.15, 0.15), seed=1337) -> pd.DataFrame:\n",
    "    assert abs(sum(fracs) - 1.0) < 1e-9\n",
    "    rng = np.random.default_rng(seed)\n",
    "\n",
    "    spk_df = df[[\"speaker_id\", \"label_num\", \"label_str\"]].drop_duplicates().copy()\n",
    "    split_rows = []\n",
    "\n",
    "    for lbl in [0, 1]:\n",
    "        spks = spk_df[spk_df[\"label_num\"] == lbl][\"speaker_id\"].tolist()\n",
    "        rng.shuffle(spks)\n",
    "        n = len(spks)\n",
    "        n_train = int(round(fracs[0] * n))\n",
    "        n_val = int(round(fracs[1] * n))\n",
    "\n",
    "        train_spks = spks[:n_train]\n",
    "        val_spks = spks[n_train:n_train + n_val]\n",
    "        test_spks = spks[n_train + n_val:]\n",
    "\n",
    "        split_rows += [{\"speaker_id\": s, \"split\": \"train\"} for s in train_spks]\n",
    "        split_rows += [{\"speaker_id\": s, \"split\": \"val\"} for s in val_spks]\n",
    "        split_rows += [{\"speaker_id\": s, \"split\": \"test\"} for s in test_spks]\n",
    "\n",
    "    return pd.DataFrame(split_rows)\n",
    "\n",
    "spk_split = d4_split_speakers(cand_df, D4_SPLIT_FRACS, D4_RANDOM_SEED)\n",
    "\n",
    "cand_df = cand_df.drop(columns=[\"split\"], errors=\"ignore\")\n",
    "cand_df = cand_df.merge(spk_split, on=\"speaker_id\", how=\"left\", validate=\"many_to_one\")\n",
    "\n",
    "if cand_df[\"split\"].isna().any():\n",
    "    ex = cand_df.loc[cand_df[\"split\"].isna(), \"speaker_id\"].drop_duplicates().head(10).tolist()\n",
    "    raise RuntimeError(f\"Some speakers did not get a split assignment. Example: {ex}\")\n",
    "\n",
    "# -------------------------\n",
    "# Finalize: move kept candidates into clips/<split>/ and build manifest rows\n",
    "# Inputs: kept candidate WAVs + split assignment\n",
    "# Outputs: final WAVs + manifest rows pointing to final paths\n",
    "# -------------------------\n",
    "global_counter = 0\n",
    "final_rows: List[Dict] = []\n",
    "\n",
    "pbar2 = tqdm(cand_df.itertuples(index=False), total=len(cand_df),\n",
    "             desc=\"D4 finalize (move kept)\", dynamic_ncols=True)\n",
    "\n",
    "for r in pbar2:\n",
    "    global_counter += 1\n",
    "\n",
    "    label_tag = d4_pd_hc(r.label_str)\n",
    "    task5 = str(r.task)\n",
    "    spk = str(r.speaker_id)\n",
    "\n",
    "    out_name = d4_safe(f\"D4_{label_tag}_{spk}_{task5}_{global_counter:06d}.wav\")\n",
    "    out_path = os.path.join(D4_CLIPS_DIR, r.split, out_name)\n",
    "\n",
    "    cand_path = getattr(r, \"clip_path_cand\")\n",
    "    if not os.path.exists(cand_path):\n",
    "        warnings_df = pd.concat([warnings_df, pd.DataFrame([{\n",
    "            \"dataset\": \"D4\",\n",
    "            \"speaker_id\": spk,\n",
    "            \"source_path\": r.source_path,\n",
    "            \"warning_type\": \"missing_candidate_file\",\n",
    "            \"detail\": cand_path\n",
    "        }])], ignore_index=True)\n",
    "        continue\n",
    "\n",
    "    shutil.move(cand_path, out_path)\n",
    "\n",
    "    final_rows.append({\n",
    "        \"split\": r.split,\n",
    "        \"dataset\": \"D4\",\n",
    "        \"task\": task5,\n",
    "        \"speaker_id\": spk,\n",
    "        \"sample_id\": r.sample_id,\n",
    "        \"label_str\": r.label_str,\n",
    "        \"label_num\": int(r.label_num),\n",
    "        \"age\": np.nan,\n",
    "        \"sex\": r.sex if pd.notna(r.sex) else np.nan,\n",
    "        \"speaker_key_rel\": r.speaker_key_rel,\n",
    "        \"clip_path\": out_path,\n",
    "        \"duration_sec\": float(r.duration_sec),\n",
    "        \"source_path\": r.source_path,\n",
    "        \"clip_start_sec\": float(r.clip_start_sec),\n",
    "        \"clip_end_sec\": float(r.clip_end_sec),\n",
    "        \"sr_hz\": int(r.sr_hz),\n",
    "        \"channels\": 1,\n",
    "        \"clip_is_contiguous\": True,\n",
    "    })\n",
    "\n",
    "# Purpose: remove temporary staging directory when done\n",
    "try:\n",
    "    if os.path.isdir(D4_CAND_DIR):\n",
    "        leftovers = list(os.scandir(D4_CAND_DIR))\n",
    "        for ent in leftovers:\n",
    "            try:\n",
    "                os.remove(ent.path)\n",
    "            except Exception:\n",
    "                pass\n",
    "        try:\n",
    "            os.rmdir(D4_CAND_DIR)\n",
    "        except Exception:\n",
    "            pass\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "manifest_df = pd.DataFrame(final_rows)\n",
    "\n",
    "# Purpose: enforce canonical schema and column order\n",
    "for c in MANIFEST_COLS:\n",
    "    if c not in manifest_df.columns:\n",
    "        manifest_df[c] = np.nan\n",
    "manifest_df = manifest_df[MANIFEST_COLS].copy()\n",
    "\n",
    "# -------------------------\n",
    "# Save artifacts (manifests, warnings, summary, run config)\n",
    "# Outputs: CSV/JSON files under manifests/, logs/, config/\n",
    "# -------------------------\n",
    "manifest_all_path = os.path.join(D4_MANIFESTS_DIR, \"manifest_all.csv\")\n",
    "manifest_df.to_csv(manifest_all_path, index=False)\n",
    "\n",
    "for sp in [\"train\", \"val\", \"test\"]:\n",
    "    p = os.path.join(D4_MANIFESTS_DIR, f\"manifest_{sp}.csv\")\n",
    "    manifest_df.loc[manifest_df[\"split\"] == sp].to_csv(p, index=False)\n",
    "\n",
    "warnings_path = os.path.join(D4_LOGS_DIR, \"preprocess_warnings.csv\")\n",
    "warnings_df.to_csv(warnings_path, index=False)\n",
    "\n",
    "summary = {\n",
    "    \"dataset\": \"D4\",\n",
    "    \"source_root\": D4_IPVS_DIR,\n",
    "    \"target_sr\": int(D4_TARGET_SR),\n",
    "    \"webrtcvad_available\": bool(HAVE_WEBRTCVAD),\n",
    "    \"scipy_available\": bool(HAVE_SCIPY),\n",
    "    \"n_source_files_total_found\": int(d4_total_wavs),\n",
    "    \"n_source_files_used_after_filter\": int(len(d4_index)),\n",
    "    \"n_output_clips_total\": int(len(manifest_df)),\n",
    "    \"n_speakers\": int(manifest_df[\"speaker_id\"].nunique()) if len(manifest_df) else 0,\n",
    "    \"label_counts_clips\": manifest_df[\"label_str\"].value_counts(dropna=False).to_dict() if len(manifest_df) else {},\n",
    "    \"split_counts_clips\": manifest_df[\"split\"].value_counts(dropna=False).to_dict() if len(manifest_df) else {},\n",
    "    \"task_counts_clips\": manifest_df[\"task\"].value_counts(dropna=False).to_dict() if len(manifest_df) else {},\n",
    "    \"n_warnings\": int(len(warnings_df)),\n",
    "    \"notes\": \"At most 1 clip per source file. Candidates written to clips/_candidates, then capped/split and moved to clips/<split>.\",\n",
    "}\n",
    "summary_path = os.path.join(D4_LOGS_DIR, \"dataset_summary.json\")\n",
    "with open(summary_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "run_cfg = {\n",
    "    \"dataset\": \"D4\",\n",
    "    \"paths\": {\n",
    "        \"dataset_dir\": D4_IPVS_DIR,\n",
    "        \"out_root\": D4_OUT_ROOT,\n",
    "        \"clips_dir\": D4_CLIPS_DIR,\n",
    "        \"candidate_dir\": D4_CAND_DIR,\n",
    "        \"manifest_all\": manifest_all_path,\n",
    "        \"warnings_csv\": warnings_path,\n",
    "        \"summary_json\": summary_path,\n",
    "    },\n",
    "    \"folder_structure\": \"clips/<split>/ (flat) with temporary clips/_candidates during run\",\n",
    "    \"filename_format\": \"D4_{HC|PD}_{speaker_id}_{task<=5}_{global_index:06d}.wav\",\n",
    "    \"manifest_schema_order\": MANIFEST_COLS,\n",
    "    \"cap_policy\": {\"groupby\": [\"speaker_id\", \"task\"], \"max_per_group\": int(D4_MAX_CLIPS_PER_SPK_PER_TASK)},\n",
    "    \"clip_boundary_policy\": \"clip_start_sec/clip_end_sec are source-timeline (post-resample). clip_is_contiguous=True.\",\n",
    "    \"one_clip_per_source_file\": True,\n",
    "    \"one_clip_selection_rule\": \"Longest voiced segment; vowel=2s centered/padded, reading=first 8s or true duration if shorter (>=min).\",\n",
    "    \"audio_processing\": {\n",
    "        \"sr_hz\": int(D4_TARGET_SR),\n",
    "        \"hum_remove\": bool(D4_HUM_REMOVE),\n",
    "        \"hum_base_hz\": float(D4_HUM_BASE_HZ),\n",
    "        \"highpass_cutoff_hz\": float(D4_HP_CUTOFF_HZ),\n",
    "        \"leveling\": \"manual RMS-like + peak limit (no pyloudnorm)\",\n",
    "        \"peak_ceiling\": float(D4_PEAK_CEIL),\n",
    "        \"max_gain_db\": float(D4_MAX_GAIN_DB),\n",
    "    },\n",
    "}\n",
    "run_cfg_path = os.path.join(D4_CONFIG_DIR, \"run_config.json\")\n",
    "with open(run_cfg_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(run_cfg, f, indent=2)\n",
    "\n",
    "print(\"\\nDONE: D4 preprocessing\")\n",
    "print(\"Manifest:\", manifest_all_path)\n",
    "print(\"Warnings:\", warnings_path)\n",
    "print(\"Summary:\", summary_path)\n",
    "print(\"Run config:\", run_cfg_path)\n",
    "print(\"Clips written:\", int(len(manifest_df)))\n",
    "\n",
    "# -------------------------\n",
    "# Quick sanity prints\n",
    "# Purpose: confirm uniform audio format and clean speaker split\n",
    "# -------------------------\n",
    "print(\"\\nSanity checks:\")\n",
    "print(\"Unique SR:\", sorted(manifest_df[\"sr_hz\"].unique().tolist()) if len(manifest_df) else [])\n",
    "print(\"Unique channels:\", sorted(manifest_df[\"channels\"].unique().tolist()) if len(manifest_df) else [])\n",
    "print(\"Speakers per split x label (by speaker):\")\n",
    "print(\n",
    "    manifest_df[[\"speaker_id\", \"split\", \"label_str\"]]\n",
    "    .drop_duplicates()\n",
    "    .groupby([\"split\", \"label_str\"])\n",
    "    .size()\n",
    ")\n",
    "\n",
    "spk_split_chk = manifest_df[[\"speaker_id\", \"split\"]].drop_duplicates()\n",
    "dup = spk_split_chk.groupby(\"speaker_id\")[\"split\"].nunique()\n",
    "bad = dup[dup > 1]\n",
    "print(\"Speakers appearing in multiple splits:\", int(len(bad)))\n",
    "if len(bad) > 0:\n",
    "    print(\"Example bad speaker ids:\", bad.index.tolist()[:10])\n",
    "    raise RuntimeError(\"Speaker appears in more than one split.\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 966,
     "referenced_widgets": [
      "08606791d15f4477affdc8e62f541592",
      "0f43557bb9ca4659ad4d45dea09e24cd",
      "b77c1dd8a22443cfaabf6343edcc592c",
      "3b4e42ff1073492fb32e231feb6450ac",
      "9001b85f1bf64b85b4f96e27312ba54f",
      "b731738d806d46768ded19c2f53a74c0",
      "37cc4a47085c4becab1a4e5cd6706a3f",
      "5da4b1b4ff044f7ab433cd642f4830fb",
      "1639cfd19c454a63b08e534078fd40ea",
      "f22f8f651b5e415bae549f0bd815665c",
      "b9da9142be2047b2a9cc8287d0264343",
      "b349de4348a244e58c8bc95e5859d0f1",
      "61fb37b837804acd855c48d3e5a89fa2",
      "93b59fdeac5f4c42bc417ca0c1edfabb",
      "4cef306d23d34c69965456a06d56cfba",
      "a1950ab59e1748efbe380a2503b329d4",
      "b3688a03fdfa49f28afcf2b36a67efed",
      "777530cc5f6b49839bdb781634d353a6",
      "f43044aa89414a6d88f91ac97a739411",
      "54d91a7af82248d999b5cc304c42fa71",
      "69ffa46fe9734434b0c6e272a1b882ee",
      "a7d0c0205f604108a6cba08ba047d332"
     ]
    },
    "id": "TL25cMo0jbsd"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "The following cell preprocesses Dataset D5 (MDVR-KCL) into the standard `preprocessed_v1` folder structure used across the project. It scans the raw D5 WAV files and keeps only those that can be clearly linked to a speaker ID, a task type, and a label (Healthy or Parkinson). The cell then creates clean and consistent training clips along with a single combined manifest. A strict rule is enforced so that **no more than one final clip is produced from each source WAV file**. To make the process more reliable when working on Google Drive, each processed clip is written right away to a temporary `_candidates` folder, and only the selected clips are later moved into the final `clips/train`, `clips/val`, and `clips/test` folders.\n",
    "\n",
    "Audio processing follows a fixed and repeatable pipeline. Each source file is loaded, converted to mono, resampled to 16 kHz, and the first 40 seconds are removed to skip setup sounds or non target audio. When SciPy is available, an optional high pass filter is applied to reduce very low frequency noise. Speech regions are then detected using voice activity detection, with WebRTC VAD used when available and an energy based fallback otherwise. Before normalizing volume, a **loudness based gate** is applied to the detected speech regions. Quieter segments are dropped under the assumption that they are more likely to be background speech or another speaker, while the main speaker is typically louder and closer to the microphone. The gate starts strict and gradually relaxes until enough speech is available to build an 8 second clip. The remaining segments are stitched together, then volume normalization is applied using an RMS target with peak limiting, without using pyloudnorm. From this stitched signal, the first full 8 seconds are taken as the final clip. If less than 8 seconds of usable audio remains, the file is skipped and a warning is recorded.\n",
    "\n",
    "After all candidate clips are created, the cell applies a limit of up to 8 clips per speaker and task, randomly selecting clips when more are available, and deletes any unselected candidate WAV files. Speakers are then split into training, validation, and test sets at the speaker level (70/15/15), while keeping Healthy and Parkinson speakers proportionally balanced in each split. The selected clips are moved into their final split folders using a standardized filename format, and a final `manifest_all.csv` is written with a consistent column order used throughout the project. The cell also generates `logs/preprocess_warnings.csv` to record any issues, `logs/dataset_summary.json` to summarize counts and rules, and `config/run_config.json` to document all settings and paths. Finally, the temporary `_candidates` folder is removed and sanity checks are run to confirm correct speaker splits and expected audio settings."
   ],
   "metadata": {
    "id": "tCfKgyQltLDL"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# ============================================================\n",
    "# D5 Preprocessing v1 (MDVR-KCL) — Loudness-Gated VAD Stitching (1 clip per file) -- NOT USED: Preprocessing v2 is used further\n",
    "# Inputs: raw D5 WAV files under the dataset folder\n",
    "# Outputs: clips/<split>/ WAVs (flat), manifests/manifest_all.csv,\n",
    "#          config/run_config.json, logs/preprocess_warnings.csv, logs/dataset_summary.json\n",
    "# Notes: Writes temporary candidates to clips/_candidates/ during processing, then moves kept clips.\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import math\n",
    "import random\n",
    "import time\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple, Dict, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import soundfile as sf\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# -------------------------\n",
    "# Drive mount check\n",
    "# Purpose: ensure dataset and output folders are reachable in Colab\n",
    "# -------------------------\n",
    "if not os.path.isdir(\"/content/drive/MyDrive\"):\n",
    "    from google.colab import drive  # type: ignore\n",
    "    drive.mount(\"/content/drive\")\n",
    "\n",
    "# -------------------------\n",
    "# Optional dependencies\n",
    "# Purpose: enable VAD and higher-quality resampling/filtering when available\n",
    "# Inputs: none\n",
    "# Outputs: flags showing which optional libraries are available\n",
    "# -------------------------\n",
    "def _d5_try_import_webrtcvad():\n",
    "    try:\n",
    "        import webrtcvad  # type: ignore\n",
    "        return webrtcvad, True\n",
    "    except Exception:\n",
    "        return None, False\n",
    "\n",
    "webrtcvad, D5_HAVE_WEBRTCVAD = _d5_try_import_webrtcvad()\n",
    "if not D5_HAVE_WEBRTCVAD:\n",
    "    try:\n",
    "        import subprocess, sys\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"-q\", \"install\", \"webrtcvad\"])\n",
    "    except Exception:\n",
    "        pass\n",
    "    webrtcvad, D5_HAVE_WEBRTCVAD = _d5_try_import_webrtcvad()\n",
    "\n",
    "try:\n",
    "    from scipy.signal import resample_poly, butter, filtfilt  # type: ignore\n",
    "    D5_HAVE_SCIPY = True\n",
    "except Exception:\n",
    "    D5_HAVE_SCIPY = False\n",
    "\n",
    "print(\"webrtcvad available?\", D5_HAVE_WEBRTCVAD)\n",
    "print(\"scipy available?\", D5_HAVE_SCIPY)\n",
    "\n",
    "# -------------------------\n",
    "# Dataset paths and standardized outputs\n",
    "# Inputs: raw dataset folder\n",
    "# Outputs: preprocessed_v1 folder tree (clips/manifests/config/logs)\n",
    "# -------------------------\n",
    "D5_PROJECT_DIR  = \"/content/drive/MyDrive/AI_PD_Project\"\n",
    "D5_DATASET_DIR  = f\"{D5_PROJECT_DIR}/Datasets/D5-English (MDVR-KCL)\"\n",
    "D5_DIR          = f\"{D5_DATASET_DIR}/26-29_09_2017_KCL\"\n",
    "\n",
    "# Outputs\n",
    "D5_OUT_ROOT     = f\"{D5_DATASET_DIR}/preprocessed_v1\"\n",
    "D5_CLIPS_DIR    = f\"{D5_OUT_ROOT}/clips\"\n",
    "D5_CAND_DIR     = f\"{D5_CLIPS_DIR}/_candidates\"     # temporary staging during this run\n",
    "D5_MANIFEST_DIR = f\"{D5_OUT_ROOT}/manifests\"\n",
    "D5_CONFIG_DIR   = f\"{D5_OUT_ROOT}/config\"\n",
    "D5_LOGS_DIR     = f\"{D5_OUT_ROOT}/logs\"\n",
    "\n",
    "for p in [D5_OUT_ROOT, D5_CLIPS_DIR, D5_CAND_DIR, D5_MANIFEST_DIR, D5_CONFIG_DIR, D5_LOGS_DIR]:\n",
    "    os.makedirs(p, exist_ok=True)\n",
    "for sp in [\"train\", \"val\", \"test\"]:\n",
    "    os.makedirs(os.path.join(D5_CLIPS_DIR, sp), exist_ok=True)\n",
    "\n",
    "# Purpose: avoid mixing candidate files from earlier runs\n",
    "if os.path.isdir(D5_CAND_DIR):\n",
    "    try:\n",
    "        shutil.rmtree(D5_CAND_DIR)\n",
    "    except Exception:\n",
    "        pass\n",
    "os.makedirs(D5_CAND_DIR, exist_ok=True)\n",
    "\n",
    "print(\"\\nD5_DIR exists?\", os.path.exists(D5_DIR))\n",
    "if not os.path.exists(D5_DIR):\n",
    "    raise FileNotFoundError(f\"D5_DIR not found: {D5_DIR}\")\n",
    "\n",
    "# -------------------------\n",
    "# Preprocessing configuration\n",
    "# Purpose: keep audio format consistent and enforce single-clip policy\n",
    "# -------------------------\n",
    "D5_SR = 16000\n",
    "D5_RANDOM_SEED = 1337\n",
    "random.seed(D5_RANDOM_SEED)\n",
    "np.random.seed(D5_RANDOM_SEED)\n",
    "\n",
    "# Purpose: skip long lead-in content common in some recordings\n",
    "D5_SKIP_SEC = 40.0\n",
    "\n",
    "# Purpose: simple, stable loudness control without external loudness libraries\n",
    "D5_TARGET_RMS_DBFS = -20.0\n",
    "D5_PEAK_LIMIT_DBFS = -1.0\n",
    "D5_MIN_RMS_DBFS    = -60.0\n",
    "D5_MAX_GAIN_DB     = 18.0\n",
    "\n",
    "# Purpose: reduce low-frequency rumble (only if SciPy is available)\n",
    "D5_ENABLE_HIGHPASS = True\n",
    "D5_HIGHPASS_HZ     = 70.0\n",
    "D5_HIGHPASS_ORDER  = 4\n",
    "\n",
    "# Purpose: speech segment detection settings\n",
    "D5_VAD_MODE      = 3\n",
    "D5_FRAME_MS      = 30\n",
    "D5_MIN_SPEECH_MS = 200\n",
    "D5_MERGE_GAP_MS  = 200\n",
    "D5_PAD_SEC       = 0.25\n",
    "D5_MIN_KEEP_SEC  = 0.30\n",
    "\n",
    "# Purpose: enforce exactly one 8.0-second clip per source file\n",
    "D5_CLIP_SEC = 8.0\n",
    "\n",
    "# Purpose: cap per speaker and task after candidate generation\n",
    "D5_MAX_CLIPS_PER_SPK_TASK = 8\n",
    "\n",
    "# Purpose: speaker-level split for train/val/test\n",
    "D5_TRAIN_PCT, D5_VAL_PCT, D5_TEST_PCT = 0.70, 0.15, 0.15\n",
    "\n",
    "# Purpose: reliable file writes on Drive\n",
    "D5_WRITE_RETRIES = 4\n",
    "D5_WRITE_SLEEP   = 0.5\n",
    "\n",
    "# Purpose: drop quiet segments (assumed other speaker / background) before normalization\n",
    "D5_ENABLE_LOUDNESS_GATE = True\n",
    "D5_GATE_START_PCTL = 65\n",
    "D5_GATE_MIN_PCTL   = 35\n",
    "D5_GATE_STEP_PCTL  = 5\n",
    "D5_GATE_REQUIRE_SEC = D5_CLIP_SEC\n",
    "D5_VAD_SCALE_TARGET_PEAK = 0.25  # only used for VAD stability on very quiet signals\n",
    "\n",
    "# -------------------------\n",
    "# Manifest schema (canonical order)\n",
    "# Purpose: identical column names and ordering across datasets\n",
    "# Output: manifest_all.csv follows this exact order\n",
    "# -------------------------\n",
    "MANIFEST_COLS = [\n",
    "    \"split\",\"dataset\",\"task\",\"speaker_id\",\"speaker_key_rel\",\"sample_id\",\"label_str\",\"label_num\",\"age\",\"sex\",\n",
    "    \"clip_path\",\"duration_sec\",\"source_path\",\"clip_start_sec\",\"clip_end_sec\",\"clip_is_contiguous\",\"sr_hz\",\"channels\",\n",
    "]\n",
    "\n",
    "# -------------------------\n",
    "# Core audio helpers\n",
    "# Purpose: read, resample, trim, normalize, and write WAV files safely\n",
    "# -------------------------\n",
    "def d5_safe(s: str) -> str:\n",
    "    # Purpose: make file names filesystem-safe\n",
    "    return re.sub(r\"[^A-Za-z0-9_\\-\\.]+\", \"_\", str(s))\n",
    "\n",
    "def d5_db_to_lin(db: float) -> float:\n",
    "    return 10.0 ** (db / 20.0)\n",
    "\n",
    "def d5_rms_dbfs(y: np.ndarray) -> float:\n",
    "    # Purpose: RMS level estimate in dBFS for gating and normalization\n",
    "    if y is None or len(y) == 0:\n",
    "        return -120.0\n",
    "    rms = float(np.sqrt(np.mean(y.astype(np.float64) ** 2) + 1e-12))\n",
    "    return 20.0 * math.log10(max(rms, 1e-12))\n",
    "\n",
    "def d5_peak_limit(y: np.ndarray, peak_dbfs: float) -> np.ndarray:\n",
    "    # Purpose: prevent clipping after gain\n",
    "    if y is None or len(y) == 0:\n",
    "        return y\n",
    "    peak = float(np.max(np.abs(y)))\n",
    "    lim = d5_db_to_lin(peak_dbfs)\n",
    "    if peak > lim and peak > 0:\n",
    "        y = y * (lim / peak)\n",
    "    return np.clip(y, -1.0, 1.0).astype(np.float32)\n",
    "\n",
    "def d5_norm_rms_then_peak(y: np.ndarray) -> np.ndarray:\n",
    "    # Purpose: bring audio to a target RMS without boosting extremely quiet signals too much\n",
    "    if y is None or len(y) == 0:\n",
    "        return y\n",
    "    cur = d5_rms_dbfs(y)\n",
    "    if cur < D5_MIN_RMS_DBFS:\n",
    "        return d5_peak_limit(y, D5_PEAK_LIMIT_DBFS)\n",
    "    gain_db = float(D5_TARGET_RMS_DBFS - cur)\n",
    "    gain_db = float(np.clip(gain_db, -60.0, D5_MAX_GAIN_DB))\n",
    "    y2 = (y.astype(np.float32) * d5_db_to_lin(gain_db)).astype(np.float32)\n",
    "    return d5_peak_limit(y2, D5_PEAK_LIMIT_DBFS)\n",
    "\n",
    "def d5_read_mono(path: str) -> Tuple[np.ndarray, int]:\n",
    "    # Purpose: read WAV and convert to mono float32\n",
    "    x, sr = sf.read(path, always_2d=False)\n",
    "    if isinstance(x, np.ndarray) and x.ndim == 2:\n",
    "        x = x.mean(axis=1)\n",
    "    x = np.asarray(x, dtype=np.float32)\n",
    "    if not np.isfinite(x).all():\n",
    "        x = np.nan_to_num(x, nan=0.0, posinf=0.0, neginf=0.0).astype(np.float32)\n",
    "    return x, int(sr)\n",
    "\n",
    "def d5_resample(y: np.ndarray, sr_in: int, sr_out: int) -> np.ndarray:\n",
    "    # Purpose: resample to a common sample rate (16 kHz)\n",
    "    if sr_in == sr_out:\n",
    "        return y.astype(np.float32, copy=False)\n",
    "    if D5_HAVE_SCIPY:\n",
    "        g = math.gcd(sr_in, sr_out)\n",
    "        up = sr_out // g\n",
    "        down = sr_in // g\n",
    "        return resample_poly(y.astype(np.float64), up, down).astype(np.float32, copy=False)\n",
    "    n_new = int(round(len(y) * (sr_out / sr_in)))\n",
    "    if n_new <= 1:\n",
    "        return y[:1].astype(np.float32, copy=False)\n",
    "    x_old = np.linspace(0.0, 1.0, num=len(y), endpoint=False)\n",
    "    x_new = np.linspace(0.0, 1.0, num=n_new, endpoint=False)\n",
    "    return np.interp(x_new, x_old, y).astype(np.float32, copy=False)\n",
    "\n",
    "def d5_crop_skip(y: np.ndarray, sr: int, skip_sec: float) -> np.ndarray:\n",
    "    # Purpose: drop first N seconds (often contains setup noise or other voices)\n",
    "    k = int(round(sr * skip_sec))\n",
    "    if k <= 0:\n",
    "        return y\n",
    "    if k >= len(y):\n",
    "        return np.zeros((0,), dtype=np.float32)\n",
    "    return y[k:].astype(np.float32, copy=False)\n",
    "\n",
    "def d5_apply_highpass(y: np.ndarray, sr: int) -> np.ndarray:\n",
    "    # Purpose: remove very low frequencies (only when SciPy is available)\n",
    "    if not D5_ENABLE_HIGHPASS or not D5_HAVE_SCIPY:\n",
    "        return y\n",
    "    cutoff = float(D5_HIGHPASS_HZ)\n",
    "    if cutoff <= 0:\n",
    "        return y\n",
    "    nyq = 0.5 * sr\n",
    "    wn = min(0.99, cutoff / nyq)\n",
    "    b, a = butter(int(D5_HIGHPASS_ORDER), wn, btype=\"highpass\")\n",
    "    y2 = filtfilt(b, a, y.astype(np.float32)).astype(np.float32)\n",
    "    if not np.isfinite(y2).all():\n",
    "        y2 = np.nan_to_num(y2, nan=0.0, posinf=0.0, neginf=0.0).astype(np.float32)\n",
    "    return y2\n",
    "\n",
    "def d5_pcm16_bytes(y: np.ndarray) -> bytes:\n",
    "    # Purpose: VAD requires 16-bit PCM bytes\n",
    "    y = np.clip(y, -1.0, 1.0)\n",
    "    return (y * 32767.0).astype(np.int16).tobytes()\n",
    "\n",
    "def d5_task_short(task: str) -> str:\n",
    "    # Purpose: map noisy folder labels into a small task vocabulary\n",
    "    t = (task or \"\").strip().lower()\n",
    "    if t.startswith(\"read\"):\n",
    "        return \"read\"\n",
    "    if t.startswith(\"spont\"):\n",
    "        return \"spont\"\n",
    "    if t.startswith(\"vow\") or t in (\"ah\", \"a\", \"vowel\"):\n",
    "        return \"vowl\"\n",
    "    return \"unk\"\n",
    "\n",
    "def d5_label_to_tag(label_str: str) -> str:\n",
    "    # Purpose: short tag for filenames\n",
    "    return \"PD\" if str(label_str).strip().lower().startswith(\"parkinson\") else \"HC\"\n",
    "\n",
    "def d5_safe_write_wav(path: str, audio: np.ndarray, sr: int) -> None:\n",
    "    # Purpose: retry writes to reduce intermittent Drive failures\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "    last_err = None\n",
    "    for attempt in range(1, D5_WRITE_RETRIES + 1):\n",
    "        try:\n",
    "            sf.write(path, np.clip(audio, -1.0, 1.0), sr, subtype=\"PCM_16\")\n",
    "            return\n",
    "        except Exception as e:\n",
    "            last_err = e\n",
    "            time.sleep(D5_WRITE_SLEEP * attempt)\n",
    "    raise RuntimeError(f\"Failed to write WAV: {path}. Last error: {repr(last_err)}\")\n",
    "\n",
    "# -------------------------\n",
    "# Segmentation + stitching helpers\n",
    "# Purpose: find speech, optionally keep only louder segments, then stitch into one stream\n",
    "# -------------------------\n",
    "def d5_webrtc_segments(y: np.ndarray, sr: int) -> Optional[List[Tuple[int, int]]]:\n",
    "    # Output: list of (start_sample, end_sample) for speech-like regions\n",
    "    if not D5_HAVE_WEBRTCVAD:\n",
    "        return None\n",
    "    if sr not in (8000, 16000, 32000, 48000):\n",
    "        return None\n",
    "    frame_ms = int(D5_FRAME_MS)\n",
    "    if frame_ms not in (10, 20, 30):\n",
    "        frame_ms = 30\n",
    "    frame_len = int(sr * (frame_ms / 1000.0))\n",
    "    if frame_len <= 0 or len(y) < frame_len:\n",
    "        return []\n",
    "    n_frames = int(math.ceil(len(y) / frame_len))\n",
    "    pad_samp = n_frames * frame_len - len(y)\n",
    "    if pad_samp > 0:\n",
    "        y = np.concatenate([y, np.zeros(pad_samp, dtype=np.float32)], axis=0)\n",
    "    pcm = d5_pcm16_bytes(y)\n",
    "\n",
    "    vad = webrtcvad.Vad(int(D5_VAD_MODE))\n",
    "    flags = []\n",
    "    for i in range(n_frames):\n",
    "        b0 = i * frame_len * 2\n",
    "        b1 = b0 + frame_len * 2\n",
    "        flags.append(vad.is_speech(pcm[b0:b1], sr))\n",
    "\n",
    "    segs = []\n",
    "    on = False\n",
    "    s0 = 0\n",
    "    for i, f in enumerate(flags):\n",
    "        if f and not on:\n",
    "            on = True\n",
    "            s0 = i\n",
    "        elif (not f) and on:\n",
    "            on = False\n",
    "            segs.append((s0 * frame_len, i * frame_len))\n",
    "    if on:\n",
    "        segs.append((s0 * frame_len, n_frames * frame_len))\n",
    "\n",
    "    n0 = len(y) - pad_samp if pad_samp > 0 else len(y)\n",
    "    return [(max(0, s), min(n0, e)) for s, e in segs]\n",
    "\n",
    "def d5_energy_segments(y: np.ndarray, sr: int) -> List[Tuple[int, int]]:\n",
    "    # Purpose: fallback segmentation when VAD is unavailable\n",
    "    frame = int(sr * 0.02)\n",
    "    hop = frame\n",
    "    if frame <= 0 or len(y) < frame:\n",
    "        return []\n",
    "    eng = []\n",
    "    idx = []\n",
    "    for i in range(0, len(y) - frame + 1, hop):\n",
    "        w = y[i:i + frame]\n",
    "        eng.append(float(np.mean(w * w)))\n",
    "        idx.append(i)\n",
    "    eng = np.array(eng, dtype=np.float32)\n",
    "    thr = float(np.percentile(eng, 25)) * 2.5\n",
    "    thr = max(thr, 1e-8)\n",
    "    keep = eng > thr\n",
    "\n",
    "    segs = []\n",
    "    in_seg = False\n",
    "    s0 = 0\n",
    "    for k, flag in enumerate(keep):\n",
    "        if flag and not in_seg:\n",
    "            in_seg = True\n",
    "            s0 = idx[k]\n",
    "        elif (not flag) and in_seg:\n",
    "            in_seg = False\n",
    "            segs.append((s0, idx[k] + frame))\n",
    "    if in_seg:\n",
    "        segs.append((s0, idx[-1] + frame))\n",
    "    return segs\n",
    "\n",
    "def d5_merge_and_filter(segs: List[Tuple[int, int]], sr: int) -> List[Tuple[int, int]]:\n",
    "    # Purpose: merge nearby segments and drop very short ones\n",
    "    if not segs:\n",
    "        return []\n",
    "    gap = int(sr * (D5_MERGE_GAP_MS / 1000.0))\n",
    "    min_len = int(sr * (D5_MIN_SPEECH_MS / 1000.0))\n",
    "    segs = sorted(segs)\n",
    "    merged = [list(segs[0])]\n",
    "    for s, e in segs[1:]:\n",
    "        if s - merged[-1][1] <= gap:\n",
    "            merged[-1][1] = max(merged[-1][1], e)\n",
    "        else:\n",
    "            merged.append([s, e])\n",
    "    out = []\n",
    "    for s, e in merged:\n",
    "        if (e - s) >= min_len:\n",
    "            out.append((int(s), int(e)))\n",
    "    return out\n",
    "\n",
    "def d5_pad_segs(segs: List[Tuple[int, int]], sr: int, n: int) -> List[Tuple[int, int]]:\n",
    "    # Purpose: add a small buffer around detected speech\n",
    "    pad = int(sr * D5_PAD_SEC)\n",
    "    out = []\n",
    "    for s, e in segs:\n",
    "        s2 = max(0, s - pad)\n",
    "        e2 = min(n, e + pad)\n",
    "        if e2 > s2:\n",
    "            out.append((s2, e2))\n",
    "    return out\n",
    "\n",
    "def d5_scale_for_vad(y: np.ndarray) -> np.ndarray:\n",
    "    # Purpose: improve VAD stability on extremely quiet recordings (does not change final audio)\n",
    "    peak = float(np.max(np.abs(y))) if len(y) else 0.0\n",
    "    if peak <= 0:\n",
    "        return y\n",
    "    if peak < 0.02:\n",
    "        gain = D5_VAD_SCALE_TARGET_PEAK / peak\n",
    "        gain = float(np.clip(gain, 1.0, 20.0))\n",
    "        return (y.astype(np.float32) * gain).astype(np.float32)\n",
    "    return y\n",
    "\n",
    "def d5_loudness_gate_segments(\n",
    "    y_pre_norm: np.ndarray,\n",
    "    segs: List[Tuple[int, int]],\n",
    "    sr: int,\n",
    "    require_sec: float\n",
    ") -> Tuple[List[Tuple[int, int]], Dict]:\n",
    "    # Purpose: keep loud segments first, relax threshold until enough audio is available\n",
    "    info = {\n",
    "        \"enabled\": bool(D5_ENABLE_LOUDNESS_GATE),\n",
    "        \"start_pctl\": int(D5_GATE_START_PCTL),\n",
    "        \"min_pctl\": int(D5_GATE_MIN_PCTL),\n",
    "        \"step_pctl\": int(D5_GATE_STEP_PCTL),\n",
    "        \"chosen_pctl\": None,\n",
    "        \"kept_segments\": 0,\n",
    "        \"total_kept_sec\": 0.0,\n",
    "        \"fallback\": False,\n",
    "    }\n",
    "    if (not D5_ENABLE_LOUDNESS_GATE) or (not segs):\n",
    "        info[\"fallback\"] = True\n",
    "        return segs, info\n",
    "\n",
    "    seg_rms_db = []\n",
    "    for (s, e) in segs:\n",
    "        seg_rms_db.append(d5_rms_dbfs(y_pre_norm[s:e]))\n",
    "    seg_rms_db = np.asarray(seg_rms_db, dtype=np.float32)\n",
    "\n",
    "    need = int(round(sr * require_sec))\n",
    "    for p in range(int(D5_GATE_START_PCTL), int(D5_GATE_MIN_PCTL) - 1, -int(D5_GATE_STEP_PCTL)):\n",
    "        thr = float(np.percentile(seg_rms_db, p))\n",
    "        keep_mask = seg_rms_db >= thr\n",
    "        kept = [segs[i] for i in range(len(segs)) if bool(keep_mask[i])]\n",
    "        total = sum((e - s) for (s, e) in kept)\n",
    "        if total >= need:\n",
    "            info[\"chosen_pctl\"] = int(p)\n",
    "            info[\"kept_segments\"] = int(len(kept))\n",
    "            info[\"total_kept_sec\"] = float(total / sr)\n",
    "            return kept, info\n",
    "\n",
    "    # Purpose: avoid dropping everything when audio is scarce\n",
    "    info[\"fallback\"] = True\n",
    "    info[\"kept_segments\"] = int(len(segs))\n",
    "    info[\"total_kept_sec\"] = float(sum((e - s) for (s, e) in segs) / sr)\n",
    "    return segs, info\n",
    "\n",
    "def d5_stitch_voiced(y_pre_norm: np.ndarray, sr: int) -> Tuple[np.ndarray, str, Dict]:\n",
    "    # Inputs: pre-normalization audio\n",
    "    # Outputs: stitched audio, method label, and gate stats for logging\n",
    "    y_for_vad = d5_scale_for_vad(y_pre_norm)\n",
    "\n",
    "    segs = d5_webrtc_segments(y_for_vad, sr)\n",
    "    if segs is None:\n",
    "        segs = d5_energy_segments(y_for_vad, sr)\n",
    "        used = \"energy\"\n",
    "    else:\n",
    "        used = \"webrtcvad\"\n",
    "\n",
    "    segs = d5_merge_and_filter(segs, sr)\n",
    "    segs = d5_pad_segs(segs, sr, len(y_pre_norm))\n",
    "\n",
    "    if not segs:\n",
    "        return y_pre_norm.astype(np.float32, copy=False), used, {\"enabled\": bool(D5_ENABLE_LOUDNESS_GATE), \"fallback\": True}\n",
    "\n",
    "    segs2, gate_info = d5_loudness_gate_segments(y_pre_norm, segs, sr, require_sec=D5_GATE_REQUIRE_SEC)\n",
    "    if not segs2:\n",
    "        segs2 = segs\n",
    "        gate_info[\"fallback\"] = True\n",
    "\n",
    "    stitched = np.concatenate([y_pre_norm[s:e] for (s, e) in segs2], axis=0).astype(np.float32, copy=False)\n",
    "    return stitched, used, gate_info\n",
    "\n",
    "def d5_first_full_window(voiced: np.ndarray, sr: int, sec: float) -> Optional[np.ndarray]:\n",
    "    # Purpose: enforce the single-clip rule (must have full 8.0 seconds)\n",
    "    need = int(round(sr * sec))\n",
    "    if len(voiced) < need:\n",
    "        return None\n",
    "    return voiced[:need].astype(np.float32, copy=False)\n",
    "\n",
    "# -------------------------\n",
    "# Dataset scan and label parsing\n",
    "# Inputs: raw WAV file paths\n",
    "# Outputs: base_df with speaker_id, label, task, and source paths\n",
    "# -------------------------\n",
    "def d5_guess_task_from_path(p: Path) -> str:\n",
    "    parts = [x.lower() for x in p.parts]\n",
    "    if \"readtext\" in parts:\n",
    "        return \"read\"\n",
    "    if \"spontaneousdialogue\" in parts or any(\"spont\" in x for x in parts):\n",
    "        return \"spont\"\n",
    "    return \"unk\"\n",
    "\n",
    "D5_ID_RE = re.compile(r\"(?:^|_)(id)(\\d+)\", re.IGNORECASE)\n",
    "def d5_guess_speaker_id(p: Path) -> Optional[str]:\n",
    "    # Purpose: extract ID## pattern used by this dataset\n",
    "    m = D5_ID_RE.search(p.stem)\n",
    "    if not m:\n",
    "        return None\n",
    "    return f\"ID{int(m.group(2)):02d}\"\n",
    "\n",
    "def d5_guess_group_hc_pd(p: Path) -> str:\n",
    "    # Purpose: infer HC/PD from folder names or filename tokens\n",
    "    parts = [x.lower() for x in p.parts]\n",
    "    if \"pd\" in parts:\n",
    "        return \"PD\"\n",
    "    if \"hc\" in parts:\n",
    "        return \"HC\"\n",
    "    s = p.stem.lower()\n",
    "    if re.search(r\"(?:^|_)pd(?:_|$)\", s):\n",
    "        return \"PD\"\n",
    "    if re.search(r\"(?:^|_)hc(?:_|$)\", s):\n",
    "        return \"HC\"\n",
    "    return \"Unknown\"\n",
    "\n",
    "wav_paths = sorted(Path(D5_DIR).rglob(\"*.wav\"))\n",
    "print(\"\\nD5 total WAV files found:\", len(wav_paths))\n",
    "\n",
    "rows = []\n",
    "for p in tqdm(wav_paths, desc=\"D5 scan\", dynamic_ncols=True):\n",
    "    spk = d5_guess_speaker_id(p)\n",
    "    grp = d5_guess_group_hc_pd(p)\n",
    "    task = d5_guess_task_from_path(p)\n",
    "\n",
    "    # Purpose: keep only files that can be assigned speaker, label, and task\n",
    "    if spk is None:\n",
    "        continue\n",
    "    if grp not in (\"HC\", \"PD\"):\n",
    "        continue\n",
    "\n",
    "    label_str = \"Healthy\" if grp == \"HC\" else \"Parkinson\"\n",
    "    label_num = 0 if grp == \"HC\" else 1\n",
    "\n",
    "    rows.append({\n",
    "        \"dataset\": \"D5\",\n",
    "        \"speaker_id\": spk,\n",
    "        \"audio_path\": str(p),\n",
    "        \"task\": task,\n",
    "        \"group_code\": grp,\n",
    "        \"label_str\": label_str,\n",
    "        \"label_num\": int(label_num),\n",
    "        \"sample_id\": p.name,\n",
    "    })\n",
    "\n",
    "base_df = pd.DataFrame(rows)\n",
    "print(\"D5 usable rows after scan:\", len(base_df))\n",
    "if len(base_df) == 0:\n",
    "    raise RuntimeError(\"No usable D5 WAV rows found after parsing (speaker_id + HC/PD + task).\")\n",
    "\n",
    "print(\"Label counts (rows):\")\n",
    "print(base_df[\"label_str\"].value_counts(dropna=False))\n",
    "print(\"Task counts (rows):\")\n",
    "print(base_df[\"task\"].value_counts(dropna=False))\n",
    "\n",
    "# -------------------------\n",
    "# Source processing → immediate candidate write (one candidate per source)\n",
    "# Inputs: base_df rows\n",
    "# Outputs: candidate WAVs in clips/_candidates/ and cand_df metadata table\n",
    "# -------------------------\n",
    "cand_meta: List[Dict] = []\n",
    "warn_rows: List[Dict] = []\n",
    "\n",
    "global_cand_counter = 0\n",
    "proc_df = base_df.sort_values([\"speaker_id\", \"task\", \"audio_path\"]).reset_index(drop=True)\n",
    "\n",
    "pbar = tqdm(\n",
    "    proc_df.itertuples(index=False),\n",
    "    total=len(proc_df),\n",
    "    desc=\"D5 preprocess (1 clip per source; gate+stitch; write candidates)\",\n",
    "    dynamic_ncols=True\n",
    ")\n",
    "\n",
    "for r in pbar:\n",
    "    try:\n",
    "        # Pipeline: read → resample → skip lead-in → optional high-pass\n",
    "        y, sr0 = d5_read_mono(r.audio_path)\n",
    "        y = d5_resample(y, sr0, D5_SR)\n",
    "        y = d5_crop_skip(y, D5_SR, D5_SKIP_SEC)\n",
    "\n",
    "        if len(y) == 0:\n",
    "            warn_rows.append({\n",
    "                \"dataset\": \"D5\", \"speaker_id\": r.speaker_id, \"source_path\": r.audio_path,\n",
    "                \"warning_type\": \"skip_after_initial_trim_empty\",\n",
    "                \"detail\": f\"skip_sec={D5_SKIP_SEC}\",\n",
    "            })\n",
    "            continue\n",
    "\n",
    "        if D5_ENABLE_HIGHPASS and D5_HAVE_SCIPY:\n",
    "            y = d5_apply_highpass(y, D5_SR)\n",
    "\n",
    "        # Key step: gate and stitch before normalization to avoid amplifying quiet background voices\n",
    "        stitched, seg_used, gate_info = d5_stitch_voiced(y, D5_SR)\n",
    "\n",
    "        if len(stitched) < int(D5_SR * D5_MIN_KEEP_SEC):\n",
    "            warn_rows.append({\n",
    "                \"dataset\": \"D5\", \"speaker_id\": r.speaker_id, \"source_path\": r.audio_path,\n",
    "                \"warning_type\": \"stitched_too_short\",\n",
    "                \"detail\": f\"seg_used={seg_used}, stitched_sec={len(stitched)/D5_SR:.3f}\",\n",
    "            })\n",
    "            continue\n",
    "\n",
    "        # Normalize only after the final stitched stream is chosen\n",
    "        stitched = d5_norm_rms_then_peak(stitched)\n",
    "\n",
    "        # Single-clip rule: must have a full 8 seconds, otherwise drop this source\n",
    "        one = d5_first_full_window(stitched, D5_SR, D5_CLIP_SEC)\n",
    "        if one is None:\n",
    "            warn_rows.append({\n",
    "                \"dataset\": \"D5\", \"speaker_id\": r.speaker_id, \"source_path\": r.audio_path,\n",
    "                \"warning_type\": \"too_short_for_full_clip\",\n",
    "                \"detail\": f\"stitched_sec={len(stitched)/D5_SR:.3f} < {D5_CLIP_SEC}; gate={gate_info}\",\n",
    "            })\n",
    "            continue\n",
    "\n",
    "        # Write candidate immediately (staging file)\n",
    "        global_cand_counter += 1\n",
    "        cand_name = d5_safe(f\"CAND_{global_cand_counter:08d}.wav\")\n",
    "        cand_path = os.path.join(D5_CAND_DIR, cand_name)\n",
    "        d5_safe_write_wav(cand_path, one, D5_SR)\n",
    "\n",
    "        cand_meta.append({\n",
    "            \"dataset\": \"D5\",\n",
    "            \"speaker_id\": str(r.speaker_id),\n",
    "            \"task\": d5_task_short(r.task),\n",
    "            \"label_str\": str(r.label_str),\n",
    "            \"label_num\": int(r.label_num),\n",
    "            \"sample_id\": str(r.sample_id),\n",
    "            \"source_path\": str(r.audio_path),\n",
    "            \"group_code\": str(r.group_code),\n",
    "            \"duration_sec\": float(len(one) / D5_SR),\n",
    "            \"clip_path_cand\": cand_path,\n",
    "            \"segmentation_used\": seg_used,\n",
    "            \"loudness_gate_info\": json.dumps(gate_info),\n",
    "        })\n",
    "\n",
    "    except Exception as e:\n",
    "        warn_rows.append({\n",
    "            \"dataset\": \"D5\",\n",
    "            \"speaker_id\": getattr(r, \"speaker_id\", \"\"),\n",
    "            \"source_path\": getattr(r, \"audio_path\", \"\"),\n",
    "            \"warning_type\": \"preprocess_error\",\n",
    "            \"detail\": repr(e),\n",
    "        })\n",
    "\n",
    "print(\"\\nD5 candidates written:\", len(cand_meta))\n",
    "if len(cand_meta) == 0:\n",
    "    raise RuntimeError(\"No D5 candidates produced. Check input audio and parsing.\")\n",
    "\n",
    "cand_df = pd.DataFrame(cand_meta)\n",
    "\n",
    "# -------------------------\n",
    "# Cap candidates per speaker and task (then delete unkept candidates)\n",
    "# Inputs: candidate table + candidate files\n",
    "# Outputs: cap_df (kept candidates), deleted candidate files removed from staging\n",
    "# -------------------------\n",
    "rng = np.random.default_rng(D5_RANDOM_SEED)\n",
    "keep_idx: List[int] = []\n",
    "\n",
    "for (spk, task), g in cand_df.groupby([\"speaker_id\", \"task\"], sort=False):\n",
    "    idx = g.index.to_numpy()\n",
    "    if len(idx) <= D5_MAX_CLIPS_PER_SPK_TASK:\n",
    "        keep_idx.extend(idx.tolist())\n",
    "    else:\n",
    "        chosen = rng.choice(idx, size=D5_MAX_CLIPS_PER_SPK_TASK, replace=False)\n",
    "        keep_idx.extend(chosen.tolist())\n",
    "\n",
    "keep_idx = sorted(set(keep_idx))\n",
    "cap_df = cand_df.loc[keep_idx].reset_index(drop=True)\n",
    "\n",
    "to_delete = cand_df.loc[~cand_df.index.isin(keep_idx), \"clip_path_cand\"].tolist()\n",
    "deleted = 0\n",
    "for p in to_delete:\n",
    "    try:\n",
    "        if os.path.exists(p):\n",
    "            os.remove(p)\n",
    "            deleted += 1\n",
    "    except Exception as e:\n",
    "        warn_rows.append({\n",
    "            \"dataset\": \"D5\",\n",
    "            \"speaker_id\": \"\",\n",
    "            \"source_path\": \"\",\n",
    "            \"warning_type\": \"candidate_delete_failed\",\n",
    "            \"detail\": f\"{p} :: {repr(e)}\",\n",
    "        })\n",
    "\n",
    "print(\"D5 clips after cap:\", int(len(cap_df)))\n",
    "print(\"Deleted unkept candidates:\", deleted)\n",
    "\n",
    "# -------------------------\n",
    "# Speaker-level split (stratified by label_str)\n",
    "# Inputs: unique speakers from kept candidates\n",
    "# Outputs: split assignment merged back onto cap_df\n",
    "# -------------------------\n",
    "spk_tbl = cap_df[[\"speaker_id\", \"label_str\"]].drop_duplicates().copy()\n",
    "rng_py = random.Random(D5_RANDOM_SEED)\n",
    "\n",
    "split_rows = []\n",
    "for lab_name, g in spk_tbl.groupby(\"label_str\"):\n",
    "    spks = g[\"speaker_id\"].tolist()\n",
    "    rng_py.shuffle(spks)\n",
    "    n = len(spks)\n",
    "\n",
    "    n_train = int(round(n * D5_TRAIN_PCT))\n",
    "    n_val = int(round(n * D5_VAL_PCT))\n",
    "    n_train = min(n_train, n)\n",
    "    n_val = min(n_val, n - n_train)\n",
    "\n",
    "    train = spks[:n_train]\n",
    "    val = spks[n_train:n_train + n_val]\n",
    "    test = spks[n_train + n_val:]\n",
    "\n",
    "    split_rows += [{\"speaker_id\": s, \"split\": \"train\"} for s in train]\n",
    "    split_rows += [{\"speaker_id\": s, \"split\": \"val\"} for s in val]\n",
    "    split_rows += [{\"speaker_id\": s, \"split\": \"test\"} for s in test]\n",
    "\n",
    "spk_split = pd.DataFrame(split_rows)\n",
    "cap_df[\"speaker_id\"] = cap_df[\"speaker_id\"].astype(str)\n",
    "spk_split[\"speaker_id\"] = spk_split[\"speaker_id\"].astype(str)\n",
    "cap_df = cap_df.merge(spk_split, on=\"speaker_id\", how=\"left\", validate=\"many_to_one\")\n",
    "\n",
    "if cap_df[\"split\"].isna().any():\n",
    "    ex = cap_df.loc[cap_df[\"split\"].isna(), \"speaker_id\"].drop_duplicates().head(10).tolist()\n",
    "    raise RuntimeError(f\"Some speakers did not get a split assignment. Example: {ex}\")\n",
    "\n",
    "print(\"\\nSplit counts (capped candidates):\")\n",
    "print(cap_df[\"split\"].value_counts())\n",
    "\n",
    "# -------------------------\n",
    "# Finalize: move candidates into clips/<split>/ and build manifest rows\n",
    "# Inputs: kept candidate WAVs + split assignment\n",
    "# Outputs: final WAVs in clips/<split>/ and manifest_df rows pointing to final paths\n",
    "# -------------------------\n",
    "manifest_rows: List[Dict] = []\n",
    "global_final_idx = 0\n",
    "\n",
    "pbar2 = tqdm(cap_df.itertuples(index=False), total=len(cap_df),\n",
    "             desc=\"D5 finalize (move kept)\", dynamic_ncols=True)\n",
    "\n",
    "for r in pbar2:\n",
    "    global_final_idx += 1\n",
    "    tag = d5_label_to_tag(r.label_str)\n",
    "\n",
    "    # Purpose: stable, readable filenames for downstream training and debugging\n",
    "    out_name = d5_safe(f\"D5_{tag}_{r.speaker_id}_{r.task}_{global_final_idx:06d}.wav\")\n",
    "    out_path = os.path.join(D5_CLIPS_DIR, r.split, out_name)\n",
    "\n",
    "    cand_path = getattr(r, \"clip_path_cand\")\n",
    "    if not os.path.exists(cand_path):\n",
    "        warn_rows.append({\n",
    "            \"dataset\": \"D5\",\n",
    "            \"speaker_id\": r.speaker_id,\n",
    "            \"source_path\": r.source_path,\n",
    "            \"warning_type\": \"missing_candidate_file\",\n",
    "            \"detail\": cand_path,\n",
    "        })\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        shutil.move(cand_path, out_path)\n",
    "    except Exception as e:\n",
    "        warn_rows.append({\n",
    "            \"dataset\": \"D5\",\n",
    "            \"speaker_id\": r.speaker_id,\n",
    "            \"source_path\": r.source_path,\n",
    "            \"warning_type\": \"move_error\",\n",
    "            \"detail\": repr(e),\n",
    "        })\n",
    "        continue\n",
    "\n",
    "    manifest_rows.append({\n",
    "        \"split\": r.split,\n",
    "        \"dataset\": \"D5\",\n",
    "        \"task\": r.task,\n",
    "        \"speaker_id\": r.speaker_id,\n",
    "        \"speaker_key_rel\": np.nan,\n",
    "        \"sample_id\": r.sample_id,\n",
    "        \"label_str\": r.label_str,\n",
    "        \"label_num\": int(r.label_num),\n",
    "        \"age\": np.nan,\n",
    "        \"sex\": np.nan,\n",
    "        \"clip_path\": out_path,\n",
    "        \"duration_sec\": float(r.duration_sec),\n",
    "        \"source_path\": r.source_path,\n",
    "        \"clip_start_sec\": np.nan,\n",
    "        \"clip_end_sec\": np.nan,\n",
    "        \"clip_is_contiguous\": False,  # stitched stream, not a single contiguous region in the source\n",
    "        \"sr_hz\": int(D5_SR),\n",
    "        \"channels\": 1,\n",
    "    })\n",
    "\n",
    "# Purpose: remove temporary staging directory when done\n",
    "try:\n",
    "    if os.path.isdir(D5_CAND_DIR):\n",
    "        leftovers = list(os.scandir(D5_CAND_DIR))\n",
    "        for ent in leftovers:\n",
    "            try:\n",
    "                os.remove(ent.path)\n",
    "            except Exception:\n",
    "                pass\n",
    "        try:\n",
    "            os.rmdir(D5_CAND_DIR)\n",
    "        except Exception:\n",
    "            pass\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "manifest_df = pd.DataFrame(manifest_rows)\n",
    "warnings_df = pd.DataFrame(warn_rows)\n",
    "\n",
    "# Purpose: enforce canonical schema and column order\n",
    "for c in MANIFEST_COLS:\n",
    "    if c not in manifest_df.columns:\n",
    "        manifest_df[c] = np.nan\n",
    "manifest_df = manifest_df[MANIFEST_COLS].copy()\n",
    "\n",
    "# -------------------------\n",
    "# Save required artifacts (manifest, warnings, summary, run config)\n",
    "# Outputs: CSV/JSON files under manifests/, logs/, config/\n",
    "# -------------------------\n",
    "manifest_all_path = os.path.join(D5_MANIFEST_DIR, \"manifest_all.csv\")\n",
    "warnings_path     = os.path.join(D5_LOGS_DIR, \"preprocess_warnings.csv\")\n",
    "summary_path      = os.path.join(D5_LOGS_DIR, \"dataset_summary.json\")\n",
    "run_cfg_path      = os.path.join(D5_CONFIG_DIR, \"run_config.json\")\n",
    "\n",
    "manifest_df.to_csv(manifest_all_path, index=False)\n",
    "warnings_df.to_csv(warnings_path, index=False)\n",
    "\n",
    "summary = {\n",
    "    \"dataset\": \"D5\",\n",
    "    \"source_root\": D5_DIR,\n",
    "    \"sr_hz\": int(D5_SR),\n",
    "    \"webrtcvad_available\": bool(D5_HAVE_WEBRTCVAD),\n",
    "    \"scipy_available\": bool(D5_HAVE_SCIPY),\n",
    "    \"policies\": {\n",
    "        \"skip_initial_sec\": float(D5_SKIP_SEC),\n",
    "        \"highpass\": {\"enabled\": bool(D5_ENABLE_HIGHPASS and D5_HAVE_SCIPY), \"hz\": float(D5_HIGHPASS_HZ)},\n",
    "        \"segmentation\": \"webrtcvad if available else energy\",\n",
    "        \"loudness_gate\": {\n",
    "            \"enabled\": bool(D5_ENABLE_LOUDNESS_GATE),\n",
    "            \"start_percentile\": int(D5_GATE_START_PCTL),\n",
    "            \"min_percentile\": int(D5_GATE_MIN_PCTL),\n",
    "            \"step_percentile\": int(D5_GATE_STEP_PCTL),\n",
    "            \"require_sec\": float(D5_GATE_REQUIRE_SEC),\n",
    "            \"note\": \"gate and stitch on pre-normalization audio; normalize after stitching\",\n",
    "        },\n",
    "        \"normalization\": \"manual RMS gain + peak limiting; no pyloudnorm\",\n",
    "        \"clip\": {\n",
    "            \"one_clip_per_source\": True,\n",
    "            \"sec\": float(D5_CLIP_SEC),\n",
    "            \"selection_rule\": \"first full 8 s from gated+stitched stream; drop if shorter\",\n",
    "        },\n",
    "        \"cap\": {\"grouping\": [\"speaker_id\", \"task\"], \"max_per_group\": int(D5_MAX_CLIPS_PER_SPK_TASK)},\n",
    "        \"split\": {\"unit\": \"speaker\", \"train\": D5_TRAIN_PCT, \"val\": D5_VAL_PCT, \"test\": D5_TEST_PCT, \"stratified_by\": \"label_str\"},\n",
    "        \"clip_is_contiguous\": False,\n",
    "    },\n",
    "    \"counts\": {\n",
    "        \"n_raw_wavs_found\": int(len(wav_paths)),\n",
    "        \"n_rows_after_scan\": int(len(base_df)),\n",
    "        \"n_candidates_before_cap\": int(len(cand_df)),\n",
    "        \"n_candidates_after_cap\": int(len(cap_df)),\n",
    "        \"n_clips_written\": int(len(manifest_df)),\n",
    "        \"split_counts_clips\": manifest_df[\"split\"].value_counts(dropna=False).to_dict() if len(manifest_df) else {},\n",
    "        \"label_counts_clips\": manifest_df[\"label_str\"].value_counts(dropna=False).to_dict() if len(manifest_df) else {},\n",
    "        \"task_counts_clips\": manifest_df[\"task\"].value_counts(dropna=False).to_dict() if len(manifest_df) else {},\n",
    "        \"n_warnings\": int(len(warnings_df)),\n",
    "    },\n",
    "}\n",
    "\n",
    "with open(summary_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "run_cfg = {\n",
    "    \"dataset\": \"D5\",\n",
    "    \"paths\": {\n",
    "        \"dataset_dir\": D5_DIR,\n",
    "        \"out_root\": D5_OUT_ROOT,\n",
    "        \"clips_dir\": D5_CLIPS_DIR,\n",
    "        \"candidate_dir\": D5_CAND_DIR,\n",
    "        \"manifest_all\": manifest_all_path,\n",
    "        \"warnings_csv\": warnings_path,\n",
    "        \"summary_json\": summary_path,\n",
    "    },\n",
    "    \"audio\": {\n",
    "        \"sr_hz\": int(D5_SR),\n",
    "        \"skip_initial_sec\": float(D5_SKIP_SEC),\n",
    "        \"highpass\": {\"enabled\": bool(D5_ENABLE_HIGHPASS), \"hz\": float(D5_HIGHPASS_HZ), \"order\": int(D5_HIGHPASS_ORDER)},\n",
    "        \"vad\": {\"mode\": int(D5_VAD_MODE), \"frame_ms\": int(D5_FRAME_MS), \"min_speech_ms\": int(D5_MIN_SPEECH_MS), \"merge_gap_ms\": int(D5_MERGE_GAP_MS), \"pad_sec\": float(D5_PAD_SEC)},\n",
    "        \"loudness_gate\": {\"enabled\": bool(D5_ENABLE_LOUDNESS_GATE), \"start_pctl\": int(D5_GATE_START_PCTL), \"min_pctl\": int(D5_GATE_MIN_PCTL), \"step_pctl\": int(D5_GATE_STEP_PCTL)},\n",
    "        \"normalization\": {\"target_rms_dbfs\": float(D5_TARGET_RMS_DBFS), \"peak_limit_dbfs\": float(D5_PEAK_LIMIT_DBFS), \"min_rms_dbfs\": float(D5_MIN_RMS_DBFS), \"max_gain_db\": float(D5_MAX_GAIN_DB)},\n",
    "        \"clip_sec\": float(D5_CLIP_SEC),\n",
    "        \"one_per_source\": True,\n",
    "    },\n",
    "    \"split\": {\"train\": D5_TRAIN_PCT, \"val\": D5_VAL_PCT, \"test\": D5_TEST_PCT, \"unit\": \"speaker\"},\n",
    "    \"cap\": {\"max_per_speaker_task\": int(D5_MAX_CLIPS_PER_SPK_TASK)},\n",
    "    \"seed\": int(D5_RANDOM_SEED),\n",
    "}\n",
    "\n",
    "with open(run_cfg_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(run_cfg, f, indent=2)\n",
    "\n",
    "print(\"\\nDONE: D5 preprocessing\")\n",
    "print(\"Manifest:\", manifest_all_path)\n",
    "print(\"Warnings:\", warnings_path)\n",
    "print(\"Summary:\", summary_path)\n",
    "print(\"Run config:\", run_cfg_path)\n",
    "print(\"Clips written:\", int(len(manifest_df)))\n",
    "\n",
    "# -------------------------\n",
    "# Quick sanity prints\n",
    "# Purpose: confirm uniform audio format and clean speaker split\n",
    "# -------------------------\n",
    "print(\"\\nSanity checks:\")\n",
    "print(\"Unique SR:\", sorted(manifest_df[\"sr_hz\"].unique().tolist()) if len(manifest_df) else [])\n",
    "print(\"Unique channels:\", sorted(manifest_df[\"channels\"].unique().tolist()) if len(manifest_df) else [])\n",
    "print(\"Speakers per split x label (by speaker):\")\n",
    "if len(manifest_df):\n",
    "    print(\n",
    "        manifest_df[[\"speaker_id\", \"split\", \"label_str\"]]\n",
    "        .drop_duplicates()\n",
    "        .groupby([\"split\", \"label_str\"])\n",
    "        .size()\n",
    "    )\n",
    "spk_split_chk = manifest_df[[\"speaker_id\", \"split\"]].drop_duplicates()\n",
    "dup = spk_split_chk.groupby(\"speaker_id\")[\"split\"].nunique()\n",
    "bad = dup[dup > 1]\n",
    "print(\"Speakers appearing in multiple splits:\", int(len(bad)))\n",
    "if len(bad) > 0:\n",
    "    print(\"Example bad speaker ids:\", bad.index.tolist()[:10])\n",
    "    raise RuntimeError(\"Speaker appears in more than one split.\")"
   ],
   "metadata": {
    "id": "BBuWacXdGzBa",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 964,
     "referenced_widgets": [
      "a2da968c0b53455f8b53b9f44287b563",
      "5462d5cb837640768748a9ae28b8d098",
      "1bca69e839b547208551208f7c388200",
      "f7119bb64ea84011a859a79872624c5c",
      "e23b6951ed034bb8a1aca5d0cfa1c1c4",
      "749acf86f4814d87848c2f1077081be1",
      "b9e4ceb361614871b446d902a2af386f",
      "3729d3182e184d5281d731308f94020e",
      "1e18f3425d4a450c943d479822a70203",
      "5d2e666551464da08fc4523682caa663",
      "3b77bd79f7dc40fe9c83c0bc11c1f293",
      "45cc4708659048d2a9ef2368b170cc8f",
      "596874dec79c4bb4bd13c5e63036d43f",
      "83de4fb1d1e2464c962bc6f024332395",
      "8f71497ec0ed4cdca37b76c13914f2d6",
      "ac59418b113946ccb4b232ebbcbd03c9",
      "14a426c781984cd29f3009cb11db01a0",
      "76130128282b4d44b75954638853ac93",
      "9e43d78fdb444a198264f24ffb26448f",
      "90e39cc18af94d119c8e414694c3b9c8",
      "0711e5fab8304ecd8b8f3601490d4be4",
      "9dde82bd69c64fe7a14ba4a25392e9ab",
      "be7a321dcbf3425088e22b18da99186c",
      "c9ee59cf039249789e03008be77ed179",
      "ebf98973ad484dbfb763ef9fbea985ef",
      "6c0dd6a96bac4c31b54565da96aa9c1d",
      "506e218e50074d64a163056e7979f313",
      "7694560c9fdb4cfca0b3acf223043bde",
      "7e509a86ff2d4f79896e15d39943cb71",
      "efe22a711f64404b9221cb023e807a9d",
      "e760c31ee89f4e489733c3e520fa0024",
      "904f8d2387d741d6a175dabd62c50c28",
      "f8cd80fe9e87412baf69b2d220a6aad6"
     ]
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "The following cell rebuilds Dataset D5 into a new `preprocessed_v2` folder by changing **only** the train, validation, and test split. The audio itself is not modified or reprocessed. All clips created earlier in `preprocessed_v1` are reused exactly as they are, with filenames kept exactly the same. Only the split assignment and folder location are updated. This makes it possible to test a new split setup while keeping the audio data identical.\n",
    "\n",
    "The cell begins by mounting Google Drive if needed, defining the D5 input and output paths, and creating the full `preprocessed_v2` folder structure (`clips/train`, `clips/val`, `clips/test`, `manifests`, `logs`, and `config`). It checks that the original v1 manifest exists and prints the source and destination paths so it is clear which dataset version is being used.\n",
    "\n",
    "Next, a new **speaker level split policy** is defined using a 50% train, 20% validation, and 30% test split, with a fixed random seed to ensure the results can be reproduced. Helper functions are prepared to safely write files, copy or move audio clips with retries, and ensure that all JSON and CSV outputs are written in a safe and consistent way.\n",
    "\n",
    "The cell then loads the **v1 manifest**, verifies that all required columns are present, filters strictly to D5 entries, and converts any literal `\"NaN\"` strings into true missing values. Before continuing, it checks that every audio file listed in the v1 manifest actually exists on disk.\n",
    "\n",
    "A new **speaker level split** is created and stratified by label (Healthy versus Parkinson’s), ensuring that each speaker appears in only one of the train, validation, or test splits. This new split information is merged back into the clip level table, replacing the old split assignments from v1.\n",
    "\n",
    "For each clip, the cell enforces a strict file transfer rule. If the clip already exists in the correct v2 split folder with the same file size, it is reused. If it exists in a different v2 split folder from a previous run, it is moved. Otherwise, it is copied from the v1 location. This guarantees that **each filename exists in exactly one split folder** in v2 and avoids leftover duplicates.\n",
    "\n",
    "After all files are transferred, the manifest is rewritten so that `clip_path` points to the new v2 locations, while all other metadata remains unchanged. The cell then writes the required outputs: a new `manifest_all.csv`, a warnings file for any transfer issues, a CSV listing the speaker split used, a dataset summary JSON with counts and split details, and a run configuration JSON that documents the rebuild policy.\n",
    "\n",
    "Finally, the cell runs thorough checks to confirm that no speaker appears in more than one split, every manifest entry points to an existing file, and no filename appears in multiple split folders. A clear summary is printed showing clip counts, speaker counts by split and label, and any warnings that were recorded."
   ],
   "metadata": {
    "id": "2p1KaQ7Z3mkz"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# ============================================================\n",
    "# D5 Split-Only Rebuild → preprocessed_v2 (50/20/30 speaker split)\n",
    "# Inputs: preprocessed_v1/manifests/manifest_all.csv + existing v1 clips\n",
    "# Outputs: preprocessed_v2/clips/<split>/ (same filenames), manifests/manifest_all.csv,\n",
    "#          logs/preprocess_warnings.csv + dataset_summary.json, config/run_config.json\n",
    "# Notes: No audio work. Only speaker split and file relocation.\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import shutil\n",
    "import random\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# -------------------------\n",
    "# Drive mount check\n",
    "# Purpose: ensure preprocessed_v1 and preprocessed_v2 paths are reachable in Colab\n",
    "# -------------------------\n",
    "if not os.path.isdir(\"/content/drive/MyDrive\"):\n",
    "    from google.colab import drive  # type: ignore\n",
    "    drive.mount(\"/content/drive\")\n",
    "\n",
    "# -------------------------\n",
    "# Dataset paths and output folders\n",
    "# Inputs: v1 manifest and v1 clip files\n",
    "# Outputs: v2 folder tree (clips/manifests/config/logs)\n",
    "# -------------------------\n",
    "D5_PROJECT_DIR   = \"/content/drive/MyDrive/AI_PD_Project\"\n",
    "D5_DATASET_DIR   = f\"{D5_PROJECT_DIR}/Datasets/D5-English (MDVR-KCL)\"\n",
    "\n",
    "D5_V1_ROOT       = Path(f\"{D5_DATASET_DIR}/preprocessed_v1\")\n",
    "D5_V2_ROOT       = Path(f\"{D5_DATASET_DIR}/preprocessed_v2\")\n",
    "\n",
    "D5_V1_MANIFEST   = D5_V1_ROOT / \"manifests\" / \"manifest_all.csv\"\n",
    "\n",
    "D5_V2_CLIPS_DIR  = D5_V2_ROOT / \"clips\"\n",
    "D5_V2_MAN_DIR    = D5_V2_ROOT / \"manifests\"\n",
    "D5_V2_CFG_DIR    = D5_V2_ROOT / \"config\"\n",
    "D5_V2_LOGS_DIR   = D5_V2_ROOT / \"logs\"\n",
    "\n",
    "# Purpose: create v2 structure without touching v1\n",
    "for p in [D5_V2_ROOT, D5_V2_CLIPS_DIR, D5_V2_MAN_DIR, D5_V2_CFG_DIR, D5_V2_LOGS_DIR]:\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "for sp in [\"train\", \"val\", \"test\"]:\n",
    "    (D5_V2_CLIPS_DIR / sp).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Purpose: fail early if v1 manifest is missing\n",
    "if not D5_V1_MANIFEST.exists():\n",
    "    raise FileNotFoundError(f\"Missing D5 v1 manifest: {str(D5_V1_MANIFEST)}\")\n",
    "\n",
    "print(\"D5 v1 manifest:\", str(D5_V1_MANIFEST))\n",
    "print(\"D5 v2 out root:\", str(D5_V2_ROOT))\n",
    "\n",
    "# -------------------------\n",
    "# Split configuration (speaker-level)\n",
    "# Purpose: rebuild only the train/val/test speaker assignment\n",
    "# -------------------------\n",
    "D5_SEED = 1337\n",
    "D5_TRAIN_PCT, D5_VAL_PCT, D5_TEST_PCT = 0.50, 0.20, 0.30\n",
    "\n",
    "# Purpose: make Drive file transfer more reliable\n",
    "COPY_RETRIES = 4\n",
    "COPY_SLEEP   = 0.5\n",
    "\n",
    "# -------------------------\n",
    "# Canonical manifest column order\n",
    "# Purpose: keep the manifest consistent across datasets/runs\n",
    "# -------------------------\n",
    "MANIFEST_COLS = [\n",
    "    \"split\",\"dataset\",\"task\",\"speaker_id\",\"speaker_key_rel\",\"sample_id\",\"label_str\",\"label_num\",\"age\",\"sex\",\n",
    "    \"clip_path\",\"duration_sec\",\"source_path\",\"clip_start_sec\",\"clip_end_sec\",\"clip_is_contiguous\",\"sr_hz\",\"channels\",\n",
    "]\n",
    "\n",
    "# -------------------------\n",
    "# Safe writers and transfer helpers\n",
    "# Purpose: avoid partial files and handle already-copied runs cleanly\n",
    "# -------------------------\n",
    "def atomic_write_text(path: Path, text: str):\n",
    "    # Write to temp and replace to reduce risk of partial outputs\n",
    "    tmp = path.with_suffix(path.suffix + \".tmp\")\n",
    "    with open(tmp, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(text)\n",
    "    os.replace(tmp, path)\n",
    "\n",
    "def atomic_write_json(path: Path, obj: dict):\n",
    "    atomic_write_text(path, json.dumps(obj, indent=2))\n",
    "\n",
    "def atomic_write_csv(path: Path, df: pd.DataFrame):\n",
    "    # NaN values stay blank in CSV (no literal \"NaN\")\n",
    "    tmp = path.with_suffix(path.suffix + \".tmp\")\n",
    "    df.to_csv(tmp, index=False, na_rep=\"\")\n",
    "    os.replace(tmp, path)\n",
    "\n",
    "def json_safe(obj):\n",
    "    # Convert numpy types and tuple keys so JSON dumps cleanly\n",
    "    if isinstance(obj, dict):\n",
    "        out = {}\n",
    "        for k, v in obj.items():\n",
    "            if isinstance(k, tuple):\n",
    "                k2 = \" | \".join([str(x) for x in k])\n",
    "            else:\n",
    "                k2 = str(k) if not isinstance(k, (str, int, float, bool)) and k is not None else k\n",
    "            out[k2] = json_safe(v)\n",
    "        return out\n",
    "    if isinstance(obj, (list, tuple)):\n",
    "        return [json_safe(x) for x in obj]\n",
    "    if isinstance(obj, (np.integer,)):\n",
    "        return int(obj)\n",
    "    if isinstance(obj, (np.floating,)):\n",
    "        return float(obj)\n",
    "    return obj\n",
    "\n",
    "def split_counts(n: int) -> Tuple[int, int, int]:\n",
    "    \"\"\"Deterministic 50/20/30 with rounding and guardrails.\"\"\"\n",
    "    if n <= 0:\n",
    "        return 0, 0, 0\n",
    "\n",
    "    n_train = int(round(n * D5_TRAIN_PCT))\n",
    "    n_val   = int(round(n * D5_VAL_PCT))\n",
    "\n",
    "    n_train = min(max(n_train, 0), n)\n",
    "    n_val   = min(max(n_val, 0), n - n_train)\n",
    "    n_test  = n - n_train - n_val\n",
    "\n",
    "    # Purpose: avoid empty splits when enough speakers exist\n",
    "    if n >= 3:\n",
    "        if n_val == 0:\n",
    "            if n_train > 1:\n",
    "                n_train -= 1; n_val += 1\n",
    "            elif n_test > 1:\n",
    "                n_test -= 1; n_val += 1\n",
    "        if n_test == 0:\n",
    "            if n_train > 1:\n",
    "                n_train -= 1; n_test += 1\n",
    "            elif n_val > 1:\n",
    "                n_val -= 1; n_test += 1\n",
    "        if n_train == 0:\n",
    "            if n_test > 1:\n",
    "                n_test -= 1; n_train += 1\n",
    "            elif n_val > 1:\n",
    "                n_val -= 1; n_train += 1\n",
    "\n",
    "    n_train = min(n_train, n)\n",
    "    n_val   = min(n_val, n - n_train)\n",
    "    n_test  = n - n_train - n_val\n",
    "    return n_train, n_val, n_test\n",
    "\n",
    "def file_size(path: Path) -> Optional[int]:\n",
    "    # Purpose: compare files without reading full contents\n",
    "    try:\n",
    "        return path.stat().st_size\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def retry_copy(src: Path, dst: Path):\n",
    "    # Purpose: retry copy to reduce intermittent Drive failures\n",
    "    dst.parent.mkdir(parents=True, exist_ok=True)\n",
    "    last_err = None\n",
    "    for attempt in range(1, COPY_RETRIES + 1):\n",
    "        try:\n",
    "            shutil.copy2(str(src), str(dst))\n",
    "            return\n",
    "        except Exception as e:\n",
    "            last_err = e\n",
    "            time.sleep(COPY_SLEEP * attempt)\n",
    "    raise RuntimeError(f\"Failed to copy: {str(src)} -> {str(dst)}. Last error: {repr(last_err)}\")\n",
    "\n",
    "def safe_transfer_clip(\n",
    "    src_v1: Path,\n",
    "    dst_v2: Path,\n",
    "    fname: str,\n",
    "    v2_clips_dir: Path,\n",
    ") -> Tuple[str, Optional[str]]:\n",
    "    \"\"\"\n",
    "    Goal: ensure dst_v2 has the correct clip with the same filename.\n",
    "\n",
    "    Priority:\n",
    "      1) If dst exists and size matches v1 → keep it\n",
    "      2) Else if found in another v2 split and size matches v1 → move it\n",
    "      3) Else → copy from v1\n",
    "\n",
    "    Returns:\n",
    "      (action, detail)\n",
    "      action in {\"SKIP_OK\", \"MOVED_FROM_OTHER_SPLIT\", \"COPIED_FROM_V1\", \"ERROR\"}\n",
    "    \"\"\"\n",
    "    src_size = file_size(src_v1)\n",
    "    if src_size is None or (not src_v1.exists()):\n",
    "        return \"ERROR\", f\"v1 source missing or unreadable: {str(src_v1)}\"\n",
    "\n",
    "    # 1) already correct\n",
    "    if dst_v2.exists():\n",
    "        dst_size = file_size(dst_v2)\n",
    "        if dst_size == src_size:\n",
    "            return \"SKIP_OK\", None\n",
    "        # wrong-sized dst is treated as stale; repair below\n",
    "\n",
    "    # 2) check if the clip is in the wrong v2 split folder (leftover from earlier run)\n",
    "    other_paths = []\n",
    "    for sp in [\"train\", \"val\", \"test\"]:\n",
    "        cand = v2_clips_dir / sp / fname\n",
    "        if cand.exists():\n",
    "            other_paths.append(cand)\n",
    "\n",
    "    for cand in other_paths:\n",
    "        cand_size = file_size(cand)\n",
    "        if cand_size == src_size:\n",
    "            dst_v2.parent.mkdir(parents=True, exist_ok=True)\n",
    "            try:\n",
    "                os.replace(str(cand), str(dst_v2))\n",
    "                return \"MOVED_FROM_OTHER_SPLIT\", str(cand)\n",
    "            except Exception as e:\n",
    "                return \"ERROR\", f\"move failed {str(cand)} -> {str(dst_v2)} :: {repr(e)}\"\n",
    "\n",
    "    # 3) copy from v1\n",
    "    try:\n",
    "        retry_copy(src_v1, dst_v2)\n",
    "        if file_size(dst_v2) != src_size:\n",
    "            return \"ERROR\", f\"copy size mismatch {str(src_v1)} -> {str(dst_v2)}\"\n",
    "        return \"COPIED_FROM_V1\", None\n",
    "    except Exception as e:\n",
    "        return \"ERROR\", f\"copy failed {str(src_v1)} -> {str(dst_v2)} :: {repr(e)}\"\n",
    "\n",
    "# -------------------------\n",
    "# Load v1 manifest and validate clip files\n",
    "# Inputs: v1 manifest_all.csv\n",
    "# Outputs: m1 (v1 rows for D5), verified that v1 clip paths exist\n",
    "# -------------------------\n",
    "m1 = pd.read_csv(D5_V1_MANIFEST)\n",
    "\n",
    "missing = [c for c in MANIFEST_COLS if c not in m1.columns]\n",
    "if missing:\n",
    "    raise RuntimeError(f\"v1 manifest missing columns: {missing}\")\n",
    "\n",
    "m1 = m1[m1[\"dataset\"].astype(str) == \"D5\"].copy()\n",
    "if len(m1) == 0:\n",
    "    raise RuntimeError(\"No D5 rows found in v1 manifest.\")\n",
    "\n",
    "# Purpose: keep missing values as real NaN, not the string \"NaN\"\n",
    "for c in [\"age\", \"sex\", \"duration_sec\", \"clip_start_sec\", \"clip_end_sec\", \"speaker_id\", \"task\", \"sample_id\", \"speaker_key_rel\"]:\n",
    "    if c in m1.columns:\n",
    "        m1[c] = m1[c].replace(\"NaN\", np.nan)\n",
    "\n",
    "# Purpose: fail early if any v1 clip is missing\n",
    "missing_files = [p for p in m1[\"clip_path\"].astype(str).tolist() if not os.path.exists(p)]\n",
    "if missing_files:\n",
    "    print(\"Example missing clip_path (first 10):\")\n",
    "    for x in missing_files[:10]:\n",
    "        print(\"  \", x)\n",
    "    raise FileNotFoundError(f\"Missing {len(missing_files)} v1 clip files. Fix before rebuilding v2.\")\n",
    "\n",
    "print(\"D5 v1 clips verified:\", len(m1))\n",
    "\n",
    "# -------------------------\n",
    "# Build new speaker split (stratified by label_str)\n",
    "# Inputs: unique speakers from v1\n",
    "# Outputs: spk_split table: one split per speaker\n",
    "# -------------------------\n",
    "spk_tbl = (\n",
    "    m1[[\"speaker_id\", \"label_str\", \"label_num\"]]\n",
    "    .drop_duplicates()\n",
    "    .copy()\n",
    ")\n",
    "\n",
    "rng = random.Random(D5_SEED)\n",
    "speaker_split_rows: List[Dict] = []\n",
    "\n",
    "for lab, g in spk_tbl.groupby(\"label_str\", sort=True):\n",
    "    spks = g[\"speaker_id\"].astype(str).tolist()\n",
    "    rng.shuffle(spks)\n",
    "\n",
    "    n = len(spks)\n",
    "    n_train, n_val, n_test = split_counts(n)\n",
    "\n",
    "    train_spks = spks[:n_train]\n",
    "    val_spks   = spks[n_train:n_train + n_val]\n",
    "    test_spks  = spks[n_train + n_val:]\n",
    "\n",
    "    speaker_split_rows += [{\"speaker_id\": s, \"split\": \"train\", \"label_str\": lab} for s in train_spks]\n",
    "    speaker_split_rows += [{\"speaker_id\": s, \"split\": \"val\",   \"label_str\": lab} for s in val_spks]\n",
    "    speaker_split_rows += [{\"speaker_id\": s, \"split\": \"test\",  \"label_str\": lab} for s in test_spks]\n",
    "\n",
    "spk_split = pd.DataFrame(speaker_split_rows)\n",
    "\n",
    "# Purpose: ensure each speaker is assigned exactly once\n",
    "if spk_split[\"speaker_id\"].duplicated().any():\n",
    "    dup = spk_split.loc[spk_split[\"speaker_id\"].duplicated(), \"speaker_id\"].tolist()[:10]\n",
    "    raise RuntimeError(f\"Speaker appears twice in split mapping. Examples: {dup}\")\n",
    "\n",
    "print(\"\\nNew speaker split (counts):\")\n",
    "print(spk_split.groupby([\"split\", \"label_str\"]).size())\n",
    "\n",
    "# -------------------------\n",
    "# Apply new split to every clip row\n",
    "# Inputs: v1 clip-level rows + speaker split table\n",
    "# Outputs: m2 (clip rows with fresh split values)\n",
    "# -------------------------\n",
    "m2 = m1.copy()\n",
    "\n",
    "# Purpose: prevent split_x / split_y columns during merge\n",
    "if \"split\" in m2.columns:\n",
    "    m2 = m2.drop(columns=[\"split\"])\n",
    "\n",
    "m2[\"speaker_id\"] = m2[\"speaker_id\"].astype(str)\n",
    "spk_split[\"speaker_id\"] = spk_split[\"speaker_id\"].astype(str)\n",
    "\n",
    "m2 = m2.merge(spk_split[[\"speaker_id\", \"split\"]], on=\"speaker_id\", how=\"left\", validate=\"many_to_one\")\n",
    "\n",
    "if m2[\"split\"].isna().any():\n",
    "    ex = m2.loc[m2[\"split\"].isna(), \"speaker_id\"].drop_duplicates().head(10).tolist()\n",
    "    raise RuntimeError(f\"Some speakers did not receive a split. Examples: {ex}\")\n",
    "\n",
    "# -------------------------\n",
    "# Transfer clips into v2 (same filenames, new split folders)\n",
    "# Inputs: v1 clip_path + new split assignment\n",
    "# Outputs: v2 clip files placed under clips/<split>/ with no duplicates across splits\n",
    "# -------------------------\n",
    "warn_rows: List[Dict] = []\n",
    "actions = {\"SKIP_OK\": 0, \"MOVED_FROM_OTHER_SPLIT\": 0, \"COPIED_FROM_V1\": 0, \"ERROR\": 0}\n",
    "\n",
    "# Purpose: avoid filename collisions in v2 split folders\n",
    "fn_counts = m2[\"clip_path\"].astype(str).map(lambda p: os.path.basename(p)).value_counts()\n",
    "dupe_fns = fn_counts[fn_counts > 1]\n",
    "if len(dupe_fns) > 0:\n",
    "    ex = dupe_fns.head(10).to_dict()\n",
    "    raise RuntimeError(f\"Duplicate clip basenames found in v1 manifest (would collide in v2). Examples: {ex}\")\n",
    "\n",
    "pbar = tqdm(m2.itertuples(index=False), total=len(m2), desc=\"D5 v2: transfer clips\", dynamic_ncols=True)\n",
    "for r in pbar:\n",
    "    src_v1 = Path(str(r.clip_path))\n",
    "    fname  = os.path.basename(str(r.clip_path))\n",
    "    dst_v2 = D5_V2_CLIPS_DIR / str(r.split) / fname\n",
    "\n",
    "    action, detail = safe_transfer_clip(src_v1=src_v1, dst_v2=dst_v2, fname=fname, v2_clips_dir=D5_V2_CLIPS_DIR)\n",
    "    actions[action] = actions.get(action, 0) + 1\n",
    "\n",
    "    if action == \"ERROR\":\n",
    "        warn_rows.append({\n",
    "            \"dataset\": \"D5\",\n",
    "            \"speaker_id\": str(r.speaker_id),\n",
    "            \"source_path\": str(r.source_path),\n",
    "            \"warning_type\": \"transfer_failed\",\n",
    "            \"detail\": detail,\n",
    "        })\n",
    "\n",
    "# Purpose: update manifest paths to the v2 clip locations (same filename, new folder)\n",
    "m2 = m2.copy()\n",
    "m2[\"clip_path\"] = [\n",
    "    str(D5_V2_CLIPS_DIR / sp / os.path.basename(p))\n",
    "    for sp, p in zip(m2[\"split\"].astype(str).tolist(), m2[\"clip_path\"].astype(str).tolist())\n",
    "]\n",
    "\n",
    "# -------------------------\n",
    "# Write v2 artifacts (manifest, warnings, summary, config, speaker split)\n",
    "# Outputs: files under preprocessed_v2/manifests, logs, config\n",
    "# -------------------------\n",
    "for c in MANIFEST_COLS:\n",
    "    if c not in m2.columns:\n",
    "        m2[c] = np.nan\n",
    "m2 = m2[MANIFEST_COLS].copy()\n",
    "\n",
    "# Purpose: keep true missing values (never literal \"NaN\")\n",
    "for c in MANIFEST_COLS:\n",
    "    if m2[c].dtype == object:\n",
    "        m2[c] = m2[c].replace(\"NaN\", np.nan)\n",
    "\n",
    "v2_manifest_path = D5_V2_MAN_DIR / \"manifest_all.csv\"\n",
    "v2_warn_path     = D5_V2_LOGS_DIR / \"preprocess_warnings.csv\"\n",
    "v2_summary_path  = D5_V2_LOGS_DIR / \"dataset_summary.json\"\n",
    "v2_cfg_path      = D5_V2_CFG_DIR / \"run_config.json\"\n",
    "v2_spk_split_csv = D5_V2_LOGS_DIR / f\"speaker_split_seed{D5_SEED}.csv\"\n",
    "\n",
    "atomic_write_csv(v2_manifest_path, m2)\n",
    "\n",
    "warnings_df = pd.DataFrame(warn_rows)\n",
    "atomic_write_csv(v2_warn_path, warnings_df)\n",
    "\n",
    "spk_split.sort_values([\"split\", \"label_str\", \"speaker_id\"]).to_csv(v2_spk_split_csv, index=False)\n",
    "\n",
    "speakers_per_split_x_label = spk_split.groupby([\"split\", \"label_str\"]).size().to_dict()\n",
    "\n",
    "summary = {\n",
    "    \"dataset\": \"D5\",\n",
    "    \"note\": \"Split-only rebuild: v2 contains v1 clips relocated into new split folders; no audio reprocessing. If a clip existed in the wrong v2 split, it was moved.\",\n",
    "    \"source_preprocessed_root\": str(D5_V1_ROOT),\n",
    "    \"out_root\": str(D5_V2_ROOT),\n",
    "    \"seed\": int(D5_SEED),\n",
    "    \"split_policy\": {\n",
    "        \"unit\": \"speaker\",\n",
    "        \"train\": float(D5_TRAIN_PCT),\n",
    "        \"val\": float(D5_VAL_PCT),\n",
    "        \"test\": float(D5_TEST_PCT),\n",
    "        \"stratified_by\": \"label_str\",\n",
    "    },\n",
    "    \"transfer_actions\": actions,\n",
    "    \"counts\": {\n",
    "        \"n_clips_total\": int(len(m2)),\n",
    "        \"split_counts_clips\": m2[\"split\"].value_counts(dropna=False).to_dict(),\n",
    "        \"label_counts_clips\": m2[\"label_str\"].value_counts(dropna=False).to_dict(),\n",
    "        \"task_counts_clips\": m2[\"task\"].value_counts(dropna=False).to_dict(),\n",
    "        \"n_speakers_total\": int(spk_tbl[\"speaker_id\"].nunique()),\n",
    "        \"speakers_per_split_x_label\": json_safe(speakers_per_split_x_label),\n",
    "        \"n_warnings\": int(len(warnings_df)),\n",
    "    },\n",
    "    \"artifacts\": {\n",
    "        \"manifest_all_csv\": str(v2_manifest_path),\n",
    "        \"warnings_csv\": str(v2_warn_path),\n",
    "        \"speaker_split_csv\": str(v2_spk_split_csv),\n",
    "    },\n",
    "}\n",
    "atomic_write_json(v2_summary_path, summary)\n",
    "\n",
    "run_cfg = {\n",
    "    \"dataset\": \"D5\",\n",
    "    \"mode\": \"split_only_rebuild\",\n",
    "    \"seed\": int(D5_SEED),\n",
    "    \"split\": {\n",
    "        \"unit\": \"speaker\",\n",
    "        \"train\": float(D5_TRAIN_PCT),\n",
    "        \"val\": float(D5_VAL_PCT),\n",
    "        \"test\": float(D5_TEST_PCT),\n",
    "        \"stratified_by\": \"label_str\",\n",
    "    },\n",
    "    \"paths\": {\n",
    "        \"source_preprocessed_v1\": str(D5_V1_ROOT),\n",
    "        \"out_root\": str(D5_V2_ROOT),\n",
    "        \"clips_dir\": str(D5_V2_CLIPS_DIR),\n",
    "        \"manifest_all\": str(v2_manifest_path),\n",
    "        \"warnings_csv\": str(v2_warn_path),\n",
    "        \"summary_json\": str(v2_summary_path),\n",
    "        \"speaker_split_csv\": str(v2_spk_split_csv),\n",
    "    },\n",
    "    \"transfer_policy\": {\n",
    "        \"priority\": [\n",
    "            \"keep dst if exists and same size as v1\",\n",
    "            \"else move from other v2 split folder if matches size\",\n",
    "            \"else copy from v1\"\n",
    "        ],\n",
    "        \"preserve_filename\": True,\n",
    "        \"no_audio_processing\": True,\n",
    "    },\n",
    "}\n",
    "atomic_write_json(v2_cfg_path, run_cfg)\n",
    "\n",
    "# -------------------------\n",
    "# Sanity checks (speaker integrity + file existence + no stale duplicates)\n",
    "# Inputs: m2 + v2 clips folders\n",
    "# Outputs: raises error if anything is inconsistent\n",
    "# -------------------------\n",
    "# A) Each speaker must be in exactly one split\n",
    "spk_chk = m2[[\"speaker_id\", \"split\"]].drop_duplicates()\n",
    "bad = spk_chk.groupby(\"speaker_id\")[\"split\"].nunique()\n",
    "bad = bad[bad > 1]\n",
    "if len(bad) > 0:\n",
    "    raise RuntimeError(f\"Speaker appears in multiple splits. Examples: {bad.index.tolist()[:10]}\")\n",
    "\n",
    "# B) Every manifest row must point to an existing v2 file\n",
    "missing_v2 = [p for p in m2[\"clip_path\"].astype(str).tolist() if not os.path.exists(p)]\n",
    "if missing_v2:\n",
    "    print(\"Example missing v2 clip_path (first 10):\")\n",
    "    for x in missing_v2[:10]:\n",
    "        print(\"  \", x)\n",
    "    raise FileNotFoundError(f\"Missing {len(missing_v2)} v2 clip files after transfer. Check warnings/logs.\")\n",
    "\n",
    "# C) A filename must not exist in more than one v2 split folder\n",
    "fn_to_splits: Dict[str, List[str]] = {}\n",
    "for sp in [\"train\", \"val\", \"test\"]:\n",
    "    sp_dir = D5_V2_CLIPS_DIR / sp\n",
    "    for fn in os.listdir(sp_dir):\n",
    "        if fn.lower().endswith(\".wav\"):\n",
    "            fn_to_splits.setdefault(fn, []).append(sp)\n",
    "dupe_across = {fn: sps for fn, sps in fn_to_splits.items() if len(sps) > 1}\n",
    "if len(dupe_across) > 0:\n",
    "    ex = list(dupe_across.items())[:10]\n",
    "    raise RuntimeError(f\"Found same filename in multiple v2 split folders (stale duplicates). Examples: {ex}\")\n",
    "\n",
    "print(\"\\nDONE: D5 preprocessed_v2 split-only rebuild (50/20/30)\")\n",
    "print(\"Transfer actions:\", actions)\n",
    "print(\"v2 Manifest:\", str(v2_manifest_path))\n",
    "print(\"v2 Warnings:\", str(v2_warn_path))\n",
    "print(\"v2 Summary:\", str(v2_summary_path))\n",
    "print(\"v2 Run config:\", str(v2_cfg_path))\n",
    "print(\"v2 Speaker split:\", str(v2_spk_split_csv))\n",
    "print(\"\\nClip counts (v2):\")\n",
    "print(m2[\"split\"].value_counts())\n",
    "print(\"\\nSpeakers per split x label:\")\n",
    "print(spk_split.groupby([\"split\", \"label_str\"]).size())\n",
    "if len(warnings_df):\n",
    "    print(\"\\nWarnings count:\", len(warnings_df))\n",
    "    print(warnings_df[\"warning_type\"].value_counts().head(10))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 709,
     "referenced_widgets": [
      "4dcfb6c283aa4991a90e454e8781cb67",
      "09c16cf0e03640a984f5f8509f07eb86",
      "460778b37b82494b8d8563c74ba8173d",
      "9cba442c5e034207a0015420da3ce94b",
      "6f2256e55183467e87132921bd16274d",
      "e0e421146c0842e5b665ed78150f933e",
      "6b69975de8a445b49401c97e58120fb6",
      "9b5fe8a9d451409790e5efc4fd7fb791",
      "035c78eb27db4d3ea04094c27b91f969",
      "8567ddcded1c44de818f21173b668cc3",
      "6a64fab3a9ae471daf78b5a1845ac150"
     ]
    },
    "id": "qveAfQsbSJNZ"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "The following cell preprocesses Dataset D6 (Ah Sound from Figshare) into a clean and standardized folder that can be used for model training. It mounts Google Drive if needed, installs and imports the required audio tools (including WebRTC VAD when available), defines the input locations for Healthy and Parkinson’s recordings along with the demographics Excel file, and creates the output folders under `preprocessed_v1` (clips, manifests, config, and logs). At the start, it clears the temporary `clips/_candidates` folder and performs basic checks so missing inputs are detected right away.\n",
    "\n",
    "The cell then cleans and standardizes the audio in a consistent way. Each source WAV file is loaded, converted to mono if needed, resampled to 16 kHz, and normalized using an RMS based gain with a peak limit to prevent clipping. Speech regions are detected using WebRTC VAD when available, or a simple energy based fallback otherwise, and small padding is added around detected speech. A strict rule is applied so that exactly one clip is created from each original source file. This clip is produced by taking a centered segment of up to 2.0 seconds from the speech focused audio, without padding if the segment is shorter. If voice detection finds too little usable speech, the code falls back to using the original audio and records a warning.\n",
    "\n",
    "As each file is processed, the clip is written immediately to a staging folder (`clips/_candidates`) to reduce the risk of data loss if the run is interrupted. At the same time, a metadata row is collected for each candidate, including label, speaker ID, sample ID, age, sex, source path, duration, and clip timing. Any errors or unusual cases, such as empty audio after processing, very short VAD output, or file read and write issues, are logged in a warnings table for later review.\n",
    "\n",
    "After all candidate clips are written, the cell optionally applies a cap policy that limits the number of clips per speaker per task, using a fixed random seed for repeatability, and removes any unkept candidate WAV files. It then splits speakers into training, validation, and test sets at the speaker level (70% train, 15% validation, 15% test), while keeping Healthy and Parkinson’s speakers balanced across splits. The selected clips are moved from the staging folder into their final locations (`clips/train`, `clips/val`, and `clips/test`) using a standardized filename format, and a final manifest is created with a consistent structure and updated clip paths.\n",
    "\n",
    "Finally, the cell writes the required outputs: `manifests/manifest_all.csv` for training and evaluation, `logs/preprocess_warnings.csv` with all recorded warnings, `logs/dataset_summary.json` summarizing counts and split details, and `config/run_config.json` describing the preprocessing settings and folder layout. Any remaining temporary files are cleaned up when possible, and a summary is printed showing where the outputs were saved and how many clips and warnings were generated."
   ],
   "metadata": {
    "id": "ES_fykYLsetT"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# ============================================================\n",
    "# D6 Preprocessing v1 — Ah Sound (Figshare), One Clip per Source, Standard Outputs\n",
    "# Inputs: HC/PD WAV folders + demographics Excel\n",
    "# Outputs: clips/<split>/ WAVs (flat), manifests/manifest_all.csv, config/run_config.json,\n",
    "#          logs/preprocess_warnings.csv, logs/dataset_summary.json\n",
    "# ============================================================\n",
    "# Key rules:\n",
    "# - Exactly 1 candidate clip per source WAV (no multi-clip splitting)\n",
    "# - Write candidates immediately, then cap, then speaker split, then move kept clips\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import math\n",
    "import random\n",
    "import time\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple, Dict, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import soundfile as sf\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# -------------------------\n",
    "# Drive mount check\n",
    "# Purpose: ensure dataset paths are reachable in Colab\n",
    "# -------------------------\n",
    "if not os.path.isdir(\"/content/drive/MyDrive\"):\n",
    "    from google.colab import drive  # type: ignore\n",
    "    drive.mount(\"/content/drive\")\n",
    "\n",
    "# -------------------------\n",
    "# Optional dependencies\n",
    "# Purpose: use WebRTC VAD when available; fall back if missing\n",
    "# -------------------------\n",
    "def _d6_try_import_webrtcvad():\n",
    "    try:\n",
    "        import webrtcvad  # type: ignore\n",
    "        return webrtcvad, True\n",
    "    except Exception:\n",
    "        return None, False\n",
    "\n",
    "webrtcvad, D6_HAVE_WEBRTCVAD = _d6_try_import_webrtcvad()\n",
    "if not D6_HAVE_WEBRTCVAD:\n",
    "    !pip -q install webrtcvad\n",
    "    webrtcvad, D6_HAVE_WEBRTCVAD = _d6_try_import_webrtcvad()\n",
    "\n",
    "try:\n",
    "    from scipy.signal import resample_poly  # type: ignore\n",
    "    D6_HAVE_SCIPY = True\n",
    "except Exception:\n",
    "    D6_HAVE_SCIPY = False\n",
    "\n",
    "# -------------------------\n",
    "# Dataset paths and output structure\n",
    "# Inputs: raw HC/PD folders and Excel metadata\n",
    "# Outputs: standardized preprocessed_v1 folder layout\n",
    "# -------------------------\n",
    "D6_DATASET_DIR = \"/content/drive/MyDrive/AI_PD_Project/Datasets/D6-Ah Sound (Figshare)\"\n",
    "D6_HC_ROOT = os.path.join(D6_DATASET_DIR, \"HC_AH\", \"HC_AH\")\n",
    "D6_PD_ROOT = os.path.join(D6_DATASET_DIR, \"PD_AH\", \"PD_AH\")\n",
    "D6_META_XLSX = os.path.join(D6_DATASET_DIR, \"Demographics_age_sex.xlsx\")\n",
    "\n",
    "# Outputs (standardized)\n",
    "D6_OUT_ROOT     = os.path.join(D6_DATASET_DIR, \"preprocessed_v1\")\n",
    "D6_CLIPS_DIR    = os.path.join(D6_OUT_ROOT, \"clips\")\n",
    "D6_CAND_DIR     = os.path.join(D6_CLIPS_DIR, \"_candidates\")  # staging area during the run\n",
    "D6_MANIFEST_DIR = os.path.join(D6_OUT_ROOT, \"manifests\")\n",
    "D6_CONFIG_DIR   = os.path.join(D6_OUT_ROOT, \"config\")\n",
    "D6_LOGS_DIR     = os.path.join(D6_OUT_ROOT, \"logs\")\n",
    "\n",
    "for p in [D6_OUT_ROOT, D6_CLIPS_DIR, D6_MANIFEST_DIR, D6_CONFIG_DIR, D6_LOGS_DIR]:\n",
    "    os.makedirs(p, exist_ok=True)\n",
    "for sp in [\"train\", \"val\", \"test\"]:\n",
    "    os.makedirs(os.path.join(D6_CLIPS_DIR, sp), exist_ok=True)\n",
    "\n",
    "# Purpose: avoid mixing candidates from older runs\n",
    "if os.path.isdir(D6_CAND_DIR):\n",
    "    try:\n",
    "        shutil.rmtree(D6_CAND_DIR)\n",
    "    except Exception:\n",
    "        pass\n",
    "os.makedirs(D6_CAND_DIR, exist_ok=True)\n",
    "\n",
    "# -------------------------\n",
    "# Fail-fast checks\n",
    "# Purpose: stop early if inputs are missing\n",
    "# -------------------------\n",
    "print(\"D6_DATASET_DIR exists?\", os.path.exists(D6_DATASET_DIR))\n",
    "print(\"D6_HC_ROOT exists?\", os.path.exists(D6_HC_ROOT))\n",
    "print(\"D6_PD_ROOT exists?\", os.path.exists(D6_PD_ROOT))\n",
    "print(\"Demographics_age_sex.xlsx exists?\", os.path.exists(D6_META_XLSX))\n",
    "print(\"webrtcvad available?\", D6_HAVE_WEBRTCVAD)\n",
    "print(\"scipy available?\", D6_HAVE_SCIPY)\n",
    "\n",
    "if not os.path.exists(D6_DATASET_DIR):\n",
    "    raise FileNotFoundError(f\"D6_DATASET_DIR not found: {D6_DATASET_DIR}\")\n",
    "if not os.path.exists(D6_HC_ROOT) or not os.path.exists(D6_PD_ROOT):\n",
    "    raise FileNotFoundError(\"Missing HC or PD root folders. Check HC_AH/HC_AH and PD_AH/PD_AH.\")\n",
    "if not os.path.exists(D6_META_XLSX):\n",
    "    raise FileNotFoundError(f\"Missing metadata Excel: {D6_META_XLSX}\")\n",
    "\n",
    "# -------------------------\n",
    "# Processing configuration\n",
    "# Purpose: standardize audio, clip length, splitting, and logging\n",
    "# -------------------------\n",
    "D6_SR = 16000\n",
    "D6_RANDOM_SEED = 1337\n",
    "random.seed(D6_RANDOM_SEED)\n",
    "np.random.seed(D6_RANDOM_SEED)\n",
    "\n",
    "D6_TASK5 = \"vowl\"         # task label stored in the manifest\n",
    "D6_TARGET_SEC = 2.0       # target clip length; no padding if shorter\n",
    "\n",
    "# Manual normalization (no external loudness dependency)\n",
    "D6_TARGET_RMS_DBFS = -20.0\n",
    "D6_PEAK_LIMIT_DBFS = -1.0\n",
    "D6_MIN_RMS_DBFS    = -60.0\n",
    "D6_MAX_GAIN_DB     = 18.0\n",
    "\n",
    "# VAD settings (used only to find voiced regions)\n",
    "D6_VAD_MODE       = 2\n",
    "D6_FRAME_MS       = 30\n",
    "D6_MIN_SPEECH_MS  = 200\n",
    "D6_MERGE_GAP_MS   = 200\n",
    "D6_PAD_SEC        = 0.25\n",
    "D6_MIN_KEEP_SEC   = 0.30\n",
    "\n",
    "# Cap policy (applied after candidates are created)\n",
    "D6_MAX_CLIPS_PER_SPK_TASK = 8\n",
    "\n",
    "# Speaker-level split ratios\n",
    "D6_TRAIN_PCT, D6_VAL_PCT, D6_TEST_PCT = 0.70, 0.15, 0.15\n",
    "\n",
    "# Reliable write retries (Drive can be flaky)\n",
    "D6_WRITE_RETRIES = 4\n",
    "D6_WRITE_SLEEP   = 0.5\n",
    "\n",
    "# -------------------------\n",
    "# Standard manifest schema\n",
    "# Output: manifest_all.csv uses these columns in this order\n",
    "# -------------------------\n",
    "MANIFEST_COLS = [\n",
    "    \"split\",\"dataset\",\"task\",\"speaker_id\",\"sample_id\",\n",
    "    \"label_str\",\"label_num\",\"age\",\"sex\",\"speaker_key_rel\",\n",
    "    \"clip_path\",\"duration_sec\",\"source_path\",\n",
    "    \"clip_start_sec\",\"clip_end_sec\",\"sr_hz\",\"channels\",\n",
    "    \"clip_is_contiguous\",\n",
    "]\n",
    "\n",
    "# -------------------------\n",
    "# Utility helpers (names, audio stats, I/O)\n",
    "# Purpose: keep core loop readable and consistent\n",
    "# -------------------------\n",
    "def d6_safe(s: str) -> str:\n",
    "    return re.sub(r\"[^A-Za-z0-9_\\-\\.]+\", \"_\", str(s))\n",
    "\n",
    "def d6_db_to_lin(db: float) -> float:\n",
    "    return 10.0 ** (db / 20.0)\n",
    "\n",
    "def d6_rms_dbfs(y: np.ndarray) -> float:\n",
    "    if y is None or len(y) == 0:\n",
    "        return -120.0\n",
    "    rms = float(np.sqrt(np.mean(y.astype(np.float64) ** 2) + 1e-12))\n",
    "    return 20.0 * math.log10(max(rms, 1e-12))\n",
    "\n",
    "def d6_peak_limit(y: np.ndarray, peak_dbfs: float) -> np.ndarray:\n",
    "    # Purpose: avoid clipping after gain changes\n",
    "    if y is None or len(y) == 0:\n",
    "        return y\n",
    "    peak = float(np.max(np.abs(y)))\n",
    "    lim = d6_db_to_lin(peak_dbfs)\n",
    "    if peak > lim and peak > 0:\n",
    "        y = y * (lim / peak)\n",
    "    return np.clip(y, -1.0, 1.0).astype(np.float32)\n",
    "\n",
    "def d6_norm_rms_then_peak(y: np.ndarray) -> np.ndarray:\n",
    "    # Purpose: normalize loudness while protecting peaks\n",
    "    if y is None or len(y) == 0:\n",
    "        return y\n",
    "    cur = d6_rms_dbfs(y)\n",
    "    if cur < D6_MIN_RMS_DBFS:\n",
    "        return d6_peak_limit(y, D6_PEAK_LIMIT_DBFS)\n",
    "    gain_db = float(D6_TARGET_RMS_DBFS - cur)\n",
    "    gain_db = float(np.clip(gain_db, -60.0, D6_MAX_GAIN_DB))\n",
    "    y2 = (y.astype(np.float32) * d6_db_to_lin(gain_db)).astype(np.float32)\n",
    "    return d6_peak_limit(y2, D6_PEAK_LIMIT_DBFS)\n",
    "\n",
    "def d6_read_mono(path: str) -> Tuple[np.ndarray, int]:\n",
    "    # Purpose: load audio and ensure a clean mono float32 vector\n",
    "    x, sr = sf.read(path, always_2d=False)\n",
    "    if isinstance(x, np.ndarray) and x.ndim == 2:\n",
    "        x = x.mean(axis=1)\n",
    "    x = np.asarray(x, dtype=np.float32)\n",
    "    if not np.isfinite(x).all():\n",
    "        x = np.nan_to_num(x, nan=0.0, posinf=0.0, neginf=0.0).astype(np.float32)\n",
    "    return x, int(sr)\n",
    "\n",
    "def d6_resample(y: np.ndarray, sr_in: int, sr_out: int) -> np.ndarray:\n",
    "    # Purpose: resample to 16 kHz for all clips\n",
    "    if sr_in == sr_out:\n",
    "        return y.astype(np.float32, copy=False)\n",
    "    if D6_HAVE_SCIPY:\n",
    "        g = math.gcd(sr_in, sr_out)\n",
    "        up = sr_out // g\n",
    "        down = sr_in // g\n",
    "        return resample_poly(y.astype(np.float64), up, down).astype(np.float32, copy=False)\n",
    "    n_new = int(round(len(y) * (sr_out / sr_in)))\n",
    "    if n_new <= 1:\n",
    "        return y[:1].astype(np.float32, copy=False)\n",
    "    x_old = np.linspace(0.0, 1.0, num=len(y), endpoint=False)\n",
    "    x_new = np.linspace(0.0, 1.0, num=n_new, endpoint=False)\n",
    "    return np.interp(x_new, x_old, y).astype(np.float32, copy=False)\n",
    "\n",
    "def d6_pcm16_bytes(y: np.ndarray) -> bytes:\n",
    "    # Purpose: WebRTC VAD expects 16-bit PCM bytes\n",
    "    y = np.clip(y, -1.0, 1.0)\n",
    "    return (y * 32767.0).astype(np.int16).tobytes()\n",
    "\n",
    "def d6_merge(segs: List[Tuple[int,int]], sr: int) -> List[Tuple[int,int]]:\n",
    "    # Purpose: merge close speech segments and drop very short ones\n",
    "    if not segs:\n",
    "        return []\n",
    "    gap = int(sr * (D6_MERGE_GAP_MS/1000.0))\n",
    "    min_len = int(sr * (D6_MIN_SPEECH_MS/1000.0))\n",
    "    segs = sorted(segs)\n",
    "    merged = [list(segs[0])]\n",
    "    for s,e in segs[1:]:\n",
    "        if s - merged[-1][1] <= gap:\n",
    "            merged[-1][1] = max(merged[-1][1], e)\n",
    "        else:\n",
    "            merged.append([s,e])\n",
    "    out = []\n",
    "    for s,e in merged:\n",
    "        if (e-s) >= min_len:\n",
    "            out.append((int(s), int(e)))\n",
    "    return out\n",
    "\n",
    "def d6_pad(segs: List[Tuple[int,int]], sr: int, n: int) -> List[Tuple[int,int]]:\n",
    "    # Purpose: add small context around detected speech\n",
    "    pad = int(sr * D6_PAD_SEC)\n",
    "    out = []\n",
    "    for s,e in segs:\n",
    "        s2 = max(0, s-pad)\n",
    "        e2 = min(n, e+pad)\n",
    "        if e2 > s2:\n",
    "            out.append((s2,e2))\n",
    "    return out\n",
    "\n",
    "def d6_webrtc_segments(y: np.ndarray, sr: int) -> Optional[List[Tuple[int,int]]]:\n",
    "    # Purpose: get voiced segments using WebRTC VAD (when possible)\n",
    "    if not D6_HAVE_WEBRTCVAD:\n",
    "        return None\n",
    "    if sr not in (8000, 16000, 32000, 48000):\n",
    "        return None\n",
    "    frame_ms = int(D6_FRAME_MS)\n",
    "    if frame_ms not in (10, 20, 30):\n",
    "        frame_ms = 30\n",
    "    frame_len = int(sr * (frame_ms/1000.0))\n",
    "    if frame_len <= 0 or len(y) < frame_len:\n",
    "        return []\n",
    "\n",
    "    n_frames = int(math.ceil(len(y)/frame_len))\n",
    "    pad_samp = n_frames*frame_len - len(y)\n",
    "    if pad_samp > 0:\n",
    "        y = np.concatenate([y, np.zeros(pad_samp, dtype=np.float32)], axis=0)\n",
    "\n",
    "    pcm = d6_pcm16_bytes(y)\n",
    "    vad = webrtcvad.Vad(int(D6_VAD_MODE))\n",
    "    flags = []\n",
    "    for i in range(n_frames):\n",
    "        s = i*frame_len*2\n",
    "        e = s + frame_len*2\n",
    "        flags.append(vad.is_speech(pcm[s:e], sr))\n",
    "\n",
    "    segs = []\n",
    "    on = False\n",
    "    s0 = 0\n",
    "    for i,f in enumerate(flags):\n",
    "        if f and not on:\n",
    "            on = True\n",
    "            s0 = i\n",
    "        elif (not f) and on:\n",
    "            on = False\n",
    "            segs.append((s0*frame_len, i*frame_len))\n",
    "    if on:\n",
    "        segs.append((s0*frame_len, n_frames*frame_len))\n",
    "\n",
    "    n0 = len(y) - pad_samp if pad_samp > 0 else len(y)\n",
    "    return [(max(0,s), min(n0,e)) for s,e in segs]\n",
    "\n",
    "def d6_energy_segments(y: np.ndarray, sr: int) -> List[Tuple[int,int]]:\n",
    "    # Purpose: fallback voiced detection using short-time energy\n",
    "    frame = int(sr * 0.02)\n",
    "    hop = frame\n",
    "    if frame <= 0 or len(y) < frame:\n",
    "        return []\n",
    "    eng = []\n",
    "    idx = []\n",
    "    for i in range(0, len(y) - frame + 1, hop):\n",
    "        w = y[i:i+frame]\n",
    "        eng.append(float(np.mean(w*w)))\n",
    "        idx.append(i)\n",
    "    eng = np.array(eng, dtype=np.float32)\n",
    "    thr = float(np.percentile(eng, 25)) * 2.5\n",
    "    thr = max(thr, 1e-8)\n",
    "    keep = eng > thr\n",
    "\n",
    "    segs = []\n",
    "    in_seg = False\n",
    "    s0 = 0\n",
    "    for k, flag in enumerate(keep):\n",
    "        if flag and not in_seg:\n",
    "            in_seg = True\n",
    "            s0 = idx[k]\n",
    "        elif (not flag) and in_seg:\n",
    "            in_seg = False\n",
    "            segs.append((s0, idx[k] + frame))\n",
    "    if in_seg:\n",
    "        segs.append((s0, idx[-1] + frame))\n",
    "    return segs\n",
    "\n",
    "def d6_voiced_concat(y: np.ndarray, sr: int) -> Tuple[np.ndarray, str]:\n",
    "    # Purpose: keep voiced parts and stitch them together into one stream\n",
    "    segs = d6_webrtc_segments(y, sr)\n",
    "    if segs is None:\n",
    "        segs = d6_energy_segments(y, sr)\n",
    "        used = \"energy\"\n",
    "    else:\n",
    "        used = \"webrtcvad\"\n",
    "    segs = d6_merge(segs, sr)\n",
    "    segs = d6_pad(segs, sr, len(y))\n",
    "    if not segs:\n",
    "        return y.astype(np.float32, copy=False), used\n",
    "    return np.concatenate([y[s:e] for s,e in segs], axis=0).astype(np.float32, copy=False), used\n",
    "\n",
    "def d6_choose_clip_center(voiced: np.ndarray, sr: int, target_sec: float) -> Tuple[np.ndarray, float, float]:\n",
    "    # Purpose: return ONE clip (<= target_sec), centered if longer, never padded\n",
    "    if voiced is None or len(voiced) == 0:\n",
    "        return np.zeros((0,), dtype=np.float32), 0.0, 0.0\n",
    "    target_len = int(round(sr * target_sec))\n",
    "    n = len(voiced)\n",
    "    if target_len <= 0:\n",
    "        return voiced.astype(np.float32, copy=False), 0.0, float(n)/sr\n",
    "    if n >= target_len:\n",
    "        start = (n - target_len) // 2\n",
    "        end = start + target_len\n",
    "        return voiced[start:end].astype(np.float32, copy=False), float(start)/sr, float(end)/sr\n",
    "    return voiced.astype(np.float32, copy=False), 0.0, float(n)/sr\n",
    "\n",
    "def d6_find_col(df: pd.DataFrame, candidates: List[str]) -> Optional[str]:\n",
    "    # Purpose: locate metadata columns despite small naming differences\n",
    "    cols_lower = {c.lower(): c for c in df.columns}\n",
    "    for cand in candidates:\n",
    "        if cand.lower() in cols_lower:\n",
    "            return cols_lower[cand.lower()]\n",
    "    for c in df.columns:\n",
    "        cl = c.lower()\n",
    "        for cand in candidates:\n",
    "            if cand.lower() in cl:\n",
    "                return c\n",
    "    return None\n",
    "\n",
    "def d6_list_wavs(root: str) -> List[str]:\n",
    "    # Purpose: collect all WAVs under a root folder\n",
    "    return sorted([str(p) for p in Path(root).rglob(\"*.wav\")])\n",
    "\n",
    "def d6_safe_write_wav(path: str, audio: np.ndarray, sr: int) -> None:\n",
    "    # Purpose: reliable write with retries\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "    last_err = None\n",
    "    for attempt in range(1, D6_WRITE_RETRIES + 1):\n",
    "        try:\n",
    "            sf.write(path, np.clip(audio, -1.0, 1.0), sr, subtype=\"PCM_16\")\n",
    "            return\n",
    "        except Exception as e:\n",
    "            last_err = e\n",
    "            time.sleep(D6_WRITE_SLEEP * attempt)\n",
    "    raise RuntimeError(f\"Failed to write WAV: {path}. Last error: {repr(last_err)}\")\n",
    "\n",
    "# -------------------------\n",
    "# Build file index (one row per source WAV)\n",
    "# Inputs: raw HC and PD WAV lists\n",
    "# Output: d6_files table used for metadata merge and processing\n",
    "# -------------------------\n",
    "hc_wavs = d6_list_wavs(D6_HC_ROOT)\n",
    "pd_wavs = d6_list_wavs(D6_PD_ROOT)\n",
    "\n",
    "print(\"\\nHC wav count:\", len(hc_wavs))\n",
    "print(\"PD wav count:\", len(pd_wavs))\n",
    "\n",
    "def d6_build_file_df(wavs: List[str], label_str: str) -> pd.DataFrame:\n",
    "    # Purpose: attach labels and IDs based on file names\n",
    "    rows = []\n",
    "    for fp in wavs:\n",
    "        base = os.path.basename(fp)\n",
    "        stem = Path(fp).stem\n",
    "        rows.append({\n",
    "            \"dataset\": \"D6\",\n",
    "            \"task\": D6_TASK5,\n",
    "            \"label_str\": label_str,\n",
    "            \"label_num\": 1 if label_str.lower() == \"parkinson\" else 0,\n",
    "            \"speaker_id\": str(stem),\n",
    "            \"sample_id\": str(base),\n",
    "            \"source_path\": str(fp),\n",
    "        })\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "d6_files = pd.concat(\n",
    "    [d6_build_file_df(hc_wavs, \"Healthy\"), d6_build_file_df(pd_wavs, \"Parkinson\")],\n",
    "    ignore_index=True\n",
    ")\n",
    "\n",
    "print(\"D6 file rows:\", len(d6_files))\n",
    "print(\"Label counts (rows):\")\n",
    "print(d6_files[\"label_str\"].value_counts(dropna=False))\n",
    "\n",
    "if len(d6_files) == 0:\n",
    "    raise RuntimeError(\"No D6 wav files found under HC/PD roots.\")\n",
    "\n",
    "# -------------------------\n",
    "# Load demographics and merge onto file table\n",
    "# Inputs: Demographics_age_sex.xlsx\n",
    "# Output: d6 table with age and sex (when available)\n",
    "# -------------------------\n",
    "d6_demo = pd.read_excel(D6_META_XLSX)\n",
    "\n",
    "col_sample = d6_find_col(d6_demo, [\"Sample ID\", \"SampleID\", \"Sample_Id\", \"ID\"])\n",
    "col_age    = d6_find_col(d6_demo, [\"Age\"])\n",
    "col_sex    = d6_find_col(d6_demo, [\"Sex\", \"Gender\"])\n",
    "\n",
    "if col_sample is None:\n",
    "    raise ValueError(f\"Could not find a Sample ID column in {D6_META_XLSX}. Columns: {list(d6_demo.columns)}\")\n",
    "\n",
    "d6_demo[col_sample] = d6_demo[col_sample].astype(str).str.strip()\n",
    "d6_demo[\"__sample_id_wav__\"] = d6_demo[col_sample].apply(lambda x: x if x.lower().endswith(\".wav\") else f\"{x}.wav\")\n",
    "\n",
    "keep_cols = [\"__sample_id_wav__\"]\n",
    "if col_age: keep_cols.append(col_age)\n",
    "if col_sex: keep_cols.append(col_sex)\n",
    "\n",
    "d6 = d6_files.merge(\n",
    "    d6_demo[keep_cols],\n",
    "    left_on=\"sample_id\",\n",
    "    right_on=\"__sample_id_wav__\",\n",
    "    how=\"left\"\n",
    ").drop(columns=[\"__sample_id_wav__\"], errors=\"ignore\")\n",
    "\n",
    "if col_age:\n",
    "    d6 = d6.rename(columns={col_age: \"age\"})\n",
    "else:\n",
    "    d6[\"age\"] = np.nan\n",
    "\n",
    "if col_sex:\n",
    "    d6 = d6.rename(columns={col_sex: \"sex\"})\n",
    "else:\n",
    "    d6[\"sex\"] = np.nan\n",
    "\n",
    "missing_both = d6[\"age\"].isna() & d6[\"sex\"].isna()\n",
    "print(\"\\nRows total:\", len(d6))\n",
    "print(\"Rows missing BOTH age and sex:\", int(missing_both.sum()))\n",
    "\n",
    "# -------------------------\n",
    "# Create candidate clips (stream-write)\n",
    "# Inputs: each source WAV path\n",
    "# Outputs: one candidate WAV per source in clips/_candidates and a candidate table\n",
    "# -------------------------\n",
    "cand_rows: List[Dict] = []\n",
    "warn_rows: List[Dict] = []\n",
    "cand_counter = 0\n",
    "\n",
    "pbar = tqdm(d6.itertuples(index=False), total=len(d6),\n",
    "            desc=\"D6 preprocess (1 clip per source; write candidates)\", dynamic_ncols=True)\n",
    "\n",
    "for r in pbar:\n",
    "    try:\n",
    "        src = str(r.source_path)\n",
    "\n",
    "        # Load -> resample -> normalize\n",
    "        y, sr0 = d6_read_mono(src)\n",
    "        y = d6_resample(y, sr0, D6_SR)\n",
    "        y = d6_norm_rms_then_peak(y)\n",
    "\n",
    "        # Extract voiced stream (or fall back to original if too short)\n",
    "        voiced, vad_used = d6_voiced_concat(y, D6_SR)\n",
    "        if len(voiced) < int(D6_SR * D6_MIN_KEEP_SEC):\n",
    "            voiced = y\n",
    "            warn_rows.append({\n",
    "                \"dataset\":\"D6\",\n",
    "                \"speaker_id\": str(r.speaker_id),\n",
    "                \"source_path\": src,\n",
    "                \"warning_type\":\"vad_too_short_fallback_original\",\n",
    "                \"detail\": f\"vad_used={vad_used}\"\n",
    "            })\n",
    "\n",
    "        # One clip only: center-crop up to target seconds (no padding)\n",
    "        clip, st, en = d6_choose_clip_center(voiced, D6_SR, D6_TARGET_SEC)\n",
    "\n",
    "        if clip is None or len(clip) == 0:\n",
    "            warn_rows.append({\n",
    "                \"dataset\":\"D6\",\n",
    "                \"speaker_id\": str(r.speaker_id),\n",
    "                \"source_path\": src,\n",
    "                \"warning_type\":\"empty_after_processing\",\n",
    "                \"detail\": \"clip length 0\"\n",
    "            })\n",
    "            continue\n",
    "\n",
    "        # Write candidate immediately\n",
    "        cand_counter += 1\n",
    "        cand_name = d6_safe(f\"CAND_{cand_counter:08d}.wav\")\n",
    "        cand_path = os.path.join(D6_CAND_DIR, cand_name)\n",
    "        d6_safe_write_wav(cand_path, clip.astype(np.float32, copy=False), D6_SR)\n",
    "\n",
    "        cand_rows.append({\n",
    "            \"dataset\":\"D6\",\n",
    "            \"task\": D6_TASK5,\n",
    "            \"speaker_id\": str(r.speaker_id),\n",
    "            \"sample_id\": str(r.sample_id),\n",
    "            \"label_str\": str(r.label_str),\n",
    "            \"label_num\": int(r.label_num),\n",
    "            \"age\": r.age if pd.notna(r.age) else np.nan,\n",
    "            \"sex\": r.sex if pd.notna(r.sex) else np.nan,\n",
    "            \"speaker_key_rel\": np.nan,\n",
    "            \"source_path\": src,\n",
    "            \"duration_sec\": float(len(clip)/D6_SR),\n",
    "            \"clip_start_sec\": float(st),\n",
    "            \"clip_end_sec\": float(en),\n",
    "            \"sr_hz\": int(D6_SR),\n",
    "            \"channels\": 1,\n",
    "            \"clip_is_contiguous\": True,\n",
    "            \"clip_path_cand\": cand_path,\n",
    "        })\n",
    "\n",
    "    except Exception as e:\n",
    "        warn_rows.append({\n",
    "            \"dataset\":\"D6\",\n",
    "            \"speaker_id\": getattr(r, \"speaker_id\", \"\"),\n",
    "            \"source_path\": getattr(r, \"source_path\", \"\"),\n",
    "            \"warning_type\":\"preprocess_error\",\n",
    "            \"detail\": repr(e)\n",
    "        })\n",
    "\n",
    "cand_df = pd.DataFrame(cand_rows)\n",
    "warnings_df = pd.DataFrame(warn_rows)\n",
    "\n",
    "print(\"\\nD6 candidates written:\", len(cand_df))\n",
    "print(\"Candidate dir:\", D6_CAND_DIR)\n",
    "\n",
    "if len(cand_df) == 0:\n",
    "    if len(warnings_df):\n",
    "        print(\"\\nSample warnings (first 10):\")\n",
    "        print(warnings_df.head(10).to_string(index=False))\n",
    "    raise RuntimeError(\"No D6 candidates produced. See warnings above (if any).\")\n",
    "\n",
    "# -------------------------\n",
    "# Cap candidates per speaker/task and delete the rest\n",
    "# Inputs: cand_df and cap limit\n",
    "# Outputs: capped cand_df and fewer files in clips/_candidates\n",
    "# -------------------------\n",
    "def d6_apply_cap_and_delete(df: pd.DataFrame, max_k: int, seed: int) -> pd.DataFrame:\n",
    "    if df.empty:\n",
    "        return df.copy()\n",
    "\n",
    "    rng = np.random.default_rng(seed)\n",
    "    keep_idx: List[int] = []\n",
    "    for (spk, task), g in df.groupby([\"speaker_id\", \"task\"], sort=False):\n",
    "        idx = g.index.to_numpy()\n",
    "        if len(idx) <= max_k:\n",
    "            keep_idx.extend(idx.tolist())\n",
    "        else:\n",
    "            chosen = rng.choice(idx, size=max_k, replace=False)\n",
    "            keep_idx.extend(chosen.tolist())\n",
    "\n",
    "    keep_idx = sorted(set(keep_idx))\n",
    "    keep_set = set(keep_idx)\n",
    "\n",
    "    # Purpose: remove unneeded candidate files to save space\n",
    "    to_delete = df.loc[~df.index.isin(list(keep_set)), \"clip_path_cand\"].tolist()\n",
    "    deleted = 0\n",
    "    for p in to_delete:\n",
    "        try:\n",
    "            if os.path.exists(p):\n",
    "                os.remove(p)\n",
    "                deleted += 1\n",
    "        except Exception as e:\n",
    "            nonlocal_warn.append({\n",
    "                \"dataset\":\"D6\",\n",
    "                \"speaker_id\":\"\",\n",
    "                \"source_path\":\"\",\n",
    "                \"warning_type\":\"candidate_delete_failed\",\n",
    "                \"detail\": f\"{p} :: {repr(e)}\"\n",
    "            })\n",
    "\n",
    "    return df.loc[keep_idx].reset_index(drop=True), deleted\n",
    "\n",
    "# Purpose: collect warnings from inside cap function without changing behavior\n",
    "nonlocal_warn: List[Dict] = []\n",
    "\n",
    "cand_df, deleted_count = d6_apply_cap_and_delete(cand_df, D6_MAX_CLIPS_PER_SPK_TASK, D6_RANDOM_SEED)\n",
    "if nonlocal_warn:\n",
    "    warnings_df = pd.concat([warnings_df, pd.DataFrame(nonlocal_warn)], ignore_index=True)\n",
    "\n",
    "print(\"D6 clips after cap:\", len(cand_df))\n",
    "print(\"Deleted unkept candidates:\", int(deleted_count))\n",
    "\n",
    "# -------------------------\n",
    "# Speaker-level split (label-stratified)\n",
    "# Inputs: speakers after cap\n",
    "# Outputs: a split label for each speaker, then merged onto cand_df\n",
    "# -------------------------\n",
    "spk_tbl = cand_df[[\"speaker_id\",\"label_str\"]].drop_duplicates().copy()\n",
    "rng = random.Random(D6_RANDOM_SEED)\n",
    "\n",
    "split_rows = []\n",
    "for lab_name, g in spk_tbl.groupby(\"label_str\"):\n",
    "    spks = g[\"speaker_id\"].tolist()\n",
    "    rng.shuffle(spks)\n",
    "    n = len(spks)\n",
    "\n",
    "    n_train = int(round(n * D6_TRAIN_PCT))\n",
    "    n_val   = int(round(n * D6_VAL_PCT))\n",
    "    n_train = min(n_train, n)\n",
    "    n_val   = min(n_val, n - n_train)\n",
    "\n",
    "    train = spks[:n_train]\n",
    "    val   = spks[n_train:n_train+n_val]\n",
    "    test  = spks[n_train+n_val:]\n",
    "\n",
    "    split_rows += [{\"speaker_id\": s, \"split\":\"train\"} for s in train]\n",
    "    split_rows += [{\"speaker_id\": s, \"split\":\"val\"} for s in val]\n",
    "    split_rows += [{\"speaker_id\": s, \"split\":\"test\"} for s in test]\n",
    "\n",
    "spk_split = pd.DataFrame(split_rows)\n",
    "cand_df = cand_df.merge(spk_split, on=\"speaker_id\", how=\"left\")\n",
    "\n",
    "# Purpose: ensure every kept speaker received a split\n",
    "if \"split\" not in cand_df.columns or cand_df[\"split\"].isna().any():\n",
    "    ex = cand_df.loc[cand_df.get(\"split\").isna() if \"split\" in cand_df.columns else [True]*len(cand_df), \"speaker_id\"] \\\n",
    "                .drop_duplicates().head(10).tolist()\n",
    "    raise RuntimeError(f\"Some speakers did not get split. Example: {ex}\")\n",
    "\n",
    "for sp in [\"train\",\"val\",\"test\"]:\n",
    "    os.makedirs(os.path.join(D6_CLIPS_DIR, sp), exist_ok=True)\n",
    "\n",
    "print(\"\\nSplit counts (clips):\")\n",
    "print(cand_df[\"split\"].value_counts(dropna=False))\n",
    "print(\"Label by split (clips):\")\n",
    "print(pd.crosstab(cand_df[\"split\"], cand_df[\"label_str\"]))\n",
    "\n",
    "# -------------------------\n",
    "# Finalize clips and build manifest\n",
    "# Inputs: kept candidate WAVs + split labels\n",
    "# Outputs: clips/<split>/ WAVs and manifest rows\n",
    "# -------------------------\n",
    "final_rows: List[Dict] = []\n",
    "global_idx = 0\n",
    "\n",
    "pbarw = tqdm(cand_df.itertuples(index=False), total=len(cand_df),\n",
    "             desc=\"D6 finalize (move kept candidates)\", dynamic_ncols=True)\n",
    "\n",
    "for r in pbarw:\n",
    "    global_idx += 1\n",
    "\n",
    "    # Purpose: name files consistently without relying on folder structure\n",
    "    tag = \"PD\" if str(r.label_str).lower().startswith(\"parkinson\") else \"HC\"\n",
    "    spk = str(r.speaker_id)\n",
    "    split = str(r.split)\n",
    "\n",
    "    out_name = d6_safe(f\"D6_{tag}_{spk}_{D6_TASK5}_{global_idx:06d}.wav\")\n",
    "    out_path = os.path.join(D6_CLIPS_DIR, split, out_name)\n",
    "\n",
    "    cand_path = str(r.clip_path_cand)\n",
    "    if not os.path.exists(cand_path):\n",
    "        warnings_df = pd.concat([warnings_df, pd.DataFrame([{\n",
    "            \"dataset\":\"D6\",\n",
    "            \"speaker_id\": spk,\n",
    "            \"source_path\": str(r.source_path),\n",
    "            \"warning_type\":\"missing_candidate_file\",\n",
    "            \"detail\": cand_path\n",
    "        }])], ignore_index=True)\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        shutil.move(cand_path, out_path)\n",
    "    except Exception as e:\n",
    "        warnings_df = pd.concat([warnings_df, pd.DataFrame([{\n",
    "            \"dataset\":\"D6\",\n",
    "            \"speaker_id\": spk,\n",
    "            \"source_path\": str(r.source_path),\n",
    "            \"warning_type\":\"candidate_move_failed\",\n",
    "            \"detail\": f\"{cand_path} -> {out_path} :: {repr(e)}\"\n",
    "        }])], ignore_index=True)\n",
    "        continue\n",
    "\n",
    "    final_rows.append({\n",
    "        \"split\": split,\n",
    "        \"dataset\": \"D6\",\n",
    "        \"task\": D6_TASK5,\n",
    "        \"speaker_id\": spk,\n",
    "        \"sample_id\": str(r.sample_id),\n",
    "        \"label_str\": str(r.label_str),\n",
    "        \"label_num\": int(r.label_num),\n",
    "        \"age\": r.age if pd.notna(r.age) else np.nan,\n",
    "        \"sex\": r.sex if pd.notna(r.sex) else np.nan,\n",
    "        \"speaker_key_rel\": np.nan,\n",
    "        \"clip_path\": out_path,\n",
    "        \"duration_sec\": float(r.duration_sec),\n",
    "        \"source_path\": str(r.source_path),\n",
    "        \"clip_start_sec\": float(r.clip_start_sec),\n",
    "        \"clip_end_sec\": float(r.clip_end_sec),\n",
    "        \"sr_hz\": int(D6_SR),\n",
    "        \"channels\": 1,\n",
    "        \"clip_is_contiguous\": True,\n",
    "    })\n",
    "\n",
    "manifest_df = pd.DataFrame(final_rows)\n",
    "\n",
    "# Purpose: enforce schema and column order even if some fields are missing\n",
    "for c in MANIFEST_COLS:\n",
    "    if c not in manifest_df.columns:\n",
    "        manifest_df[c] = np.nan\n",
    "manifest_df = manifest_df[MANIFEST_COLS].copy()\n",
    "\n",
    "# Purpose: remove staging folder when possible\n",
    "try:\n",
    "    if os.path.isdir(D6_CAND_DIR):\n",
    "        leftovers = list(os.scandir(D6_CAND_DIR))\n",
    "        for ent in leftovers:\n",
    "            try:\n",
    "                os.remove(ent.path)\n",
    "            except Exception:\n",
    "                pass\n",
    "        try:\n",
    "            os.rmdir(D6_CAND_DIR)\n",
    "        except Exception:\n",
    "            pass\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# -------------------------\n",
    "# Save standardized outputs\n",
    "# Outputs: manifest_all.csv, preprocess_warnings.csv, dataset_summary.json, run_config.json\n",
    "# -------------------------\n",
    "manifest_all_path = os.path.join(D6_MANIFEST_DIR, \"manifest_all.csv\")\n",
    "warnings_path     = os.path.join(D6_LOGS_DIR, \"preprocess_warnings.csv\")\n",
    "summary_path      = os.path.join(D6_LOGS_DIR, \"dataset_summary.json\")\n",
    "run_cfg_path      = os.path.join(D6_CONFIG_DIR, \"run_config.json\")\n",
    "\n",
    "manifest_df.to_csv(manifest_all_path, index=False)\n",
    "warnings_df.to_csv(warnings_path, index=False)\n",
    "\n",
    "summary = {\n",
    "    \"dataset\": \"D6\",\n",
    "    \"source_root\": D6_DATASET_DIR,\n",
    "    \"hc_root\": D6_HC_ROOT,\n",
    "    \"pd_root\": D6_PD_ROOT,\n",
    "    \"metadata_xlsx\": D6_META_XLSX,\n",
    "    \"sr_hz\": int(D6_SR),\n",
    "    \"webrtcvad_available\": bool(D6_HAVE_WEBRTCVAD),\n",
    "    \"scipy_available\": bool(D6_HAVE_SCIPY),\n",
    "    \"task\": D6_TASK5,\n",
    "    \"one_clip_per_source_file\": True,\n",
    "    \"candidate_write_timing\": \"written immediately to clips/_candidates then moved to clips/<split>\",\n",
    "    \"cap_policy\": {\n",
    "        \"applied_stage\": \"after_candidates_before_splitting\",\n",
    "        \"grouping\": [\"speaker_id\", \"task\"],\n",
    "        \"max_per_group\": int(D6_MAX_CLIPS_PER_SPK_TASK)\n",
    "    },\n",
    "    \"splits\": {\"train\": D6_TRAIN_PCT, \"val\": D6_VAL_PCT, \"test\": D6_TEST_PCT},\n",
    "    \"counts\": {\n",
    "        \"n_source_files\": int(len(d6_files)),\n",
    "        \"n_candidates_written_pre_cap\": int(len(cand_rows)),\n",
    "        \"n_candidates_post_cap\": int(len(cand_df)),\n",
    "        \"n_clips_written\": int(len(manifest_df)),\n",
    "        \"split_counts_clips\": manifest_df[\"split\"].value_counts(dropna=False).to_dict() if len(manifest_df) else {},\n",
    "        \"label_counts_clips\": manifest_df[\"label_str\"].value_counts(dropna=False).to_dict() if len(manifest_df) else {},\n",
    "        \"n_warnings\": int(len(warnings_df)),\n",
    "        \"deleted_unkept_candidates\": int(deleted_count),\n",
    "    }\n",
    "}\n",
    "with open(summary_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "run_cfg = {\n",
    "    \"dataset\": \"D6\",\n",
    "    \"paths\": {\n",
    "        \"dataset_dir\": D6_DATASET_DIR,\n",
    "        \"hc_root\": D6_HC_ROOT,\n",
    "        \"pd_root\": D6_PD_ROOT,\n",
    "        \"metadata_xlsx\": D6_META_XLSX,\n",
    "        \"out_root\": D6_OUT_ROOT,\n",
    "        \"clips_dir\": D6_CLIPS_DIR,\n",
    "        \"candidate_dir\": D6_CAND_DIR,\n",
    "        \"manifest_all\": manifest_all_path,\n",
    "        \"warnings_csv\": warnings_path,\n",
    "        \"summary_json\": summary_path,\n",
    "    },\n",
    "    \"standard_structure\": {\n",
    "        \"clips\": \"clips/<split>/ (flat) with temporary clips/_candidates during run\",\n",
    "        \"manifests\": \"manifests/manifest_all.csv\",\n",
    "        \"config\": \"config/run_config.json\",\n",
    "        \"logs\": [\"logs/preprocess_warnings.csv\", \"logs/dataset_summary.json\"],\n",
    "    },\n",
    "    \"filename_format\": \"D6_{HC|PD}_{speaker_id}_{task<=5}_{global_index:06d}.wav\",\n",
    "    \"task_short\": D6_TASK5,\n",
    "    \"one_clip_per_source_file\": True,\n",
    "    \"clip_selection\": {\n",
    "        \"method\": \"center-crop voiced-concatenated stream\",\n",
    "        \"target_sec\": float(D6_TARGET_SEC),\n",
    "        \"padding\": False\n",
    "    },\n",
    "    \"normalization\": {\n",
    "        \"method\": \"manual RMS gain + peak limit\",\n",
    "        \"target_rms_dbfs\": float(D6_TARGET_RMS_DBFS),\n",
    "        \"peak_limit_dbfs\": float(D6_PEAK_LIMIT_DBFS),\n",
    "        \"min_rms_dbfs\": float(D6_MIN_RMS_DBFS),\n",
    "        \"max_gain_db\": float(D6_MAX_GAIN_DB),\n",
    "        \"pyloudnorm\": False\n",
    "    },\n",
    "    \"vad\": {\n",
    "        \"method\": \"webrtcvad if available else energy fallback\",\n",
    "        \"mode\": int(D6_VAD_MODE),\n",
    "        \"frame_ms\": int(D6_FRAME_MS),\n",
    "        \"min_speech_ms\": int(D6_MIN_SPEECH_MS),\n",
    "        \"merge_gap_ms\": int(D6_MERGE_GAP_MS),\n",
    "        \"pad_sec\": float(D6_PAD_SEC),\n",
    "        \"min_keep_sec\": float(D6_MIN_KEEP_SEC)\n",
    "    },\n",
    "    \"cap\": {\"max_per_speaker_task\": int(D6_MAX_CLIPS_PER_SPK_TASK)},\n",
    "    \"seed\": int(D6_RANDOM_SEED),\n",
    "}\n",
    "with open(run_cfg_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(run_cfg, f, indent=2)\n",
    "\n",
    "print(\"\\nDONE: D6 preprocessing\")\n",
    "print(\"Manifest:\", manifest_all_path)\n",
    "print(\"Warnings:\", warnings_path)\n",
    "print(\"Summary:\", summary_path)\n",
    "print(\"Run config:\", run_cfg_path)\n",
    "print(\"Clips written:\", int(len(manifest_df)))\n",
    "print(\"Warnings:\", int(len(warnings_df)))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 845,
     "referenced_widgets": [
      "f32ebf25994440d1bf44823068463e34",
      "9e4adc6c61404fd291c0602b4e7df13d",
      "c19b89173aee465e86313e628d84826b",
      "0ce017efdd5a46d1bcce2a8e4371dcec",
      "23ffe527f2554ad18dfea37c58d26b28",
      "4e7ea6cedfa74fd08eca902d742ceca8",
      "981687e49e1840cb98c1088037242d6d",
      "a95ac282ad474be6a6757e5264a0340b",
      "cf94765b49bb4527a04bcf70a6e577f9",
      "a5234dc506a24944a19599435ed2d17c",
      "f647ed86a52a434a9dcea44a18e3414e",
      "8d0df96c00e54ef39a3e707f377d3a9a",
      "36de0b0071a5484c9b1c39e8be09dbfc",
      "7239f4a254d94376bfa1a956982a273e",
      "eedbfc20a0574061b962685a7ab60045",
      "b2ab046aedd44e278f35724628a60674",
      "c96bf5bceca04eaab2022b04d687dba2",
      "d6a86e6bb27747a4bcfbb55f9c2fb73f",
      "0de128f806b845b381580c2366cea8a6",
      "37ba480e9f4a43dda434b1813e18b91b",
      "029bbbf9c7de4843a6598e5dcd0928f5",
      "f1856c7bbb934929a8226fa24a1dca02"
     ]
    },
    "id": "W2j-ktXeiCQw"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "#Training and Validation of D1, D2, D4, D5 and D6 Datasets"
   ],
   "metadata": {
    "id": "KStB9jutkB7m"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The following cell trains and validates the baseline model on Dataset D1 (NeuroVoz Spanish) using only the preprocessed audio clips listed in `manifests/manifest_all.csv`. It is written to be robust against failures by checking inputs early, showing progress bars, and saving outputs into a new time stamped experiment folder so previous runs are never overwritten. The Wav2Vec2 backbone remains frozen, and only the small task specific heads are trained.\n",
    "\n",
    "The cell starts by importing all required libraries and setting the D1 output root path (`DX_OUT_ROOT`) along with the manifest location. An experiment record is then created using an experiment tag and a timestamp, and the full output folder structure is built as\n",
    "`<DX_OUT_ROOT>/trainval_runs/exp_<tag>_<timestamp>/run_<dataset>_seed####/`.\n",
    "\n",
    "Next, fixed training settings are defined to ensure repeatable results. These include up to 10 epochs, an effective batch size of 64 using gradient accumulation, a learning rate of 1e-3, early stopping after two validation AUROC non improvements, three fixed random seeds (1337, 2024, 7777), mixed precision on the GPU for speed, and a fixed probability threshold of 0.5 for threshold based metrics. The main run settings and GPU information are printed.\n",
    "\n",
    "The cell then loads `manifest_all.csv`, checks that all required columns are present, keeps only rows marked as `train` or `val`, and infers the dataset name from the most common `dataset` value if that column exists. It prints the number of training and validation rows along with class counts for Healthy and Parkinson’s clips, and stops immediately if either split is empty. A progress bar based check then confirms that all audio files listed in `clip_path` exist, failing early and reporting example missing paths if any are found.\n",
    "\n",
    "Each row is assigned to a task group, where `task == \"vowl\"` is treated as vowel speech and all other values are treated as non vowel speech. A custom dataset loader reads each audio clip, enforces a 16 kHz sample rate, converts audio to mono if needed, and creates an attention mask. For vowel clips, trailing near zero padding is detected using a small amplitude threshold and masked out so padded silence does not affect learning. For non vowel clips, the attention mask contains all ones. A custom collator pads audio clips and attention masks within each batch to a common length.\n",
    "\n",
    "The model uses a frozen Wav2Vec2 backbone with a two head classifier on top. After feature extraction, a masked mean pooling step summarizes each clip. Two small pre head blocks, each made of LayerNorm and Dropout, are used, one for vowel clips and one for non vowel clips, followed by separate linear classifiers. To avoid issues under mixed precision, all head computations are forced to run in float32. Each batch routes data through the correct head based on the task group.\n",
    "\n",
    "For each random seed, the cell creates a seed specific run folder, builds data loaders, and runs a short warm up by loading three training batches to catch disk or batching problems early. Training then proceeds epoch by epoch with validation after each epoch, tracking validation AUROC. Early stopping is applied when AUROC does not improve for two consecutive epochs. Only the best validation epoch is saved, including the best head weights (`best_heads.pt`), validation plots (`roc_curve.png` and `confusion_matrix.png` at threshold 0.5), and a `metrics.json` file containing the best AUROC and epoch, dataset sizes, label counts, run settings, and additional threshold based metrics such as accuracy, precision, recall or sensitivity, specificity, F1 score, MCC, and a two sided Fisher exact test p value.\n",
    "\n",
    "After all three seeds complete, the cell prints the AUROC for each seed and computes the mean AUROC with a 95% confidence interval using a t distribution with n=3. A single `summary_trainval.json` file is written for the experiment, and the same summary is appended as one line to the global history log at\n",
    "`<DX_OUT_ROOT>/trainval_runs/history_index.jsonl`.\n",
    "\n",
    "Finally, the cell prints the path to the experiment folder and unassigns the Colab runtime to cleanly stop the L4 GPU session."
   ],
   "metadata": {
    "id": "vKgfj8HNjwSw"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# ============================================================\n",
    "# D1 Train + Val — Frozen Wav2Vec2 + Two Task Heads, Full Run History\n",
    "# Inputs: manifest_all.csv (train/val rows) and audio clips referenced by clip_path\n",
    "# Outputs: per-seed run folders (metrics.json, roc_curve.png, confusion_matrix.png, best_heads.pt),\n",
    "#          plus one experiment summary_trainval.json and one appended history_index.jsonl record\n",
    "# ============================================================\n",
    "\n",
    "import os, json, math, random, time, warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import soundfile as sf\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from transformers import Wav2Vec2Model\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, confusion_matrix, matthews_corrcoef\n",
    "from scipy.stats import fisher_exact\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# -------------------------\n",
    "# Dataset root and manifest location\n",
    "# Input: dataset output root created by preprocessing\n",
    "# Output: MANIFEST_ALL path used for train/val tables\n",
    "# -------------------------\n",
    "DX_OUT_ROOT = \"/content/drive/MyDrive/AI_PD_Project/Datasets/D1-NeuroVoz-Castillan Spanish/preprocessed_v1\"\n",
    "MANIFEST_ALL = f\"{DX_OUT_ROOT}/manifests/manifest_all.csv\"\n",
    "\n",
    "# -------------------------\n",
    "# Experiment folder naming (keeps older runs intact)\n",
    "# Output: one EXP_ROOT folder for this execution\n",
    "# -------------------------\n",
    "EXPERIMENT_TAG = \"frozen_LNDO\"   # short label used in folder and summary files\n",
    "RUN_STAMP = time.strftime(\"%Y%m%d_%H%M%S\")  # unique timestamp per run\n",
    "TRAINVAL_ROOT = Path(DX_OUT_ROOT) / \"trainval_runs\"\n",
    "EXP_ROOT = TRAINVAL_ROOT / f\"exp_{EXPERIMENT_TAG}_{RUN_STAMP}\"\n",
    "EXP_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# -------------------------\n",
    "# Training settings (same across seeds)\n",
    "# Purpose: stable and comparable runs\n",
    "# -------------------------\n",
    "MAX_EPOCHS     = 10\n",
    "EFFECTIVE_BS   = 64\n",
    "PER_DEVICE_BS  = 16\n",
    "GRAD_ACCUM     = max(1, EFFECTIVE_BS // PER_DEVICE_BS)\n",
    "\n",
    "LR             = 1e-3\n",
    "PATIENCE       = 2\n",
    "SEEDS          = [1337, 2024, 7777]\n",
    "\n",
    "BACKBONE_CKPT  = \"facebook/wav2vec2-base\"\n",
    "SR_EXPECTED    = 16000\n",
    "\n",
    "# Purpose: detect where trailing padding starts for vowel clips\n",
    "TINY_THRESH    = 1e-4\n",
    "\n",
    "# Purpose: small regularization in the trainable head path\n",
    "DROPOUT_P      = 0.2\n",
    "\n",
    "# Purpose: stable loading from Drive-backed files\n",
    "NUM_WORKERS    = 0\n",
    "PIN_MEMORY     = False\n",
    "\n",
    "# Purpose: fixed operating point for confusion matrix and threshold metrics\n",
    "VAL_THRESHOLD  = 0.5\n",
    "\n",
    "# Purpose: speed on GPU while keeping head math stable (heads forced FP32 later)\n",
    "USE_AMP        = True\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "try:\n",
    "    torch.set_float32_matmul_precision(\"high\")\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", message=\"None of the inputs have requires_grad=True. Gradients will be None\")\n",
    "warnings.filterwarnings(\"ignore\", message=\"Passing `gradient_checkpointing` to a config initialization is deprecated\")\n",
    "\n",
    "print(\"EXPERIMENT ROOT:\", str(EXP_ROOT))\n",
    "print(\"DEVICE:\", DEVICE)\n",
    "if DEVICE.type == \"cuda\":\n",
    "    print(\"GPU:\", torch.cuda.get_device_name(0))\n",
    "print(\"PER_DEVICE_BS:\", PER_DEVICE_BS, \"| GRAD_ACCUM:\", GRAD_ACCUM, \"| EFFECTIVE_BS:\", PER_DEVICE_BS * GRAD_ACCUM)\n",
    "print(\"NUM_WORKERS:\", NUM_WORKERS, \"| PIN_MEMORY:\", PIN_MEMORY)\n",
    "print(\"VAL_THRESHOLD:\", VAL_THRESHOLD)\n",
    "print(\"USE_AMP:\", bool(USE_AMP and DEVICE.type == \"cuda\"))\n",
    "\n",
    "# -------------------------\n",
    "# Load and filter manifest (train/val only)\n",
    "# Inputs: manifest_all.csv\n",
    "# Outputs: train_df, val_df, dataset_id, and basic counts printed\n",
    "# -------------------------\n",
    "if not os.path.exists(MANIFEST_ALL):\n",
    "    raise FileNotFoundError(f\"Missing manifest_all.csv: {MANIFEST_ALL}\")\n",
    "\n",
    "m = pd.read_csv(MANIFEST_ALL)\n",
    "req_cols = {\"split\", \"clip_path\", \"label_num\", \"task\"}\n",
    "missing = [c for c in sorted(req_cols) if c not in m.columns]\n",
    "if missing:\n",
    "    raise ValueError(f\"Manifest missing required columns: {missing}. Found: {list(m.columns)}\")\n",
    "\n",
    "m = m[m[\"split\"].isin([\"train\", \"val\"])].copy()\n",
    "if len(m) == 0:\n",
    "    raise RuntimeError(\"After filtering to split in {train,val}, manifest has 0 rows.\")\n",
    "\n",
    "# Purpose: pick the dominant dataset label if multiple are present\n",
    "if \"dataset\" in m.columns and m[\"dataset\"].notna().any():\n",
    "    dataset_id = str(m[\"dataset\"].value_counts(dropna=True).idxmax())\n",
    "    m = m[m[\"dataset\"].astype(str) == dataset_id].copy()\n",
    "else:\n",
    "    dataset_id = \"DX\"\n",
    "\n",
    "# Purpose: keep a consistent set of columns even if some are missing\n",
    "keep_cols = [\"clip_path\", \"label_num\", \"task\", \"speaker_id\", \"duration_sec\", \"split\"]\n",
    "for c in keep_cols:\n",
    "    if c not in m.columns:\n",
    "        m[c] = np.nan\n",
    "m = m[keep_cols].copy()\n",
    "\n",
    "train_df = m[m[\"split\"] == \"train\"].copy().reset_index(drop=True)\n",
    "val_df   = m[m[\"split\"] == \"val\"].copy().reset_index(drop=True)\n",
    "\n",
    "print(f\"\\nDataset inferred: {dataset_id}\")\n",
    "print(f\"Train rows: {len(train_df)} | Val rows: {len(val_df)}\")\n",
    "print(\"Train label counts:\", train_df[\"label_num\"].value_counts(dropna=False).to_dict())\n",
    "print(\"Val label counts:\",   val_df[\"label_num\"].value_counts(dropna=False).to_dict())\n",
    "\n",
    "if len(train_df) == 0 or len(val_df) == 0:\n",
    "    raise RuntimeError(\"Train or Val split has 0 rows.\")\n",
    "\n",
    "# -------------------------\n",
    "# Clip existence check (progress bar)\n",
    "# Inputs: train_df/val_df clip_path\n",
    "# Output: fail early with a few missing examples\n",
    "# -------------------------\n",
    "def _fail_fast_missing_paths(df: pd.DataFrame, name: str):\n",
    "    missing_paths = []\n",
    "    for p in tqdm(df[\"clip_path\"].astype(str).tolist(), desc=f\"Check {name} clip_path exists\", dynamic_ncols=True):\n",
    "        if not os.path.exists(p):\n",
    "            missing_paths.append(p)\n",
    "            if len(missing_paths) >= 10:\n",
    "                break\n",
    "    if missing_paths:\n",
    "        raise FileNotFoundError(f\"{name}: missing clip_path(s). Examples: {missing_paths}\")\n",
    "\n",
    "_fail_fast_missing_paths(train_df, \"TRAIN\")\n",
    "_fail_fast_missing_paths(val_df, \"VAL\")\n",
    "\n",
    "# -------------------------\n",
    "# Task grouping (vowel vs other)\n",
    "# Purpose: route each clip to the matching head\n",
    "# Output: task_group column used by dataset and model\n",
    "# -------------------------\n",
    "def _task_group(task_val) -> str:\n",
    "    return \"vowel\" if str(task_val) == \"vowl\" else \"other\"\n",
    "\n",
    "train_df[\"task_group\"] = train_df[\"task\"].apply(_task_group)\n",
    "val_df[\"task_group\"]   = val_df[\"task\"].apply(_task_group)\n",
    "\n",
    "# -------------------------\n",
    "# Dataset and collator (dynamic padding + custom mask)\n",
    "# Inputs: one manifest row per sample\n",
    "# Outputs: padded batches with input_values, attention_mask, labels, task_group\n",
    "# -------------------------\n",
    "class AudioManifestDataset(Dataset):\n",
    "    def __init__(self, df: pd.DataFrame):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        row = self.df.iloc[idx]\n",
    "        clip_path = str(row[\"clip_path\"])\n",
    "        label = int(row[\"label_num\"])\n",
    "        task_group = str(row[\"task_group\"])\n",
    "\n",
    "        if not os.path.exists(clip_path):\n",
    "            raise FileNotFoundError(f\"Missing clip_path: {clip_path}\")\n",
    "\n",
    "        # Purpose: load audio, convert to mono, force float32\n",
    "        y, sr = sf.read(clip_path, always_2d=False)\n",
    "        if isinstance(y, np.ndarray) and y.ndim > 1:\n",
    "            y = np.mean(y, axis=1)\n",
    "        y = np.asarray(y, dtype=np.float32)\n",
    "\n",
    "        # Purpose: enforce training sample rate\n",
    "        if int(sr) != SR_EXPECTED:\n",
    "            raise ValueError(f\"Sample rate mismatch (expected {SR_EXPECTED}): {clip_path} has sr={sr}\")\n",
    "\n",
    "        # Purpose: hide trailing padding in vowel clips; other clips are fully visible\n",
    "        attn = np.ones((len(y),), dtype=np.int64)\n",
    "        if task_group == \"vowel\":\n",
    "            k = -1\n",
    "            for j in range(len(y) - 1, -1, -1):\n",
    "                if abs(float(y[j])) > TINY_THRESH:\n",
    "                    k = j\n",
    "                    break\n",
    "            if k >= 0:\n",
    "                attn[:k+1] = 1\n",
    "                attn[k+1:] = 0\n",
    "            else:\n",
    "                attn[:] = 1\n",
    "        else:\n",
    "            attn[:] = 1\n",
    "\n",
    "        return {\n",
    "            \"input_values\": torch.from_numpy(y),\n",
    "            \"attention_mask\": torch.from_numpy(attn),\n",
    "            \"labels\": torch.tensor(label, dtype=torch.long),\n",
    "            \"task_group\": task_group,\n",
    "        }\n",
    "\n",
    "def collate_fn(batch):\n",
    "    # Purpose: pad each batch to its own max length, and pad mask with zeros\n",
    "    max_len = int(max(b[\"input_values\"].numel() for b in batch))\n",
    "    input_vals, attn_masks, labels, task_groups = [], [], [], []\n",
    "    for b in batch:\n",
    "        x = b[\"input_values\"]\n",
    "        a = b[\"attention_mask\"]\n",
    "        pad = max_len - x.numel()\n",
    "        if pad > 0:\n",
    "            x = torch.cat([x, torch.zeros(pad, dtype=x.dtype)], dim=0)\n",
    "            a = torch.cat([a, torch.zeros(pad, dtype=a.dtype)], dim=0)\n",
    "        input_vals.append(x)\n",
    "        attn_masks.append(a)\n",
    "        labels.append(b[\"labels\"])\n",
    "        task_groups.append(b[\"task_group\"])\n",
    "    return {\n",
    "        \"input_values\": torch.stack(input_vals, dim=0),\n",
    "        \"attention_mask\": torch.stack(attn_masks, dim=0),\n",
    "        \"labels\": torch.stack(labels, dim=0),\n",
    "        \"task_group\": task_groups,\n",
    "    }\n",
    "\n",
    "# -------------------------\n",
    "# Model: frozen backbone + per-task LN/Dropout + per-task linear head\n",
    "# Inputs: padded audio batch + attention_mask + task_group\n",
    "# Outputs: loss and logits (PD probability comes from softmax(logits)[:,1])\n",
    "# -------------------------\n",
    "class Wav2Vec2TwoHeadClassifier(nn.Module):\n",
    "    def __init__(self, ckpt: str, dropout_p: float):\n",
    "        super().__init__()\n",
    "\n",
    "        # Purpose: backbone features are fixed; only heads train\n",
    "        self.backbone = Wav2Vec2Model.from_pretrained(ckpt)\n",
    "        for p in self.backbone.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "        H = int(self.backbone.config.hidden_size)\n",
    "\n",
    "        # Purpose: task-specific normalization and dropout before each head\n",
    "        self.pre_vowel = nn.Sequential(nn.LayerNorm(H), nn.Dropout(float(dropout_p)))\n",
    "        self.pre_other = nn.Sequential(nn.LayerNorm(H), nn.Dropout(float(dropout_p)))\n",
    "\n",
    "        self.head_vowel = nn.Linear(H, 2)\n",
    "        self.head_other = nn.Linear(H, 2)\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    def masked_mean_pool(self, last_hidden, attn_mask_samples):\n",
    "        # Purpose: pool only where attention_mask is 1 (after converting to feature time)\n",
    "        feat_mask = self.backbone._get_feature_vector_attention_mask(last_hidden.shape[1], attn_mask_samples)\n",
    "        feat_mask = feat_mask.to(last_hidden.device).unsqueeze(-1).type_as(last_hidden)\n",
    "        masked = last_hidden * feat_mask\n",
    "        denom = feat_mask.sum(dim=1).clamp(min=1.0)\n",
    "        return masked.sum(dim=1) / denom\n",
    "\n",
    "    def _heads_fp32(self, x_fp_any: torch.Tensor, head: nn.Module) -> torch.Tensor:\n",
    "        # Purpose: keep head math in float32 even when autocast is enabled\n",
    "        x = x_fp_any.float()\n",
    "        if x.is_cuda:\n",
    "            with torch.amp.autocast(device_type=\"cuda\", enabled=False):\n",
    "                return head(x)\n",
    "        return head(x)\n",
    "\n",
    "    def forward(self, input_values, attention_mask, labels, task_group):\n",
    "        # Purpose: avoid gradients through the frozen backbone\n",
    "        with torch.no_grad():\n",
    "            out = self.backbone(input_values=input_values, attention_mask=attention_mask)\n",
    "            last_hidden = out.last_hidden_state\n",
    "\n",
    "        pooled = self.masked_mean_pool(last_hidden, attention_mask)  # [B,H]\n",
    "\n",
    "        # Purpose: compute both routes in FP32 so routing is dtype-safe\n",
    "        z_v = self.pre_vowel(pooled.float())\n",
    "        z_o = self.pre_other(pooled.float())\n",
    "\n",
    "        logits_v = self._heads_fp32(z_v, self.head_vowel)  # FP32\n",
    "        logits_o = self._heads_fp32(z_o, self.head_other)  # FP32\n",
    "\n",
    "        # Purpose: select the matching head per sample\n",
    "        is_vowel = torch.tensor([tg == \"vowel\" for tg in task_group], device=pooled.device, dtype=torch.bool)\n",
    "        logits = logits_o.clone()\n",
    "        if is_vowel.any():\n",
    "            logits[is_vowel] = logits_v[is_vowel]\n",
    "\n",
    "        loss = self.loss_fn(logits, labels)\n",
    "        return loss, logits\n",
    "\n",
    "# -------------------------\n",
    "# Validation metrics and figures\n",
    "# Inputs: y_true and PD probability\n",
    "# Outputs: AUROC, threshold metrics, ROC and confusion PNGs\n",
    "# -------------------------\n",
    "def compute_auc(y_true, y_prob):\n",
    "    y_true = np.asarray(y_true, dtype=np.int64)\n",
    "    y_prob = np.asarray(y_prob, dtype=np.float64)\n",
    "    if len(np.unique(y_true)) < 2:\n",
    "        return float(\"nan\")\n",
    "    return float(roc_auc_score(y_true, y_prob))\n",
    "\n",
    "def compute_threshold_metrics(y_true, y_prob, thr=0.5):\n",
    "    y_true = np.asarray(y_true, dtype=np.int64)\n",
    "    y_prob = np.asarray(y_prob, dtype=np.float64)\n",
    "    y_pred = (y_prob >= float(thr)).astype(np.int64)\n",
    "\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=[0, 1])\n",
    "    TN, FP, FN, TP = int(cm[0, 0]), int(cm[0, 1]), int(cm[1, 0]), int(cm[1, 1])\n",
    "\n",
    "    eps = 1e-12\n",
    "    acc = (TP + TN) / max(1, (TP + TN + FP + FN))\n",
    "    prec = TP / (TP + FP + eps)\n",
    "    rec = TP / (TP + FN + eps)     # sensitivity\n",
    "    f1 = 2 * prec * rec / (prec + rec + eps)\n",
    "    spec = TN / (TN + FP + eps)\n",
    "\n",
    "    try:\n",
    "        mcc = float(matthews_corrcoef(y_true, y_pred)) if len(np.unique(y_pred)) > 1 else float(\"nan\")\n",
    "    except Exception:\n",
    "        mcc = float(\"nan\")\n",
    "\n",
    "    # Purpose: quick association test on [[TN,FP],[FN,TP]]\n",
    "    try:\n",
    "        _, pval = fisher_exact([[TN, FP], [FN, TP]], alternative=\"two-sided\")\n",
    "        pval = float(pval)\n",
    "    except Exception:\n",
    "        pval = float(\"nan\")\n",
    "\n",
    "    return {\n",
    "        \"threshold\": float(thr),\n",
    "        \"confusion_matrix\": {\"TN\": TN, \"FP\": FP, \"FN\": FN, \"TP\": TP},\n",
    "        \"accuracy\": float(acc),\n",
    "        \"precision\": float(prec),\n",
    "        \"recall\": float(rec),\n",
    "        \"f1_score\": float(f1),\n",
    "        \"sensitivity\": float(rec),\n",
    "        \"specificity\": float(spec),\n",
    "        \"mcc\": float(mcc),\n",
    "        \"p_value_fisher_two_sided\": float(pval),\n",
    "    }\n",
    "\n",
    "def save_roc_curve_png(y_true, y_prob, out_png):\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_prob)\n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr)\n",
    "    plt.plot([0, 1], [0, 1], linestyle=\"--\")\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(\"ROC Curve (Val)\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_png, dpi=150)\n",
    "    plt.close()\n",
    "\n",
    "def save_confusion_png(y_true, y_prob, out_png, thr=0.5):\n",
    "    y_pred = (np.asarray(y_prob) >= float(thr)).astype(int)\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=[0, 1])\n",
    "    plt.figure()\n",
    "    plt.imshow(cm)\n",
    "    plt.xticks([0, 1], [\"Healthy(0)\", \"PD(1)\"])\n",
    "    plt.yticks([0, 1], [\"Healthy(0)\", \"PD(1)\"])\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"True\")\n",
    "    plt.title(f\"Confusion Matrix (Val, thr={thr:.2f})\")\n",
    "    for (i, j), v in np.ndenumerate(cm):\n",
    "        plt.text(j, i, str(v), ha=\"center\", va=\"center\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_png, dpi=150)\n",
    "    plt.close()\n",
    "\n",
    "# -------------------------\n",
    "# Deterministic seeding\n",
    "# Purpose: stabilize shuffling and initialization per seed\n",
    "# -------------------------\n",
    "def set_all_seeds(seed: int):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# -------------------------\n",
    "# One seed run: train, validate, save best epoch artifacts\n",
    "# Inputs: seed, train_df, val_df\n",
    "# Outputs: run folder files and values used in the experiment summary\n",
    "# -------------------------\n",
    "def run_trainval_once(seed: int):\n",
    "    set_all_seeds(seed)\n",
    "\n",
    "    run_dir = EXP_ROOT / f\"run_{dataset_id}_seed{seed}\"\n",
    "    run_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    train_ds = AudioManifestDataset(train_df)\n",
    "    val_ds   = AudioManifestDataset(val_df)\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=PER_DEVICE_BS, shuffle=True,\n",
    "                              num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY, collate_fn=collate_fn)\n",
    "    val_loader   = DataLoader(val_ds, batch_size=PER_DEVICE_BS, shuffle=False,\n",
    "                              num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY, collate_fn=collate_fn)\n",
    "\n",
    "    # Purpose: quick I/O and batching smoke test\n",
    "    print(f\"\\n[seed={seed}] Warm-up: loading 3 train batches...\")\n",
    "    t0 = time.time()\n",
    "    it = iter(train_loader)\n",
    "    for i in range(3):\n",
    "        _ = next(it)\n",
    "        print(f\"  loaded warmup batch {i+1}/3\")\n",
    "    print(f\"[seed={seed}] Warm-up done in {time.time()-t0:.2f}s\")\n",
    "\n",
    "    model = Wav2Vec2TwoHeadClassifier(BACKBONE_CKPT, dropout_p=DROPOUT_P).to(DEVICE)\n",
    "\n",
    "    # Purpose: only heads and pre-head blocks train\n",
    "    trainable_params = (\n",
    "        list(model.pre_vowel.parameters()) + list(model.pre_other.parameters()) +\n",
    "        list(model.head_vowel.parameters()) + list(model.head_other.parameters())\n",
    "    )\n",
    "    opt = torch.optim.Adam(trainable_params, lr=LR)\n",
    "\n",
    "    use_amp = bool(USE_AMP and DEVICE.type == \"cuda\")\n",
    "    scaler = torch.amp.GradScaler(\"cuda\", enabled=use_amp)\n",
    "\n",
    "    best_auc = -1.0\n",
    "    best_epoch = -1\n",
    "    no_improve = 0\n",
    "    best_state = None\n",
    "    best_val_probs = None\n",
    "    best_val_true = None\n",
    "    best_thr_metrics = None\n",
    "\n",
    "    for epoch in range(1, MAX_EPOCHS + 1):\n",
    "        # Train\n",
    "        model.train()\n",
    "        train_losses = []\n",
    "        opt.zero_grad(set_to_none=True)\n",
    "\n",
    "        pbar = tqdm(train_loader, desc=f\"[seed={seed}] Train epoch {epoch}\", dynamic_ncols=True)\n",
    "        step = 0\n",
    "        for batch in pbar:\n",
    "            step += 1\n",
    "            input_values = batch[\"input_values\"].to(DEVICE, non_blocking=False)\n",
    "            attention_mask = batch[\"attention_mask\"].to(DEVICE, non_blocking=False)\n",
    "            labels = batch[\"labels\"].to(DEVICE, non_blocking=False)\n",
    "            task_group = batch[\"task_group\"]\n",
    "\n",
    "            with torch.amp.autocast(device_type=\"cuda\", enabled=use_amp):\n",
    "                loss, _ = model(input_values, attention_mask, labels, task_group)\n",
    "                loss = loss / GRAD_ACCUM\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            train_losses.append(float(loss.detach().cpu().item()) * GRAD_ACCUM)\n",
    "\n",
    "            if (step % GRAD_ACCUM) == 0:\n",
    "                scaler.step(opt)\n",
    "                scaler.update()\n",
    "                opt.zero_grad(set_to_none=True)\n",
    "\n",
    "        # Purpose: flush last partial accumulation if needed\n",
    "        if (step % GRAD_ACCUM) != 0:\n",
    "            scaler.step(opt)\n",
    "            scaler.update()\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "\n",
    "        avg_train_loss = float(np.mean(train_losses)) if train_losses else float(\"nan\")\n",
    "\n",
    "        # Validate\n",
    "        model.eval()\n",
    "        all_probs, all_true = [], []\n",
    "        vpbar = tqdm(val_loader, desc=f\"[seed={seed}] Val epoch {epoch}\", dynamic_ncols=True)\n",
    "        with torch.inference_mode():\n",
    "            for batch in vpbar:\n",
    "                input_values = batch[\"input_values\"].to(DEVICE, non_blocking=False)\n",
    "                attention_mask = batch[\"attention_mask\"].to(DEVICE, non_blocking=False)\n",
    "                labels = batch[\"labels\"].to(DEVICE, non_blocking=False)\n",
    "                task_group = batch[\"task_group\"]\n",
    "\n",
    "                with torch.amp.autocast(device_type=\"cuda\", enabled=use_amp):\n",
    "                    _, logits = model(input_values, attention_mask, labels, task_group)\n",
    "\n",
    "                probs = torch.softmax(logits.float(), dim=-1)[:, 1].detach().cpu().numpy().astype(np.float64)\n",
    "                all_probs.extend(probs.tolist())\n",
    "                all_true.extend(labels.detach().cpu().numpy().astype(np.int64).tolist())\n",
    "\n",
    "        val_auc = compute_auc(all_true, all_probs)\n",
    "        print(f\"seed={seed} | epoch {epoch:02d}/{MAX_EPOCHS} | train_loss={avg_train_loss:.5f} | val_AUROC={val_auc:.5f}\")\n",
    "\n",
    "        # Purpose: keep only the best epoch by VAL AUROC\n",
    "        improved = (not math.isnan(val_auc)) and (val_auc > best_auc + 1e-12)\n",
    "        if improved:\n",
    "            best_auc = float(val_auc)\n",
    "            best_epoch = int(epoch)\n",
    "            no_improve = 0\n",
    "            best_state = {\n",
    "                \"pre_vowel\": {k: v.detach().cpu().clone() for k, v in model.pre_vowel.state_dict().items()},\n",
    "                \"pre_other\": {k: v.detach().cpu().clone() for k, v in model.pre_other.state_dict().items()},\n",
    "                \"head_vowel\": {k: v.detach().cpu().clone() for k, v in model.head_vowel.state_dict().items()},\n",
    "                \"head_other\": {k: v.detach().cpu().clone() for k, v in model.head_other.state_dict().items()},\n",
    "            }\n",
    "            best_val_probs = list(all_probs)\n",
    "            best_val_true  = list(all_true)\n",
    "            best_thr_metrics = compute_threshold_metrics(best_val_true, best_val_probs, thr=VAL_THRESHOLD)\n",
    "        else:\n",
    "            no_improve += 1\n",
    "\n",
    "        if no_improve >= PATIENCE:\n",
    "            break\n",
    "\n",
    "    if best_state is None or best_val_probs is None or best_val_true is None or best_thr_metrics is None:\n",
    "        raise RuntimeError(\"No best epoch captured. Validation AUROC may be NaN due to single-class validation split.\")\n",
    "\n",
    "    # Save best epoch artifacts only\n",
    "    best_heads_path = run_dir / \"best_heads.pt\"\n",
    "    torch.save(best_state, str(best_heads_path))\n",
    "\n",
    "    roc_png = run_dir / \"roc_curve.png\"\n",
    "    cm_png  = run_dir / \"confusion_matrix.png\"\n",
    "    ytrue_np = np.asarray(best_val_true, dtype=np.int64)\n",
    "    yprob_np = np.asarray(best_val_probs, dtype=np.float64)\n",
    "\n",
    "    save_roc_curve_png(ytrue_np, yprob_np, str(roc_png))\n",
    "    save_confusion_png(ytrue_np, yprob_np, str(cm_png), thr=VAL_THRESHOLD)\n",
    "\n",
    "    metrics = {\n",
    "        \"dataset\": dataset_id,\n",
    "        \"seed\": int(seed),\n",
    "        \"best_val_auroc\": float(best_auc),\n",
    "        \"best_epoch\": int(best_epoch),\n",
    "        \"n_train\": int(len(train_df)),\n",
    "        \"n_val\": int(len(val_df)),\n",
    "        \"label_counts_train\": train_df[\"label_num\"].value_counts(dropna=False).to_dict(),\n",
    "        \"label_counts_val\": val_df[\"label_num\"].value_counts(dropna=False).to_dict(),\n",
    "        \"experiment_tag\": EXPERIMENT_TAG,\n",
    "        \"run_stamp\": RUN_STAMP,\n",
    "        \"dropout_p\": float(DROPOUT_P),\n",
    "        \"lr\": float(LR),\n",
    "        \"effective_batch_size\": int(PER_DEVICE_BS * GRAD_ACCUM),\n",
    "        \"per_device_batch_size\": int(PER_DEVICE_BS),\n",
    "        \"grad_accum_steps\": int(GRAD_ACCUM),\n",
    "        \"val_threshold\": float(VAL_THRESHOLD),\n",
    "        \"backbone_ckpt\": BACKBONE_CKPT,\n",
    "        \"threshold_metrics_best_epoch\": best_thr_metrics,\n",
    "    }\n",
    "    metrics_path = run_dir / \"metrics.json\"\n",
    "    with open(metrics_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(metrics, f, indent=2)\n",
    "\n",
    "    print(f\"[seed={seed}] WROTE:\")\n",
    "    print(\" \", str(metrics_path))\n",
    "    print(\" \", str(roc_png))\n",
    "    print(\" \", str(cm_png))\n",
    "    print(\" \", str(best_heads_path))\n",
    "\n",
    "    return float(best_auc), str(run_dir), best_thr_metrics\n",
    "\n",
    "# -------------------------\n",
    "# Run all seeds and write experiment-level summary\n",
    "# Inputs: SEEDS list\n",
    "# Outputs: summary_trainval.json and appended history_index.jsonl\n",
    "# -------------------------\n",
    "aucs = []\n",
    "run_dirs = []\n",
    "per_seed_metrics = []\n",
    "\n",
    "for seed in SEEDS:\n",
    "    a, rd, thrm = run_trainval_once(seed)\n",
    "    aucs.append(a)\n",
    "    run_dirs.append(rd)\n",
    "    per_seed_metrics.append({\"seed\": int(seed), \"best_val_auroc\": float(a), \"threshold_metrics_best_epoch\": thrm})\n",
    "\n",
    "t_crit = 4.302652729911275  # df=2, 95% CI\n",
    "n = len(aucs)\n",
    "mean_auc = float(np.mean(aucs))\n",
    "std_auc = float(np.std(aucs, ddof=1)) if n > 1 else 0.0\n",
    "half_width = float(t_crit * (std_auc / math.sqrt(n))) if n > 1 else 0.0\n",
    "\n",
    "print(\"\\nAUROC by seed:\")\n",
    "for s, a in zip(SEEDS, aucs):\n",
    "    print(f\"  seed {s}: {a:.6f}\")\n",
    "print(f\"\\nMean AUROC: {mean_auc:.6f}\")\n",
    "print(f\"95% CI (t, n=3): [{mean_auc - half_width:.6f}, {mean_auc + half_width:.6f}]\")\n",
    "\n",
    "exp_summary = {\n",
    "    \"dataset\": dataset_id,\n",
    "    \"dx_out_root\": DX_OUT_ROOT,\n",
    "    \"experiment_tag\": EXPERIMENT_TAG,\n",
    "    \"run_stamp\": RUN_STAMP,\n",
    "    \"exp_root\": str(EXP_ROOT),\n",
    "    \"run_dirs\": run_dirs,\n",
    "    \"seeds\": SEEDS,\n",
    "    \"aurocs\": [float(x) for x in aucs],\n",
    "    \"mean_auroc\": float(mean_auc),\n",
    "    \"t_crit_df2_95\": float(t_crit),\n",
    "    \"half_width_95_ci\": float(half_width),\n",
    "    \"ci95\": [float(mean_auc - half_width), float(mean_auc + half_width)],\n",
    "    \"n_train\": int(len(train_df)),\n",
    "    \"n_val\": int(len(val_df)),\n",
    "    \"label_counts_train\": train_df[\"label_num\"].value_counts(dropna=False).to_dict(),\n",
    "    \"label_counts_val\": val_df[\"label_num\"].value_counts(dropna=False).to_dict(),\n",
    "    \"effective_batch_size\": int(PER_DEVICE_BS * GRAD_ACCUM),\n",
    "    \"per_device_batch_size\": int(PER_DEVICE_BS),\n",
    "    \"grad_accum_steps\": int(GRAD_ACCUM),\n",
    "    \"val_threshold\": float(VAL_THRESHOLD),\n",
    "    \"backbone_ckpt\": BACKBONE_CKPT,\n",
    "    \"dropout_p\": float(DROPOUT_P),\n",
    "    \"lr\": float(LR),\n",
    "    \"per_seed_best_epoch_metrics\": per_seed_metrics,\n",
    "}\n",
    "\n",
    "summary_path = EXP_ROOT / \"summary_trainval.json\"\n",
    "with open(summary_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(exp_summary, f, indent=2)\n",
    "\n",
    "# Output: append one line per experiment so older records remain intact\n",
    "history_path = TRAINVAL_ROOT / \"history_index.jsonl\"\n",
    "with open(history_path, \"a\", encoding=\"utf-8\") as f:\n",
    "    f.write(json.dumps(exp_summary) + \"\\n\")\n",
    "\n",
    "print(\"\\nWROTE per-experiment summary:\", str(summary_path))\n",
    "print(\"APPENDED global history index:\", str(history_path))\n",
    "print(\"\\nOpen this folder to access artifacts:\", str(EXP_ROOT))\n",
    "\n",
    "# -------------------------\n",
    "# Stop runtime (release GPU)\n",
    "# -------------------------\n",
    "print(\"\\nAll done. Unassigning the runtime to stop the L4 instance...\")\n",
    "try:\n",
    "    from google.colab import runtime  # type: ignore\n",
    "    print(\"Calling runtime.unassign() now.\")\n",
    "    runtime.unassign()\n",
    "except Exception as e:\n",
    "    print(\"Could not unassign runtime automatically. You can stop the runtime manually in Colab.\")\n",
    "    print(\"Reason:\", repr(e))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "6a2238a227b6428cafe8a39b5d4650e3",
      "22e6f3ab48f749a2bea4ed50b64cbe4c",
      "463ad170c53a40288841b1e27b045860",
      "a04eda5ff0584aff92b2bc5194d8f9ba",
      "3c253e4141f145a5825849e42abadeda",
      "c09d4f6c232e40359731f4187099a735",
      "6f3c8b45ac474821b014c7610114d2fd",
      "65f43c9680884701bc9367d21cf6acfc",
      "29247cbe324f42caad835848248f4e36",
      "3dbe6466862e4621af2ccab312c0b70a",
      "ae51d76bbd964f6a9fcb736ee9a59e92",
      "c7e6b112d15f48f98d410804eefcf786",
      "58a3f6a04202457fb1cb0a0f38860bad",
      "5afd69055ab74ed69c2d122fbfffdd0d",
      "e1b923dc1def48af94a75e2946a5faa6",
      "80b8092ce34843a686f8140930cadd53",
      "1db804eecb664648ab050b8b175d253c",
      "0b81f22ec5bd495d8ff848748c248a81",
      "45c00d45f5e74fbe9efb34e4da76f133",
      "acd230b5dc0d47f08fdc42122eda01ba",
      "8d43cfe0fbd64baca5c8e00b1ea92d63",
      "29174440ede746cfa0d6344aa7670f81",
      "304c5aa6af45440bad263cc52d2b5f1f",
      "b7f4259403fa4e6a8b437d61744407bf",
      "e1b71507586c40ccbbc4dd0e247a2ff6",
      "ca0555f8d75d47b88330517b61b357f1",
      "168dbe0d16204642b2ae35f33ad8d2e5",
      "7822172146a04f9394fdaa5c84dfff2a",
      "ffd21f2b0c864838a1cc0e39764fbfc0",
      "420dacb82cac49de8984f9ba165d775a",
      "60bf8ace5eba47289594d0bc6f0e906f",
      "fd73fa98d06e4b5196a5c0eddb0d90e3",
      "6a2ab9db6af8409589e74ae911cafb52",
      "6f9aa9b25ba94f139763e921ea07fec3",
      "60aa3432e51b40c1af7c9667983151f5",
      "78b55c2afc78496f983ccd05b2f6318d",
      "0bc70c48deab4221a388f74c09062c7b",
      "8ea5a04b53d047a2a6dbf97066b81279",
      "6ef91f4daa07425ba4fbe16bf39cb60b",
      "55202af64d8945adaeb85506d892a626",
      "0ef98505a7c849adb570ae77a17efbf6",
      "500b29c02a8d4f20a221552241033ab4",
      "756f16eeb8984d14a94449827fac2897",
      "25eaeac15fdf43d195a9c8ba78c0c69d",
      "a946ea8a34c042448b09ec7b6369e0a4",
      "dd76237938a1487ea29cb886ab9adc10",
      "998334641ab54df2a998de144a4e96bc",
      "dbc08c695e48452a923c5ca1bd1dd08d",
      "d524b6947d874a93b18db5bd74feeea8",
      "e1e24a49768249a184b04e3dd81c7402",
      "870945f383c54c9c92be0003adf0ca2f",
      "91487e529d48414ea5f7ded46321c286",
      "c0a62d0b0ffb47aca3ccb5aa634c82f5",
      "323c453e572b463db6e87ac1f7b13d0d",
      "3410a8e9d4b243a698c17bf6c87635fc",
      "d0d21b43e04e438694022b6f07cd39a2",
      "27f99e0c53f64aaf893806aab5dcc1a4",
      "c4319bdf39bf4185a9c6a5fefec59c68",
      "53f4ea4f364f482e98b1858b84525e54",
      "675c717c7b6f4bf38ead35ea67426bce",
      "a52c347111cd499fb9af0f6c780cb46f",
      "cc3d28e6f5004cb797097b6f785bbca3",
      "126a0d5684ec458f8f95d401fa91b139",
      "f83fdb04d5d74a15bc5f94e763b4b7b3",
      "e9317a033f9747f39a69189d58ae0361",
      "5d4071e922b244748e750c293a6a10d5",
      "474b30414da94c72881c622994456d66",
      "1f1b57ff6d8345cfb6559505760ee52f",
      "0cdab742562343d0bae21f7c967344e6",
      "278fa5cfba1741f48b8448eacf18ba73",
      "92e753dc0f814d95b2e2cf185f3cd7e0",
      "3e6ab0f847904bb6bb14cc0a74fb09d6",
      "24a9497bf26a4f06b3273ea733edee7e",
      "28217d073b264d069ef0539e9c069727",
      "e0e63b37937a4fb08009313cdd72dd1b",
      "955729c9379f4e5588319ae0ca46ada3",
      "9b879f794f8c4a8db8a9ef423383f316",
      "854e0b5170844500a95b25f6d15b4e75",
      "42eb6c1ace564e41bd5105950d1c5d7e",
      "ddf5f30fa59f4a8eb79fa3b57a122b60",
      "d58d88b8fe14457fb2eace5a455f1591",
      "348443392996454a844930cf3cd6dae1",
      "5c66bc198bb94b88b741bcd58de6331a",
      "7947c87ad4944de49250b731b9ed18cd",
      "8b808200d5c041eb9bc35055d903a2c9",
      "8b86bb9e115d40c19f17d2f7089d6fa3",
      "e0bd73e3ce2b488c99ee22fc8064418e",
      "7a5616465d4c48b1b065d5ca7ad36b15",
      "2648c40158654e008d1736209fd3f13e",
      "7802784ba95d48c4ab042bb3456707d2",
      "44a843b7cfa543a8b42466cde6ed4961",
      "cf407cba146b40a9a81d90afd63483e0",
      "343ad849c1954cf1b4147cf33a072de9",
      "e4db6d479fb14afba76f8168447f64ed",
      "f547fd56d2814f5897ea42a1cc13f01d",
      "59523bb859694dd0befbbf47849f5357",
      "97863cf5042d42d6bc4a26a8ed34aaff",
      "05bc60aaf1104a41897967dbdb4fb62b",
      "750bbb13e93f446a94be56b1c2a8671b",
      "de14a0782ba747e4b96d11d9202ff75b",
      "de09686bda3c492f921708d853e51673",
      "921cafa0a6ac4c9284c6aa03b2efe2cd",
      "ec805929237d4863a3206a7e95b92bf3",
      "32be5d92f9c64e92b43f3ece5b296bb1",
      "4bffd32ce4174674a8a2f959131b7a26",
      "77ee25b32e214ec085b4923d31d5021a",
      "4b1b7edc3c3646c4a5acd431b0dc931a",
      "88bf1b15256b4bfa8c369b48b8441feb",
      "a071869a0cb848d8af56e7c78f407461",
      "9784229ea38a4cb1ab29e1d7d247794f",
      "1d5e0a73827844db9a52476f52d530e0",
      "56f1551aa8474a8ead4f4d98be847ec2",
      "fb67c1602db44c16b2db718458c8ec9a",
      "4ff8670a0b64426e9c36fd38c67965e3",
      "b124c7ea305d4bf4a537f264fbe3ddf9",
      "4200e243445d413482129eeaf8c9a6aa",
      "61d0da1dc1234010a2b3851444250768",
      "e9a517e665204491be2b239b2ec9115f",
      "c91dc67d3a6041ac927834c1e3ad288e",
      "565aefeb9f76469b87e8bdcb6a1a1bae",
      "99489951b3164e57bdb4790e79656118",
      "2012adada4ae480d89e8e34d97d2cfc6",
      "292744da7c50499db0fdbdf12f693e45",
      "7bbe9cba48e44b7396b189fab39a682a",
      "0947328022ad48e09da38f588c8c705e",
      "73507399868f46c3b68b95659b48ca68",
      "17cba525674c4d57be57e29b5befb04f",
      "bfee649214fd4b888889d6af9302434b",
      "faaa5fa3cc3b402b85d8062096787e08",
      "3590eabb3ba04bd183c35e599ac31348",
      "5e963e1927834320a0e240f1710e6acf",
      "e10e0935bb334d8caee5e4f33e5e0725",
      "166a9ad87fa74fcd956acc97d69493d1",
      "7697a2806934470fb7ea690bb1ebe84c",
      "4e120bb0ff844031b7732470aef44cc4",
      "2ec6b74cee2e4dccaaea1bc81ae6b3e3",
      "7eeb8371e0b74ac8b0fcfe93996ce48d",
      "cb310c04f44a40aa8eb0104f79fd3555",
      "679fe89197434c008c6615e833cf876e",
      "8e9622043aaf4466b06a2ce54459a651",
      "3f58cb7694904ec3b3e7aefc4c08c0aa",
      "9c9d94aafe544a71a73c7c4fe81200eb",
      "ba9e0ad00cf149ee8ea1350e72180370",
      "2f48e3d73a5045ec904f22c91f2fb54b",
      "4b15a1dceaaf467eb1adab1fab44325c",
      "3cc715c1376e490ebb0d8d1946cffb3c",
      "7c6ca4282af143caa136936d752d0ffc",
      "fedd5a7f453947cfbc9b7bb80faea5f9",
      "04370f06ab3c4b0f83c4bc9879debea2",
      "1304cfa2db7f414fac91bcf520ab713d",
      "db929f6729fc4aa097616dff5a717671",
      "c33c414b73a548a49b7fc135cae7d73b",
      "3fe6a2d56a1849f78db85848c9831970",
      "14e4f1b9353e4ba089abf46c4385fc62",
      "bdfbc108ab8f4499b49e2701ef391899",
      "7c4b3ffa0f3044dab4f0d4e5f3ba3511",
      "dab231ea06e747688a35c45cae8b63f5",
      "9f1273e4220f4f70a68f9e4971415c97",
      "336df7fec90244d387b775628c2db290",
      "dcf494d0ebc74a3a9e3613f3d26d9fcb",
      "9156f4ecd5114781b050d204413d63b3",
      "3aa6817d06b94db0a63394dbd7e05d45",
      "e97912771e3a4098bb506db590ae4e28",
      "1899b819f159447bafbb30c19a46b49a",
      "24a3960ca8134c3484dc1c5219da7733",
      "cffce09d570a4f45988afc57e22009cf",
      "44301f60ea5b4633896a52ce605fa6e8",
      "bc71e07bebe847149d9c4fdae88b0bb6",
      "eb11b3f7415e4b74a4abcb1289206e16",
      "fd09426e8d2a4c559739b3620b4be9bb",
      "953d4f784e6b4fafb920dda12977b9ef",
      "3832dbb51f5c4cf489a380dc20edde34",
      "4be3a7abb54542bc9b874d5dafa85bb8",
      "4ae6223cd84c44c0ab27ef873561e413",
      "8a5d135035f544aabbbb05c80703c273",
      "69702596f32a46a78fcfbfda8524d300",
      "397ef1c90dfe47699a61a04247def5e6",
      "d798d3939d694d4fb7c0906f8b0649d1",
      "3e2ea36926e641239e138108f84547b0",
      "896917894a5a4e9897f355e696bab026",
      "f0228935dc3a4a539c7d325495c0c440",
      "dd82d4bc232b4d9aabb1dd737abf0aec",
      "6d8ee6f0aff84087a68c9a5667c49114",
      "bf621eed4ca84fb787a4a6cc7cb97376",
      "c7b4f4e30c754df2b77133d879db3492",
      "f0798eeffd5144a394e2561e688761bd",
      "2e79001055ca4144a38a855e2c66469f",
      "675e41c0a7df4154b938a9e99d6bcb47",
      "1db8e9f80c7f4bc0bd124506c2809e46",
      "6d169ebb429046f6abcd0ce136c9ff27",
      "7bb670601c8845af879b4aa0eddd25d0",
      "7aca00af4d14413b99afe3796009d7e0",
      "9bb19f3497674652b8a4a5ed42b20b5a",
      "88d704100fba4024a51aabb633eb44c5",
      "2aea4b5cdcbe4af4a6d06471ccab10c3",
      "ad4f34799c1f4c8fbcbf092793e5ebb0",
      "2d3911f4f2f54bb092311b8a87690896",
      "03ddbee316684b9183d9e2bd6ee13405",
      "9dde76f408d64f0dac7ac97a88dfdeed",
      "d57605a2e8cc41c4821d8a5e6782e75c",
      "a264b9b14dc346b18c08a3018e09a8c4",
      "881a48a549db4b40bf18c97c55285910",
      "be4a87c3f0794a288dd4258b0936099b",
      "6eb34598d3bf4573a7b5d4c2e5ec1212",
      "21386d6cfba64e54a79741ee0bfb015d",
      "49cab8c36acb4823b363eb9b210acc7a",
      "5f89014a4b214cdf91adbfe0fa012f9f",
      "f37fb93bb60148d1ad6d97668cc40f18",
      "9256cb0028584c049c9cd1373ae2caa4",
      "7c9fc7599a5f4b699ce7af10bd58b351",
      "a1947ba00616420dbf9fa695aac13652",
      "3524cd1ae3d2405f83c825de48ca2f2d",
      "a673f04c6af8493aa2ef5b998efcda01",
      "8c782054edf447c58ce489c7c672a702",
      "4480089f39f944d383b2026038cadac4",
      "cab3a5d66a6b418580909d4163ffd639",
      "91d18439d8b94374950f67d376623f25",
      "8dae36ce212d482ea5d4529c8117a3cd",
      "21d442380b54462a83766f9d3d45626e",
      "aa0eb5b3759c4559a448eb25bd7d6255",
      "a9e4377af5b443a19bc43bf5f6651047",
      "04c84a1689394248994f055fd3f310eb",
      "4bd969c6a34346648069ec45d41b9cab",
      "7746c16956d74e709f6caa80437f9b7d",
      "cc542870acca43119c8f154d8418afd9",
      "771e6dba2268485b9f09a5c4b5182027",
      "a1fe7e66b6c84de487b24c3750b8ba83",
      "94254068ebaa4a13b2f3b017de08d02e",
      "b1ffb30148924b728071ab1922c515e0",
      "f9d99e5a329a4d78a7f67c31b89bad83",
      "d674082ee72c4ec7910356895aaeaead",
      "a2d802d37b684356b767edca6ed7ec5a",
      "9cfd7dd486f24f5a8b25f32b00122e7c",
      "9110ec4aecbc43e4a0d97c7a1d21929c",
      "2f456c59d42a4f829f69c3187597487e",
      "6b45f8675691460f9077666f5c9527e3",
      "e956a38b84b44680a2f24b4312bdac30",
      "98b349a1bc29417a94ceafdea100fc74",
      "5b53123af40c418ca5656dde946002ca",
      "bca42210e2f24d6f875921ae24f79671",
      "70b54343a81047d39b23dc8aa82c480f",
      "0e011cb8f3d0479ea2c36b1099719112",
      "dc17fb9f151a4bb48c9d1cf62d87709b",
      "1404c36d82c64faa8d167e05ac229b39",
      "4f2cb2f2380d47869e80c3fd1fa981ea",
      "6230cd16d7574deb8c6f1dd472f2b666",
      "c4fd9ea30e2f45798c72e7a01e781a9a",
      "f0c01de1be53445b9b111e966cdb890b",
      "4fc0a41205364286925dd6f40eb4e9f9",
      "5e84ad87049a483eaaf81c3a949c2ff0",
      "ac6b7403b9c84f2b929d4eb4c4bb9f0a",
      "ea33391438724e2aa8368d8f55c25a04",
      "5889c18c0caf409fa0579ff74cfc92d5",
      "3c5c3a1318a64297bc2d132c4d8ec667",
      "d521dc4f393c451aada5209ed16729d4",
      "dfbe914edf14400aaa16f136db059368",
      "796b008243c04eb7ab9c72f7e9a4b646",
      "11c661985e014229903a8587c0adb920",
      "b8bb9c58c3104dadafcfaa8fff340ea7",
      "da623c273e4040dba96c36da3e56d5d9",
      "872cf34fcc7d4ddca46014b51179745b",
      "d74d7629a24d47938c4a37d97ed5f358",
      "009d2f9195a0481d885fddd5224afc6e",
      "9dbd7d785aa74dbe92e6e09d258938e2",
      "0770a5e9181848d0a0b151ee271dbeea",
      "a956730ef3014f6fa38abc0508e238e8",
      "f9167a214b54468d864c07c8627e0184",
      "1ddee245a262452daef1885cc8ff16a6",
      "f712f3019b8f495092c2bfe4ec27e13c",
      "0489b12b17a94940a5fcce6e0b774be1",
      "ffc4204afff44311944320325b4d04b6",
      "e20c30e1145a4420927e52f1c9a647df",
      "aa3d43b0bcef4c17906dc71c54c3b67a",
      "eb6b5c21b7d94cd98ab2725bb37ff844",
      "162a025c58ee4879b8895ddda4852141",
      "34df50e23b864900bbe648eda6747cdd",
      "f17b1800995a44d490d24a3ab2bd7500",
      "b8221d356e9b4da19410a8ff82b733e2",
      "1ec4125681e4452cb18996fd36de0a31",
      "6b852f9174dd4c17aed81c44de974f7e",
      "6e24e85c89324bd486c684d00cea1ca8",
      "37686d3787f74aeb9229c574733a8be4",
      "24f93de81cb043ac8b384c4ccf4172f4",
      "28b88afaa39f4593bad6c0a6f1df8b1f",
      "22b8edf602bf44a5b21dadc6332a2343",
      "1ce5b31f97414b1088c2f530175c4ba2",
      "d3c4e3233f2e42fb8fc82f970330e2b8",
      "79900c9b92ba48679cba9f13245277e0",
      "5c57a53b22de4480a9ce4db19f9c20b3",
      "3b3ef97ca9a44fddad1707ca0e2f8d7a",
      "a82ea665b22d46b5b458a27d30762f21",
      "a2f6d6d73f794e89b300d91ee8b695a7",
      "bbe8ad294bbe43d5807ffd661f071d36",
      "f22147ab07214a6491fad2091f0fdb18",
      "a8f4c24b6e4c4bdf893cb1172d5f19f6",
      "50b1dd2197d248fb9417d7f6d1a80c6e",
      "794e0320067c4620ac892ece0d5f051d",
      "59b6948d9db749c590eb4182e03285b1",
      "2edab8688e44466fa77734bc8a13361d",
      "64d43008a32b4eee9ffb6b6e70403c28",
      "96886a1985624a41bc8aa1c37220121c",
      "519b7de4bdeb4625a4e89526f7603667",
      "2f4c7129a6004431a4ee7cf5f5b43eaa",
      "2d93ab6e5376485aa59c0a89f021002c",
      "5a5fb6685ce34c4f8441c9d0386a269a",
      "bc3a4a364ffb43198af532194ac162ab",
      "5e917b059d2649be973a017475d203ed",
      "01367149abf54ff5953451227281e4aa",
      "e3711a257ba448128a5edac13b53db51",
      "005936b5ccb14abb8cfd5f31a9380285",
      "34491dbbf7f14b0ebc3ce6d3d3a242e5",
      "17d7d83e7f3441d882a8aff3eeefcab2",
      "f9f763b447e2479c96762d06530755fd",
      "34587c7be7934bb4a86ce64787947690",
      "5e01de870cea44c995e70b9fff7c2b13",
      "6d8f28bfed2c421a9223efed7647b760",
      "a379dfdbac474185940f308d89588765",
      "748b5787e10a4676a7421851456c4776",
      "777249747a4b43919142d68da281dbff",
      "35782b39097f4a719263d52606a2db01",
      "b675751d87c34919aaf2f049c12495d1",
      "f45ac0ef499741a49db2ef2b17b59314",
      "8cf134fd0df44604a760264fc59bcacc",
      "bd996eec692747e2b3367d4fe7208c0d",
      "41c4bc223fa94431a6d1cc181d4f9b9c",
      "e5c3f64d9c454bc6af0d31c4ef0c2419",
      "5b3c3a90d50c406798b31e751f342efe",
      "1475a5989f3e4980a8aa9f1c7f9ab7d4",
      "c075d2e690f54563a35ab8a148e17820",
      "7584e0dd407c40bc9dee83a2eb63ce50",
      "41f1c4454c0a4803810d4ab90e36844a",
      "6794452e272c48d0b5909da4374c99fd",
      "4a984d610dae4e998234d074466392ee",
      "fb43ff0ca4284e4eba322d4466246a56",
      "5000e083878e4179a1271d8f595d997d",
      "f543a37a953b46c7abba9dddeab85cd6",
      "81c642978ac846bbb6c642c1d033bd9a",
      "625b44bcbdba4016b82c0ec30fa8cdf4",
      "2fccdf3da7344d6a945a68049141b858",
      "f7d2df439d14416daba6261bb10c2ea5",
      "dfddcdfc6e76427eabe4244e2928c229",
      "dfed14d0180b40f782867e90727dd60a",
      "c24c6a99a4924dc09cec43b4010e2261",
      "93f56243ab544c419a625a3666f532d6",
      "5d98ba4223834ffd99450af2957e063d",
      "f2c52f6972d14df98e4f6d7550f5139b",
      "21f03d85eb7b40908816230bebb1faab",
      "1d3321b562c34ebf80f1307682785132",
      "eda020fe39c64db89be3e039b8d65aed",
      "c018e6b7104649428b1bb2dce57fef12",
      "29a065e2afff4b2b96de2db24723a29e",
      "070d4b983bca43319c9850e56f104ea0",
      "498ceed7ce614de5918ae82966e79a38",
      "2bad364cabba403aa2ba78f8191b4d50",
      "00048f73f63345b4a7c084e58a4a5ac4",
      "9975f0d0a8b94202bfb9fa6a7a753509",
      "dbe3ede8bdf5483f99e07972d2c00f9c",
      "b97e27666c744aefa8d8d44e57d8a705",
      "7e3718613a894e4ca66a5f30c33fe438",
      "b46236ca578c4c18a4d0d6322ca73827",
      "342ec7ba21544a179002442cb7180460",
      "6a63e30760834e4ab3957f19f6df43ba",
      "f401ce7b88b04ea2bd85a2944267d4b3",
      "273904d45ab84f2bb0fc7211300a748f",
      "ff4ea8ed7f1b4051b35610634910f693",
      "74b4d50154154cc395c59510843eabf9",
      "5e785c9520124f549fddcc4b03edf70f",
      "31623348064448968d23ea1ac1c5adad",
      "9752b777eca74848b09d330ac9666982",
      "92a2ec35b0ea434697a6088c7eb66878",
      "d7f84890064c405f9eccbb6a96436cef",
      "07dc1995289c4692be8b7ed71a79f4e1",
      "417c5cdc405e423e859acac83a64a886",
      "b010741079e24bb195143560578d2647",
      "71d1a69f98544bde832c98234e983881",
      "3ee0b08c095c4c4583cc358278f87689",
      "f58278c4456d4b3087c1da4df56a4bad",
      "6391748e0b9248959be9f348928d979b",
      "6748d93ec46e4a598cf94188a85c200a",
      "8b7accac7c9c44728fe09c436d41ba80",
      "98374a4aefe344e484131d7f60bd86a0",
      "c1cc1a6f0ebb49eda456890b09de89e2",
      "49b1a24e2b2a48048473ff12c42c0b20",
      "3b13bf47c3e84aae85fffc13fceb91f0",
      "bb5769dd559d419b9aa8081c771f4030",
      "e60927e9dce04bfcaa4e74dc8c9ed3fd",
      "d8b6c15490d24b28887b22df26e7a70e",
      "ada938fc12b44747a91128a51ae404d9",
      "c1291413c8e04f32a8939a4c795d09b1",
      "4e7d309dc7554159996de0492ae78161",
      "806de2a257c34ebdb88e5f33403da81e",
      "8f4f666e32e04e4bb06ce0c84f2c45b3",
      "ece6639207834692bbe3888bc018fcfa",
      "7311db3362e14bee9c48feb55c755a0c",
      "ae457952474e46ea9f4261415c8ad6c2",
      "efba74d1752342439baa2c709f7d2593",
      "cc1e87d7b6714dea9740038f28c58aad",
      "1d4b9d9317c84347b2ad345832aa0ea4",
      "21c4db9349104124981986f15205ff65",
      "27ac0b2d422f434ba7ffa98cec0aef50",
      "e67c88817b744eb2b2e124017620903a",
      "1a193eba607b4384ae3c1ac40c72314f",
      "ca0c885932674b2b9ba31f6f5fc56946",
      "7c1abd31f8e940eba93568489fca1918",
      "d34bd84358ae4afbb6c2d529f5dae7cd",
      "36d5191e272a490493d5db4f62096878",
      "2989ff546ed848099226aaac7e392cc2",
      "1f6000ea8aa0451ebac3340e44f44121",
      "6411f7e5c917456fa6752673279507a1",
      "734a429ca22244b18026d555fa8d121c",
      "b7cf48c5ef4d46fd91108ce32c05b5d0",
      "9c70da15ced044eabaf9c7d4beee1785",
      "229146cb3b1849dabfb74b334c16d86a",
      "4ed96cd588b34025867d4f3c3c4b17c1",
      "8a2c7db8c6d548f4882b0cbea1c4d391",
      "bc2edfbddbee414eb6f8df79dea04aff",
      "6d37db82652f41f8a6ded81975335149",
      "13912157a4704d64924b86ba758a886b",
      "e7adf4ec2573413a955cae36e2f99974",
      "25f7bc4cbb0f437c89e83cba3d39e3ac",
      "abfb1b81d54c4512b07900f185936ea0",
      "05414f5ec42240c2b6b64cc7b74043a9",
      "ed96a0742305453fb7f99f3d4e68c939",
      "d8f78902323643b7857efe790a0357cf",
      "f19805794135430aa606fb5f09c4d668",
      "abfac72d8b6c4d7d92e31d2939024e57",
      "3e7b5db97d0c46c98f4f9d64ce567d36",
      "4bf84e8718284be1a428b56304361b02",
      "881c4d03695f432ba70cb44e0734fafb",
      "32e675d067734faa88a7da9871be3e36",
      "3f90fd6d07d34576810f7487b6c4837e",
      "ff8704050e8e490490dc903df7388ffc",
      "ad9018ba7da0418c887b9fed4c79c2ed",
      "9c7a4649cb4b4490aecd719aa20b266d",
      "2ff336e47c524cf79f528f7164cac459",
      "e7a5016edbe14c5cbafdcc6539b3623e",
      "ff27993ed5584e26bb8bbd6ce234a90f",
      "415020f89c774cceb8d1af41faf45934",
      "b3e0a94ca4f947c48ba693d262ce8fac",
      "720c2155f0e14955a1e4fae7725a15df",
      "62d3a02ff50f4d87ac1b1c33bc332ccb",
      "4996eabb2886459d98405afab63e6663",
      "dbf6780cfe494f5fb1ea31b6766cbbd2",
      "667d55899cfc405f92ca4ad8e696afbf",
      "e72213a19e04479998e8adf35c1775b5",
      "7ecf83522ad846daab5273bc2f5519e3",
      "b00c47741be549ee8eb0edbbc87a96a5",
      "f71a721f8a14414bb03bc0eb6a57ef19",
      "9a615180070b430b9a7409b0faceaccb",
      "75c2df77a82c44e8a3e20df5b724a9c1",
      "89f9fb8c8ae24c76920ceb88264adec3",
      "9b57a8495ee84512bd99585a5b7df65b",
      "82ca9ac0ffee422d9980cb1090039a34",
      "60563bd6aece46c1af2672e2ea805f2c",
      "e49c9a0d605b4df1a1802ab0cce78030",
      "97057d2c16e14e858415cb939494ee15",
      "e9732f30042046ef8f31dfe513b37188",
      "8861af7c29534bc284dbfc7d30b5b85c",
      "5249cff20c6343a2923c569bd8ef645f",
      "6e8a4a05c4424ec69fdead5ccd6d0cd7",
      "6ff77c4cabc94e939fb15074b0589d18",
      "897380843253492f96ff2e11305ab874",
      "7bc87031a96f4be2b952ae6cd84060ae",
      "f75a7414a1ef46f49d27a802afc92021",
      "ffcda5c20c024eeaa3f01d989b15ed96",
      "eb26af2dd1914528838e87599a2f557c",
      "3b72b98c681d4d799f51263d117f8ba9",
      "638c992016f64b869c87aad8039cdb57",
      "318c4ca379b44e60851f8d7789b0868a",
      "d361e6bbaaee4d9caea4d85708a7e304",
      "49f0a730b2854a719bf3a1a6953128c2",
      "1f1d732d6d7d42c991e57cf84367c4c8",
      "c7455b96d4f5449b8cfbdf9954cfd512",
      "09404027cc104a5f81b30585100543a1",
      "2990490ea3fc4f06b6c54cef21965490",
      "51be00e995114474a28124dd7d1add2f",
      "b7616bee89c1449fa5ada3461da4c058",
      "166b055dedfd41d5ab455e3399169a7b",
      "6235f5bfbec246828c7a95b54753477e",
      "3a7cfc8cd8014e4f92bc5cd2037fc4e6",
      "0939e55a717244249741d7dcee7d43c6",
      "e9aaf3d152314e698d59f7e5f81466a1",
      "1fb29114186f4ab7858a916195c93e28",
      "aab3f4a71c334df99a6c57c3718df04f",
      "bbb2bd4df10b4b59ba998b7eb8011fce",
      "60208c64c5454c748eccdfce2e7038bd",
      "9dd8d4109dfc44919b5ff22a52a5606b",
      "f1fa3120f7ed463aa4ab5e62afb5232d",
      "17d05bb33deb495e8d91ad6fbdbbff83",
      "3cbf2bd21a9e4bb8964efc8c61de1909",
      "0370255d4f844988a2c7ec81e4e7944f",
      "992f8f070e3246488a9c7f23866c1b0a",
      "831b485129e546cb892a26b8e6d59803",
      "a7f88a85adc540dda3bcc2923854912c",
      "01fc9d801a4d4e5fbd751c77fafea4a4",
      "b47f2f35d7134da39f43e83e983d69fc",
      "64428e4cac9a4a888862b8178024c467",
      "c171f596958c497eabab34b7984d1c81",
      "bfe59f40f8d5432585d05ef57b722f52",
      "3075202eeb64454a9315540050be5ab1",
      "98d2f325973d4bb799e6817bc1a770cd",
      "1fb520ea4b864f77b7f9344e5879bd31",
      "33d94a9bb59c4daabe4c73dca4b13f4c",
      "6f41b78db70e44ffb3a5936e76a9ce60",
      "95444c851e514f8d81f347731152e167",
      "92ee1c925d504e9a8aa3a2a8ed779c39",
      "0fec90f0eb3a4c4fb0d8eb9dfc405716",
      "6205ac80dbde42d8a9c1bea5dbb7df70",
      "4960d7d99260405a92c4bc4aa38e4a32",
      "ebfc9fed4fb94a75a9517a1d5158b05a",
      "add245f7198547de87652a43b5a629bc",
      "11f6c7187fd44cc88680daed26201e4a",
      "de258c2ea6404c70b20376fad8e77b91",
      "5762be35871c4eb3a6967e1dcf553b1f",
      "cb35d8f1492c4653aa716c5d51da8ab2",
      "37c46f35c6c349f2b99af62686de53fa",
      "0434e3279dc24c7eafd7b2e7d02b03dd",
      "725ffb8575ab4860944356ce0fddcb38",
      "7ad0aa08f6d84febb3b899a1c80f6a72",
      "a8edcdd463ee4ca98fa7295de3ede4b9",
      "0992d38f56464bcbb956851a0fbcf90e",
      "8f2d9c95a9d547588a55cf5c3fafd083",
      "aaa1a5b1e6a8484dbbdc34de9d166d72",
      "fd4b3115b0624b4cb3e604f53060e2cc",
      "660349e7ed4c4a16822924c82f43c1f9",
      "4a62b1ebc5ca4291a8d163b61ed7ab3e",
      "928362436ec74de4b811e1541e1df114",
      "1076fbd506df4e6f938f4bea3077017f",
      "90f5c5c9dd2549a5b1efafa535fbd5a4",
      "a4c9afcf624e49d2830f09b85401063b",
      "19ab8006e19d4ef58ea544d978db44cf",
      "693805de72fa444ab101a8b6728408b3",
      "b3f0bb682af844f686466f6b2c2f9b1d",
      "541eea18ce8c45eb971258026b296d13",
      "085c9b39f30448bab62ecf14707a9ccd",
      "fc3318c8a27442f69f8e39ef548d2ec9",
      "2a74a5e9ae3d49acb4e127596f8d5c69",
      "1eda760253f64e89a37ccf58b4d8445b",
      "eac80085e23844f38e87a516f25b3307",
      "f0d7154206984866a3163c6f91425f55",
      "59b9bdcf899c4ba191249493ca6902a3",
      "68f0f3f5468943e18ff2b55985d0af25",
      "f1a7d93e5a7f46cf974f4dcc2c4d2952",
      "5de403c27c214e9eb4ad52dc02b4b9ed",
      "ce9c94bb21cd482daedd76da75da9fe4",
      "8271686b1fde40a48a76ae73dbec2b66",
      "cfc2f7beef3e4a71a1f74cb111b4ad87",
      "8851fe9b8a7e422eaa60dec78c1fa9f7",
      "6c8bcb375dfc472686ec3ed97ce77d59",
      "1683c9a8efde433ba93f05714e61e815",
      "5a75127a91924dc2a8dd8afcd05a42e8",
      "5b7b5631ae044fbe9c756994d43af1c1",
      "24020f72ce0d433ba1935bb814d988e9",
      "c8bfcc8853f3459caf6442bd3eb72c1c",
      "99eb42fc00ad4bcab1e469d1a0a2d112",
      "5e0d4517627545bea6faac00bd57deda",
      "c94bc40cdc1f4bffaaaa71f6a9dd4eb9",
      "8f8bfe7a37014917a3d0b1cef0afa64e",
      "ae7da83beb5640068ed083d76de36cc6",
      "e316eed5d0744d27bd21d099adfea533",
      "be4f58228d3a4b49957835d6ce47e165",
      "30c2709f55d148c1a3dd67690f10349c",
      "bfaf131adf6d477098d692308f5f8de8",
      "c6c085a871f34bcebae2dc1af37f9466",
      "4bea9c4da8834b078548394a60576c81",
      "c75d7772f8864468a48be24516751099",
      "76577b6834ce4f72b77708136d736009",
      "2d6c1368ca1e43fd8d893570c520c4e6",
      "67e2b02731ae4fed8910e7c37fddf074",
      "8fe23b0db0b8496fa353557e91cf43cc",
      "7a514ff434cf4c429dad4fd84ffbac5c",
      "8bac06782c8748fd88d8dac333b2964e",
      "881a8a79427b4ed29a29bb0b7da44ebc",
      "4cd52c7121814d17bafff33421965cc7",
      "9eb9090f92ff4228a26fe06308808a56",
      "89b3dbc80af345bb86abc3ae1e0b491a",
      "032228d344644dfc987abb383cf85368",
      "1b68b3df32964a7ebc69d486c05561a9",
      "e067cac6c48c47fd8359a3c4676f44c6",
      "2478536fe2d542d594711040b033afc2",
      "8d75b4020d1a4b40af8124b77d8441de",
      "6f74e93ddec74f8eb6786ca321c2d392",
      "f2bc6d6a80e4497298d475a6c9602b1f"
     ]
    },
    "id": "9F9mzio_ZD8E"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "The following cell trains and validates a Parkinson’s versus Healthy classifier on Dataset D2 (Slovak) using only the preprocessed dataset folder and its `manifests/manifest_all.csv`. It performs training and validation only, with no test step. The cell is set up to stop early if required files or inputs are missing, and it saves all outputs in a new time stamped experiment folder so previous results are not overwritten.\n",
    "\n",
    "The cell starts by importing the required libraries and setting `DX_OUT_ROOT` to the D2 preprocessed dataset path, along with the path to `manifest_all.csv`. It then defines fixed run settings to ensure repeatable results, including up to 10 epochs, an effective batch size of 64 using gradient accumulation, a learning rate of 1e-3, early stopping after two validation AUROC non improvements, three fixed random seeds (1337, 2024, 7777), a fixed decision threshold of 0.5 for threshold based metrics, and mixed precision when running on an L4 GPU. An output folder structure matching the other datasets is created at\n",
    "`<DX_OUT_ROOT>/trainval_runs/exp_<tag>_<timestamp>/run_<dataset>_seed####/`, and a global history log file (`history_index.jsonl`) is prepared.\n",
    "\n",
    "Next, the cell loads the manifest file, checks that all required columns are present, and keeps only rows where the split is `train` or `val`. If available, the dataset name is inferred from the most common value in the `dataset` column. It prints the number of training and validation rows along with label counts. Each row is assigned to a simple task group, where `task == \"vowl\"` is treated as vowel speech and all other tasks are treated as non vowel speech. Before training begins, a progress bar based check confirms that all audio files listed in `clip_path` exist on disk.\n",
    "\n",
    "A custom dataset class is used to load audio clips from disk, enforcing a 16 kHz sample rate and converting multi channel audio to mono. An attention mask is created for each clip. For vowel clips, the end of real audio is detected using a small amplitude threshold and trailing near zero padding is masked out so the model does not learn from artificial silence. For other clips, the attention mask contains only ones. A custom collator then pads each batch to the length of the longest clip and pads the attention masks in the same way.\n",
    "\n",
    "The model consists of a frozen Wav2Vec2 backbone with two small trainable heads, one for vowel clips and one for non vowel clips. Each head uses LayerNorm, Dropout, and a Linear layer that outputs two classes. The backbone weights are not updated during training. When mixed precision is enabled, the head computations are forced to run in float32 for stability.\n",
    "\n",
    "For each of the three random seeds, the cell creates a seed specific run folder and data loaders, and runs a short warm up by loading three training batches to catch disk or batching issues early. Training then proceeds epoch by epoch with validation at the end of each epoch. Validation AUROC is tracked and early stopping is applied after two epochs without improvement. Only the best validation epoch is kept for each seed, and the following files are saved in the seed’s run folder: `best_heads.pt` with the trained head weights and metadata, `roc_curve.png` and `confusion_matrix.png` showing validation results at threshold 0.5, and a `metrics.json` file with the best AUROC, best epoch, dataset sizes, run settings, and threshold based metrics at 0.5 including accuracy, precision, recall or sensitivity, specificity, F1 score, MCC, and the Fisher exact test p value.\n",
    "\n",
    "After all seeds finish, the cell prints the AUROC for each seed and computes the mean AUROC with a 95% confidence interval using a t distribution with n=3. A single `summary_trainval.json` file is written for the full experiment, and the same summary is appended as one line to the global `trainval_runs/history_index.jsonl` file.\n",
    "\n",
    "Finally, the cell prints the main output folder path and unassigns the Colab runtime to cleanly stop the L4 GPU session."
   ],
   "metadata": {
    "id": "1gAMaK1IkTed"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# ============================================================\n",
    "# D2 Train + Val — Frozen Backbone, Two Heads, LN + Dropout, Full Run History\n",
    "# Inputs: manifest_all.csv (train/val only) and the audio clips referenced by clip_path\n",
    "# Outputs: per-seed run folders with metrics.json, roc_curve.png, confusion_matrix.png, best_heads.pt\n",
    "#          plus one experiment summary_trainval.json and one appended history_index.jsonl record\n",
    "# ============================================================\n",
    "\n",
    "# -------------------------\n",
    "# Imports and core libraries\n",
    "# Purpose: audio I/O, model training, metrics, and plotting\n",
    "# -------------------------\n",
    "import os\n",
    "import json\n",
    "import math\n",
    "import time\n",
    "import random\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import soundfile as sf\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, confusion_matrix, matthews_corrcoef\n",
    "from scipy.stats import fisher_exact\n",
    "\n",
    "from transformers import Wav2Vec2Model\n",
    "\n",
    "# -------------------------\n",
    "# D2 dataset root and manifest location\n",
    "# Input: dataset output root created by preprocessing\n",
    "# Output: MANIFEST_ALL path used for train/val tables\n",
    "# -------------------------\n",
    "D2_OUT_ROOT = \"/content/drive/MyDrive/AI_PD_Project/Datasets/D2-Slovak (EWA-DB)/EWA-DB/preprocessed_v1\"\n",
    "DX_OUT_ROOT = D2_OUT_ROOT  # standardized variable name the rest of the code uses\n",
    "\n",
    "MANIFEST_ALL = f\"{DX_OUT_ROOT}/manifests/manifest_all.csv\"\n",
    "\n",
    "# -------------------------\n",
    "# Run settings and training defaults\n",
    "# Purpose: fixed settings for consistent comparisons across runs\n",
    "# -------------------------\n",
    "MAX_EPOCHS = 10\n",
    "EFFECTIVE_BS = 64\n",
    "\n",
    "PER_DEVICE_BS = 16\n",
    "GRAD_ACCUM = max(1, EFFECTIVE_BS // PER_DEVICE_BS)\n",
    "\n",
    "LR = 1e-3\n",
    "EARLY_STOP_PATIENCE = 2\n",
    "\n",
    "SEEDS = [1337, 2024, 7777]\n",
    "\n",
    "# Purpose: detect where trailing padding starts in vowel clips\n",
    "TINY_THRESHOLD = 1e-4  # mandatory\n",
    "\n",
    "# Purpose: stable DataLoader behavior on Drive-backed storage\n",
    "NUM_WORKERS = 0\n",
    "PIN_MEMORY = False\n",
    "\n",
    "BACKBONE_CKPT = \"facebook/wav2vec2-base\"\n",
    "\n",
    "# Purpose: small regularization in the heads\n",
    "HEAD_DROPOUT_P = 0.10\n",
    "\n",
    "# Purpose: faster training on GPU while keeping heads stable (heads forced FP32 later)\n",
    "USE_AMP = True\n",
    "\n",
    "# Purpose: fixed operating point for confusion matrix and threshold metrics\n",
    "VAL_THRESHOLD = 0.5\n",
    "\n",
    "# -------------------------\n",
    "# Output folders and history tracking\n",
    "# Output: one EXP_ROOT folder per execution, plus a global JSONL index\n",
    "# -------------------------\n",
    "EXPERIMENT_TAG = \"frozen_LNDO\"   # keep same meaning as D1 tag\n",
    "RUN_STAMP = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "TRAINVAL_ROOT = Path(DX_OUT_ROOT) / \"trainval_runs\"\n",
    "EXP_ROOT = TRAINVAL_ROOT / f\"exp_{EXPERIMENT_TAG}_{RUN_STAMP}\"\n",
    "EXP_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "HISTORY_INDEX_PATH = TRAINVAL_ROOT / \"history_index.jsonl\"\n",
    "\n",
    "# -------------------------\n",
    "# Device and environment printout\n",
    "# Purpose: capture key runtime settings in the notebook output\n",
    "# -------------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "gpu_name = torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"CPU\"\n",
    "\n",
    "print(\"EXPERIMENT ROOT:\", str(EXP_ROOT))\n",
    "print(\"DEVICE:\", device)\n",
    "if device.type == \"cuda\":\n",
    "    print(\"GPU:\", gpu_name)\n",
    "print(\"PER_DEVICE_BS:\", PER_DEVICE_BS, \"| GRAD_ACCUM:\", GRAD_ACCUM, \"| EFFECTIVE_BS:\", PER_DEVICE_BS * GRAD_ACCUM)\n",
    "print(\"NUM_WORKERS:\", NUM_WORKERS, \"| PIN_MEMORY:\", PIN_MEMORY)\n",
    "print(\"VAL_THRESHOLD:\", VAL_THRESHOLD)\n",
    "print(\"USE_AMP:\", bool(USE_AMP and device.type == \"cuda\"))\n",
    "print(\"\")\n",
    "\n",
    "# -------------------------\n",
    "# Manifest existence check\n",
    "# Input: MANIFEST_ALL\n",
    "# Output: fail early with a clear error if missing\n",
    "# -------------------------\n",
    "if not os.path.exists(MANIFEST_ALL):\n",
    "    raise FileNotFoundError(f\"Missing manifest_all.csv: {MANIFEST_ALL}\")\n",
    "\n",
    "# -------------------------\n",
    "# Build train/val tables from the manifest\n",
    "# Inputs: manifest_all.csv\n",
    "# Outputs: train_df and val_df, plus basic counts printed to output\n",
    "# -------------------------\n",
    "m = pd.read_csv(MANIFEST_ALL)\n",
    "\n",
    "required_cols = [\"split\", \"clip_path\", \"label_num\", \"task\", \"speaker_id\", \"duration_sec\"]\n",
    "for c in required_cols:\n",
    "    if c not in m.columns:\n",
    "        raise ValueError(f\"manifest_all.csv missing required column: {c}\")\n",
    "\n",
    "# Purpose: train/val only in this cell\n",
    "m = m[m[\"split\"].isin([\"train\", \"val\"])].copy()\n",
    "if len(m) == 0:\n",
    "    raise RuntimeError(\"No rows with split in {'train','val'} found in manifest_all.csv\")\n",
    "\n",
    "# Purpose: pick the dominant dataset label if multiple datasets are present\n",
    "if \"dataset\" in m.columns:\n",
    "    dataset_id = str(m[\"dataset\"].astype(str).value_counts().idxmax())\n",
    "    m = m[m[\"dataset\"].astype(str) == dataset_id].copy()\n",
    "else:\n",
    "    dataset_id = \"DX\"\n",
    "\n",
    "train_df = m[m[\"split\"] == \"train\"].copy().reset_index(drop=True)\n",
    "val_df   = m[m[\"split\"] == \"val\"].copy().reset_index(drop=True)\n",
    "\n",
    "keep_cols = [\"clip_path\", \"label_num\", \"task\", \"speaker_id\", \"duration_sec\", \"split\"]\n",
    "train_df = train_df[keep_cols].copy()\n",
    "val_df   = val_df[keep_cols].copy()\n",
    "\n",
    "def _label_counts(df: pd.DataFrame) -> Dict[int, int]:\n",
    "    vc = df[\"label_num\"].astype(int).value_counts().to_dict()\n",
    "    return {0: int(vc.get(0, 0)), 1: int(vc.get(1, 0))}\n",
    "\n",
    "print(f\"Dataset inferred: {dataset_id}\")\n",
    "print(f\"Train rows: {len(train_df)} | Val rows: {len(val_df)}\")\n",
    "print(f\"Train label counts: {_label_counts(train_df)}\")\n",
    "print(f\"Val label counts: {_label_counts(val_df)}\")\n",
    "\n",
    "# -------------------------\n",
    "# Task grouping (vowel vs other)\n",
    "# Purpose: route each clip through the matching head\n",
    "# Output: task_group column used by dataset and model\n",
    "# -------------------------\n",
    "def make_task_group(task: Any) -> str:\n",
    "    return \"vowel\" if str(task) == \"vowl\" else \"other\"\n",
    "\n",
    "train_df[\"task_group\"] = train_df[\"task\"].apply(make_task_group)\n",
    "val_df[\"task_group\"]   = val_df[\"task\"].apply(make_task_group)\n",
    "\n",
    "# -------------------------\n",
    "# Clip path existence check (progress bar)\n",
    "# Inputs: train_df/val_df clip_path values\n",
    "# Output: fail early with a few missing examples\n",
    "# -------------------------\n",
    "def check_paths_exist(df: pd.DataFrame, desc: str) -> None:\n",
    "    paths = df[\"clip_path\"].astype(str).tolist()\n",
    "    missing = []\n",
    "    for p in tqdm(paths, desc=desc, dynamic_ncols=True):\n",
    "        if not os.path.exists(p):\n",
    "            missing.append(p)\n",
    "            if len(missing) >= 10:\n",
    "                break\n",
    "    if missing:\n",
    "        raise FileNotFoundError(f\"{desc}: missing clip_path(s), first examples:\\n\" + \"\\n\".join(missing))\n",
    "\n",
    "check_paths_exist(train_df, \"Check TRAIN clip_path exists\")\n",
    "check_paths_exist(val_df, \"Check VAL clip_path exists\")\n",
    "\n",
    "# -------------------------\n",
    "# Dataset: read audio and build per-sample attention masks\n",
    "# Inputs: one manifest row (clip_path, label_num, task_group)\n",
    "# Output: tensors for input_values, attention_mask, labels, and task_group\n",
    "# -------------------------\n",
    "class AudioClipsDataset(Dataset):\n",
    "    def __init__(self, df: pd.DataFrame):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Dict[str, Any]:\n",
    "        row = self.df.iloc[idx]\n",
    "        path = str(row[\"clip_path\"])\n",
    "        label = int(row[\"label_num\"])\n",
    "        tg = str(row[\"task_group\"])\n",
    "\n",
    "        # Purpose: load audio, convert to mono, force float32\n",
    "        x, sr = sf.read(path, always_2d=True)\n",
    "        if x.shape[1] != 1:\n",
    "            x = x.mean(axis=1, keepdims=True)\n",
    "        x = x[:, 0].astype(np.float32, copy=False)\n",
    "\n",
    "        # Purpose: enforce training sample rate\n",
    "        if int(sr) != 16000:\n",
    "            raise RuntimeError(f\"Sample rate is not 16kHz for {path}: got {sr}\")\n",
    "\n",
    "        # Purpose: hide trailing padding in vowel clips; keep other clips fully visible\n",
    "        if tg == \"vowel\":\n",
    "            k = None\n",
    "            for i in range(len(x) - 1, -1, -1):\n",
    "                if abs(float(x[i])) > TINY_THRESHOLD:\n",
    "                    k = i\n",
    "                    break\n",
    "            if k is None:\n",
    "                attn = np.ones((len(x),), dtype=np.int64)\n",
    "            else:\n",
    "                attn = np.zeros((len(x),), dtype=np.int64)\n",
    "                attn[:k+1] = 1\n",
    "        else:\n",
    "            attn = np.ones((len(x),), dtype=np.int64)\n",
    "\n",
    "        return {\n",
    "            \"input_values\": torch.tensor(x, dtype=torch.float32),\n",
    "            \"attention_mask\": torch.tensor(attn, dtype=torch.long),\n",
    "            \"labels\": torch.tensor(label, dtype=torch.long),\n",
    "            \"task_group\": tg,\n",
    "        }\n",
    "\n",
    "# -------------------------\n",
    "# Collator: dynamic padding and mask preservation\n",
    "# Inputs: list of dataset items with variable length\n",
    "# Output: padded batch tensors + task_group list\n",
    "# -------------------------\n",
    "def collate_batch(batch: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
    "    lengths = [b[\"input_values\"].shape[0] for b in batch]\n",
    "    max_len = int(max(lengths)) if lengths else 0\n",
    "\n",
    "    B = len(batch)\n",
    "    x = torch.zeros((B, max_len), dtype=torch.float32)\n",
    "    msk = torch.zeros((B, max_len), dtype=torch.long)\n",
    "    y = torch.zeros((B,), dtype=torch.long)\n",
    "    tg = []\n",
    "\n",
    "    for i, b in enumerate(batch):\n",
    "        t = b[\"input_values\"]\n",
    "        a = b[\"attention_mask\"]\n",
    "        n = t.shape[0]\n",
    "        x[i, :n] = t\n",
    "        msk[i, :n] = a\n",
    "        y[i] = b[\"labels\"]\n",
    "        tg.append(b[\"task_group\"])\n",
    "\n",
    "    return {\"input_values\": x, \"attention_mask\": msk, \"labels\": y, \"task_group\": tg}\n",
    "\n",
    "# -------------------------\n",
    "# Model: frozen Wav2Vec2 backbone + two small heads\n",
    "# Inputs: audio batch + attention_mask + task_group\n",
    "# Outputs: loss and logits, using the matching head per sample\n",
    "# -------------------------\n",
    "class FrozenW2V2TwoHead(nn.Module):\n",
    "    def __init__(self, backbone_ckpt: str, dropout_p: float):\n",
    "        super().__init__()\n",
    "\n",
    "        # Purpose: load backbone once and freeze it\n",
    "        self.backbone = Wav2Vec2Model.from_pretrained(backbone_ckpt)\n",
    "        for p in self.backbone.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "        H = int(self.backbone.config.hidden_size)\n",
    "\n",
    "        # Purpose: small trainable heads, one per task group\n",
    "        self.head_vowel = nn.Sequential(\n",
    "            nn.LayerNorm(H),\n",
    "            nn.Dropout(dropout_p),\n",
    "            nn.Linear(H, 2),\n",
    "        )\n",
    "        self.head_other = nn.Sequential(\n",
    "            nn.LayerNorm(H),\n",
    "            nn.Dropout(dropout_p),\n",
    "            nn.Linear(H, 2),\n",
    "        )\n",
    "\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    def masked_mean_pool(self, last_hidden: torch.Tensor, attn_mask_samples: torch.Tensor) -> torch.Tensor:\n",
    "        # Purpose: map the sample-space mask to feature-space mask for pooling\n",
    "        B, T_feat, H = last_hidden.shape\n",
    "        feat_mask = self.backbone._get_feature_vector_attention_mask(T_feat, attn_mask_samples)  # [B,T_feat] bool\n",
    "        feat_mask_f = feat_mask.to(last_hidden.dtype)\n",
    "        denom = feat_mask_f.sum(dim=1).clamp(min=1.0).unsqueeze(-1)\n",
    "        pooled = (last_hidden * feat_mask_f.unsqueeze(-1)).sum(dim=1) / denom\n",
    "        return pooled\n",
    "\n",
    "    def _heads_fp32(self, pooled_any: torch.Tensor, head: nn.Module) -> torch.Tensor:\n",
    "        # Purpose: run heads in float32 even when autocast is enabled\n",
    "        pooled_fp32 = pooled_any.float()\n",
    "        if pooled_fp32.is_cuda:\n",
    "            with torch.amp.autocast(device_type=\"cuda\", enabled=False):\n",
    "                return head(pooled_fp32)\n",
    "        return head(pooled_fp32)\n",
    "\n",
    "    def forward(self, input_values: torch.Tensor, attention_mask: torch.Tensor, labels: torch.Tensor, task_group: List[str]):\n",
    "        out = self.backbone(input_values=input_values, attention_mask=attention_mask)\n",
    "        last_hidden = out.last_hidden_state  # [B, T_feat, H]\n",
    "\n",
    "        pooled = self.masked_mean_pool(last_hidden, attention_mask)  # [B, H]\n",
    "\n",
    "        # Purpose: fill logits by routing each sample to its head\n",
    "        B = pooled.shape[0]\n",
    "        logits = torch.zeros((B, 2), dtype=torch.float32, device=pooled.device)\n",
    "\n",
    "        idx_v = [i for i, tg in enumerate(task_group) if tg == \"vowel\"]\n",
    "        idx_o = [i for i, tg in enumerate(task_group) if tg != \"vowel\"]\n",
    "\n",
    "        if idx_v:\n",
    "            pv = pooled[idx_v]\n",
    "            lv = self._heads_fp32(pv, self.head_vowel)   # FP32\n",
    "            logits[idx_v] = lv\n",
    "\n",
    "        if idx_o:\n",
    "            po = pooled[idx_o]\n",
    "            lo = self._heads_fp32(po, self.head_other)   # FP32\n",
    "            logits[idx_o] = lo\n",
    "\n",
    "        loss = self.loss_fn(logits, labels)\n",
    "        return loss, logits\n",
    "\n",
    "# -------------------------\n",
    "# Metrics and plots (validation only)\n",
    "# Inputs: y_true and PD probability\n",
    "# Outputs: AUROC, threshold metrics, and PNG figures\n",
    "# -------------------------\n",
    "def compute_val_auroc(probs1: np.ndarray, y_true: np.ndarray) -> float:\n",
    "    if len(np.unique(y_true)) < 2:\n",
    "        return float(\"nan\")\n",
    "    return float(roc_auc_score(y_true, probs1))\n",
    "\n",
    "def compute_threshold_metrics(y_true: np.ndarray, probs1: np.ndarray, thr: float = 0.5) -> Dict[str, Any]:\n",
    "    y_true = y_true.astype(int)\n",
    "    y_pred = (probs1 >= thr).astype(int)\n",
    "\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=[0, 1])\n",
    "    if cm.shape != (2, 2):\n",
    "        TN = FP = FN = TP = 0\n",
    "    else:\n",
    "        TN, FP, FN, TP = cm[0, 0], cm[0, 1], cm[1, 0], cm[1, 1]\n",
    "\n",
    "    eps = 1e-12\n",
    "    accuracy = (TP + TN) / max(1, (TP + TN + FP + FN))\n",
    "    precision = TP / (TP + FP + eps)\n",
    "    recall = TP / (TP + FN + eps)            # sensitivity\n",
    "    f1 = 2 * precision * recall / (precision + recall + eps)\n",
    "    specificity = TN / (TN + FP + eps)\n",
    "\n",
    "    mcc = float(matthews_corrcoef(y_true, y_pred)) if len(np.unique(y_pred)) > 1 else float(\"nan\")\n",
    "\n",
    "    # Purpose: quick association test between predictions and truth\n",
    "    try:\n",
    "        _, p_value = fisher_exact([[TN, FP], [FN, TP]], alternative=\"two-sided\")\n",
    "        p_value = float(p_value)\n",
    "    except Exception:\n",
    "        p_value = float(\"nan\")\n",
    "\n",
    "    return {\n",
    "        \"threshold\": float(thr),\n",
    "        \"confusion_matrix\": {\"TN\": int(TN), \"FP\": int(FP), \"FN\": int(FN), \"TP\": int(TP),\n",
    "        },\n",
    "        \"accuracy\": float(accuracy),\n",
    "        \"precision\": float(precision),\n",
    "        \"recall\": float(recall),\n",
    "        \"f1_score\": float(f1),\n",
    "        \"sensitivity\": float(recall),\n",
    "        \"specificity\": float(specificity),\n",
    "        \"mcc\": float(mcc),\n",
    "        \"p_value_fisher_two_sided\": float(p_value),\n",
    "    }\n",
    "\n",
    "def save_roc_curve(y_true: np.ndarray, probs1: np.ndarray, out_png: str) -> None:\n",
    "    fpr, tpr, _ = roc_curve(y_true, probs1)\n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr)\n",
    "    plt.plot([0, 1], [0, 1], linestyle=\"--\")\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(\"ROC Curve (Val)\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_png, dpi=160)\n",
    "    plt.close()\n",
    "\n",
    "def save_confusion_matrix(y_true: np.ndarray, probs1: np.ndarray, out_png: str, thr: float = 0.5) -> None:\n",
    "    y_pred = (probs1 >= thr).astype(int)\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=[0, 1])\n",
    "    plt.figure()\n",
    "    plt.imshow(cm, interpolation=\"nearest\")\n",
    "    plt.title(f\"Confusion Matrix (Val) @ thr={thr:.2f}\")\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"True\")\n",
    "    plt.xticks([0, 1], [\"HC(0)\", \"PD(1)\"])\n",
    "    plt.yticks([0, 1], [\"HC(0)\", \"PD(1)\"])\n",
    "    for (i, j), v in np.ndenumerate(cm):\n",
    "        plt.text(j, i, str(v), ha=\"center\", va=\"center\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_png, dpi=160)\n",
    "    plt.close()\n",
    "\n",
    "# -------------------------\n",
    "# Deterministic seeding\n",
    "# Purpose: stabilize shuffling and initialization per seed\n",
    "# -------------------------\n",
    "def set_all_seeds(seed: int) -> None:\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# -------------------------\n",
    "# Warm-up loader\n",
    "# Purpose: catch I/O and collation issues before training starts\n",
    "# -------------------------\n",
    "def warmup_loader(loader: DataLoader, n_batches: int = 3, seed: int = 0) -> None:\n",
    "    t0 = time.time()\n",
    "    print(f\"[seed={seed}] Warm-up: loading {n_batches} train batches...\")\n",
    "    it = iter(loader)\n",
    "    for i in range(n_batches):\n",
    "        _ = next(it)\n",
    "        print(f\"  loaded warmup batch {i+1}/{n_batches}\")\n",
    "    dt = time.time() - t0\n",
    "    print(f\"[seed={seed}] Warm-up done in {dt:.2f}s\")\n",
    "\n",
    "# -------------------------\n",
    "# One seed run: train, validate, save best epoch\n",
    "# Inputs: seed, train_df, val_df\n",
    "# Outputs: run folder artifacts + metrics dict used for experiment summary\n",
    "# -------------------------\n",
    "def run_one_seed(seed: int) -> Dict[str, Any]:\n",
    "    set_all_seeds(seed)\n",
    "\n",
    "    run_dir_path = EXP_ROOT / f\"run_{dataset_id}_seed{seed}\"\n",
    "    run_dir_path.mkdir(parents=True, exist_ok=True)\n",
    "    run_dir = str(run_dir_path)  # keep downstream path handling unchanged\n",
    "\n",
    "    train_ds = AudioClipsDataset(train_df)\n",
    "    val_ds   = AudioClipsDataset(val_df)\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_ds,\n",
    "        batch_size=PER_DEVICE_BS,\n",
    "        shuffle=True,\n",
    "        num_workers=NUM_WORKERS,\n",
    "        pin_memory=PIN_MEMORY,\n",
    "        collate_fn=collate_batch,\n",
    "        drop_last=False,\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_ds,\n",
    "        batch_size=PER_DEVICE_BS,\n",
    "        shuffle=False,\n",
    "        num_workers=NUM_WORKERS,\n",
    "        pin_memory=PIN_MEMORY,\n",
    "        collate_fn=collate_batch,\n",
    "        drop_last=False,\n",
    "    )\n",
    "\n",
    "    warmup_loader(train_loader, n_batches=3, seed=seed)\n",
    "\n",
    "    model = FrozenW2V2TwoHead(BACKBONE_CKPT, HEAD_DROPOUT_P).to(device)\n",
    "\n",
    "    # Purpose: only head weights update\n",
    "    params = list(model.head_vowel.parameters()) + list(model.head_other.parameters())\n",
    "    opt = torch.optim.Adam(params, lr=LR)\n",
    "\n",
    "    use_amp = bool(USE_AMP and device.type == \"cuda\")\n",
    "    scaler = torch.amp.GradScaler(\"cuda\", enabled=use_amp)\n",
    "\n",
    "    best_auroc = -1.0\n",
    "    best_epoch = -1\n",
    "    best_state = None\n",
    "    no_improve = 0\n",
    "\n",
    "    best_probs = None\n",
    "    best_ytrue = None\n",
    "\n",
    "    for epoch in range(1, MAX_EPOCHS + 1):\n",
    "        # Train loop\n",
    "        model.train()\n",
    "        opt.zero_grad(set_to_none=True)\n",
    "\n",
    "        running_loss = 0.0\n",
    "        n_steps = 0\n",
    "\n",
    "        pbar = tqdm(train_loader, desc=f\"[seed={seed}] Train epoch {epoch}\", dynamic_ncols=True)\n",
    "        for step, batch in enumerate(pbar, start=1):\n",
    "            x = batch[\"input_values\"].to(device, non_blocking=True)\n",
    "            msk = batch[\"attention_mask\"].to(device, non_blocking=True)\n",
    "            y = batch[\"labels\"].to(device, non_blocking=True)\n",
    "            tg = batch[\"task_group\"]\n",
    "\n",
    "            with torch.amp.autocast(device_type=\"cuda\", enabled=use_amp):\n",
    "                loss, _ = model(x, msk, y, tg)\n",
    "                loss = loss / GRAD_ACCUM\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "\n",
    "            if (step % GRAD_ACCUM) == 0:\n",
    "                scaler.step(opt)\n",
    "                scaler.update()\n",
    "                opt.zero_grad(set_to_none=True)\n",
    "\n",
    "            running_loss += float(loss.item()) * GRAD_ACCUM\n",
    "            n_steps += 1\n",
    "\n",
    "        # Purpose: flush last partial accumulation if needed\n",
    "        if (n_steps % GRAD_ACCUM) != 0:\n",
    "            scaler.step(opt)\n",
    "            scaler.update()\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "\n",
    "        avg_train_loss = running_loss / max(1, n_steps)\n",
    "\n",
    "        # Validation loop\n",
    "        model.eval()\n",
    "        y_true_all = []\n",
    "        p1_all = []\n",
    "\n",
    "        pbarv = tqdm(val_loader, desc=f\"[seed={seed}] Val epoch {epoch}\", dynamic_ncols=True)\n",
    "        with torch.no_grad():\n",
    "            for batch in pbarv:\n",
    "                x = batch[\"input_values\"].to(device, non_blocking=True)\n",
    "                msk = batch[\"attention_mask\"].to(device, non_blocking=True)\n",
    "                y = batch[\"labels\"].to(device, non_blocking=True)\n",
    "                tg = batch[\"task_group\"]\n",
    "\n",
    "                with torch.amp.autocast(device_type=\"cuda\", enabled=use_amp):\n",
    "                    _, logits = model(x, msk, y, tg)\n",
    "\n",
    "                probs = torch.softmax(logits.float(), dim=-1)[:, 1]\n",
    "                y_true_all.append(y.detach().cpu().numpy())\n",
    "                p1_all.append(probs.detach().cpu().numpy())\n",
    "\n",
    "        y_true = np.concatenate(y_true_all, axis=0).astype(int)\n",
    "        p1 = np.concatenate(p1_all, axis=0).astype(np.float64)\n",
    "\n",
    "        val_auroc = compute_val_auroc(p1, y_true)\n",
    "\n",
    "        print(f\"seed={seed} | epoch {epoch:02d}/{MAX_EPOCHS} | train_loss={avg_train_loss:.5f} | val_AUROC={val_auroc:.5f}\")\n",
    "\n",
    "        # Purpose: keep only the best epoch by VAL AUROC\n",
    "        improved = (not math.isnan(val_auroc)) and (val_auroc > best_auroc + 1e-12)\n",
    "        if improved:\n",
    "            best_auroc = float(val_auroc)\n",
    "            best_epoch = int(epoch)\n",
    "            no_improve = 0\n",
    "\n",
    "            best_state = {\n",
    "                \"head_vowel\": {k: v.detach().cpu() for k, v in model.head_vowel.state_dict().items()},\n",
    "                \"head_other\": {k: v.detach().cpu() for k, v in model.head_other.state_dict().items()},\n",
    "                \"backbone_ckpt\": BACKBONE_CKPT,\n",
    "                \"hidden_size\": int(model.backbone.config.hidden_size),\n",
    "                \"dropout_p\": float(HEAD_DROPOUT_P),\n",
    "                \"dataset_id\": dataset_id,\n",
    "                \"seed\": int(seed),\n",
    "                \"val_threshold\": float(VAL_THRESHOLD),\n",
    "            }\n",
    "            best_probs = p1.copy()\n",
    "            best_ytrue = y_true.copy()\n",
    "        else:\n",
    "            no_improve += 1\n",
    "            if no_improve >= EARLY_STOP_PATIENCE:\n",
    "                break\n",
    "\n",
    "    # Save best epoch artifacts only\n",
    "    metrics_path = os.path.join(run_dir, \"metrics.json\")\n",
    "    roc_png = os.path.join(run_dir, \"roc_curve.png\")\n",
    "    cm_png = os.path.join(run_dir, \"confusion_matrix.png\")\n",
    "    heads_path = os.path.join(run_dir, \"best_heads.pt\")\n",
    "\n",
    "    if best_state is None or best_probs is None or best_ytrue is None:\n",
    "        raise RuntimeError(f\"[seed={seed}] No valid best_state captured; cannot write artifacts.\")\n",
    "\n",
    "    save_roc_curve(best_ytrue, best_probs, roc_png)\n",
    "    save_confusion_matrix(best_ytrue, best_probs, cm_png, thr=VAL_THRESHOLD)\n",
    "    torch.save(best_state, heads_path)\n",
    "\n",
    "    thr_metrics = compute_threshold_metrics(best_ytrue, best_probs, thr=VAL_THRESHOLD)\n",
    "\n",
    "    # Output: metrics.json (includes best AUROC and threshold metrics)\n",
    "    metrics = {\n",
    "        \"dataset\": dataset_id,\n",
    "        \"seed\": int(seed),\n",
    "        \"best_val_auroc\": float(best_auroc),\n",
    "        \"best_epoch\": int(best_epoch),\n",
    "        \"max_epochs\": int(MAX_EPOCHS),\n",
    "        \"early_stop_patience\": int(EARLY_STOP_PATIENCE),\n",
    "        \"n_train\": int(len(train_df)),\n",
    "        \"n_val\": int(len(val_df)),\n",
    "        \"label_counts_train\": _label_counts(train_df),\n",
    "        \"label_counts_val\": _label_counts(val_df),\n",
    "        \"backbone_ckpt\": BACKBONE_CKPT,\n",
    "        \"frozen_backbone\": True,\n",
    "        \"heads\": {\n",
    "            \"vowel\": \"LayerNorm + Dropout + Linear(768->2)\",\n",
    "            \"other\": \"LayerNorm + Dropout + Linear(768->2)\",\n",
    "        },\n",
    "        \"threshold_metrics_best_epoch\": thr_metrics,\n",
    "        \"padding_mask_rule\": {\n",
    "            \"tiny_threshold\": TINY_THRESHOLD,\n",
    "            \"vowel\": \"trailing abs(x) > tiny_threshold => mask=1 else 0\",\n",
    "            \"other\": \"all ones\",\n",
    "            \"batch_pad\": \"zero pad + mask zeros\",\n",
    "        },\n",
    "        \"batching\": {\n",
    "            \"per_device_bs\": int(PER_DEVICE_BS),\n",
    "            \"grad_accum\": int(GRAD_ACCUM),\n",
    "            \"effective_bs\": int(PER_DEVICE_BS * GRAD_ACCUM),\n",
    "        },\n",
    "        \"optimizer\": {\"name\": \"Adam\", \"lr\": float(LR)},\n",
    "        \"amp\": bool(use_amp),\n",
    "        \"device\": str(device),\n",
    "        \"gpu\": str(gpu_name),\n",
    "        \"experiment_tag\": EXPERIMENT_TAG,\n",
    "        \"run_stamp\": RUN_STAMP,\n",
    "        \"val_threshold\": float(VAL_THRESHOLD),\n",
    "        \"dropout_p\": float(HEAD_DROPOUT_P),\n",
    "        \"lr_scalar\": float(LR),\n",
    "        \"effective_batch_size\": int(PER_DEVICE_BS * GRAD_ACCUM),\n",
    "        \"per_device_batch_size\": int(PER_DEVICE_BS),\n",
    "        \"grad_accum_steps\": int(GRAD_ACCUM),\n",
    "        \"paths\": {\n",
    "            \"manifest_all\": MANIFEST_ALL,\n",
    "            \"run_dir\": run_dir,\n",
    "            \"roc_curve_png\": roc_png,\n",
    "            \"confusion_matrix_png\": cm_png,\n",
    "            \"best_heads_pt\": heads_path,\n",
    "        },\n",
    "    }\n",
    "\n",
    "    with open(metrics_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(metrics, f, indent=2)\n",
    "\n",
    "    print(f\"[seed={seed}] WROTE:\\n  {metrics_path}\\n  {roc_png}\\n  {cm_png}\\n  {heads_path}\\n\")\n",
    "\n",
    "    return metrics\n",
    "\n",
    "# -------------------------\n",
    "# Run all seeds and write experiment summary\n",
    "# Inputs: SEEDS list\n",
    "# Outputs: summary_trainval.json and appended history_index.jsonl\n",
    "# -------------------------\n",
    "results = []\n",
    "for sd in SEEDS:\n",
    "    results.append(run_one_seed(sd))\n",
    "\n",
    "aurocs = np.array([r[\"best_val_auroc\"] for r in results], dtype=np.float64)\n",
    "\n",
    "print(\"AUROC by seed:\")\n",
    "for r in results:\n",
    "    print(f\"  seed {r['seed']}: {r['best_val_auroc']:.6f}\")\n",
    "\n",
    "t_crit = 4.302652729911275  # df=2, 95% CI\n",
    "n = len(aurocs)\n",
    "mean_auroc = float(np.mean(aurocs))\n",
    "std_auc = float(np.std(aurocs, ddof=1)) if n > 1 else 0.0\n",
    "half_width = float(t_crit * (std_auc / math.sqrt(n))) if n > 1 else 0.0\n",
    "ci_low = float(mean_auroc - half_width)\n",
    "ci_high = float(mean_auroc + half_width)\n",
    "\n",
    "print(\"\")\n",
    "print(f\"Mean AUROC: {mean_auroc:.6f}\")\n",
    "print(f\"95% CI (t, n=3): [{ci_low:.6f}, {ci_high:.6f}]\")\n",
    "print(\"\")\n",
    "\n",
    "# Purpose: keep a compact per-seed record inside the experiment summary\n",
    "per_seed_metrics = []\n",
    "run_dirs = []\n",
    "for r in results:\n",
    "    per_seed_metrics.append({\n",
    "        \"seed\": int(r[\"seed\"]),\n",
    "        \"best_val_auroc\": float(r[\"best_val_auroc\"]),\n",
    "        \"threshold_metrics_best_epoch\": r[\"threshold_metrics_best_epoch\"],\n",
    "    })\n",
    "    run_dirs.append(r[\"paths\"][\"run_dir\"])\n",
    "\n",
    "# Output: one summary JSON for the whole experiment\n",
    "exp_summary = {\n",
    "    \"dataset\": dataset_id,\n",
    "    \"dx_out_root\": DX_OUT_ROOT,\n",
    "    \"experiment_tag\": EXPERIMENT_TAG,\n",
    "    \"run_stamp\": RUN_STAMP,\n",
    "    \"exp_root\": str(EXP_ROOT),\n",
    "    \"run_dirs\": run_dirs,\n",
    "    \"seeds\": SEEDS,\n",
    "    \"aurocs\": [float(x) for x in aurocs.tolist()],  # note: keep spelling consistent with earlier summaries\n",
    "    \"mean_auroc\": float(mean_auroc),\n",
    "    \"t_crit_df2_95\": float(t_crit),\n",
    "    \"half_width_95_ci\": float(half_width),\n",
    "    \"ci95\": [float(ci_low), float(ci_high)],\n",
    "    \"n_train\": int(len(train_df)),\n",
    "    \"n_val\": int(len(val_df)),\n",
    "    \"label_counts_train\": _label_counts(train_df),\n",
    "    \"label_counts_val\": _label_counts(val_df),\n",
    "    \"effective_batch_size\": int(PER_DEVICE_BS * GRAD_ACCUM),\n",
    "    \"per_device_batch_size\": int(PER_DEVICE_BS),\n",
    "    \"grad_accum_steps\": int(GRAD_ACCUM),\n",
    "    \"val_threshold\": float(VAL_THRESHOLD),\n",
    "    \"backbone_ckpt\": BACKBONE_CKPT,\n",
    "    \"dropout_p\": float(HEAD_DROPOUT_P),\n",
    "    \"lr\": float(LR),\n",
    "    \"per_seed_best_epoch_metrics\": per_seed_metrics,\n",
    "}\n",
    "\n",
    "summary_path = Path(EXP_ROOT) / \"summary_trainval.json\"\n",
    "with open(summary_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(exp_summary, f, indent=2)\n",
    "\n",
    "# Output: append one line per experiment so older records remain intact\n",
    "with open(HISTORY_INDEX_PATH, \"a\", encoding=\"utf-8\") as f:\n",
    "    f.write(json.dumps(exp_summary) + \"\\n\")\n",
    "\n",
    "print(\"WROTE per-experiment summary:\", str(summary_path))\n",
    "print(\"APPENDED global history index:\", str(HISTORY_INDEX_PATH))\n",
    "print(\"\")\n",
    "print(\"Open this folder to access artifacts:\", str(EXP_ROOT))\n",
    "\n",
    "# -------------------------\n",
    "# Stop runtime (release GPU)\n",
    "# -------------------------\n",
    "print(\"\\nAll done. Unassigning the runtime to stop the L4 instance...\")\n",
    "try:\n",
    "    from google.colab import runtime  # type: ignore\n",
    "    print(\"Calling runtime.unassign() now.\")\n",
    "    runtime.unassign()\n",
    "except Exception as e:\n",
    "    print(\"Could not unassign runtime automatically. You can stop the runtime manually in Colab.\")\n",
    "    print(\"Reason:\", repr(e))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 2749,
     "referenced_widgets": [
      "9f63b87696964d41a19ce2ee2b965d64",
      "2c6b409514a045d697587d435ba4d88e",
      "08a5eaee9df846cab4e0014cd37bc669",
      "00a06b5191b240a6a6738c740b291589",
      "9a12f595dc0f44b08c813379aada3163",
      "93c22c46ac934e7184a2e32f41cfa104",
      "f0249f1e35bf4d2d823a7eb10f38e811",
      "c91e9e3417b647f9b41f6f2c35508942",
      "87e96ebbc12d4861a4667427d550c595",
      "9719c8c795534830b581717d8c070439",
      "de5e19421a0946ed88e5bfc403b3b392",
      "11e8971d38754bd49dcaeaf0c463bd12",
      "ea63a262cd7049fbab68b7ad5b23d027",
      "63a6a2478a8a4d0aa02758552a6d3638",
      "f9052e2f20a54e5282c6a9f2c7f98391",
      "30d604bebf4d4d08ad9895337c27b38d",
      "1e42ca03eb22478f98900bf9fb02f712",
      "090316d4236e44dfbdd8581333692feb",
      "7d50ac14d45747d1ad0b9c8deb27a1d1",
      "7fdb09517c7e4a108bb0f5b0b060a733",
      "16cdfeaf032441909ea1e1dff97311a8",
      "48f9e54e744745ad9c8714d1f396f49a",
      "d2d2ead05077441a8c9229763550f33b",
      "8bbe5c9b38ff40a8992b202c9d3473b2",
      "c6b953a9546f427d955dfe85d7ae455c",
      "6f36460654344593b6dc277e52e52250",
      "68c4ad4ee53b4320bebf8e4c9ae191e3",
      "745c0450ebb44e30801e59df5c7d4668",
      "cc0ad6af196e419597a8423c8de394ad",
      "3387e40cecd34d76972f08c817eb1294",
      "eee3ff89377c4602afd51a929f6d61b1",
      "272bf98ce568479999b579b4286a0b8f",
      "1bcb24e4d8c14d01a9f6a57f63539a14",
      "0a3e1f30b0db4c0eadf65ca71ddefbd5",
      "af3e39edd425487987827e1f37da94f2",
      "70ae6ecc6395403291dca001f8594456",
      "923dc8bede69433abb57757867b446bd",
      "acfeff2b8d304888889fd4bd923fed43",
      "cb46730c30bc4a4e994cc70f207eff52",
      "60b9245ac9a848e88308f88afa357859",
      "c805fbed811c40a7b8524e1aea11a45a",
      "aece57319b67444bbf1aa0575d33a57f",
      "344b9031c761455e8b5526af53d014aa",
      "3aec1de41e924deca110dd1835ca6ace",
      "899327e0a06041a099b323fe7fde602d",
      "a4721ff1a8874c9498631293466e8484",
      "7c07bd9ab54e423a96dd435501bea560",
      "626b35ce9de641eeab19a7fd0e87a8ac",
      "702607038f384fa59d91e5cf0962cce2",
      "0e8fd1cf539346f581f351a61f503c12",
      "83481ddab4454b46b2b579f2959a0113",
      "b56d9e6246f64358a5c7fa9521d1e182",
      "81c0a6ce45ac4930bea4c70ad5d99d8c",
      "3638dc65fe5c4463a0eb65f59a9dfa6b",
      "eadc640644b34c8a8ed3b6c0e118a60e",
      "ea5b544ee96844948a3edffc514096c5",
      "9a8065fd46794534816330f6abc8c112",
      "5cb2902b633b4b06a312c7c7ee279379",
      "3bd9001eab5b4701be3c0a4a4cfe6417",
      "e3d2bc6e84ad455a8c9d13cc98c05cbe",
      "164747bdefb740048ef9ab2cef971211",
      "336f6077dcce4d448390798314dc5b56",
      "1d96323bbaa64d37bc3ffc4a30c29db0",
      "c2aebe72019245dfab098171fe72060b",
      "4d65d314ce8d48cdb7e4ffe82862d049",
      "1e1b3b9c49454b2faefed1ab340306ba",
      "f49ab0f7bcce4fd683b55f0ef0b71b39",
      "5bba581430f94a7dab999193cf91d704",
      "7dff88502b414afc8688e45dbfb9e40f",
      "da976e2ae1b946349cd725b6a802462e",
      "f1fd15977938485a9b677fcd67c292e1",
      "4ce0f85cf5e2405a941ef927ead37f71",
      "5fecdc7248574bb493aea460d34908ec",
      "101a8c514910454da00d3c4ac9ea4df3",
      "bc4920a491324df99c90a93c1faef29a",
      "afc40312354142fba10f274149ebc1f6",
      "514aa19419ac45ffa436dc474d052a82",
      "b77d0a47e266498d808d21a05f40e229",
      "a3306c7ca9164508b1321ef6872a2d08",
      "a545beac33f040829da1a5c3cf2fd4d3",
      "40a0e380663244ea8fe40f3628f9c9d4",
      "556e9913d6ed4f3bb03f3c88e9e2abc8",
      "94d84275a1c0484980f33afd68b120cc",
      "7168b97c28714edb8db6ec8885e2014f",
      "5191064e664b43bfa4060db3e5f15500",
      "48bd04f2565449f2ba11a44b4f2802ee",
      "c0f7b723e05743e0a3d1cd6cbe0f83cb",
      "b266f4334ae940e2aa8b4cac214929c5",
      "d7a46dcd03be423b83f3fcbde89b6bd7",
      "1a35d46c66ab43e8b08e1aa8344531b6",
      "b3edfb02c2ed49be99bdfa417420a746",
      "dfbfa2895eec4e15afd8bd6861c6e028",
      "79f125f480504eec94f924b1f7c885ba",
      "760bf56230dd49d0bff3f6ed9824111c",
      "ec22cc34e5b64514b1d28b47f88bfa20",
      "d0f8b248b26c4bfeb027aba37fa027ef",
      "8eb117979805428db93fea00ffb1383b",
      "5a47158ff08047b28b82e73c2324d292",
      "2f339152a348429aa456d42c45a886cd",
      "fce6d99d8e2340e8b681ba3444d4bb53",
      "e5dbb99a7852489d87f57ef224ad6899",
      "5f58eca612b1453888e0b859b35ca3ca",
      "907b241cf1854f3580109f98b7b166bc",
      "d16f5ac6be6d47898f99632ecf6fc4d4",
      "48887c11286c4ee1b0aa1ef35626ed18",
      "9d7317197cdc43d788c36ebbb8b2080f",
      "cd74b577355c42cc9e4e216099ce0896",
      "d2789cfdd9174e9194594a301449311c",
      "732017e34d024c9ca982dbf9cee22d77",
      "f3ee1f1565e94b6096561c05388a16d5",
      "5431d87ed4d746d9847cd615e9be681b",
      "7ebf9e0218ce4c3f9af720cc82c6c751",
      "872c609dab2f4990995f7b971c57c405",
      "d664f0b32a1f4557907d410e32987ad1",
      "02ad557ed7984d41a8d0e9e55d44c1bd",
      "434b453c81c4455990b89eed041386cc",
      "cf8e524fb5ff49879afe3a59a933f796",
      "7d4937b4d04246669b87bb3b6605a5ce",
      "09b1855691e74a00837df0c99ed262bd",
      "de0ab092dcbe4e5c9457658bdd91adb4",
      "ffaf7fc0a3c54612acf7a269d5c75746",
      "f4fe002eab38455c826fdfd569c19b02",
      "da9c7cb582374f8789a945dd0f2c394d",
      "5f65c40aa05141ceb310c857ade819e1",
      "35734661814f44669b3ce0a083e3b5e9",
      "25bb672fc33b4f4f9c14ca848dfc642f",
      "d25c85ac20fb4877bb860009d0766abd",
      "f5b7f310a0d64f29ae6e9dc4c71331c9",
      "4a2ec5c37bdb4b959ce35ef69fd3bebc",
      "4b8a80fd1f584c448bb083928e1defc7",
      "4157d76b8def4990afa12d3f81964255",
      "5cefac4e6cb84153b41b4e133b59cb77",
      "e7e91af02c7e41b597ff13e784544254",
      "18afd0156d8a4b5eb89458889b83755d",
      "493bf1b00d73468f80ed441c19b3d3ae",
      "33c3392ac39c43a59999df3bc4cbde5d",
      "c85a69844f8c462089e9f1732b8aafa7",
      "cd5fc412f3974d7da7573e4de298952c",
      "2df3ea244b1b49c49a209818519bcd5f",
      "c1a71144a725432fbc7b4dc0342641dd",
      "e61c679296f04ebcaee0be0a61161956",
      "f0cc44699df24f85ac74a529c5df4cb2",
      "fa608bd7a37b4b63991017e7298fde7b",
      "baa9048ac15f497584c50bb8f0f70edb",
      "8a7e9edfe0f4484980dbe0ac4157ce5c",
      "922ca9e7547346fcabe0b48fe2cafc67",
      "e8c3fcd220e44555af9235662e0a1b1c",
      "89c7457449f24b44afbf2f409d7519dc",
      "af4f53b5b09242b0b3d96078e157fc04",
      "70a8d06b12a54637b1a6dd2be43f2bd9",
      "e138645363644e0d971e92a81b5a85a4",
      "8f670d7c9c0b44189e6ecc2bc4bb6807",
      "998ed13a43f0411c9a77bda1a549f126",
      "73209e62f2e6421282e4ddf787e5c995",
      "231165ccdbb84265871e922d008906c8",
      "5c13196670244dabbf5653b12dc4532a",
      "917360a4bcc2472894a151e4b2b4ef24",
      "8c3a655b821e404281e4e064f6c6c9f7",
      "705d84232ea449a5a0d152a155dd7b04",
      "d75121397d1c4d2e88bcafe7be9647ce",
      "5b44cb3b3cb245bba112046a5dfc5298",
      "b532c46a9a514a108776a04255d70d7c",
      "39fa2bac97734454849fab70d36e719d",
      "5eefa5667a9c42c8a50a8b2166228213",
      "8a336a03eb134eb6b5ff12486d63c852",
      "ab8d79d01e0a43289f9288041bb19937",
      "4ad63b742b9144458e41bbb6d9e7d94e",
      "37ac8301c0e341d98e1d864106780950",
      "a2fb3948e29549bf9d2e4e86683de5e5",
      "ac0fef010f134c92a55b5521b36b09b1",
      "ff808b68cd624374be26acb7c764b662",
      "3ff61f18f7a1477f9e2d3035f6a914ab",
      "8140efdf14f64df6a532f0f57540b48f",
      "64a6607c1a4d460d82e4ec69a77020f5",
      "0c562967b5984706a0a97ffbbbd6d6fb",
      "c7583b3452e545e8b723f30001e9b5f3",
      "43190112f0094c84958a28902f49c0af",
      "f6fd7e43a174445b94896c7b7ccd0682",
      "02c502e9493a47b9a1eb8027508bd0d4",
      "cf3c00ee9f0345bba9fe00290cc79813",
      "18f2928511e4452abd568dbac08d381f",
      "0dd03ee701d04018b1b45d6eef9ca5d0",
      "c4004964de69475aae1b65b10268bda7",
      "f4858be7e27a4bffbad903ffca8df195",
      "e80377736d2d456395f83d62b8ba67f7",
      "dad588ec565e498388d758f4ad9d268b",
      "576e726b29e3463392e3f7f2404573ab",
      "bf041b6fbf6f4c309688c12ded2dd7f0",
      "df9ccb71884f40bbadd7034c3d834dd4",
      "dfd6e7bb70d744869432ce29c25dd7c3",
      "8cefb31d91bf4f569ea130eae29b57f2",
      "3ee051cb6b7c42618bf7c60d20d3212b",
      "34a115e26a204e019c54d689f03d0ae1",
      "db59ef820ea44aebaf74e9f9d9b53c67",
      "e65fe8b25e1a4a9c8f36498c9adaee7c",
      "f865c7219357454e89a2df18cf6e6b95",
      "1f36c0fb7e8647ac882520b69dcb1c68",
      "429dbf07b1bb4c5a902546b974247176",
      "7d5495ecc7ca42b1af3ef4a62a5093e7",
      "57c55445ead74289a2ee9fbc5810b5ee",
      "3b04237a7c144a639c1f4c0eaf6c93fc",
      "3899b74f5fee4881a2d7f048aa56d687",
      "6b3b00df3db040809b89a607c8606ee6",
      "bbc92b81edd94b33acec2cac58fed61d",
      "5b3cbd014dff4eb5b1a9bd606df4e477",
      "1b1eb9f8a1744a36bca29a78711592ac",
      "c0b9258f561b498d9e951d07a0bd2937",
      "50d9716a43d54dd0907593da71ba2feb",
      "f4ae4942bbe645b789cce446ad4c7a31",
      "b108e8874e404a9fa0a32ec04f75ffdb",
      "274cd6cde0f14f9892aa21ded13efc3d",
      "544b02cc76ab44e392a6ce518c95ef82",
      "4f9bdbd0eade414592e2a07024321888",
      "d2499505fb63463dae19b6697ab5108a",
      "36e18ccdedff4f3d9068351658cc3896",
      "f1cce734f004454d8c03a16587bf130c",
      "b1c74b5ca3b3447fa2a5fb50eefe9950",
      "7f8df5c27874405dacdb7ece40f6aad1",
      "30568fecc62e41bca9dd0f6d4a79a761",
      "ad13b9d76db5430487327f53003bec66",
      "308e5a5c99d24b3e89b42426083e0802",
      "8a63567fc074439da897812751d66abc",
      "5080e8c9e2a1462bbe285952c47a0f02",
      "ef0350ac38564ab2a2153f5e9396b969",
      "0ccc9fd375f84b5f83cdfd7936431143",
      "c21d7b29a56a474bbedef8e9256de0b1",
      "01e1a2879ad1487eaa6d41a97b6933a2",
      "20fe0ac5a67b43f09ac48a83f15f3932",
      "5f699edc27244a04b00573526625b28e",
      "c26152ffe5f34e5187a93f0ffdcf99e8",
      "0ceb63c262f8497090e427559faba1e0",
      "456fad47b68f4a7ba715a2825fa88de0",
      "583bec2cf5c04ebf9956cd06518f9b4a",
      "0e9b876f34854b4599dc5e7747fee337",
      "1b5306ac52eb4b438743c914e9ad9aaa",
      "f22370037fb542babbfb2e2be5a9a0d8",
      "f9b5852db9e9496f92c497f4b88d2781",
      "7e14b2bdfbe94498a94fe0e7c95d9718",
      "fdeb45864e414d30969eb2bdb26fee32",
      "141e96a00f2c402fb812e20c248eda6c",
      "0c1a9d4f6b214f449538627d7aa53d13",
      "baf18aa363ef49e49b3f925131099075",
      "5f0111bdc1f44f3c98598927bfd00d79",
      "e431320f4c1a436c87db2a736e33fc9b",
      "8676284e6a1f4ede85656d0d1fde0980",
      "13b1e9daf5284dcbbd39bc3257f1ac2a",
      "fdee286e123f4b19ba18ede28e9f03db",
      "ea2e7bdd488d4fe192a83ebab8a28e9a",
      "66a3dc1ac76246a196462167391275a7",
      "02cf371bf46a4849a7107ac28b9eba30",
      "e2e40a514ae548b3ad64642ab6977c81",
      "9bd3de379e5147318d9e6562f1344c25",
      "fe9ce38ee5974611a53730486a8e0509",
      "143aa22d340f4711bfd01510ff0210ff",
      "9fdda92d2500472aa70ef9c012bc9a53",
      "5d009e4532504687b0f61581e91da978",
      "8bc9c43c61174d87913e99da39bee20b",
      "21323ee9b3d040d4aa4c8c1a52a1b86f",
      "cce2b06f522e4c08936b7c7b47f41e85",
      "2687b491346044e39e7c4c35e3064889",
      "dbc5f0b0ceb04eb49fb04f10f97732d5",
      "7f2ac22f70fc4fc59ff2722cc66f40f4",
      "626ba580536640cebd1a55bd6aa3511e",
      "afe1966c27eb4abb8c807f92d0e20fe9",
      "6f288e1990a44a97ba56b161a4907ef0",
      "9624d97c600f4ba78493b4a00815913e",
      "b8952ab36d8f4f8abca047072c4a417d",
      "7642eb280c1e4cdf9fb195a6659898ea",
      "2d9cc4d17eae440691671f9655318e43",
      "b064717d6d264522979763eeeea6e0ea",
      "d09346d5cee5474c93a012e827f6e936",
      "f1420d1b4f21423ab34d75dcd6d19143",
      "2a4b995475954d70a1528a67c6ef50d8",
      "6eb4a5c898944f5b90f8ac65a7e8428f",
      "a99e2eff125a42c88732f869159ccf78",
      "9e5adb79fe2648d3ac283fd9d8ab03cd",
      "662c6aa2926d4eb99e4ccec80811a1bd",
      "5d61f906ecf24543aecf3aac83519c11",
      "3e18675fdbf842a09d2e238f17f447cf",
      "927285198b1647d9999ef5a7a68151a7",
      "8986465844a4465cb31d2858717f94e8",
      "f7627618fcfd48859efde1fe17affa10",
      "858b773930c54b27b9f5ca3e32202008",
      "2f05c5f6df384862928bfb69bda3acaf",
      "5d3ff3daef454538810bb29ba3e24fab",
      "2ce9cec2001c4ea1a21ac8efa91196fd",
      "2a08a688acf5454fbc618da01cccb035",
      "c3394e3a65a44221ae565daddb25d697",
      "f6cfe51ef4ca4b139500733326db5e77",
      "b2590fbf67a84b7fb97e5d88d3bf81ce",
      "782c4c6612174a2cbd4e0befe71effcd",
      "e74b2484e79b4223bf821f462664d9c3",
      "c0931c81893e4859b81cb714c2445aaa",
      "2fe48fb10e894b2fb339e33c24eabfe1",
      "10f6c07ccfa5434581362f01f7e8930a",
      "8cd7e767392e4ad987397327926658f6",
      "d0c4ea6eb3614e039486adf7e2bd573e",
      "5813f05e8d7e4601949ce87b56d2582b",
      "ee94965b1c5546478b1c6ed217ed119c",
      "8776c45ecda94af088d8fbea394f405c",
      "223701624d8a45fb8848b4fb044c424e",
      "172b135bee63499492acf386703ca543",
      "f1144cafd647489d9d3058d41ce78420",
      "4fa7e10fd1a84a7eb78fac864c75738e",
      "bc86038639784ed4a94acc4718812561",
      "1bbaa23e27d944658a7b3e1ddabe363e",
      "4615fa2e9fda4a89b7d11af0799cdb3b",
      "65a91875f0af4aa6abf205096c7ef5b8",
      "d8daa934fd384bc7aaa1c34b63722e29",
      "1c2fce4017dd4cd1b1e0be0d49ced25c",
      "13672bde908647cb9053ac345159bc99",
      "5d13f3ad9f264a87bdb89ab595abb867",
      "0940a629326343a9a84b9772386e0589",
      "4b07f5bd94f2465aa5d168134e4eaf1f",
      "a27590550989419eb3d7f8a558770e45",
      "1be9667b672b427697ff568f8dba3438",
      "71589af71b47459490146a703746ca36",
      "11944d3a3e7c4a1aa8068454fc7ab35f",
      "69197e7f883c4c938a648a545562bcf0",
      "289a2f8ae1394a2d887e15078e7190b3",
      "162936c87e3047ee9c197e2311f76078",
      "d789fe7735af4647b3a291e25b356915",
      "16def0a11d114ee7b8beeef4e69550d6",
      "2969fe70951343379af82c70e05a9280",
      "e437d4e0d8c14bbc84cea34de87f3e14",
      "f04bf59fd8234316bf51f51b6f479844",
      "7360cd599068424cad4deab0bc3a9e13",
      "0714b48e57d1438e85bd9498ec7652be",
      "ebe8030f1a7a49aa9e9c30a2463209dc",
      "6945279cca6847ad813c18eb099bff1a",
      "81b0286a1c3e41baa6a0e8d699256e13",
      "24013fa9fade462d85887b191ccea5f6",
      "aa7bc37530964289a6bcee268941eefa",
      "c923d5abb2f540b59ea4ceb798412796",
      "deb18bd4c3394468b777310f87cfd4aa",
      "1934a83e028b43bcaa994b2bc0e32d27",
      "79e89b98d6bb435caf7e04ac47fd6f37",
      "b9e5117117c34ec284491793b28bd6eb",
      "f04aa0a7e0be4ccca6fcafb38b306ad8",
      "f171f586c9f04a50a15d566049266288",
      "bcf401f0c6af4d348eb15323fea2922d",
      "f4f1475fa4b345a8ab542c0a8561534f",
      "6558b9449585493693a2550b69a37efc",
      "6880e9a6f60046e6b1acfa031b50b082",
      "b592ecee2f624e6fa4450e0ab0b5ad0d",
      "9a648f57a7cd45fa85d4e913df01bcc1",
      "ff47a20143084a2f923b680c47e1b8d7",
      "581bdf48d01d470cb9a98328035a0e83",
      "a76517ca9d8c403baf00c8e6339212b2",
      "a50a2e9336654bb7a311d3ea30eda926",
      "4509f7fd12334136963a2717a98f62d8",
      "9af5c095015a4749a16e24103a13bea6",
      "78880a4ab14b4636809e6e5c15530c03",
      "2acb51f115c84840901b576c5900f8e5",
      "916b590d97e24f7c9997a9a21327a59d",
      "b8db048aa997413f83f5d80035912b5b",
      "87d2d9cad24d4acbbdd5244f898334a4",
      "187ee9a498ed49e5a9343915464f2829",
      "d72cd6feab8b46a69c6bcbd5a6ec2627",
      "9bb7021ffe714a8fad2a608265a87f7f",
      "78977e2708a9459caa52511e67700a95",
      "576a374adebe4f2c8942e2177cbc7fc0",
      "9f3712f76fc048fda79fc135c8f4989b",
      "e3ab5d81cac24c9a9ea65438e73c26e2",
      "c1a186088b264a99a71f8af8fb1810dd",
      "94d0f042730d49c9a60c48ff64179f04",
      "9913273ab6a34d758725202bd116188a",
      "61c9eede20074cf1bb446d2b27c4e0e8",
      "b9ab9a6ebbcc407089078cebf00564b7",
      "3545d4b964094bd192dbf7aede64d192",
      "3499bac784184a708d253580ace5e8bd",
      "e41ff52b91e3439f9cb506287946fd71",
      "d50bea47ac5b4d67a335c69ca5107630",
      "c4b88c33257d45c5a991e005228a6375",
      "e57931549f93422d8cbe8f3ec4eeccbc",
      "88a06550024548fbb713cbc1635c2565",
      "e68d0f187eb24a9eb33855884f3707fc",
      "3631a71ce1df4917ab28a5e8e1dba57f",
      "4e1886f34b9e47e78fbce5ddba3e1857",
      "c9b4ddf2769c4359bd6db0ee8d209e53",
      "e3401261c9d3480ea3a08dbb665e8fb3",
      "46ba52c51ad54b1b8762251074ead5fa",
      "96ac95dfd76b42ae9fc8a9facee57f6d",
      "9516c2e82e3d47749d87513ac51725f1",
      "3bed6d88fccd4312996108d9116a35b5",
      "1f3bd60168b94d64a2ccd0250afd2ea2",
      "79296d97c0dc4ea7b1651647d0c91a0a",
      "6bfbaba456c44e9882a7f67193a29480",
      "438fb4485b484b38b37dd476778ebf88",
      "cc11b3d57b494f4e9a2e79258b9c1ead",
      "b32ed6b53c914930b1271e79ab39a1d9",
      "0e06df084b6a4cf0a910b75f0d08aa5f",
      "d3e47f9deba44416b39c565b873698d2",
      "eac25b50e7e04be5b9481c002e4584b6",
      "da96389a539244679daa2aee871304eb",
      "61ba50ee521f417cb236f6d9189af9c0",
      "cf2d5e651d474140986667dd9e54ab28",
      "a123c85e90b04d838c2e30091a1e328c",
      "bfc6643de42146c1b85c61d520d5b248",
      "0a60e86c01564b048b8bc4337faf67df",
      "6dadc36fc92c4c06831fc6d838593742",
      "87a9d82dfb334a59a2ca7e0d7378867c",
      "22c4cb22e1df497f8231689d58d54737",
      "69c9a24e69584dcda5e3b82396020bb9",
      "6283bc6de84c400d80b00076ff004002",
      "114f4c4b978c48a4826bd0561883d7c8",
      "3e18476b8b324e39b76dc7e20dba4923"
     ]
    },
    "id": "T-Dxq8N6DQYy"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "The following cell trains and validates the D4 model using only the training and validation splits from the prepared dataset folder (`DX_OUT_ROOT`) and its `manifests/manifest_all.csv`. It is written to be robust against failures by running early checks, showing progress bars, and saving each run in its own folder so results are never overwritten.\n",
    "\n",
    "The cell expects `DX_OUT_ROOT` to already be defined from the D4 preprocessing step. It creates a new experiment folder under `trainval_runs/` using a fixed tag (`frozen_LNDO`) and a timestamp, which keeps each run separate for later reference. Training settings are fixed for consistency across runs, including three random seeds (1337, 2024, 7777), up to 10 epochs, an effective batch size of 64 using gradient accumulation, a learning rate of 1e-3, early stopping with a patience of 2 epochs, and a required audio sample rate of 16 kHz.\n",
    "\n",
    "The manifest is then loaded and filtered to keep only rows marked as `train` or `val`. The dataset name is inferred from the most common value in the `dataset` column. The cell prints the size of each split and the label counts, and stops immediately if any audio file paths listed in the manifest do not exist. Each clip is also assigned to a simple task group, where `vowl` is treated as vowel speech and all other values are treated as non vowel speech.\n",
    "\n",
    "A custom dataset and batch collation step is used to load audio files, convert stereo audio to mono when needed, and confirm that the sample rate is exactly 16 kHz. An attention mask is created so that padded silence at the end of vowel clips, identified by near zero values, is ignored during training. Each batch is padded to the length of the longest clip in that batch, and the attention masks are padded in the same way so the model does not learn from added padding.\n",
    "\n",
    "The model consists of a frozen Wav2Vec2 backbone, downloaded using safetensors for compatibility, and two small trainable classifiers, one for vowel clips and one for other clips. Each classifier includes a small pre head block made of LayerNorm and Dropout followed by a linear layer. Only these small blocks and heads are trained, while the backbone remains fixed.\n",
    "\n",
    "For each random seed, the cell builds the training and validation data loaders and runs a short warm up by loading three training batches to catch input output or batching issues early. Training then proceeds epoch by epoch with validation at the end of each epoch, and validation AUROC is computed. Early stopping is applied when AUROC does not improve for two consecutive epochs. Only the best validation epoch for each seed is kept, and its outputs are saved under `exp_<tag>_<timestamp>/run_<dataset>_seed####/`.\n",
    "\n",
    "For each seed, the saved outputs include `best_heads.pt` with the trained head weights and pre head blocks, `roc_curve.png` showing the validation ROC curve, `confusion_matrix.png` showing the validation confusion matrix at a threshold of 0.5, and a `metrics.json` file containing the best epoch, best validation AUROC, dataset counts, key hyperparameters, and additional threshold based metrics at 0.5 such as accuracy, precision, recall or sensitivity, specificity, F1 score, MCC, and a Fisher exact test p value.\n",
    "\n",
    "After all three seeds finish, the cell prints the AUROC for each seed and the mean AUROC with a 95% confidence interval computed using a t distribution with n=3. It then writes a `summary_trainval.json` file inside the experiment folder with the full summary across seeds, and appends a one line record to `trainval_runs/history_index.jsonl` as a global log of experiments.\n",
    "\n",
    "Finally, the cell prints the output folder locations and unassigns the Colab runtime to cleanly shut down the L4 GPU session."
   ],
   "metadata": {
    "id": "2irq4OwZ13XX"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# =========================\n",
    "# D4 Train + Val — Frozen Backbone, Two Heads, Full Run History\n",
    "# Inputs: manifest_all.csv (train/val only) and the audio clips referenced by clip_path\n",
    "# Outputs: per-seed run folders with metrics.json, roc_curve.png, confusion_matrix.png, best_heads.pt\n",
    "#          plus one experiment summary_trainval.json and one appended history_index.jsonl record\n",
    "# =========================\n",
    "\n",
    "import os, json, math, random, time, warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import soundfile as sf\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from transformers import Wav2Vec2Model\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, roc_curve,\n",
    "    confusion_matrix, accuracy_score,\n",
    "    precision_recall_fscore_support,\n",
    "    matthews_corrcoef\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# -------------------------\n",
    "# Dataset root from preprocessing runtime\n",
    "# Purpose: reuse the dataset output root already set by preprocessing\n",
    "# Input: DX_OUT_ROOT must exist in memory\n",
    "# -------------------------\n",
    "if \"DX_OUT_ROOT\" not in globals():\n",
    "    raise RuntimeError(\"DX_OUT_ROOT is not defined in the runtime. Run the dataset preprocessing cell first.\")\n",
    "DX_OUT_ROOT = str(globals()[\"DX_OUT_ROOT\"])\n",
    "MANIFEST_ALL = f\"{DX_OUT_ROOT}/manifests/manifest_all.csv\"\n",
    "\n",
    "# -------------------------\n",
    "# Experiment folder naming (no overwrites)\n",
    "# Purpose: keep results from every run, even when rerunning the notebook\n",
    "# Output: EXP_ROOT holds all per-seed outputs for this execution\n",
    "# -------------------------\n",
    "EXPERIMENT_TAG = \"frozen_LNDO\"\n",
    "RUN_STAMP = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "TRAINVAL_ROOT = Path(DX_OUT_ROOT) / \"trainval_runs\"\n",
    "EXP_ROOT = TRAINVAL_ROOT / f\"exp_{EXPERIMENT_TAG}_{RUN_STAMP}\"\n",
    "EXP_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# -------------------------\n",
    "# Training defaults\n",
    "# Purpose: fixed settings for consistent comparison across datasets\n",
    "# -------------------------\n",
    "MAX_EPOCHS     = 10\n",
    "EFFECTIVE_BS   = 64\n",
    "PER_DEVICE_BS  = 16\n",
    "GRAD_ACCUM     = max(1, EFFECTIVE_BS // PER_DEVICE_BS)\n",
    "\n",
    "LR             = 1e-3\n",
    "PATIENCE       = 2\n",
    "SEEDS          = [1337, 2024, 7777]\n",
    "\n",
    "BACKBONE_CKPT  = \"facebook/wav2vec2-base\"\n",
    "SR_EXPECTED    = 16000\n",
    "TINY_THRESH    = 1e-4\n",
    "\n",
    "# Small pre-head block (trainable)\n",
    "DROPOUT_P      = 0.2\n",
    "\n",
    "# Drive-friendly loader defaults\n",
    "NUM_WORKERS    = 0\n",
    "PIN_MEMORY     = False\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "try:\n",
    "    torch.set_float32_matmul_precision(\"high\")\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", message=\"None of the inputs have requires_grad=True. Gradients will be None\")\n",
    "warnings.filterwarnings(\"ignore\", message=\"Passing `gradient_checkpointing` to a config initialization is deprecated\")\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"huggingface_hub\")\n",
    "\n",
    "print(\"DX_OUT_ROOT:\", DX_OUT_ROOT)\n",
    "print(\"MANIFEST_ALL:\", MANIFEST_ALL)\n",
    "print(\"DEVICE:\", DEVICE)\n",
    "if DEVICE.type == \"cuda\":\n",
    "    print(\"GPU:\", torch.cuda.get_device_name(0))\n",
    "print(\"PER_DEVICE_BS:\", PER_DEVICE_BS, \"| GRAD_ACCUM:\", GRAD_ACCUM, \"| EFFECTIVE_BS:\", PER_DEVICE_BS * GRAD_ACCUM)\n",
    "print(\"NUM_WORKERS:\", NUM_WORKERS, \"| PIN_MEMORY:\", PIN_MEMORY)\n",
    "print(\"EXPERIMENT_TAG:\", EXPERIMENT_TAG, \"| RUN_STAMP:\", RUN_STAMP)\n",
    "\n",
    "# -------------------------\n",
    "# Manifest load and split filtering\n",
    "# Input: manifest_all.csv\n",
    "# Output: train_df and val_df for this dataset only\n",
    "# -------------------------\n",
    "if not os.path.exists(MANIFEST_ALL):\n",
    "    raise FileNotFoundError(f\"Missing manifest_all.csv: {MANIFEST_ALL}\")\n",
    "\n",
    "m = pd.read_csv(MANIFEST_ALL)\n",
    "\n",
    "# Purpose: fail early if required columns are missing\n",
    "req_cols = {\"split\", \"clip_path\", \"label_num\", \"task\"}\n",
    "missing = [c for c in sorted(req_cols) if c not in m.columns]\n",
    "if missing:\n",
    "    raise ValueError(f\"Manifest missing required columns: {missing}. Found: {list(m.columns)}\")\n",
    "\n",
    "# Purpose: this cell is train/val only\n",
    "m = m[m[\"split\"].isin([\"train\", \"val\"])].copy()\n",
    "if len(m) == 0:\n",
    "    raise RuntimeError(\"After filtering to split in {train,val}, manifest has 0 rows.\")\n",
    "\n",
    "# Purpose: pick the dominant dataset label if the file contains multiple datasets\n",
    "if \"dataset\" in m.columns and m[\"dataset\"].notna().any():\n",
    "    dataset_id = str(m[\"dataset\"].value_counts(dropna=True).idxmax())\n",
    "    m = m[m[\"dataset\"].astype(str) == dataset_id].copy()\n",
    "else:\n",
    "    dataset_id = \"DX\"\n",
    "\n",
    "# Purpose: keep a consistent set of columns even if some are missing\n",
    "keep_cols = [\"clip_path\", \"label_num\", \"task\", \"speaker_id\", \"duration_sec\", \"split\"]\n",
    "for c in keep_cols:\n",
    "    if c not in m.columns:\n",
    "        m[c] = np.nan\n",
    "m = m[keep_cols].copy()\n",
    "\n",
    "train_df = m[m[\"split\"] == \"train\"].copy().reset_index(drop=True)\n",
    "val_df   = m[m[\"split\"] == \"val\"].copy().reset_index(drop=True)\n",
    "\n",
    "print(f\"\\nDataset inferred: {dataset_id}\")\n",
    "print(f\"Train rows: {len(train_df)} | Val rows: {len(val_df)}\")\n",
    "print(\"Train label counts:\", train_df[\"label_num\"].value_counts(dropna=False).to_dict())\n",
    "print(\"Val label counts:\",   val_df[\"label_num\"].value_counts(dropna=False).to_dict())\n",
    "\n",
    "# Purpose: avoid training on empty tables\n",
    "if len(train_df) == 0 or len(val_df) == 0:\n",
    "    raise RuntimeError(\"Train or Val split has 0 rows.\")\n",
    "\n",
    "# -------------------------\n",
    "# Clip path existence check\n",
    "# Input: clip_path values in train_df and val_df\n",
    "# Output: raises with a few missing examples instead of failing mid-training\n",
    "# -------------------------\n",
    "def _fail_fast_missing_paths(df: pd.DataFrame, name: str):\n",
    "    missing_paths = []\n",
    "    for p in tqdm(df[\"clip_path\"].astype(str).tolist(), desc=f\"Check {name} clip_path exists\", dynamic_ncols=True):\n",
    "        if not os.path.exists(p):\n",
    "            missing_paths.append(p)\n",
    "            if len(missing_paths) >= 10:\n",
    "                break\n",
    "    if missing_paths:\n",
    "        raise FileNotFoundError(f\"{name}: missing clip_path(s). Examples: {missing_paths}\")\n",
    "\n",
    "_fail_fast_missing_paths(train_df, \"TRAIN\")\n",
    "_fail_fast_missing_paths(val_df, \"VAL\")\n",
    "\n",
    "# -------------------------\n",
    "# Task grouping\n",
    "# Purpose: route each clip to the vowel head or the other head\n",
    "# Output: task_group column used by dataset and model\n",
    "# -------------------------\n",
    "def _task_group(task_val) -> str:\n",
    "    return \"vowel\" if str(task_val) == \"vowl\" else \"other\"\n",
    "\n",
    "train_df[\"task_group\"] = train_df[\"task\"].apply(_task_group)\n",
    "val_df[\"task_group\"]   = val_df[\"task\"].apply(_task_group)\n",
    "\n",
    "# -------------------------\n",
    "# Dataset and batch collation\n",
    "# Inputs: manifest rows (clip_path, label_num, task_group)\n",
    "# Outputs: padded batches with attention_mask kept aligned with audio samples\n",
    "# -------------------------\n",
    "class AudioManifestDataset(Dataset):\n",
    "    def __init__(self, df: pd.DataFrame):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        row = self.df.iloc[idx]\n",
    "        clip_path = str(row[\"clip_path\"])\n",
    "        label = int(row[\"label_num\"])\n",
    "        task_group = str(row[\"task_group\"])\n",
    "\n",
    "        if not os.path.exists(clip_path):\n",
    "            raise FileNotFoundError(f\"Missing clip_path: {clip_path}\")\n",
    "\n",
    "        # Purpose: load audio, convert stereo to mono, force float32\n",
    "        y, sr = sf.read(clip_path, always_2d=False)\n",
    "        if isinstance(y, np.ndarray) and y.ndim > 1:\n",
    "            y = np.mean(y, axis=1)\n",
    "        y = np.asarray(y, dtype=np.float32)\n",
    "\n",
    "        # Purpose: enforce a single training sample rate\n",
    "        if int(sr) != SR_EXPECTED:\n",
    "            raise ValueError(f\"Sample rate mismatch (expected {SR_EXPECTED}): {clip_path} has sr={sr}\")\n",
    "\n",
    "        # Purpose: build an attention mask in sample space\n",
    "        attn = np.ones((len(y),), dtype=np.int64)\n",
    "\n",
    "        # Purpose: for vowel clips, hide trailing zero padding so pooling ignores it\n",
    "        if task_group == \"vowel\":\n",
    "            k = -1\n",
    "            for j in range(len(y) - 1, -1, -1):\n",
    "                if abs(float(y[j])) > TINY_THRESH:\n",
    "                    k = j\n",
    "                    break\n",
    "            if k >= 0:\n",
    "                attn[:k+1] = 1\n",
    "                attn[k+1:] = 0\n",
    "            else:\n",
    "                attn[:] = 1\n",
    "        else:\n",
    "            attn[:] = 1\n",
    "\n",
    "        return {\n",
    "            \"input_values\": torch.from_numpy(y),                 # float32 [T]\n",
    "            \"attention_mask\": torch.from_numpy(attn),            # int64   [T]\n",
    "            \"labels\": torch.tensor(label, dtype=torch.long),     # int64   []\n",
    "            \"task_group\": task_group,                            # str\n",
    "        }\n",
    "\n",
    "def collate_fn(batch):\n",
    "    # Purpose: pad all clips in the batch to the same length using zeros\n",
    "    max_len = int(max(b[\"input_values\"].numel() for b in batch))\n",
    "    input_vals, attn_masks, labels, task_groups = [], [], [], []\n",
    "    for b in batch:\n",
    "        x = b[\"input_values\"]\n",
    "        a = b[\"attention_mask\"]\n",
    "        pad = max_len - x.numel()\n",
    "        if pad > 0:\n",
    "            x = torch.cat([x, torch.zeros(pad, dtype=x.dtype)], dim=0)\n",
    "            a = torch.cat([a, torch.zeros(pad, dtype=a.dtype)], dim=0)\n",
    "        input_vals.append(x)\n",
    "        attn_masks.append(a)\n",
    "        labels.append(b[\"labels\"])\n",
    "        task_groups.append(b[\"task_group\"])\n",
    "    return {\n",
    "        \"input_values\": torch.stack(input_vals, dim=0),      # [B,T]\n",
    "        \"attention_mask\": torch.stack(attn_masks, dim=0),    # [B,T]\n",
    "        \"labels\": torch.stack(labels, dim=0),                # [B]\n",
    "        \"task_group\": task_groups,                           # list[str]\n",
    "    }\n",
    "\n",
    "# -------------------------\n",
    "# Model definition\n",
    "# Inputs: backbone checkpoint and dropout probability\n",
    "# Output: loss and logits (PD vs Healthy) chosen by task_group\n",
    "# -------------------------\n",
    "class Wav2Vec2TwoHeadClassifier(nn.Module):\n",
    "    def __init__(self, ckpt: str, dropout_p: float):\n",
    "        super().__init__()\n",
    "\n",
    "        # Purpose: load weights via safetensors for safer, consistent loading\n",
    "        self.backbone = Wav2Vec2Model.from_pretrained(\n",
    "            ckpt,\n",
    "            use_safetensors=True,      # <-- critical fix\n",
    "            local_files_only=False\n",
    "        )\n",
    "        for p in self.backbone.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "        H = int(self.backbone.config.hidden_size)\n",
    "\n",
    "        # Purpose: small per-task feature cleanup before each head\n",
    "        self.pre_vowel = nn.Sequential(nn.LayerNorm(H), nn.Dropout(float(dropout_p)))\n",
    "        self.pre_other = nn.Sequential(nn.LayerNorm(H), nn.Dropout(float(dropout_p)))\n",
    "\n",
    "        self.head_vowel = nn.Linear(H, 2)\n",
    "        self.head_other = nn.Linear(H, 2)\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    def masked_mean_pool(self, last_hidden, attn_mask_samples):\n",
    "        # Purpose: convert sample-level mask to feature-level mask for pooling\n",
    "        feat_mask = self.backbone._get_feature_vector_attention_mask(last_hidden.shape[1], attn_mask_samples)\n",
    "        feat_mask = feat_mask.to(last_hidden.device).unsqueeze(-1).type_as(last_hidden)\n",
    "        masked = last_hidden * feat_mask\n",
    "        denom = feat_mask.sum(dim=1).clamp(min=1.0)\n",
    "        return masked.sum(dim=1) / denom\n",
    "\n",
    "    def forward(self, input_values, attention_mask, labels, task_group):\n",
    "        # Purpose: backbone stays frozen and runs without gradients\n",
    "        with torch.no_grad():\n",
    "            out = self.backbone(input_values=input_values, attention_mask=attention_mask)\n",
    "            last_hidden = out.last_hidden_state  # [B,T',H]\n",
    "\n",
    "        pooled = self.masked_mean_pool(last_hidden, attention_mask)  # [B,H]\n",
    "        pooled = pooled.float()  # keep heads stable and avoid dtype surprises\n",
    "\n",
    "        z_v = self.pre_vowel(pooled)\n",
    "        z_o = self.pre_other(pooled)\n",
    "\n",
    "        logits_v = self.head_vowel(z_v)  # float32\n",
    "        logits_o = self.head_other(z_o)  # float32\n",
    "\n",
    "        # Purpose: choose the matching head per sample\n",
    "        is_vowel = torch.tensor([tg == \"vowel\" for tg in task_group], device=pooled.device, dtype=torch.bool)\n",
    "        logits = logits_o.clone()\n",
    "        if is_vowel.any():\n",
    "            logits[is_vowel] = logits_v[is_vowel]\n",
    "\n",
    "        loss = self.loss_fn(logits, labels)\n",
    "        return loss, logits\n",
    "\n",
    "# -------------------------\n",
    "# Metric and plot helpers (VAL)\n",
    "# Inputs: y_true and predicted PD probability\n",
    "# Outputs: AUROC, threshold metrics, and PNG plots\n",
    "# -------------------------\n",
    "def compute_auc(y_true, y_prob):\n",
    "    y_true = np.asarray(y_true, dtype=np.int64)\n",
    "    y_prob = np.asarray(y_prob, dtype=np.float64)\n",
    "    if len(np.unique(y_true)) < 2:\n",
    "        return float(\"nan\")\n",
    "    return float(roc_auc_score(y_true, y_prob))\n",
    "\n",
    "def save_roc_curve_png(y_true, y_prob, out_png):\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_prob)\n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr)\n",
    "    plt.plot([0, 1], [0, 1], linestyle=\"--\")\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(\"ROC Curve (Val)\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_png, dpi=150)\n",
    "    plt.close()\n",
    "\n",
    "def save_confusion_png(y_true, y_prob, out_png):\n",
    "    # Purpose: show thresholded performance at a fixed 0.5 cutoff\n",
    "    y_pred = (np.asarray(y_prob) >= 0.5).astype(int)\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=[0, 1])\n",
    "    plt.figure()\n",
    "    plt.imshow(cm)\n",
    "    plt.xticks([0, 1], [\"Healthy(0)\", \"PD(1)\"])\n",
    "    plt.yticks([0, 1], [\"Healthy(0)\", \"PD(1)\"])\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"True\")\n",
    "    plt.title(\"Confusion Matrix (Val, thr=0.5)\")\n",
    "    for (i, j), v in np.ndenumerate(cm):\n",
    "        plt.text(j, i, str(v), ha=\"center\", va=\"center\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_png, dpi=150)\n",
    "    plt.close()\n",
    "\n",
    "def compute_threshold_metrics(y_true, y_prob, thr=0.5):\n",
    "    # Purpose: extra “single-number” metrics at a fixed cutoff\n",
    "    y_true = np.asarray(y_true, dtype=np.int64)\n",
    "    y_prob = np.asarray(y_prob, dtype=np.float64)\n",
    "    y_pred = (y_prob >= float(thr)).astype(np.int64)\n",
    "\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=[0, 1])\n",
    "    tn, fp, fn, tp = (cm.ravel().tolist() if cm.size == 4 else [0, 0, 0, 0])\n",
    "\n",
    "    acc = float(accuracy_score(y_true, y_pred))\n",
    "    prec, rec, f1, _ = precision_recall_fscore_support(y_true, y_pred, average=\"binary\", zero_division=0)\n",
    "    mcc = float(matthews_corrcoef(y_true, y_pred)) if len(np.unique(y_true)) > 1 else float(\"nan\")\n",
    "\n",
    "    sensitivity = float(rec)\n",
    "    specificity = float(tn / (tn + fp)) if (tn + fp) > 0 else float(\"nan\")\n",
    "\n",
    "    # Purpose: quick association test between prediction and truth\n",
    "    p_value = float(\"nan\")\n",
    "    try:\n",
    "        from scipy.stats import fisher_exact  # type: ignore\n",
    "        _, p_value = fisher_exact([[tn, fp], [fn, tp]], alternative=\"two-sided\")\n",
    "        p_value = float(p_value)\n",
    "    except Exception:\n",
    "        p_value = float(\"nan\")\n",
    "\n",
    "    return {\n",
    "        \"thr\": float(thr),\n",
    "        \"tn\": int(tn), \"fp\": int(fp), \"fn\": int(fn), \"tp\": int(tp),\n",
    "        \"accuracy\": float(acc),\n",
    "        \"precision\": float(prec),\n",
    "        \"recall\": float(rec),\n",
    "        \"f1\": float(f1),\n",
    "        \"sensitivity\": float(sensitivity),\n",
    "        \"specificity\": float(specificity),\n",
    "        \"mcc\": float(mcc),\n",
    "        \"p_value_fisher\": float(p_value),\n",
    "    }\n",
    "\n",
    "# -------------------------\n",
    "# Random seed control\n",
    "# Purpose: stabilize shuffling and weight init per run\n",
    "# -------------------------\n",
    "def set_all_seeds(seed: int):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# -------------------------\n",
    "# One seed run: train, validate, save best epoch\n",
    "# Inputs: seed, train_df, val_df\n",
    "# Outputs: artifacts in run_dir and a metrics dict for exp summary\n",
    "# -------------------------\n",
    "def run_trainval_once(seed: int):\n",
    "    set_all_seeds(seed)\n",
    "\n",
    "    run_dir = EXP_ROOT / f\"run_{dataset_id}_seed{seed}\"\n",
    "    run_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Purpose: build loaders from manifest tables\n",
    "    train_ds = AudioManifestDataset(train_df)\n",
    "    val_ds   = AudioManifestDataset(val_df)\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_ds,\n",
    "        batch_size=PER_DEVICE_BS,\n",
    "        shuffle=True,\n",
    "        num_workers=NUM_WORKERS,\n",
    "        pin_memory=PIN_MEMORY,\n",
    "        collate_fn=collate_fn\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_ds,\n",
    "        batch_size=PER_DEVICE_BS,\n",
    "        shuffle=False,\n",
    "        num_workers=NUM_WORKERS,\n",
    "        pin_memory=PIN_MEMORY,\n",
    "        collate_fn=collate_fn\n",
    "    )\n",
    "\n",
    "    # Purpose: confirm file I/O and collation are working before training starts\n",
    "    print(f\"\\n[seed={seed}] Warm-up: loading 3 train batches...\")\n",
    "    t0 = time.time()\n",
    "    it = iter(train_loader)\n",
    "    for i in range(3):\n",
    "        _ = next(it)\n",
    "        print(f\"  loaded warmup batch {i+1}/3\")\n",
    "    print(f\"[seed={seed}] Warm-up done in {time.time()-t0:.2f}s\")\n",
    "\n",
    "    model = Wav2Vec2TwoHeadClassifier(BACKBONE_CKPT, dropout_p=DROPOUT_P).to(DEVICE)\n",
    "\n",
    "    # Purpose: train only the small pre-head blocks and heads\n",
    "    trainable_params = (\n",
    "        list(model.pre_vowel.parameters()) + list(model.pre_other.parameters()) +\n",
    "        list(model.head_vowel.parameters()) + list(model.head_other.parameters())\n",
    "    )\n",
    "    opt = torch.optim.Adam(trainable_params, lr=LR)\n",
    "\n",
    "    best_auc = -1.0\n",
    "    best_epoch = -1\n",
    "    no_improve = 0\n",
    "    best_state = None\n",
    "    best_val_probs = None\n",
    "    best_val_true = None\n",
    "    best_val_metrics_thr = None\n",
    "\n",
    "    for epoch in range(1, MAX_EPOCHS + 1):\n",
    "        # Train\n",
    "        model.train()\n",
    "        train_losses = []\n",
    "        opt.zero_grad(set_to_none=True)\n",
    "\n",
    "        pbar = tqdm(train_loader, desc=f\"[seed={seed}] Train epoch {epoch}\", dynamic_ncols=True)\n",
    "        step = 0\n",
    "        for batch in pbar:\n",
    "            step += 1\n",
    "            input_values = batch[\"input_values\"].to(DEVICE, non_blocking=False)\n",
    "            attention_mask = batch[\"attention_mask\"].to(DEVICE, non_blocking=False)\n",
    "            labels = batch[\"labels\"].to(DEVICE, non_blocking=False)\n",
    "            task_group = batch[\"task_group\"]\n",
    "\n",
    "            loss, _ = model(input_values, attention_mask, labels, task_group)\n",
    "            loss = loss / GRAD_ACCUM\n",
    "            loss.backward()\n",
    "\n",
    "            train_losses.append(float(loss.detach().cpu().item()) * GRAD_ACCUM)\n",
    "\n",
    "            # Purpose: simulate a larger batch size via accumulation\n",
    "            if (step % GRAD_ACCUM) == 0:\n",
    "                opt.step()\n",
    "                opt.zero_grad(set_to_none=True)\n",
    "\n",
    "        # Purpose: handle leftover steps at the end of the epoch\n",
    "        if (step % GRAD_ACCUM) != 0:\n",
    "            opt.step()\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "\n",
    "        avg_train_loss = float(np.mean(train_losses)) if train_losses else float(\"nan\")\n",
    "\n",
    "        # Validate\n",
    "        model.eval()\n",
    "        all_probs, all_true = [], []\n",
    "        vpbar = tqdm(val_loader, desc=f\"[seed={seed}] Val epoch {epoch}\", dynamic_ncols=True)\n",
    "        with torch.inference_mode():\n",
    "            for batch in vpbar:\n",
    "                input_values = batch[\"input_values\"].to(DEVICE, non_blocking=False)\n",
    "                attention_mask = batch[\"attention_mask\"].to(DEVICE, non_blocking=False)\n",
    "                labels = batch[\"labels\"].to(DEVICE, non_blocking=False)\n",
    "                task_group = batch[\"task_group\"]\n",
    "\n",
    "                _, logits = model(input_values, attention_mask, labels, task_group)\n",
    "                probs = torch.softmax(logits, dim=-1)[:, 1].detach().cpu().numpy().astype(np.float64)\n",
    "                all_probs.extend(probs.tolist())\n",
    "                all_true.extend(labels.detach().cpu().numpy().astype(np.int64).tolist())\n",
    "\n",
    "        val_auc = compute_auc(all_true, all_probs)\n",
    "        print(f\"seed={seed} | epoch {epoch:02d}/{MAX_EPOCHS} | train_loss={avg_train_loss:.5f} | val_AUROC={val_auc:.5f}\")\n",
    "\n",
    "        # Purpose: keep only the best epoch by VAL AUROC\n",
    "        improved = (not math.isnan(val_auc)) and (val_auc > best_auc + 1e-12)\n",
    "        if improved:\n",
    "            best_auc = float(val_auc)\n",
    "            best_epoch = int(epoch)\n",
    "            no_improve = 0\n",
    "            best_state = {\n",
    "                \"pre_vowel\": {k: v.detach().cpu().clone() for k, v in model.pre_vowel.state_dict().items()},\n",
    "                \"pre_other\": {k: v.detach().cpu().clone() for k, v in model.pre_other.state_dict().items()},\n",
    "                \"head_vowel\": {k: v.detach().cpu().clone() for k, v in model.head_vowel.state_dict().items()},\n",
    "                \"head_other\": {k: v.detach().cpu().clone() for k, v in model.head_other.state_dict().items()},\n",
    "            }\n",
    "            best_val_probs = list(all_probs)\n",
    "            best_val_true  = list(all_true)\n",
    "            best_val_metrics_thr = compute_threshold_metrics(best_val_true, best_val_probs, thr=0.5)\n",
    "        else:\n",
    "            no_improve += 1\n",
    "\n",
    "        # Purpose: stop early if AUROC does not improve for PATIENCE epochs\n",
    "        if no_improve >= PATIENCE:\n",
    "            break\n",
    "\n",
    "    if best_state is None or best_val_probs is None or best_val_true is None:\n",
    "        raise RuntimeError(\"No best epoch captured. Validation AUROC may be NaN due to single-class validation split.\")\n",
    "\n",
    "    # Save best epoch artifacts\n",
    "    best_heads_path = run_dir / \"best_heads.pt\"\n",
    "    torch.save(best_state, str(best_heads_path))\n",
    "\n",
    "    roc_png = run_dir / \"roc_curve.png\"\n",
    "    cm_png  = run_dir / \"confusion_matrix.png\"\n",
    "    save_roc_curve_png(np.asarray(best_val_true, dtype=np.int64), np.asarray(best_val_probs, dtype=np.float64), str(roc_png))\n",
    "    save_confusion_png(np.asarray(best_val_true, dtype=np.int64), np.asarray(best_val_probs, dtype=np.float64), str(cm_png))\n",
    "\n",
    "    # Output: metrics.json for this seed\n",
    "    metrics = {\n",
    "        \"dataset\": dataset_id,\n",
    "        \"seed\": int(seed),\n",
    "        \"best_val_auroc\": float(best_auc),\n",
    "        \"best_epoch\": int(best_epoch),\n",
    "        \"n_train\": int(len(train_df)),\n",
    "        \"n_val\": int(len(val_df)),\n",
    "        \"label_counts_train\": train_df[\"label_num\"].value_counts(dropna=False).to_dict(),\n",
    "        \"label_counts_val\": val_df[\"label_num\"].value_counts(dropna=False).to_dict(),\n",
    "        \"experiment_tag\": EXPERIMENT_TAG,\n",
    "        \"run_stamp\": RUN_STAMP,\n",
    "        \"dropout_p\": float(DROPOUT_P),\n",
    "        \"lr\": float(LR),\n",
    "        \"effective_batch_size\": int(PER_DEVICE_BS * GRAD_ACCUM),\n",
    "        \"per_device_batch_size\": int(PER_DEVICE_BS),\n",
    "        \"grad_accum_steps\": int(GRAD_ACCUM),\n",
    "        \"backbone_ckpt\": BACKBONE_CKPT,\n",
    "        \"thr_metrics_val_thr0p5\": best_val_metrics_thr,\n",
    "    }\n",
    "    metrics_path = run_dir / \"metrics.json\"\n",
    "    with open(metrics_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(metrics, f, indent=2)\n",
    "\n",
    "    print(f\"[seed={seed}] WROTE:\")\n",
    "    print(\" \", str(metrics_path))\n",
    "    print(\" \", str(roc_png))\n",
    "    print(\" \", str(cm_png))\n",
    "    print(\" \", str(best_heads_path))\n",
    "\n",
    "    return float(best_auc), str(run_dir), metrics\n",
    "\n",
    "# -------------------------\n",
    "# Run all seeds + write experiment summary\n",
    "# Inputs: SEEDS list\n",
    "# Outputs: summary_trainval.json and appended history_index.jsonl\n",
    "# -------------------------\n",
    "aucs = []\n",
    "run_dirs = []\n",
    "per_seed_metrics = []\n",
    "\n",
    "for seed in SEEDS:\n",
    "    a, rd, met = run_trainval_once(seed)\n",
    "    aucs.append(float(a))\n",
    "    run_dirs.append(str(rd))\n",
    "    per_seed_metrics.append(met)\n",
    "\n",
    "# Purpose: simple mean and 95% CI across 3 seeds\n",
    "t_crit = 4.302652729911275  # df=2, 95% CI\n",
    "n = len(aucs)\n",
    "mean_auc = float(np.mean(aucs))\n",
    "std_auc = float(np.std(aucs, ddof=1)) if n > 1 else 0.0\n",
    "half_width = float(t_crit * (std_auc / math.sqrt(n))) if n > 1 else 0.0\n",
    "ci95 = [float(mean_auc - half_width), float(mean_auc + half_width)]\n",
    "\n",
    "print(\"\\nAUROC by seed:\")\n",
    "for s, a in zip(SEEDS, aucs):\n",
    "    print(f\"  seed {s}: {a:.6f}\")\n",
    "print(f\"\\nMean AUROC: {mean_auc:.6f}\")\n",
    "print(f\"95% CI (t, n=3): [{ci95[0]:.6f}, {ci95[1]:.6f}]\")\n",
    "\n",
    "exp_summary = {\n",
    "    \"dataset\": dataset_id,\n",
    "    \"dx_out_root\": DX_OUT_ROOT,\n",
    "    \"experiment_tag\": EXPERIMENT_TAG,\n",
    "    \"run_stamp\": RUN_STAMP,\n",
    "    \"exp_root\": str(EXP_ROOT),\n",
    "    \"run_dirs\": run_dirs,\n",
    "    \"seeds\": SEEDS,\n",
    "    \"aurocs\": [float(x) for x in aucs],\n",
    "    \"mean_auroc\": float(mean_auc),\n",
    "    \"t_crit_df2_95\": float(t_crit),\n",
    "    \"half_width_95_ci\": float(half_width),\n",
    "    \"ci95\": ci95,\n",
    "    \"n_train\": int(len(train_df)),\n",
    "    \"n_val\": int(len(val_df)),\n",
    "    \"label_counts_train\": train_df[\"label_num\"].value_counts(dropna=False).to_dict(),\n",
    "    \"label_counts_val\": val_df[\"label_num\"].value_counts(dropna=False).to_dict(),\n",
    "    \"effective_batch_size\": int(PER_DEVICE_BS * GRAD_ACCUM),\n",
    "    \"per_device_batch_size\": int(PER_DEVICE_BS),\n",
    "    \"grad_accum_steps\": int(GRAD_ACCUM),\n",
    "    \"backbone_ckpt\": BACKBONE_CKPT,\n",
    "    \"dropout_p\": float(DROPOUT_P),\n",
    "    \"lr\": float(LR),\n",
    "    \"per_seed_metrics\": per_seed_metrics,\n",
    "}\n",
    "\n",
    "summary_path = EXP_ROOT / \"summary_trainval.json\"\n",
    "with open(summary_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(exp_summary, f, indent=2)\n",
    "\n",
    "# Output: append one line so older experiments remain intact\n",
    "history_path = TRAINVAL_ROOT / \"history_index.jsonl\"\n",
    "with open(history_path, \"a\", encoding=\"utf-8\") as f:\n",
    "    f.write(json.dumps(exp_summary) + \"\\n\")\n",
    "\n",
    "print(\"\\nWROTE per-experiment summary:\", str(summary_path))\n",
    "print(\"APPENDED global history index:\", str(history_path))\n",
    "print(\"\\nOpen this folder to access artifacts:\", str(EXP_ROOT))\n",
    "\n",
    "# -------------------------\n",
    "# Stop runtime (release GPU)\n",
    "# -------------------------\n",
    "print(\"\\nAll done. Unassigning the runtime to stop the L4 instance...\")\n",
    "try:\n",
    "    from google.colab import runtime  # type: ignore\n",
    "    print(\"Calling runtime.unassign() now.\")\n",
    "    runtime.unassign()\n",
    "except Exception as e:\n",
    "    print(\"Could not unassign runtime automatically. Error:\", repr(e))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "f781a92b9f3c4258bae2aba783a446c2",
      "173cffc8f52e47839b4903cd7a4db1ac",
      "bb648ae2d52244cd80e823d7211f2118",
      "f0d90d8847cb44619504b863e8899437",
      "ee3fdf90783848119322530f516b8184",
      "85c7bcaa576b4cc3be1ac377b7ef872f",
      "b29b0bd57b6741ce8fec84cd2f48aca5",
      "5943ae31aef04b3dac095a8f6b781abe",
      "f7d201e51a5d4e8da45cd6ea8b570f66",
      "3a935ffccdb144779de50513e2bac4c4",
      "60b1992dc3094d60a88e628e1e93e0c1",
      "91f89e633607436281b90049167b49b4",
      "7f88de0baa26479198d887f9c637a252",
      "ce4a447c4e584ed994c2aac49e43fdc6",
      "f8d181ec35604768b56474b7312615f4",
      "6756bbbb08c441829fc5dcc4e47433b1",
      "dfe08de10ecf4aef906a794d96cd1841",
      "a0cb9e71b1f24749a7d2d71a5b0fcf2d",
      "7879dcace7314d8a87cf05f3f7491101",
      "9d00d7e1726e404b9a74ff109ecbcd83",
      "2d77d326708f4b6abe7d09610437ab87",
      "ee9b504e1640441fa329425f5cd91453",
      "6a83534ff4544e5ca320457c549c47c5",
      "8cfa538463a0440b9e2421182ce24cb0",
      "d61c07e2b39d40acb3e3ecfff0ff1631",
      "89fc8bf1ae6a4efdb30625a40683327e",
      "35e103d74a2b43d0a61413dceffbb5dd",
      "87ad6df65e444af2933c6b670929be60",
      "4fe9567dd8c24506a0c2c946b062f82f",
      "365f30f6a03c4c0ba5961c5cec2f4d4b",
      "ca040ebe3229421fb9af925a59e41e03",
      "7d8ca9438cd04115a02c43fd92f8306a",
      "a82393b7e762492181aa8de433f17649",
      "9d6f72c2adeb4e72b6b0baecd58213e0",
      "f0badbfb449241979a1df3f03b50c0e7",
      "55d3f56618d14cc184e18d4c45ba1851",
      "796cb57c063047a0884bddcdb6841ade",
      "7dd9125ad705476e906e4d215a6d12cc",
      "f3f371c22e724fcb8824be8ee4d458e9",
      "e6f99a454b0d4a8aa10ec63da1857b15",
      "995d252ab8274c69a55aa7779dcf87c1",
      "72d92110c5204ae6a462b32a9bfaa454",
      "82545cb3b5ea4be7beb0e60c6451d4fb",
      "5ab5d50aa5f5433e82ab06b71070a7c5",
      "79235a974abd4b2e90b2b5758c1ea1f2",
      "a9448d90bb7e417e87da9f0170e44b51",
      "0b93cc8efae4423196cdfcfd889381a4",
      "2a62ea5feb3d4b46841ca1e2efd62129",
      "2a8957cefb1841c0a4f34f59561f139b",
      "88bb23ec78464c73969f0ced95421c0e",
      "5011da94c76c4d4989796ef9682ac58b",
      "581a56bbfaa44405baa0084e0b9efe51",
      "a6642c4cebeb47ec85c61a2c73ad8222",
      "35b2098eadea4dde88906ffcae56e54b",
      "4cfe71cc60814f2d8ea674976c9c01ea",
      "c8c3248f4ea3402d844e743bd6b8a148",
      "7ef18770f2dd4305a4fab420417eb8f4",
      "7bc23ae9d3414d389dc97c4d7c30694b",
      "6eeafc587b834e3491148bafb172b989",
      "d72829b83cb6431c9b977f8f842ed07e",
      "022b2703d29e4b9ab24d459d51b2eb8f",
      "744dd3d771a244c0bf859681319a5b56",
      "9ca9b0f29cea4f9b85b8071f51d4f1e7",
      "957dae54d6b444c4b6a5a8dde3ad1856",
      "3030d4b06e2f4f848d717fa609c55900",
      "763c851970fb457c860f0de430d46743",
      "097b6f46661c400a8c4a7b31c6cf2bc3",
      "6808826640394b248eae9bd62a8b3cc1",
      "5d3e9a22f3bb4a8580d33980820a5900",
      "fb95826b82284461b64c2bd3f6a0e067",
      "e9a03cd162d143e3b5d90ab1ca30b9f1",
      "3172c1d13af34acc91c15a69fad2fcd2",
      "810921ac1d2c4418856541246aec328d",
      "04a9c535ae554a0699438a81fde16fb2",
      "c5d7343e98f44ad4b7604b3b539a77ed",
      "04a9c39de0e94d8f916482acfc23861a",
      "a686342a8a6b4cad9c3a7a755a0dcd2b",
      "99ee8617171d426d8411cfb21bb83ffc",
      "8cb5117b889c4bd89bb0dfcf1fadea1f",
      "c377abf84e2e450f941e3af96f5f34b8",
      "106594d1bf8443e784314ebc31b4e417",
      "e75ba6c447b14cd59c5b6f016cc3d5f4",
      "5e757e0f5cbe44b88ab05da9aec63a02",
      "749cbcd3fc614141a2447c705d9d1bfb",
      "771cf785d4eb44c78a89b7b4c48c8bb9",
      "712b836a203e442fb3eec70cfee8b31e",
      "89eeb5be95a54c7fae163840db483e4a",
      "fdd0c7d8c7654ac1bea9af477893cd51",
      "d47d18c77d3d46dbb54126e5c45d6aa0",
      "2f7d0cc864044c7096fcb24db2c1ccbb",
      "8b167547662f442488e4f9f320524437",
      "3aeb870d4e5240c2861bb316084b2f37",
      "1d73b9966fc446f488cbc21bc866e6ec",
      "7c96c070654644f68de4f6908c143d17",
      "646ac0a38b624009bb13917aa021c29b",
      "0e0a54c9301e49c1b541912207566d14",
      "c9567e375f514554b1c69502ab66b7bd",
      "88ddecb0a652479f87b11d36c56ac9c6",
      "936075c212924aba9ca32f640045a724",
      "f2b7064c4da7439b9ec669a1297da362",
      "e0fdc255488d47e4848d6046f04fa195",
      "574ae759095b442cb61901e45f505088",
      "8bd4fddcc203446fbf28134e613bf0fc",
      "fade533496de423eb3e2bc48069bc543",
      "e6d925e8d1b8443cbf38877f1b3d2220",
      "2fe663f504cc42658011f3e5a46afb39",
      "038a64881d1a4b1bb90f204a924fa504",
      "3cf309e1c7b8441ebdcdd50150512507",
      "723c8fa9b45b48218c3b33d218060e00",
      "69e34ec416cf45b58d4ebbd8b01cf813",
      "a41a2e10786946398c7a7d4d5a93087c",
      "f72689f941e7451b9fa30d415882af9f",
      "616b108e370c4469869b12a3481926c6",
      "58d02878081d4bb6be704d40727b1b1d",
      "6048c95ebf7248c08cbfd57c74be1d14",
      "22ee5134c8cf42edae3834090d108be0",
      "0365d78919cd4121820018cafafc4d72",
      "611a1d2cdb0f4a8b8a2c414ab1af508c",
      "17fccdae067644e494d069e67ab54d9b",
      "2e26e2ae796e4b9f9397f8bb43651905",
      "770ff74f8795463aa6d0aec051f873a4",
      "914bf548d3b34b4f8b9bd4ed428314c3",
      "0d88c6a1d1ef40e785627d29e3e3caf0",
      "3e99c4bc194e4cc7a0f67e6742f5bd1a",
      "261e6b3d03bb4cc49c027bad1b8261ed",
      "3c981430d21843c6beb8f5c3a4574167",
      "48214688e525461cb7f6bef18c8892d3",
      "f43b3b2d8a4e44f0b28c0012cf9d04be",
      "0f553988d3a04dbf9c8c7094713d023c",
      "1ab7a79f683c4faaaa3cb1caf65d5ee2",
      "6977df1eaff84033963284b3562f9daa",
      "8ccff083a8324c51b9ab45f000940705",
      "5438c423c2a74803a165ef8f62e50b4a",
      "768e1232a5284dd7a0d7ef5d36aee69d",
      "547e22f79df44c5ca094d75e04e45ae7",
      "efb0f5aa014049c6a41782dae9d629be",
      "d2f55d7bc0ac4e62b684bec1281aa8a1",
      "9022a838e3cf49468c11e87fafa38be2",
      "d72e9160b84649fa9bdb0315f7d4d9f6",
      "6b1a8e34c3a5403298e3009cc8250657",
      "ecdc1a00ee93466b838c4b15ab41e892",
      "bdf7ffbb59374bfb9fb5268daa34b2b7",
      "e2a8663c5ebc44d283fcda8fe1715b70",
      "a9d121a289b84f6fb8c5bbe5c6cf8692",
      "bf26cc85246445c8952b73c33d3f917c",
      "7e2e25173b294c108649d8139e919ee7",
      "780e1c8df6854bd7bd505b2f5a4b84f9",
      "1c925113b4bc4f22bf18dce2e6e3053e",
      "00ceb2120fa14ec3a097db7b124588a7",
      "f105734c50894c8882eefbdc4fd2c267",
      "b0ad60c0e8484338a58a479227d34fec",
      "e39e0b83b00241f6a6f466abe45c27cf",
      "8de70975b8b14ac18be47a63a5551ac1",
      "d177e62a10d74befbf0f21b967e92564",
      "300efdbe7f6247a3b11981feac35144e",
      "268287b11b9047c7b40892bd4d9e51d1",
      "6cf73a43fca145678dd8b12f89506ab5",
      "8de1b23d7a8f4484a9a3564b3ef24d28",
      "0f94493188d2463ea661d05ad4339051",
      "50a7eb26bb2648d2b7a1229546557495",
      "abfb73b5560941b39877057d6a695e71",
      "cb2db2a206bb45f8aa888156f0344e7d",
      "4b7e4b9c0da04f00884d9377b0c6dc19",
      "abf36b0896b944b4a444d618cee0024c",
      "5ad4bba05a3c40ec997beb67271bf195",
      "e05f0cf246584d4298346a5fd472910c",
      "2ba985e37acb4c7382d1ef236d095fce",
      "1e3b2bc86a3549f5902ed23f84d422bd",
      "e16d2a8ed741447f8463aa8009be1585",
      "64966b3731c14045b1d3f734166cfe7b",
      "4afa525bfb644023b010d87bc1b0ba1a",
      "ac37a9eacd844f8c84923379a4af9bba",
      "904645ac43ee4204abc15b7b7b4d09af",
      "73989e6c86e94ef7be3d3e7c50aefa27",
      "1b22c636b0c64e1dbaeebf5705d78c26",
      "328ba4796a774e928a68f41ffbfaadfa",
      "8f7dbc703f1945888d34338ae8b0edd2",
      "f8fb69ce4edc4e56b6c386b80555b2b4",
      "df3474ed323846dcbc72f3fd72179a77",
      "f7785bc052b84cb8a4ee8bd65c4d7e23",
      "1e97e2816c6a42b583a565d6cc7463ae",
      "b65480a80d7947c482f9a6a7b967b076",
      "5c9b51d4f2c7457b9fdae2f11217a0b1",
      "46fae98ce126479eb78d998b2028b3d1",
      "58ec41c53ba94152afd803bcb9f08f6d",
      "103bbe6669884f44a372502e4094d8b5",
      "d35f330f95fd40ee9e09f4eb7fb09c1b",
      "a5bcd0629b4f4b7e8c21acfb3e21c7ec",
      "2ee6b0c05e174b0eb6fd8295113267d4",
      "a5605ec3a56f4c679c7c047635d5f8f2",
      "7c5fd36feb8e4e4d8e18934153a48ace",
      "6c8c8a61a10a429185b065cf44023ce5",
      "446b24e6a0974ed0b4aac6452e193949",
      "5ff65538be074599ba40876d29494a88",
      "33d7d200e09d418b84a6ee45d3bc8be2",
      "b2d8bb15df5a4d7487fd9c84e4ccd0ea",
      "309accbed37b469e80eac82fae2d21ca",
      "8d67427d9a0049d381c74ca1bb1c0f6e",
      "5c6485ba2e774c2ebead6b82dbfaa758",
      "71e2da5cf738443589d0dc1bfcdeb5b6",
      "3c7e8a3056ea45b796a639aa4ebdb51d",
      "ff54a13583134cfbbed9866ecc149adf",
      "3ae4ba807b60415bb9d71987d2c91feb",
      "ea3f19ccd64c4dbd8048af9fed2cc135",
      "f68cc13242974c379196fdbffd9d9c4c",
      "f63dfa0bd66f4de39edb151994ff8c37",
      "6033848b007c496cab626f8d02ebb6e8",
      "2e38c9a614cc4523a3bdedb592eeb5f9",
      "4b8a950cb1e243b7b5de8e4ee1e9b7b0",
      "013fa2ff420b4ce98391eafee145d10a",
      "7fa5ed1de30c42a2ac0f2f39c97c2670",
      "06d607137795400aa1da62a61d2941f8",
      "a50ee49379054615b9ea1067248e5035",
      "7b876d65e5214c7b8c5e6f54becaba14",
      "bfb51cf09e6e424dafab4412834ac48c",
      "0f75ec23ee1946529488f77788cdd4f1",
      "4c818c513ab444f5919a71fbc82d4bf1",
      "09a4166b706644d99843115d2443c03f",
      "342363ef0f6f4abfb40930a0f526a716",
      "a1f2d5cf57bd402dae03b3a074a6dff5",
      "d7326804894a470c848a4e0dd14d0a3e",
      "cdd2fc8a97204a18b58c04ed8a832950",
      "dbc1940d0ad54e12a6c22d2b421f9b53",
      "49052eb372454ce5bb6f58f73322fac7",
      "2e34a63a1d3e4ad881472bd1805d3449",
      "6999d232dd054aed910013df367741b7",
      "0f6008a6d71245699fe70ce5c6302e53",
      "3cb785e882ea4b1789e83706d803982b",
      "ecaadcea0c3b4442bef34e53175c71c6",
      "2db2ae5f1bb748ad95d21beac57733bd",
      "353d63605d3044adba900091be07fbff",
      "90c08e6f38ff4adebda3f9c9502e1e97",
      "1cf50f43bfc343fe8a4f36d33dce287a",
      "4e6e001c85b2444e851ff290ad686300",
      "72691e10d4a14dd0a82892d7b50d0034",
      "ac54eb56b96d4a4e8b8db3e8233f9ce6",
      "d46ba71008824704b0bc5173eb1079d9",
      "483e4714d1464637a3a9f9975206f1a0",
      "1433dc20c4a6477089e17c68bc18a53c",
      "c50f3a2460c44a70b0d6f3b42efe37af",
      "34ddfea174bc4c28939c33a93f12acc3",
      "a393c13a0ab84eeba98e7e8fe264d666",
      "f7abe3ef69124291b71e490514fe429d",
      "9c383bd9b7ae499ca4ef4858c39f00ff",
      "9244c97ac72e414197d26389e7de61e0",
      "24fd7d09744f4c76a83fb0112ea6b396",
      "484ceac0bb634f98a3e3396033226038",
      "c0f44fe2a1fd40c7a7a1d3754fe9a989",
      "b9bf316616da475fad911b3b33a24c17",
      "82aceb0430bd4d6a9b7867937b837090",
      "b066803cd8f0489983e416b4958c42ea",
      "6d4433d5a42a4c0d80eb99008b40868a",
      "ffb03c7420f745bea907d26e5f653c16",
      "f94cd550b406406daff4c065e996e490",
      "dce4c423d5f9424087e1ba3c21cb6701",
      "48b28e4e3d7f4064be7bdc7f8a569045",
      "442124b71ff841568e2bfede7a653c03",
      "62c37c89f8594bc9af9e2247ead8b9c6",
      "d4c4cbf24adb4ca8b7544afcfd0ce480",
      "d276665f6201437ab1c1a5574eb57f51",
      "6ca651cecfd8439a83d99f70d2524828",
      "efc7dee1bef34e328eb021ebecea4d09",
      "ba720bb8fec747c58455754680678b6b",
      "d8b91e45e77242f9ae7f5890a76952ca",
      "4e22373668a948e8bc27e4496199a932",
      "34c6be4d0e3b4159b7a5951c4655070c",
      "f358679b58164bf3b6a5fccc27b01c66",
      "4d43c8a9c27c4904add8a20e3b6be170",
      "659f448c237b4cdf911a36de7e432a00",
      "34c4e3810c284c939f980226f1c62fe9",
      "c12ef71885d74e4c9e5953462e5d6913",
      "ceac512f6bd34dfbbfe7447f0cd319b9",
      "ba1c0430708f42a6bf4df3bf95c5a704",
      "32429780e60147a68e17274287e98b6d",
      "e781120d043c4c6da2c426c54ed7def2",
      "4575d09c195d4c04b695ef00b5168379",
      "76e23b87ceaa4cf990672bc701f79cde",
      "504413e56d7c4c8ca7cbf76df1c1af98",
      "a94469b5640c426e80fdb8f57cfa5314",
      "077fb2128a2b4f73877d9ae3937a72f2",
      "cc7700791e6b45cbbae170e4b10bcda4",
      "c3d51213fbef40c78935c064960ab891",
      "c31929e6e169483ea525cc328811e4e5",
      "07c9d43b085f4e5c8941ec395bb3ad95",
      "707e38ff361542cdb0cbb293fa2524db",
      "aee2a5b0352246f9b2c7a1a29b21ade1",
      "336d8eedf07b48108fd100abf6dc17fd",
      "00b9f6135db4429f8f6b3afd59afa0d8",
      "d7c879c3f16044a3b4dbe56e7cd33bec",
      "13780f166cc44090b67c56fc08a02e75",
      "b945aa2ee5fe4f44bcec6791ca635032",
      "317f6af957f947ff9fdb025ac68d10df",
      "7d91c0af4afd4700a909dc443a62c7cf",
      "de8702f7fffb4e8a9ec018f3c9f2fb7b",
      "1a9c6830af0c4fe8aa61d1dc6c013cec",
      "53db4e01228041548ff641632207d007",
      "c649bba7ea1845ebbbfac619f99054d0",
      "499b7a015af04c69a21daaf9323c90b1",
      "1a285792e422423bbf5405d2ec64b042",
      "46c2e891d16f4324b3f3b4b806ffc44f",
      "1560179c89564d26bba7fe2df18f5b97",
      "409e3185d8e14e4a8365dad313eaa8be",
      "d42af47513bf47ffa50ebef2aaea5293",
      "eb2dd3090b53416d8322bdde2ee32ce7",
      "b0b97ea7e9b54f9097d6218d4d5d1664",
      "d5c19f4371794065bbfbc4c1c69fc9ab",
      "31284b927b7f406581407fca799770e1",
      "a6584a12f33a4d719310522d4c5fcb54",
      "4c1868e3e00d418db0823601b8689d32",
      "d1f649df6d2d4d3389de782cda57fd6b",
      "55a6a32c38254a37a1105a044068a0b7",
      "ff3ac46d3019400e806dca7d930f4a92",
      "325ac1c17f944b9fb4a0a17f2b725b53",
      "2d7a76e9fb6a4c9d93792a3f0c0833e6",
      "7881bd86208342bdab4c5fdd1204c6ec",
      "8b4f7a09ac1c4ecd927cf398d7a0b17b",
      "773435c757a54d2ca569187bf0b06406",
      "7f1e0458d5b94144be8c9b32e8a2d10c",
      "754761f8850941e69c8d2821207d0b5f",
      "0ab0b63685db4134a7afad58c527a7b0",
      "1e6099280943420fb5de7a399aa4de4b",
      "4efe0a3d8b98468581fb150391cf5eb8",
      "84f707cf4cbe4ae2bf2b28ebbb34030a",
      "e73f09adcc034af1a0b69cc1e497fe46",
      "b1eb1f6d28634780bb4cf40a63552e93",
      "e81d153d433b4b4eae22bff243182567",
      "db3a57111b4d43fd93decbf68fa0324a",
      "10d4af4670ed4b2f9b5983b019caca2a",
      "03ab905d534b4ef5a404a04366507980",
      "4065a1be6a874c04ab2abfbed9aee1eb",
      "d7669c1f6e154fe98daa34c62686036a",
      "185324f1851148e8a5f96c706b4947fe",
      "240a612f0ca74e9bb8c934c8fe6de89c",
      "4ed76776e6f746868a1e7b83e3a51ea4",
      "e53dadf498784c268c50a1d8141462d1",
      "8aaa431a06e24c0387cdaff7a67c9135",
      "755b4915219e4286a2d92c6d562abb3e",
      "ce319be2bad74dc09c67c5758000b050",
      "78260fea7b03483da8349c9c818955d2",
      "2d71a209d7c941808cc7613fd0906fb1",
      "1bb57927f08c4c8491079c3dc638dfe2",
      "778340a5d328400092a00cc598203286",
      "2f72192b50714efe817eac0e97f01832",
      "666a34e9ca3b400ea2dfeb99caf5c587",
      "faf2a74e4c2144438e838c7480b93468",
      "d59b4255774d45639a2ef68cfbd19229",
      "24300a7a82ba48099bc6f545424fb0e4",
      "3e26407c7786402abdb0a7737552eb65",
      "c084c120349c414dbd7413ea48448a1a",
      "98cd6a8158f747d89b45b12db1e2e5ff",
      "2476e16cd5474c76aeab2c119b96adaf",
      "31083e961fbb49f88d6ba1e74b883abd",
      "d1cf63d459e24f208b1a74a31a8aa590",
      "26e57744f0ed4f389f3ad580f68c8421",
      "dd1de32603504ac5b8d4a413c7d61a66",
      "8c0b5f0cc5d34cde86d986d0c32f1f6c",
      "447209ad738a4b6e8871427cbc89ea97",
      "31b56469de674927a33ee9acaf6b6d62",
      "8faf3d56cad641308a7298fd7edb521a",
      "3eae965ce802410ea9b717b787ae0b4d",
      "2bf59befbfbe49ab87c26a01d159a4d3",
      "3eab2b18c36f4f82821459fafbd57e3b",
      "2f0a0311f4ad430293442e666e7e88d1",
      "e15fc0dca0c34bbbab54b4f8a716c69f",
      "d8dc70bb5749432bb3fa08f801b5d0cb",
      "07f76713e70d4e03a7a1d4db4fdc7133",
      "b62b9721f8f942e4ad9cd61ac845f10b",
      "e3bc89b0ffe4433b98c49afb4771c890",
      "57fdf0dfb07047edb5467d86dbdc0ad9",
      "61ab592c793e436a8b480bba08cc227f",
      "95e0e5234c9441ca9a0623eafcdf13c2",
      "cb1a5e5bfb254dddbd9a14d2fc84d53c",
      "5aa00b1f62b44941be539fd05de2c64c",
      "7f4ae13099144d1aabae86c0cf69cae1",
      "1a8d08cee76b47e3b42b10cdcc2cd153",
      "c6ef15922ac94b63b4ebcc9cd018d4f2",
      "0b5ebf5973544fb9b66cb2fd8f3b5f91",
      "c96832dddf2c49b0bf2cfe299d0ce248",
      "d9f3285b973a4dadbc239da9e74344bd",
      "414aadd1286e4607b0451f691a8b5367",
      "6d296b08030c4d1bb6e21449f5fc3d1f",
      "21f843f13ae14849b6e210a594beaefd",
      "18c26dde02de4655b166bc7d5900e9be",
      "b192f2b34723457f93ee812ad110b47d",
      "317770af697f42a286f1b73c63f84b1c",
      "84a7457af801439b9d2f4405defc062d",
      "4883d4a89bc749e599a15681d2530798",
      "3e0c64d58d8b46aa9eb103a290bc0236",
      "47ed8b451e014bffb53bdfeda1317e21",
      "e6df5ad8cee443e8a495e164616cd962",
      "8b80d6e2764c407ab30d5b9c59a7818e",
      "b35a4a23e4e64f2dad9d6c6ab558d05f",
      "31d099ac4c5a43fbb7bf1d79a490d3de",
      "122216e01ff1431fb27234f4a95f389c",
      "2a8d4ba1133946d2a7576529797a63c1",
      "d3758cb690f944358b929cc7403c3d12",
      "af4b71283a314f8087f9999769997c3d",
      "765fcf953051453b94f4e4662229f11a",
      "470ec9dabaa446f0b143804db19e6e0e",
      "549aeb86a62c4e11a86d1e2f4701e892",
      "cef2ce37054f4a42bf32d65bf61988ef",
      "582a4c3f7ad04fc98886739fcb5aa3a1",
      "f3610e22377541168deecd83a7bf81d0",
      "30b939225fc14e47b7001ef316ab3b1e",
      "ea1a5576d9ba458d8ac41247e8ad13fa",
      "2dfc2b5b990041499bb333fd63459c23",
      "51f43d5acbaa49a6bb6df753ca79c1df",
      "6f860c14699942949310ecd397a1a10a",
      "f429b1e602d54d47a98b0cc3a5ec3a26",
      "115c63e80c054115819c9cc97ffd2a71",
      "003537efbe73435593816fe28f04cb37",
      "5d98bec99ea842aa8bfe03b585f12583",
      "fa8d7f1f954142e5a4a50d48ac6f2365",
      "7f4235955f8241d798cba870380f8b82",
      "387b372215e046218c38d7a7d9b6b2dd",
      "c019ef770bf9433f92cb0639c9746dce",
      "b2c8b39ac75f40559c50d147a8ba29f7",
      "52e2d829529f419eb5a81247cdec6fd5",
      "22e40edd79324166b1808efd7ccf4d44",
      "6276bad8e5434b08af4285116c145c64",
      "c288441edf49403983e9bff60211f279",
      "aca2c7b4f3284c43b50f9014510f09cd",
      "4c5109c9901643eba5e9a965df474da5",
      "a5cf41b4c83a49f88c539d209a3ec916",
      "f33ea116e04d42f9927b2b9c1c7fcd2f",
      "3135bad26a3d49248f8c37ccdbed1ade",
      "316a6e75ba3945f9aa32a5807d7d8bf5",
      "f04b5963df274f3284dbb7329fe23482",
      "9e9c28b4647746379fbf498e3bc09528",
      "3223148ef4ff4aaba5cd784a12e353fd",
      "5bf88a0ac8db4954a60bce1bad660de4",
      "1d9c56dbabb2438ebe066b845a3458db",
      "adafc74758754fbdbcbdc9fa524b920e",
      "ec8ae966db624d18afa041d9ed60c69e",
      "f4154cfb12244bb599cd0e483f1556b5",
      "8ddbd18f6920467584dfa3baf1c2e1d8",
      "f47ca61d75c341398ccc02bb7c5716b8",
      "fcd39a7afe4b4b77b88dddf31ea9aed2",
      "09826ed287ff49dc977ed43b12554a91",
      "11b5a7b18df048e881dc80acde2c6988",
      "a47dcdf1f1524688b3be124658923cdd",
      "88e221dc64bc4452a0bba7caf775ee12",
      "e8b46924260a43808760fd79c5a9f941",
      "3718181fad084d5085555b7875a5370e",
      "6a84c00e7aec4996ac75a47a1e6493a4",
      "91be759d1890425390e2badb6dcfcdae",
      "26f37e23d39e4443a66ff2e5fd0796d1",
      "c746d69895de4b198763d221034fd1e8",
      "865779fae3ea4a8fac7631c1ba609bed",
      "95aaaeb42a5142e793c6f23cc75df205",
      "36784c2c1fde47fcb7ae8350b61ae804",
      "48695b68dd43402a9c94f3e1f32e5b42",
      "118b924a55bf48e79813d20b9b270113",
      "48183ffcc0ba4f5bada2754e97606d83",
      "937b0e8a1ce64249ad264feb4ef821b4",
      "21914bca282d4ba199c38e10bbaeb8ef",
      "feeecc85d6fd43aa9d547d6526012e60",
      "d0110441d8fb4c538aa127e24678fc4e",
      "e9ee9ac2267c4be2a96ab346726b9b12",
      "2146a49d30064de3a66ec89bd43af6ad",
      "e7d184a675d54de894d4c0756ed26746",
      "078ca7f689864506983ea062bea1c43e",
      "6f869f64cfff4f3b86ea876d8228279a",
      "c19e370422ff4a458af0f677d7e0c3e9",
      "70f3acbebd9e4bd8a67816b8221e6bd3",
      "d48835e3eb2340d89782ca33a64a8f62",
      "f0fb62e417e948418e3363e45c044898",
      "a4888864495049d78ed36516901a9782",
      "10a574b6775c491a8b34bb0ce3710d8a",
      "449ccebed4134bba96792301462b1668",
      "2c17c5956e5443ca89ef48927fcf37b0",
      "77d6b611d7f241cfa28e8b89cdcc6831",
      "2a2db2f945184913b433382fb85b8619",
      "7448b589e5aa4596955e8da4ade95ef4",
      "7dcd1e9488fd4a9cb584616f3fa91670",
      "3ec589d2f03b4397ac235f91166ddec0",
      "eede7ffb40eb4baeb18e8859f3ff9a90",
      "ba20bfad24504fde94a2a34a1642e5b2",
      "013b2b39fc874f3992db91f0a588d94f",
      "582bbceb23b6410d899ca06a46a07cc6",
      "60806c5643fd40c38906f1e1f15110b2",
      "7d7323087580401d9faac5380e0eef15",
      "2140729c4f804f84928db47a345a7def",
      "a083d4a251a945538cbd9866cb1bf9b5"
     ]
    },
    "id": "lKEaBMLHjjxX"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "The following cell trains and validates the D5 model using the updated v2 speaker splits (50% train, 20% validation, 30% test), and records the results in a way that does not overwrite earlier runs. It reads `manifests/manifest_all.csv` from the D5 v2 output folder, keeps only rows marked as `train` or `val`, infers the dataset label from the most common `dataset` value in the manifest, prints basic counts for sanity checking, and stops immediately if any referenced audio files are missing.\n",
    "\n",
    "An experiment folder is created under `trainval_runs/` using a fixed tag (`frozen_LNDO`) and a timestamp so that each run is stored separately. Within this folder, the cell runs three independent training jobs using different random seeds (1337, 2024, 7777). Training uses a frozen Wav2Vec2 feature extractor and learns only two small task specific classifiers, one for vowel clips and one for other speech. The `task` column is converted into a simple `task_group`, where `vowl` is treated as vowel and all other values are treated as other. Audio files are loaded from disk, converted to mono if needed, checked to be 16 kHz, and paired with an attention mask so trailing padded silence in vowel clips is ignored. During batching, audio is padded with zeros and the attention mask is padded in the same way so the model does not learn from padded silence.\n",
    "\n",
    "For each seed, the cell builds the training and validation data loaders and runs a short warm up by loading three batches to catch input or disk issues early. Training runs for up to 10 epochs using an effective batch size of 64 through gradient accumulation, mixed precision on the GPU, and early stopping when validation AUROC does not improve for two consecutive epochs. After each epoch, validation is run and validation AUROC is computed. Only the best validation epoch for each seed is kept.\n",
    "\n",
    "For each seed run, results are saved under `exp_<tag>_<timestamp>/run_D5_seed####/`. Saved files include `best_heads.pt` with the trained weights for the two task heads and their small LayerNorm and Dropout blocks, `roc_curve.png` showing the validation ROC curve, `confusion_matrix.png` showing the validation confusion matrix at a fixed threshold of 0.5, and `metrics.json` containing training settings, dataset sizes, the best epoch, the best validation AUROC, and additional threshold based metrics at 0.5 such as accuracy, precision, recall or sensitivity, specificity, F1 score, MCC, and Fisher’s exact test p value.\n",
    "\n",
    "After all three seeds complete, the cell prints the AUROC for each seed and the mean AUROC with a 95% confidence interval computed using a t distribution with n=3. A single `summary_trainval.json` file is written to summarize the entire experiment, including paths, settings, and per seed results, and the same summary is appended as one line to `trainval_runs/history_index.jsonl` so experiments are tracked over time. Finally, the Colab runtime is unassigned to stop the L4 GPU instance."
   ],
   "metadata": {
    "id": "m7NlDc1RlHaX"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# =========================\n",
    "# D5 Train + Val — v2 Splits, Frozen Backbone, Two Heads\n",
    "# Inputs: manifest_all.csv (train/val only) and audio clips referenced by clip_path\n",
    "# Outputs: per-seed run folders with metrics.json, roc_curve.png, confusion_matrix.png, best_heads.pt\n",
    "#          plus one exp-level summary_trainval.json and one appended history_index.jsonl record\n",
    "# =========================\n",
    "\n",
    "import os, json, math, random, time, warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import soundfile as sf\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from transformers import Wav2Vec2Model\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, confusion_matrix, matthews_corrcoef\n",
    "from scipy.stats import fisher_exact\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# -------------------------\n",
    "# Dataset root (D5 v2)\n",
    "# Purpose: pick the preprocessed version that contains the v2 train/val splits\n",
    "# Input: manifest_all.csv under this root\n",
    "# -------------------------\n",
    "# DX_OUT_ROOT = \"/content/drive/MyDrive/AI_PD_Project/Datasets/D5-English (MDVR-KCL)/preprocessed_v1\"  # modified to consider the new 50/20/30 splits path\n",
    "DX_OUT_ROOT = \"/content/drive/MyDrive/AI_PD_Project/Datasets/D5-English (MDVR-KCL)/preprocessed_v2\"    # modified to consider the new 50/20/30 splits path\n",
    "\n",
    "MANIFEST_ALL = f\"{DX_OUT_ROOT}/manifests/manifest_all.csv\"\n",
    "\n",
    "# -------------------------\n",
    "# Experiment folder naming\n",
    "# Purpose: keep a permanent run history without overwriting older experiments\n",
    "# Output: EXP_ROOT contains all per-seed run folders for this execution\n",
    "# -------------------------\n",
    "EXPERIMENT_TAG = \"frozen_LNDO\"   # change to something meaningful for paper\n",
    "RUN_STAMP = time.strftime(\"%Y%m%d_%H%M%S\")  # unique timestamp\n",
    "TRAINVAL_ROOT = Path(DX_OUT_ROOT) / \"trainval_runs\"\n",
    "EXP_ROOT = TRAINVAL_ROOT / f\"exp_{EXPERIMENT_TAG}_{RUN_STAMP}\"\n",
    "EXP_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# -------------------------\n",
    "# Training settings (fixed defaults)\n",
    "# Purpose: keep training behavior comparable across datasets\n",
    "# -------------------------\n",
    "MAX_EPOCHS     = 10\n",
    "EFFECTIVE_BS   = 64\n",
    "PER_DEVICE_BS  = 16\n",
    "GRAD_ACCUM     = max(1, EFFECTIVE_BS // PER_DEVICE_BS)\n",
    "\n",
    "LR             = 1e-3\n",
    "PATIENCE       = 2\n",
    "SEEDS          = [1337, 2024, 7777]\n",
    "\n",
    "BACKBONE_CKPT  = \"facebook/wav2vec2-base\"\n",
    "SR_EXPECTED    = 16000\n",
    "TINY_THRESH    = 1e-4\n",
    "\n",
    "# Small pre-head blocks (trainable)\n",
    "DROPOUT_P      = 0.2\n",
    "\n",
    "# Data loader stability (Drive-friendly)\n",
    "NUM_WORKERS    = 0\n",
    "PIN_MEMORY     = False\n",
    "\n",
    "# Fixed threshold used for confusion and threshold metrics (VAL only)\n",
    "VAL_THRESHOLD  = 0.5\n",
    "\n",
    "# Mixed precision (GPU only)\n",
    "USE_AMP        = True\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "try:\n",
    "    torch.set_float32_matmul_precision(\"high\")\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", message=\"None of the inputs have requires_grad=True. Gradients will be None\")\n",
    "warnings.filterwarnings(\"ignore\", message=\"Passing `gradient_checkpointing` to a config initialization is deprecated\")\n",
    "\n",
    "print(\"EXPERIMENT ROOT:\", str(EXP_ROOT))\n",
    "print(\"DEVICE:\", DEVICE)\n",
    "if DEVICE.type == \"cuda\":\n",
    "    print(\"GPU:\", torch.cuda.get_device_name(0))\n",
    "print(\"PER_DEVICE_BS:\", PER_DEVICE_BS, \"| GRAD_ACCUM:\", GRAD_ACCUM, \"| EFFECTIVE_BS:\", PER_DEVICE_BS * GRAD_ACCUM)\n",
    "print(\"NUM_WORKERS:\", NUM_WORKERS, \"| PIN_MEMORY:\", PIN_MEMORY)\n",
    "print(\"VAL_THRESHOLD:\", VAL_THRESHOLD)\n",
    "print(\"USE_AMP:\", bool(USE_AMP and DEVICE.type == \"cuda\"))\n",
    "\n",
    "# -------------------------\n",
    "# Manifest read + split filtering\n",
    "# Input: manifest_all.csv\n",
    "# Output: train_df and val_df (train/val rows only) and dataset_id label\n",
    "# -------------------------\n",
    "if not os.path.exists(MANIFEST_ALL):\n",
    "    raise FileNotFoundError(f\"Missing manifest_all.csv: {MANIFEST_ALL}\")\n",
    "\n",
    "m = pd.read_csv(MANIFEST_ALL)\n",
    "\n",
    "# Purpose: ensure the required columns exist before continuing\n",
    "req_cols = {\"split\", \"clip_path\", \"label_num\", \"task\"}\n",
    "missing = [c for c in sorted(req_cols) if c not in m.columns]\n",
    "if missing:\n",
    "    raise ValueError(f\"Manifest missing required columns: {missing}. Found: {list(m.columns)}\")\n",
    "\n",
    "# Purpose: train/val only in this cell\n",
    "m = m[m[\"split\"].isin([\"train\", \"val\"])].copy()\n",
    "if len(m) == 0:\n",
    "    raise RuntimeError(\"After filtering to split in {train,val}, manifest has 0 rows.\")\n",
    "\n",
    "# Purpose: infer the dataset label when the manifest mixes datasets\n",
    "if \"dataset\" in m.columns and m[\"dataset\"].notna().any():\n",
    "    dataset_id = str(m[\"dataset\"].value_counts(dropna=True).idxmax())\n",
    "    m = m[m[\"dataset\"].astype(str) == dataset_id].copy()\n",
    "else:\n",
    "    dataset_id = \"DX\"\n",
    "\n",
    "# Purpose: keep a consistent table shape across datasets\n",
    "keep_cols = [\"clip_path\", \"label_num\", \"task\", \"speaker_id\", \"duration_sec\", \"split\"]\n",
    "for c in keep_cols:\n",
    "    if c not in m.columns:\n",
    "        m[c] = np.nan\n",
    "m = m[keep_cols].copy()\n",
    "\n",
    "train_df = m[m[\"split\"] == \"train\"].copy().reset_index(drop=True)\n",
    "val_df   = m[m[\"split\"] == \"val\"].copy().reset_index(drop=True)\n",
    "\n",
    "print(f\"\\nDataset inferred: {dataset_id}\")\n",
    "print(f\"Train rows: {len(train_df)} | Val rows: {len(val_df)}\")\n",
    "print(\"Train label counts:\", train_df[\"label_num\"].value_counts(dropna=False).to_dict())\n",
    "print(\"Val label counts:\",   val_df[\"label_num\"].value_counts(dropna=False).to_dict())\n",
    "\n",
    "# Purpose: avoid silent runs that cannot train or validate\n",
    "if len(train_df) == 0 or len(val_df) == 0:\n",
    "    raise RuntimeError(\"Train or Val split has 0 rows.\")\n",
    "\n",
    "# -------------------------\n",
    "# Clip existence check\n",
    "# Input: clip_path from train_df/val_df\n",
    "# Output: raises early if any audio files are missing (shows a few examples)\n",
    "# -------------------------\n",
    "def _fail_fast_missing_paths(df: pd.DataFrame, name: str):\n",
    "    missing_paths = []\n",
    "    for p in tqdm(df[\"clip_path\"].astype(str).tolist(), desc=f\"Check {name} clip_path exists\", dynamic_ncols=True):\n",
    "        if not os.path.exists(p):\n",
    "            missing_paths.append(p)\n",
    "            if len(missing_paths) >= 10:\n",
    "                break\n",
    "    if missing_paths:\n",
    "        raise FileNotFoundError(f\"{name}: missing clip_path(s). Examples: {missing_paths}\")\n",
    "\n",
    "_fail_fast_missing_paths(train_df, \"TRAIN\")\n",
    "_fail_fast_missing_paths(val_df, \"VAL\")\n",
    "\n",
    "# -------------------------\n",
    "# Task grouping\n",
    "# Purpose: choose vowel head vs other head per clip\n",
    "# Output: task_group column added to both tables\n",
    "# -------------------------\n",
    "def _task_group(task_val) -> str:\n",
    "    return \"vowel\" if str(task_val) == \"vowl\" else \"other\"\n",
    "\n",
    "train_df[\"task_group\"] = train_df[\"task\"].apply(_task_group)\n",
    "val_df[\"task_group\"]   = val_df[\"task\"].apply(_task_group)\n",
    "\n",
    "# -------------------------\n",
    "# Dataset + collator (padding + attention masks)\n",
    "# Inputs: train_df/val_df clip_path and task_group\n",
    "# Outputs: batches with padded input_values and matching attention_mask\n",
    "# -------------------------\n",
    "class AudioManifestDataset(Dataset):\n",
    "    def __init__(self, df: pd.DataFrame):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        row = self.df.iloc[idx]\n",
    "        clip_path = str(row[\"clip_path\"])\n",
    "        label = int(row[\"label_num\"])\n",
    "        task_group = str(row[\"task_group\"])\n",
    "\n",
    "        if not os.path.exists(clip_path):\n",
    "            raise FileNotFoundError(f\"Missing clip_path: {clip_path}\")\n",
    "\n",
    "        # Purpose: load audio and force mono float32\n",
    "        y, sr = sf.read(clip_path, always_2d=False)\n",
    "        if isinstance(y, np.ndarray) and y.ndim > 1:\n",
    "            y = np.mean(y, axis=1)\n",
    "        y = np.asarray(y, dtype=np.float32)\n",
    "\n",
    "        # Purpose: keep a single sample rate across all training\n",
    "        if int(sr) != SR_EXPECTED:\n",
    "            raise ValueError(f\"Sample rate mismatch (expected {SR_EXPECTED}): {clip_path} has sr={sr}\")\n",
    "\n",
    "        # Purpose: mask trailing zero padding for vowel clips during pooling\n",
    "        attn = np.ones((len(y),), dtype=np.int64)\n",
    "        if task_group == \"vowel\":\n",
    "            k = -1\n",
    "            for j in range(len(y) - 1, -1, -1):\n",
    "                if abs(float(y[j])) > TINY_THRESH:\n",
    "                    k = j\n",
    "                    break\n",
    "            if k >= 0:\n",
    "                attn[:k+1] = 1\n",
    "                attn[k+1:] = 0\n",
    "            else:\n",
    "                attn[:] = 1\n",
    "        else:\n",
    "            attn[:] = 1\n",
    "\n",
    "        return {\n",
    "            \"input_values\": torch.from_numpy(y),\n",
    "            \"attention_mask\": torch.from_numpy(attn),\n",
    "            \"labels\": torch.tensor(label, dtype=torch.long),\n",
    "            \"task_group\": task_group,\n",
    "        }\n",
    "\n",
    "def collate_fn(batch):\n",
    "    # Purpose: pad to the longest audio in the batch (pads input and mask with zeros)\n",
    "    max_len = int(max(b[\"input_values\"].numel() for b in batch))\n",
    "    input_vals, attn_masks, labels, task_groups = [], [], [], []\n",
    "    for b in batch:\n",
    "        x = b[\"input_values\"]\n",
    "        a = b[\"attention_mask\"]\n",
    "        pad = max_len - x.numel()\n",
    "        if pad > 0:\n",
    "            x = torch.cat([x, torch.zeros(pad, dtype=x.dtype)], dim=0)\n",
    "            a = torch.cat([a, torch.zeros(pad, dtype=a.dtype)], dim=0)\n",
    "        input_vals.append(x)\n",
    "        attn_masks.append(a)\n",
    "        labels.append(b[\"labels\"])\n",
    "        task_groups.append(b[\"task_group\"])\n",
    "    return {\n",
    "        \"input_values\": torch.stack(input_vals, dim=0),\n",
    "        \"attention_mask\": torch.stack(attn_masks, dim=0),\n",
    "        \"labels\": torch.stack(labels, dim=0),\n",
    "        \"task_group\": task_groups,\n",
    "    }\n",
    "\n",
    "# -------------------------\n",
    "# Model: frozen Wav2Vec2 + two task heads\n",
    "# Inputs: backbone checkpoint and dropout probability\n",
    "# Output: loss and logits (PD vs Healthy) using the head selected by task_group\n",
    "# -------------------------\n",
    "class Wav2Vec2TwoHeadClassifier(nn.Module):\n",
    "    def __init__(self, ckpt: str, dropout_p: float):\n",
    "        super().__init__()\n",
    "        self.backbone = Wav2Vec2Model.from_pretrained(ckpt)\n",
    "        for p in self.backbone.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "        H = int(self.backbone.config.hidden_size)\n",
    "\n",
    "        # Purpose: small per-task feature cleanup before each head\n",
    "        self.pre_vowel = nn.Sequential(nn.LayerNorm(H), nn.Dropout(float(dropout_p)))\n",
    "        self.pre_other = nn.Sequential(nn.LayerNorm(H), nn.Dropout(float(dropout_p)))\n",
    "\n",
    "        self.head_vowel = nn.Linear(H, 2)\n",
    "        self.head_other = nn.Linear(H, 2)\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    def masked_mean_pool(self, last_hidden, attn_mask_samples):\n",
    "        # Purpose: pool frame features while ignoring masked samples\n",
    "        feat_mask = self.backbone._get_feature_vector_attention_mask(last_hidden.shape[1], attn_mask_samples)\n",
    "        feat_mask = feat_mask.to(last_hidden.device).unsqueeze(-1).type_as(last_hidden)\n",
    "        masked = last_hidden * feat_mask\n",
    "        denom = feat_mask.sum(dim=1).clamp(min=1.0)\n",
    "        return masked.sum(dim=1) / denom\n",
    "\n",
    "    def _heads_fp32(self, x_fp_any: torch.Tensor, head: nn.Module) -> torch.Tensor:\n",
    "        # Purpose: keep heads in fp32 even when AMP is enabled\n",
    "        x = x_fp_any.float()\n",
    "        if x.is_cuda:\n",
    "            with torch.amp.autocast(device_type=\"cuda\", enabled=False):\n",
    "                return head(x)\n",
    "        return head(x)\n",
    "\n",
    "    def forward(self, input_values, attention_mask, labels, task_group):\n",
    "        # Purpose: backbone is frozen, so gradients only flow through heads\n",
    "        with torch.no_grad():\n",
    "            out = self.backbone(input_values=input_values, attention_mask=attention_mask)\n",
    "            last_hidden = out.last_hidden_state\n",
    "\n",
    "        pooled = self.masked_mean_pool(last_hidden, attention_mask)  # [B,H]\n",
    "\n",
    "        z_v = self.pre_vowel(pooled.float())\n",
    "        z_o = self.pre_other(pooled.float())\n",
    "\n",
    "        logits_v = self._heads_fp32(z_v, self.head_vowel)\n",
    "        logits_o = self._heads_fp32(z_o, self.head_other)\n",
    "\n",
    "        # Purpose: apply the correct head per sample\n",
    "        is_vowel = torch.tensor([tg == \"vowel\" for tg in task_group], device=pooled.device, dtype=torch.bool)\n",
    "        logits = logits_o.clone()\n",
    "        if is_vowel.any():\n",
    "            logits[is_vowel] = logits_v[is_vowel]\n",
    "\n",
    "        loss = self.loss_fn(logits, labels)\n",
    "        return loss, logits\n",
    "\n",
    "# -------------------------\n",
    "# Metrics + plots (VAL only)\n",
    "# Inputs: val labels and predicted PD probabilities\n",
    "# Outputs: AUROC, threshold metrics, ROC curve PNG, confusion matrix PNG\n",
    "# -------------------------\n",
    "def compute_auc(y_true, y_prob):\n",
    "    y_true = np.asarray(y_true, dtype=np.int64)\n",
    "    y_prob = np.asarray(y_prob, dtype=np.float64)\n",
    "    if len(np.unique(y_true)) < 2:\n",
    "        return float(\"nan\")\n",
    "    return float(roc_auc_score(y_true, y_prob))\n",
    "\n",
    "def compute_threshold_metrics(y_true, y_prob, thr=0.5):\n",
    "    y_true = np.asarray(y_true, dtype=np.int64)\n",
    "    y_prob = np.asarray(y_prob, dtype=np.float64)\n",
    "    y_pred = (y_prob >= float(thr)).astype(np.int64)\n",
    "\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=[0, 1])\n",
    "    TN, FP, FN, TP = int(cm[0, 0]), int(cm[0, 1]), int(cm[1, 0]), int(cm[1, 1])\n",
    "\n",
    "    eps = 1e-12\n",
    "    acc = (TP + TN) / max(1, (TP + TN + FP + FN))\n",
    "    prec = TP / (TP + FP + eps)\n",
    "    rec = TP / (TP + FN + eps)\n",
    "    f1 = 2 * prec * rec / (prec + rec + eps)\n",
    "    spec = TN / (TN + FP + eps)\n",
    "\n",
    "    try:\n",
    "        mcc = float(matthews_corrcoef(y_true, y_pred)) if len(np.unique(y_pred)) > 1 else float(\"nan\")\n",
    "    except Exception:\n",
    "        mcc = float(\"nan\")\n",
    "\n",
    "    try:\n",
    "        _, pval = fisher_exact([[TN, FP], [FN, TP]], alternative=\"two-sided\")\n",
    "        pval = float(pval)\n",
    "    except Exception:\n",
    "        pval = float(\"nan\")\n",
    "\n",
    "    return {\n",
    "        \"threshold\": float(thr),\n",
    "        \"confusion_matrix\": {\"TN\": TN, \"FP\": FP, \"FN\": FN, \"TP\": TP},\n",
    "        \"accuracy\": float(acc),\n",
    "        \"precision\": float(prec),\n",
    "        \"recall\": float(rec),\n",
    "        \"f1_score\": float(f1),\n",
    "        \"sensitivity\": float(rec),\n",
    "        \"specificity\": float(spec),\n",
    "        \"mcc\": float(mcc),\n",
    "        \"p_value_fisher_two_sided\": float(pval),\n",
    "    }\n",
    "\n",
    "def save_roc_curve_png(y_true, y_prob, out_png):\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_prob)\n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr)\n",
    "    plt.plot([0, 1], [0, 1], linestyle=\"--\")\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(\"ROC Curve (Val)\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_png, dpi=150)\n",
    "    plt.close()\n",
    "\n",
    "def save_confusion_png(y_true, y_prob, out_png, thr=0.5):\n",
    "    y_pred = (np.asarray(y_prob) >= float(thr)).astype(int)\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=[0, 1])\n",
    "    plt.figure()\n",
    "    plt.imshow(cm)\n",
    "    plt.xticks([0, 1], [\"Healthy(0)\", \"PD(1)\"])\n",
    "    plt.yticks([0, 1], [\"Healthy(0)\", \"PD(1)\"])\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"True\")\n",
    "    plt.title(f\"Confusion Matrix (Val, thr={thr:.2f})\")\n",
    "    for (i, j), v in np.ndenumerate(cm):\n",
    "        plt.text(j, i, str(v), ha=\"center\", va=\"center\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_png, dpi=150)\n",
    "    plt.close()\n",
    "\n",
    "# -------------------------\n",
    "# Seed control\n",
    "# Purpose: stabilize training order and results per seed\n",
    "# -------------------------\n",
    "def set_all_seeds(seed: int):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# -------------------------\n",
    "# One seed run (train + val)\n",
    "# Inputs: seed, train_df, val_df\n",
    "# Outputs: best_heads.pt + plots + metrics.json in run_dir\n",
    "# -------------------------\n",
    "def run_trainval_once(seed: int):\n",
    "    set_all_seeds(seed)\n",
    "\n",
    "    # Purpose: keep D5 run folder names stable and easy to scan\n",
    "    # Output: run_dir for this seed under EXP_ROOT\n",
    "    # run_dir = EXP_ROOT / f\"run_{dataset_id}_seed{seed}\"\n",
    "    # run_dir.mkdir(parents=True, exist_ok=True)\n",
    "    run_dir = EXP_ROOT / f\"run_D5_seed{seed}\"  # modified to consider the new 50/20/30 splits path\n",
    "    run_dir.mkdir(parents=True, exist_ok=True)  # modified to consider the new 50/20/30 splits path\n",
    "\n",
    "    # Purpose: create loaders from the manifest tables\n",
    "    train_ds = AudioManifestDataset(train_df)\n",
    "    val_ds   = AudioManifestDataset(val_df)\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=PER_DEVICE_BS, shuffle=True,\n",
    "                              num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY, collate_fn=collate_fn)\n",
    "    val_loader   = DataLoader(val_ds, batch_size=PER_DEVICE_BS, shuffle=False,\n",
    "                              num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY, collate_fn=collate_fn)\n",
    "\n",
    "    # Warm-up\n",
    "    # Purpose: ensure the training loader is working before epochs start\n",
    "    print(f\"\\n[seed={seed}] Warm-up: loading 3 train batches...\")\n",
    "    t0 = time.time()\n",
    "    it = iter(train_loader)\n",
    "    for i in range(3):\n",
    "        _ = next(it)\n",
    "        print(f\"  loaded warmup batch {i+1}/3\")\n",
    "    print(f\"[seed={seed}] Warm-up done in {time.time()-t0:.2f}s\")\n",
    "\n",
    "    model = Wav2Vec2TwoHeadClassifier(BACKBONE_CKPT, dropout_p=DROPOUT_P).to(DEVICE)\n",
    "\n",
    "    # Purpose: only train pre-head blocks and heads (backbone stays frozen)\n",
    "    trainable_params = (\n",
    "        list(model.pre_vowel.parameters()) + list(model.pre_other.parameters()) +\n",
    "        list(model.head_vowel.parameters()) + list(model.head_other.parameters())\n",
    "    )\n",
    "    opt = torch.optim.Adam(trainable_params, lr=LR)\n",
    "\n",
    "    use_amp = bool(USE_AMP and DEVICE.type == \"cuda\")\n",
    "    scaler = torch.amp.GradScaler(\"cuda\", enabled=use_amp)\n",
    "\n",
    "    # Purpose: track the best epoch by VAL AUROC\n",
    "    best_auc = -1.0\n",
    "    best_epoch = -1\n",
    "    no_improve = 0\n",
    "    best_state = None\n",
    "    best_val_probs = None\n",
    "    best_val_true = None\n",
    "    best_thr_metrics = None\n",
    "\n",
    "    for epoch in range(1, MAX_EPOCHS + 1):\n",
    "        # Train\n",
    "        model.train()\n",
    "        train_losses = []\n",
    "        opt.zero_grad(set_to_none=True)\n",
    "\n",
    "        pbar = tqdm(train_loader, desc=f\"[seed={seed}] Train epoch {epoch}\", dynamic_ncols=True)\n",
    "        step = 0\n",
    "        for batch in pbar:\n",
    "            step += 1\n",
    "            input_values = batch[\"input_values\"].to(DEVICE, non_blocking=False)\n",
    "            attention_mask = batch[\"attention_mask\"].to(DEVICE, non_blocking=False)\n",
    "            labels = batch[\"labels\"].to(DEVICE, non_blocking=False)\n",
    "            task_group = batch[\"task_group\"]\n",
    "\n",
    "            with torch.amp.autocast(device_type=\"cuda\", enabled=use_amp):\n",
    "                loss, _ = model(input_values, attention_mask, labels, task_group)\n",
    "                loss = loss / GRAD_ACCUM\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            train_losses.append(float(loss.detach().cpu().item()) * GRAD_ACCUM)\n",
    "\n",
    "            # Purpose: update weights every GRAD_ACCUM steps to reach EFFECTIVE_BS\n",
    "            if (step % GRAD_ACCUM) == 0:\n",
    "                scaler.step(opt)\n",
    "                scaler.update()\n",
    "                opt.zero_grad(set_to_none=True)\n",
    "\n",
    "        # Purpose: handle leftover steps when epoch length is not divisible by GRAD_ACCUM\n",
    "        if (step % GRAD_ACCUM) != 0:\n",
    "            scaler.step(opt)\n",
    "            scaler.update()\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "\n",
    "        avg_train_loss = float(np.mean(train_losses)) if train_losses else float(\"nan\")\n",
    "\n",
    "        # Validate\n",
    "        model.eval()\n",
    "        all_probs, all_true = [], []\n",
    "        vpbar = tqdm(val_loader, desc=f\"[seed={seed}] Val epoch {epoch}\", dynamic_ncols=True)\n",
    "        with torch.inference_mode():\n",
    "            for batch in vpbar:\n",
    "                input_values = batch[\"input_values\"].to(DEVICE, non_blocking=False)\n",
    "                attention_mask = batch[\"attention_mask\"].to(DEVICE, non_blocking=False)\n",
    "                labels = batch[\"labels\"].to(DEVICE, non_blocking=False)\n",
    "                task_group = batch[\"task_group\"]\n",
    "\n",
    "                with torch.amp.autocast(device_type=\"cuda\", enabled=use_amp):\n",
    "                    _, logits = model(input_values, attention_mask, labels, task_group)\n",
    "\n",
    "                # Purpose: use PD probability from softmax(logits)\n",
    "                probs = torch.softmax(logits.float(), dim=-1)[:, 1].detach().cpu().numpy().astype(np.float64)\n",
    "                all_probs.extend(probs.tolist())\n",
    "                all_true.extend(labels.detach().cpu().numpy().astype(np.int64).tolist())\n",
    "\n",
    "        val_auc = compute_auc(all_true, all_probs)\n",
    "        print(f\"seed={seed} | epoch {epoch:02d}/{MAX_EPOCHS} | train_loss={avg_train_loss:.5f} | val_AUROC={val_auc:.5f}\")\n",
    "\n",
    "        # Purpose: store best weights and best-epoch VAL threshold metrics\n",
    "        improved = (not math.isnan(val_auc)) and (val_auc > best_auc + 1e-12)\n",
    "        if improved:\n",
    "            best_auc = float(val_auc)\n",
    "            best_epoch = int(epoch)\n",
    "            no_improve = 0\n",
    "            best_state = {\n",
    "                \"pre_vowel\": {k: v.detach().cpu().clone() for k, v in model.pre_vowel.state_dict().items()},\n",
    "                \"pre_other\": {k: v.detach().cpu().clone() for k, v in model.pre_other.state_dict().items()},\n",
    "                \"head_vowel\": {k: v.detach().cpu().clone() for k, v in model.head_vowel.state_dict().items()},\n",
    "                \"head_other\": {k: v.detach().cpu().clone() for k, v in model.head_other.state_dict().items()},\n",
    "            }\n",
    "            best_val_probs = list(all_probs)\n",
    "            best_val_true  = list(all_true)\n",
    "            best_thr_metrics = compute_threshold_metrics(best_val_true, best_val_probs, thr=VAL_THRESHOLD)\n",
    "        else:\n",
    "            no_improve += 1\n",
    "\n",
    "        # Purpose: early stop when VAL AUROC does not improve for PATIENCE epochs\n",
    "        if no_improve >= PATIENCE:\n",
    "            break\n",
    "\n",
    "    # Purpose: avoid writing partial artifacts if no valid best epoch exists\n",
    "    if best_state is None or best_val_probs is None or best_val_true is None or best_thr_metrics is None:\n",
    "        raise RuntimeError(\"No best epoch captured. Validation AUROC may be NaN due to single-class validation split.\")\n",
    "\n",
    "    # Save best epoch artifacts\n",
    "    best_heads_path = run_dir / \"best_heads.pt\"\n",
    "    torch.save(best_state, str(best_heads_path))\n",
    "\n",
    "    roc_png = run_dir / \"roc_curve.png\"\n",
    "    cm_png  = run_dir / \"confusion_matrix.png\"\n",
    "    ytrue_np = np.asarray(best_val_true, dtype=np.int64)\n",
    "    yprob_np = np.asarray(best_val_probs, dtype=np.float64)\n",
    "\n",
    "    save_roc_curve_png(ytrue_np, yprob_np, str(roc_png))\n",
    "    save_confusion_png(ytrue_np, yprob_np, str(cm_png), thr=VAL_THRESHOLD)\n",
    "\n",
    "    # Output: metrics.json captures the best epoch and the extra threshold metrics\n",
    "    metrics = {\n",
    "        \"dataset\": dataset_id,\n",
    "        \"seed\": int(seed),\n",
    "        \"best_val_auroc\": float(best_auc),\n",
    "        \"best_epoch\": int(best_epoch),\n",
    "        \"n_train\": int(len(train_df)),\n",
    "        \"n_val\": int(len(val_df)),\n",
    "        \"label_counts_train\": train_df[\"label_num\"].value_counts(dropna=False).to_dict(),\n",
    "        \"label_counts_val\": val_df[\"label_num\"].value_counts(dropna=False).to_dict(),\n",
    "        \"experiment_tag\": EXPERIMENT_TAG,\n",
    "        \"run_stamp\": RUN_STAMP,\n",
    "        \"dropout_p\": float(DROPOUT_P),\n",
    "        \"lr\": float(LR),\n",
    "        \"effective_batch_size\": int(PER_DEVICE_BS * GRAD_ACCUM),\n",
    "        \"per_device_batch_size\": int(PER_DEVICE_BS),\n",
    "        \"grad_accum_steps\": int(GRAD_ACCUM),\n",
    "        \"val_threshold\": float(VAL_THRESHOLD),\n",
    "        \"backbone_ckpt\": BACKBONE_CKPT,\n",
    "        \"threshold_metrics_best_epoch\": best_thr_metrics,\n",
    "    }\n",
    "    metrics_path = run_dir / \"metrics.json\"\n",
    "    with open(metrics_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(metrics, f, indent=2)\n",
    "\n",
    "    print(f\"[seed={seed}] WROTE:\")\n",
    "    print(\" \", str(metrics_path))\n",
    "    print(\" \", str(roc_png))\n",
    "    print(\" \", str(cm_png))\n",
    "    print(\" \", str(best_heads_path))\n",
    "\n",
    "    return float(best_auc), str(run_dir), best_thr_metrics\n",
    "\n",
    "# -------------------------\n",
    "# Run all seeds + write exp summary + append global history\n",
    "# Inputs: SEEDS list\n",
    "# Outputs: exp_root/summary_trainval.json and one appended history_index.jsonl record\n",
    "# -------------------------\n",
    "aucs = []\n",
    "run_dirs = []\n",
    "per_seed_metrics = []\n",
    "\n",
    "for seed in SEEDS:\n",
    "    a, rd, thrm = run_trainval_once(seed)\n",
    "    aucs.append(a)\n",
    "    run_dirs.append(rd)\n",
    "    per_seed_metrics.append({\"seed\": int(seed), \"best_val_auroc\": float(a), \"threshold_metrics_best_epoch\": thrm})\n",
    "\n",
    "t_crit = 4.302652729911275  # df=2, 95% CI\n",
    "n = len(aucs)\n",
    "mean_auc = float(np.mean(aucs))\n",
    "std_auc = float(np.std(aucs, ddof=1)) if n > 1 else 0.0\n",
    "half_width = float(t_crit * (std_auc / math.sqrt(n))) if n > 1 else 0.0\n",
    "\n",
    "print(\"\\nAUROC by seed:\")\n",
    "for s, a in zip(SEEDS, aucs):\n",
    "    print(f\"  seed {s}: {a:.6f}\")\n",
    "print(f\"\\nMean AUROC: {mean_auc:.6f}\")\n",
    "print(f\"95% CI (t, n=3): [{mean_auc - half_width:.6f}, {mean_auc + half_width:.6f}]\")\n",
    "\n",
    "exp_summary = {\n",
    "    \"dataset\": dataset_id,\n",
    "    \"dx_out_root\": DX_OUT_ROOT,\n",
    "    \"experiment_tag\": EXPERIMENT_TAG,\n",
    "    \"run_stamp\": RUN_STAMP,\n",
    "    \"exp_root\": str(EXP_ROOT),\n",
    "    \"run_dirs\": run_dirs,\n",
    "    \"seeds\": SEEDS,\n",
    "    \"aurocs\": [float(x) for x in aucs],\n",
    "    \"mean_auroc\": float(mean_auc),\n",
    "    \"t_crit_df2_95\": float(t_crit),\n",
    "    \"half_width_95_ci\": float(half_width),\n",
    "    \"ci95\": [float(mean_auc - half_width), float(mean_auc + half_width)],\n",
    "    \"n_train\": int(len(train_df)),\n",
    "    \"n_val\": int(len(val_df)),\n",
    "    \"label_counts_train\": train_df[\"label_num\"].value_counts(dropna=False).to_dict(),\n",
    "    \"label_counts_val\": val_df[\"label_num\"].value_counts(dropna=False).to_dict(),\n",
    "    \"effective_batch_size\": int(PER_DEVICE_BS * GRAD_ACCUM),\n",
    "    \"per_device_batch_size\": int(PER_DEVICE_BS),\n",
    "    \"grad_accum_steps\": int(GRAD_ACCUM),\n",
    "    \"val_threshold\": float(VAL_THRESHOLD),\n",
    "    \"backbone_ckpt\": BACKBONE_CKPT,\n",
    "    \"dropout_p\": float(DROPOUT_P),\n",
    "    \"lr\": float(LR),\n",
    "    \"per_seed_best_epoch_metrics\": per_seed_metrics,\n",
    "}\n",
    "\n",
    "summary_path = EXP_ROOT / \"summary_trainval.json\"\n",
    "with open(summary_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(exp_summary, f, indent=2)\n",
    "\n",
    "# Output: one-line append so older experiments stay intact\n",
    "history_path = TRAINVAL_ROOT / \"history_index.jsonl\"\n",
    "with open(history_path, \"a\", encoding=\"utf-8\") as f:\n",
    "    f.write(json.dumps(exp_summary) + \"\\n\")\n",
    "\n",
    "print(\"\\nWROTE per-experiment summary:\", str(summary_path))\n",
    "print(\"APPENDED global history index:\", str(history_path))\n",
    "print(\"\\nOpen this folder to access artifacts:\", str(EXP_ROOT))\n",
    "\n",
    "# -------------------------\n",
    "# Stop runtime\n",
    "# Purpose: release the GPU machine after outputs are written\n",
    "# -------------------------\n",
    "print(\"\\nAll done. Unassigning the runtime to stop the L4 instance...\")\n",
    "try:\n",
    "    from google.colab import runtime  # type: ignore\n",
    "    print(\"Calling runtime.unassign() now.\")\n",
    "    runtime.unassign()\n",
    "except Exception as e:\n",
    "    print(\"Could not unassign runtime automatically. You can stop the runtime manually in Colab.\")\n",
    "    print(\"Reason:\", repr(e))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "c0c2cd530e7741568cc15fab546078b2",
      "cf34935d16674eb0a255577379f68bad",
      "0336ebc0d89244b69e9e0a6872e860f3",
      "9de39e69109744cc97b27f4ae28af6f9",
      "19c6a34c665f40528dd3a136d2ab0ca6",
      "8cb8a44503454644b5ed10c392def721",
      "e660c9cd83564f52b110bd559f612d4e",
      "3a0afded65394f7e91bb071c8feac0df",
      "4686b0b3b62843c68357311b61ec3240",
      "cc94f46cceb54e56837d795bd82176c2",
      "217b543d62454b7d860648be1be3e7f7",
      "74ca22ece7eb4fccb16f719307d0186a",
      "6e7bd833b79d4cf7b90eb0cd800f5c79",
      "ceea69a4d816498ba814883d5f89c3c2",
      "514431fe02eb42058ad325752ce333d7",
      "ab10ef758e864c34a864a6d5882cd0a3",
      "90d0ea13d65e4e4789f8412344ce8f15",
      "9e551bf8b79a48ad8bc7fc400b06ae98",
      "e5d92312d03240339d8c1e4357957ba3",
      "21fe5190b22e4cbc883f99c8566801d1",
      "f7d44de20cac408e8b0a54f48812ddba",
      "a869001311f04db6a7193701294bac61",
      "2eaf55dee9444df4ba92bec1f7736884",
      "a82980627e8843cfbe8717c38d72e457",
      "7c729ea38d4c4c5e9d555f13262efbff",
      "bc55f80422c64dfd8468951975769713",
      "db33af613e4f47a991725dacc3593ebc",
      "ca512e42876a4f638002db52a85f6397",
      "b8e66068182540a99242c03f09a9da53",
      "bb7da9830e7c46cc89e4599175829b2a",
      "eba88f58a32e4938b4e9d95e0300315e",
      "97222ac534bc418ab81fe0c97f56dc44",
      "a7682d4b18114030bb1c80e14ec8f0ad",
      "50d9b01a785a4c0896de85a3ca89ab91",
      "aa241970e873437ab7a6cd4e12bcd617",
      "24b7d34a8adc4c8dab8a2d56c83e210f",
      "fa6dce66711848e08f175299c433800b",
      "1e7b1b81e7484fb48fb7ecee75bb4b26",
      "b869492cfa174907a4addc35c39a26a2",
      "2366028e765140c88bd18f300e82c643",
      "0ab4c29e88394c858ffdea5bc6f1f8b8",
      "71b06f9615ba417a8cb05d683124891e",
      "f4414cca09a348faa6b6e932ce3ef184",
      "a16941042a894dc8acf0e8526dc40413",
      "371497426bb94022a53a21457289cb06",
      "6256891d48da4c5896be6771e65e8ad2",
      "48141dd8cb6b41458d88803bea672c24",
      "b9b278a91ff446cf8e14dc8dbf43fdae",
      "15fc18494210418280088962c1c9ae03",
      "029ebe177d9e465190fbe082a6c8c1c6",
      "204d2602e9e243d582533eb8fc7d44e5",
      "e8d2e441549546b3b6373df9d3f6fabb",
      "6d15b9b3d5964372a5734f8caa278aeb",
      "c9d563458bb6443f998a89935d57425f",
      "e5510fca4b88415e841fd3aae6e03be9",
      "35c9ae6d7053419ea8a1aa7d65db5750",
      "b47cf2970efa44e988884579b0ed879d",
      "991511d619304733a12db92d37073bc2",
      "cf42cabf51f34d748381e1bbf5479146",
      "8d2dc40704384576b9537b2b126730ee",
      "7dd5eae4128949e1ab725ee60759e7f2",
      "629432181d3c44eea7a99c124936a20c",
      "d06fc0c9c544443db425fe21820143b8",
      "b279d2aa3f2545afa7a52b07a2e7f049",
      "bc744e6f8ccb4ee297d5a56d6d894b77",
      "1b7721569ae246c9ab455b249f248be7",
      "367fb7aa0cf746faa83663e1a023d385",
      "949b2dbaaf8f41ffac070c1aa798bb58",
      "7b886da9663042329b44bd528f71da9e",
      "9e117ea3512641bbb75db6d47b71f264",
      "268fcd604b514fd288d6d8422334aca7",
      "5efe1dcd0ad74a02a2af4306f902dd25",
      "99f2ef32bf7e470197d475c242920027",
      "aabfe24e995c4e198435be778d936f60",
      "4b3784bf9aef4a4fb8dd40c3e47dcab3",
      "7d5f0ce62d6a474a901493b58aa4a8f1",
      "af42cad8b7634ea2b06cf40e3f194bf0",
      "f9af48de34e64969b972ca1728eff43e",
      "b2669f4bfb134ed2910da71df7a0f0e3",
      "c42d1381d76e491188c30d9e962272d2",
      "4424b1f599a34f3980e7ed2d2651c247",
      "2f0f1bf551844728a64a951741694b6b",
      "92a1a795509147b7a2a2509c036b1cd7",
      "fc65674c540c428a88239554c2248d1b",
      "9bf799642f654a689d69ae78c2882687",
      "b75b4a31a044450faf6d3dbde175ff67",
      "6397fe74ca724db489011ef3c2c9fe9f",
      "ec48c790bd344e6a83f2a13003d0190a",
      "e0452235e06a41a397fcdfa408b43647",
      "11eeee7318b049d585748f3385caec60",
      "2646461c0b3c458a9019b5cb6ea5ab84",
      "1702b8d224784d9c8ed74bfa4971490e",
      "4cafa01212694304b489d5d0a726d6f6",
      "aa9365853084405dae5fdd17ee47c82d",
      "26f20b136f9046a490e34680fa887acb",
      "e4117337aff94cf5a453dab048bb0993",
      "7280b561c6144153b77190a6855e729c",
      "eb19b3536d0f46fb94d344390fb19ba4",
      "9363c31ad1b643a69ffbefae46074533",
      "babc03c54aaf44c8b068badbe8b905b4",
      "c46742402e1c46fab8973f4e22cb0521",
      "4d40dad6961c4e18b6f76653fb8d8749",
      "9caa7de93ade45e79ffad45c6ab776fe",
      "da83e8a2c2544823ba872293dd4f2ce9",
      "1d27b05749614a9ba49d8f211902c431",
      "6485b6e0c48d4401bdcb89290f638cc8",
      "ce5ab57ba8a149de829bf11ff8dcc543",
      "c132e967481a456b8ae54e949f373002",
      "280a0e179d4c461aa29ce66d46441e2d",
      "dfe1b0788018470fa1604794c33ee2e5",
      "eecdf365abfc4b1a9ad9db8dec3a8b0a",
      "58f7df0db3a74f7c84961d7b07a70e74",
      "d784b53732d8481e94cb821cd58d8cd4",
      "2dc93ec727ac433aa8ba180d9c8bf2cf",
      "66ab25b9b68544808471b496ade593e4",
      "4f4fe585061a49e3b965a958c995ed63",
      "fa1c82f2291d4b4e8e72780f7007c87a",
      "0f620cc8a425416092e1b29e51f52b25",
      "a4bb58cc3c87416188acd2b7e6ec1a95",
      "f56e0c61c39047dfa12403f91a2cd0d3",
      "7cb36a8a0fe74263aadda89050760d18",
      "a152b5b6264c493380dd2d29c0fcd504",
      "cb5106ebbb2c4c4bbb0e030bef35c593",
      "8311093324d744c5a940ccbeabe7e37b",
      "70e56f594f15408ba03caff736c3d6d9",
      "b4b8b7d4c62b44af95061d4132fc0183",
      "3535b4116e3244289956906a1e887bda",
      "55f524135d45496fad8bee0665ea0961",
      "f64dd379b42b45e6b28b0a6b6f4fd4f5",
      "59403fc005094ae8b713b66e7361a681",
      "c8f4cadf058f4340a301a000ce5c3290",
      "24b6675c8108454d8877e600fd6c190e",
      "b62d860c1434429abb377796a5842587",
      "547acaa915e7489c947de53cabaa338c",
      "010668d5a8334deabefdea88a2616a97",
      "5b6f08d7f91340ada84a381b2721913e",
      "16564677f919459481b5298b5f5cb319",
      "55a6e6412d914d4aa65bb2fa582d2fbd",
      "f1942fd5761b4c6ea5296ec79f1db0ba",
      "bcb07d9906d54b199d10ac96fc66719b",
      "8bcd9c1bff4547c58ea81d18f32ebf47",
      "3e3c57d40dae4c0aba66ca910f64b815",
      "b01883a544c94139920cdec07d780e59",
      "290d6b63b30444adbc2f4769fc7244b2",
      "2fdd053774a74fda8e13252eccc7ef65",
      "aa87d76e94c4430c840af7d0e910ad9f",
      "2802142fae274afab6a29e9fee61e61d",
      "8c6524c66acb4fdea84f60114067aeea",
      "131fde14252e455bba54fa742e67df08",
      "a745e231db96498fa98a3524f13586ef",
      "862fdbe22834431da6a33f4725b73d52",
      "48764852e70b47bda66ca3cd98db5279",
      "0d525da7477a48899f495efab63beaa0",
      "9181355ceb6d46929727efe1455a7dff",
      "db885c808118435282032be491439b79",
      "e30c456ad75146a2a634072681599af8",
      "700f3867f96344c08f836843c608b79f",
      "92af0064aef34760bd54658f546f51d4",
      "ae394b571f5a4cc08fe563a4987e8aaf",
      "1205bdc0168a4e8bb62cc25075f52b5a",
      "3ee37ca768b2440da63e025a9359a7b8",
      "e50b6fe1eb7b4279952d9e565d883c2a",
      "ce407796ad2749e7b9e83fea4474ef67",
      "74a328bb683743b8b233105110aeac24",
      "bbc6854a380c4559afbe0d93fe17715b",
      "6bd74795b3694ca7b4ae3ec47740afe3",
      "020f2b0cccb9445796bae647885070db",
      "199ff44c17e645dcb4e6308f882e0ed1",
      "60b5e7a0b4bd4786a720d4615642e417",
      "e78e4083e52745499f706927a1ec88be",
      "451e102b373c4f808088d60a0ebcd4da",
      "faae269ec2584f548d16e4040de65ee3",
      "ab3ab5dfaf3742f59675fea77ed725b8",
      "3db6b286e4d14e3b91f8ca741c1c1caf",
      "b7d73794b57c41d6be58602f6d77d8ae",
      "121ba272c03a4904a114e8692c7d4334",
      "4afa0779b62d4f25b168fd0cce2c5d2c",
      "e2f088275cde49829ca9573091e0a512",
      "8e13ca5475ba4285b89c5d3d5301c0a1",
      "5d9cb040b1724075a2139d4eb11c98e9",
      "95959e0a5fdb4d789f22c9445fac91f7",
      "75d370ba518f4666ab03ce4bf2dcafeb",
      "ce449b02ac4647f48cdff830a8f83190",
      "22dc150fe6f748459bab4e98a2ce4e22",
      "5b5bb69fdd0946d29deccd28d4266633",
      "38dfcdb4ffe04193aff9a07d515e7f3d",
      "275a71466f3943b5a0b1d171799e6b2c",
      "2362634bf25e45d4893cdc7dc3054a97",
      "e6b6ebaafff941d8b1c660ef273ad516",
      "bf8a81f2f2eb44cd9d929dc9ddd5bd47",
      "c23c81eca5694d34a3b98d3d59966ff1",
      "4b4bc432f458441c99c6483d8fba3c84",
      "bdcc26f2397c494e9f6f8d6de6f9cb9c",
      "e0738906ec544df090f1dcc8b90b5098",
      "a10f363d14eb4dd4a56d8abe3727db00",
      "2d056d16fba34072b610b71a97afc9d3",
      "496669345dec446f9043bd56595b40b2",
      "87bd94084e65478a8e1675b7f389e87a",
      "b046c896145545b2a0cf3bb9308b3075",
      "898c5bf7e58f4372a488cbc90ee7ecd5",
      "45a5ac10befb4c68aba2764e932c4b1d",
      "c2238db3303a4b51b9fd79e3a181bf10",
      "e4c0795670524f418df39ee02e86a701",
      "770625183f8f4c759cedd5745b417f61",
      "5e2d3b5395a34dee91990b35b30c8a93",
      "4046d06379204eeca108b6f1ae3d3cad",
      "4b3891b94ac140409770761089e91623",
      "142a620ef05649d1a67af50a79a7e4d7",
      "80dda3f523a6438b9480f618a1c1e06e",
      "a59fb091875e4368b283554135d1519b",
      "96c09c91c6484b2e94a559c9e38b0e76",
      "d8b7dfe179dc4d13a449b81338755b52",
      "48b2fae0824a40ce96e0698c6a0a8a5a",
      "0c2effef3e4a4cb4a3bce9cc30a30969",
      "35b5530f4bb347138812a9253f2cd9db",
      "b8cb66e76c924bd0809938fe654c3550",
      "32fc3b1236e64fa8937e670468d04e06",
      "7875c5d9cae5480680240606571d8917",
      "cf512df5c8f147118465f412d0ea0083",
      "159707f6bc3549ac90dba36f3e67d765",
      "db51f9e8d5124e4284cf5b8a435c9b55",
      "1f7d8ed8180b4d52ab5bd3dbe6d1ccb0",
      "0d9b7a53d3e64d63aadfe5a9557ce87d",
      "dc1576fb0c994290939782f63aa8f456",
      "516cd010139c4b769ca5be61f15150ff",
      "0bfdb4af6a8b41118552a599900f08a7",
      "3151a0aecc4f4852bef81be60f9d52c8",
      "ae976cc2af994c90a3ffdbe6de8f3f64",
      "6d18139ad6cc436c9afac5883c8968b7",
      "93dad9587c264e2a80a4fdc9c6a34ab9",
      "e2738a641d08464ca3d47bb607df6d0b",
      "c772b4832a8744309ba497abbcdbb9ca",
      "bfda22c7de7b4a578def2ce2ea2c9571",
      "9c46a8aecbe249e797a1b524f3b8f539",
      "7e8ef20e46dd42688420fe6c86441cdf",
      "3e6f683e7ab34d7982938c0e2f8c0621",
      "1290016d17cc43b8bb4d68c1fd2fea4a",
      "1271fa71d32b4669878d3c5552d5ec6c",
      "cd8326b5434e43f2a2e67177078b864a",
      "0d1c06dbd74d47aab8238d1d4fe98d87",
      "cad01ead2c5547658ffd593497948e98",
      "e2a43fc3a45b43609b416f2382fcca79",
      "adb65bf3dcc04b07bb9c4dbf7b5f282b",
      "a61a3a5df2864e0d9d886e8766848cb1",
      "179d809b9d574889bee6f9df747bba1a",
      "41d1ebca73114eaaaea5304d24bf8f32",
      "683bb715f1394db2906238fdb4de877d",
      "6efa3bb41415493f88a3e2546f68abea",
      "e79ac894c7d8489c9765010fe70861d6",
      "b31da1dad1a2454faaf5c1dd4c64fff6",
      "719dbfc855be49a0ab37d162bd7d501f",
      "d6b634aab6c4416983d719f1cb52d81c",
      "70625432f9774f468f9334a8e39c1484",
      "84f2c4d0e16f4cbe889fc14316ff0bd0",
      "726137e107e84a279db9bf60779dda43",
      "1ae0fdbb64e44da4adcb34eee7ccfc45",
      "2b78c2c993514547b4bb64519bacd9b2",
      "222a7d8570f443bb8544009da0caf93b",
      "dbb397d3853242f8ab8e97adb396a65d",
      "bc897ac24d75475fb20d0ed89da64fad",
      "0657e6b45ced40adbc0a728fcfc355fa",
      "faf139c87fca40c9a7432df2265fca53",
      "92d947fd19f24a498452ec25c7b821bd",
      "4c80e806f134440496b477e19a00953c",
      "69382d6ebb6c46bca85124db37a4d006",
      "ad7703362c9443bfb4618f526412b845",
      "7ae5ff4fdb694a31b0f04ee2fcfa9d32",
      "ab3c2dc001ea4e848cda202a1c03cfd4",
      "8cde658a3f4e4194bcbd2017bff02b01",
      "3aff7f8a57aa44bc946051557ed2386c",
      "f6bb1e4dce8b4563886009e7ee6beb86",
      "1d7d5e57d4674f3bb12b8c500207dba5",
      "9ad4c52d08c842ddbef03c9a58ae3f2b",
      "985b8d5545ae4052a217e3b7256d463d",
      "18bef0e937e24c6f910a3486c200594c",
      "d0d947d988f44781ae7df8fcdcae02bf",
      "5ab68600ffb5462a9955fedadce41349",
      "b988e00d0b134ce48d4f74dded0c935f",
      "1f5935a7ff384ee885a85ad08ab07169",
      "dcdb2efb4e73448eb0b7428a53190d54",
      "c0a8b5250af54542a5b2cf761b7d5bce",
      "10715724ca71476784f73a9a3ad8b1b2",
      "860157f874e4405c87503ee629212c79",
      "68ef8219a719422e990ae46b5f983663",
      "7cccc713f0ad4e3db1cdab1dee879cbc",
      "9f008719500a4d7097156537e7fe115b",
      "15bc0bf17140476da66ec809969d7cb9",
      "f8813e6b742343cf806905af9a251648",
      "6781a39cebe54a238bde1bf57f8ae99c",
      "8325d15251a445e9a5e3c75a398e1860",
      "d6641cf9dd414115958a5ff2736e0c08",
      "317680f5e3fa408c9d8f042996843e5d",
      "0282cb6c01714ef2a2a69404c741ec3a",
      "d2f500b793b14439978a0298605d6d3f",
      "611f2f77ebe448c8830a1535eb20ef13",
      "710e24323285491f9dd1163496abcdba",
      "f30f565001594ce0af1941396515a8d8",
      "a900a5b8468a4318a618eec5aa69edf3",
      "21ad020bd46f4ebebbaee1e214ef22cf",
      "e08d2ec766cb4f06b070d1ce8f626be5",
      "860671c673e14fbe8db80145dab85671",
      "90aad4b9adf84c6ca7fd3e919f83e110",
      "1c91eda82a8e42e7abb21dba67daf38c",
      "6379ffd54afb472d8be97f0f6e2eb56f",
      "8271d43f73654267a46b200faa86e0f6",
      "5f0f72d977024e2c9597d36fc3ee5489",
      "2fd48a06f9fe4d5cacec71b27d587679",
      "ec6f55b5b3744f81b140ec3f3d5dc417",
      "f805ea27acf6486e8d325063f70e259a",
      "99e218bffda449f49b1a4565f5354169",
      "77c9aee983de4a309d7c81f758051764",
      "c8a0429044994d5aacc8f663b15abfce",
      "bcfafb5b1028444b83a944c82316cdfa",
      "31757d17a8f4430bbef9fbf4e3de25ba",
      "7b8c8f8ef05344a289794ed872b8c079",
      "ed8ad92e280b4e059021ad60fa897d06",
      "ee7f2815cfb7480b8f6d1776200d6201",
      "42fec410f6164294bd23875271378527",
      "d3a37ce74af4433389562d85242115d8",
      "e9d1f4a8ac4447aab15f97cb23d04be4",
      "352296aaae0e445589fec2f241c5e7b6",
      "57a58da441fb4ddbb62fe851700e4691",
      "90be8d69c29b4c2aa548b640221e2b9f",
      "917e84fe0bbb402783ce9ebe989bc0cf",
      "ad90496d83684592a42bf1c83200a70c",
      "fb0fb5066e9e4ddf9b8f93b8a85df515",
      "acbddfbcd28948dd9acce8491e3d3799",
      "fa3eda8e28664f79be4b3427564bbb35",
      "eae32a2554564e16ae11e24592a237ee",
      "094b6fde12284fb38fd946eae7caae7d",
      "09c14f1a353e454f91c352acbefcd701",
      "0aa7c67e05114d6b802de6c2961718d7",
      "ea46b3f7f5934e24823f4019b554b10e",
      "c4a36455d1534aaab4541d7b3383fed5",
      "4752cda76ca54a23bba7acbcbb1dda5f",
      "8d8c4e869f694d11a087e71b6fe5bcf7",
      "24d26a187c4f4b09bce79a06e507dc4a",
      "70785423283d40388c7ad9cffe5b1dd7",
      "97e4fa6d9c2741cbb408963310db55d1",
      "7a26f00a2dcc4098880bf4fa6e8ca0a6",
      "186659fca3994dbb8854115a7897ef24",
      "a914510ddd4c4bde82666b9cc7006489",
      "becc2ae698f24722bc659ca8620de989",
      "422d4e354ac54fc19f7e953a9a5a26b6",
      "d4dc78aba7774e2e897668409e21c74e",
      "0f6e0222a70e4211a3d78f846e210b79",
      "116484030d694fedb33f94369c03185f",
      "52414c67aeda45528dec1054d76233e6",
      "b06b16c97bf84897abbadbf3d2021fcc",
      "09992b6643ad4fcb9b46ba9a2b56201d",
      "9a2f9f5af96b4cc79d1459480ef35e8b",
      "ccb7aaba31f44ba5b494259fdfd700e2",
      "d3d1b7c1a04545d091a24c37f3ddacba",
      "40669d242b8b4fdf90165b68277a2c67",
      "45b5ed037d624e56b5023a828e6641ad",
      "2fdf09cd19df4ddeb787d096fe7cedbe",
      "72ac4f6ac63840d2977408f9c9dcc5fa",
      "ce65c1130c794356baa245e3e4371d3e",
      "4272baf5e2d845c8b46f41aff1e4ff1c",
      "61c2cb230139407ba3076d43b9e783fd",
      "0c30bd258e504162ae298df1b565e3bf",
      "8d546e7bdfc644cdaa4fb97681a3427a",
      "dc02752ac33e497581dcf8365c51b18d",
      "f2df6ae611ae40588c74bd042d28084b",
      "9f8cd7d31b3c4c09866af45e86910d65",
      "26fd81841d324171bc93ac797568af4c",
      "94d1eb7d29304ca08d82cd1077e8eb8c",
      "53e371b6f8c4485fad5f6109983009dd",
      "7816dfb28ba84f99a65f0d3648af443a",
      "e12c8b7a679b4b009bf40a2d36802939",
      "fa154d23f7294488943c86e338ba1123",
      "b4c3d3e9a8804734a89c3e9a2b75f924",
      "ab15f92b0a5d4474bd507bae982956ef",
      "79425aeb956347e0bdcc1a271e60a0e9",
      "cdaeda05a96647558a58b0f7178e0d91",
      "475a3a5d00814359b38a0a787f83cca9",
      "f4d16e593c244b6b80d6f519ec5a1f5e",
      "4745e2f16051465089075dc13d0a0090",
      "0035de49fb6f493a810b919b7ed10773",
      "de624fff5f35425faf8a8ba91ecd2bc1",
      "3d8e6edced3f46e19576f31fcdae4738",
      "4360087caac64e36a89e8f1c115b9c49",
      "acd388a056484b2aaad89005692c2a07",
      "9c383ec1a18f40a2b196bc3b5bea2be8",
      "65af01ef7e794a8684dca32086f33732",
      "b947c267c56c415a9c605758ea55d409",
      "d0255b1288314476ada79c7947e6ec36",
      "b496b120f4f44878a705c2eaea8356a6",
      "78d48785aa0a45daa3ae94eee6f664e4",
      "63c1b5efd511404eb2c4d9689e59aa31",
      "00c46d0c79c145d8b73103d0a894cd69",
      "90743b55f4a84a598125868292a8f113",
      "207983258c7f4d428d28508f3eee58b3",
      "0553d10f09644efd98635f0916c65bd5",
      "02458b2553754c51a0746ac680470b24",
      "904b64fa7b6c42b4935abcb6442262dc",
      "f07a6cc6c7d14066981c02aaf97ae797",
      "d422d5a9a7c749a7b6fee9c79cf9fdd8",
      "b9a24f3a5ea54033a872c25f0f8464f6",
      "a50504fd51484028a6c797f81d26276e",
      "32d46dd8aba9466396faf80d12f6bc61",
      "b89572d635d84a4fa0bbdf5b29f498dc",
      "6b46a80a1eff4959a25ed3d5e597e81d",
      "cc414609646e4a20be38b88035193d01",
      "1198b81c603b4c948ce1d693098ae96d",
      "2b8d1d5b2fdd48e490bf6cd7f12b1047",
      "395fdeb6999d4d63bc360fb610aa54bd",
      "7de16e957d5f4a8cb6b9131a5a2ebd49",
      "82f0143560934e3ab7d169366648d963",
      "99a428fa650a4fd08adaa9141773f04d",
      "a356908d5e364644b63a2ef36c2b8277",
      "cbc01c0ffa3b49ab951aa5cdda8206ae",
      "400a2e99a804431290a9256635e74cc9",
      "e7982e5151b74f7ab6a179ffcdc1786a",
      "4f99614bf6494da1b1fe481f99d2be83",
      "ab19632b3f474a80b820d9e590784216",
      "12d6d17190d54132a398a3ee62fac937",
      "db2cb74c622748f78f968549b279b878",
      "05a912538b314b84901cc092a74d0016",
      "6299825320f2489ba438db059f69e9ee",
      "ed0425a248b14941a5c7c2b4beefde2a",
      "34aca3a01fa34b459d1cc4bf4dfe9c87",
      "6d7290e735494f2cac323a11d303d1c3",
      "78fdcd068f18410188e04ce6e8d979dd",
      "325c69e55fd748adb43ac35ddb6023d5",
      "c184bbdf878f43b082c67ea7b213fe4d",
      "e52a8afeac4f4f2c9e2fab25fb51fdab",
      "ceeb26e13b2b4d5b947fe7518e4e1ba8",
      "2b55205689b94ba0ac3f674e40a30675",
      "211b6be5e1a547e387af63b017077c63",
      "51f3f47c3f634d52a7caf1ec651b1d3f",
      "df07a79681e14bb98cf5f0be187584eb",
      "39f67a55beb44b6489cddcfcf5d619e8",
      "ccdb353981f34978a769aa55d6510361",
      "ece1eb30a2b04926b2ab36deb244e800",
      "4450fedb385c4965b2a286e0feca52f8",
      "1ba682144ca3468b981c23c510d9db39",
      "f342af1e5b594401944105629312a866",
      "1b32e1935c244085a8085bd278aca278",
      "202c93743e244fd78a2ffe94a895ac11",
      "ec6a323a88724d7a91b5c5ee4e7f556a",
      "ffea80ea19244eb392ec195e7f293b48",
      "4876445834e04da9b99375d3c2629595",
      "12bd73b8b5cf4f5a8fd5c72359c6338e",
      "d717f1edb7014ffa980e7d215145ecc3",
      "92479c979ca04d118cc724d6b93e03a2",
      "f9cd4c144d794c5bb0d632257318d739",
      "1c7a5d84e0824930adfc05977e60628d",
      "7ed9d5e550944cb899728e2c0e455933",
      "2c73e2da9e2e44dc863d2a144b996ed0",
      "d1cbf0daf56b4673a85f6dbb0e3f61a8",
      "a52f652c767147fa860b7eebce5ed3ad",
      "29b0df299f764ab79f0ec6246fe5651c",
      "51b6028b40634b60978c19426b0e3cd4",
      "64f49935ad4b4aa8927f6583710e0659",
      "f5ff7a88351e47f89abb350fc2f202de",
      "dac60e4993a149399678df501b9625fe",
      "e2c305c681db4f9aa6563ce696172edd",
      "fb09e8fe94e544e786c0255860c6fe42",
      "df46f368c99d4b0a954ec25bde8ddb06",
      "dd03ec5770124d429745a8b8ea31bc76",
      "720a3b6419e9426ca3cbaa44794c0596",
      "4247dcb6f32e43e28fb33c86b4c21321",
      "de85691093b84d0a986f66f326a6ac68",
      "2bda6f3b2aca4a9ebf3e71746df81fbd",
      "9bc1507f07cc47a89734b905bf71fb4a",
      "9237c14b1c1646baa5fb1cbd58344395",
      "d87fcb697485406c9a1ef978c023ae5c",
      "62ed037bf4cd41f2a09fe24616896565",
      "794c27cf093b4e3e8e5f824030dcbb44",
      "dbca85bbd48c49098f97542a63affc91",
      "43f3e999e22649eb80d90879a2fffeae",
      "214338cea8884fe4aa6e139ac7f3b466",
      "18333e53e67c46229c2264e40b48679d",
      "49e8a41dcb8d486a9ba8b00001fc0859",
      "03d43a8d091d4360a17bfec4294d2172",
      "fa9b78313862444cb6ceb5191b7a966c",
      "68ff47e41268403da8bd9d3ad5834579",
      "10666df9c0c24010b04ecf7a7426fdf0",
      "e284805a0659408cabedeca04291d64f",
      "1080e0a1c967464f9769f250609ddaa3",
      "e7c63eda8aba4f438e19d1c996075cca",
      "10f0beaea3754184bdf0794ec91a53d0",
      "00f2adc8f35c43dca7197e6464680ce7",
      "2dd69ff72a1843baaaff0f3f9c1586b2",
      "14042c4c140345b4b66eb274a38d5a42",
      "662f19c95d6d4104a193e4c8b1302a25",
      "e3e7eadeea494817bdef1b9b81d994c0",
      "1f05611eedf04e618514d7efc8826712",
      "40dab5ed0d334d92b2474751f1c6cc8d",
      "c4fc9485480e487fb9193017ae1ae896",
      "4026e5764f254f2782ab1546d2d873ba",
      "1affaa59d6a745e5af95a081103dcc6d",
      "540d39743f094b87910fd41b4a919905",
      "0c9655b5fab145d28542b4a06ed3be5a",
      "7c7744a1b1da4c65a66aaddb160ed27f",
      "d20c61ab829344f9b1f1c9f2ee39cb30",
      "2068508c03a84784a4fa3a774bf378ce",
      "76f9a12b280a4000955433ed1729b01d",
      "d0b2fd5f9a1a4de88bcf0c9dc3b43f44",
      "ce3568920f424deebd7984a377c4b524",
      "3fc42528c4a34196a34c763668655dad",
      "9601979deac047c7a288c53148b241a3",
      "73b067b78a534c27bb638558b23faa4f",
      "07dcb5ad339f41fa9ff5077e1fe35d60",
      "6c285e22394c47e7b7c72b183a600ec2",
      "0677f53c87eb4a33937209c06ed29709",
      "08ed92394ddd45fca97fa60bfe0bc412",
      "4cd74c05272d433e8850056a0dcda83b",
      "243a1123cf0b4736b87350d729d99ad5",
      "599852bc5555491280439864aebebf71",
      "4bbf262874934adaac8f05362c20625f",
      "36dc8dc2ee6644bfb9c4686642f2a0ac",
      "9b7b0da148b14d2fbf47f7b9354d387d",
      "042a7d328b624c0b8b80349e9f4e79c7",
      "4ce7afcea61842b7805830e27e4abd19",
      "ef0744d0f3dd4eb7afe4457739ae514b",
      "ce79e497577a44dab40382a3ffe3ecc1",
      "e6d17f493136405d9bcddfb86363c589",
      "4f990c8392c84fa08bdeb29d6d8c952c",
      "98023deb4d5b4e4ea8a5970c1fde2b55",
      "159fcbb8f9744cdcbb4a43d64f3f2d99",
      "5524a0a5a09f43009fa28cb5e9848048",
      "d79047cdb871422db646b25bb132c2b8",
      "8b811b4e6aa14ab9b888e437848a60b8",
      "d04e7cd2977643049a561419dfd5ab42",
      "fccee624f5a54a33b508d0bbb32c6f40",
      "9a228f8b5e654ebeaf4ccc5be72db457",
      "503381a268984c5fbae31220f5b3b228",
      "e10af2ddacc345659ea7854d16c48259",
      "029934af00a845b1921d23c94385f374",
      "987827ad68df471d8d2b338a46cc73bb",
      "a09f99737cee4b65959cdcc0f420541f",
      "683335692231482a870e25e2b10214fd",
      "80e45b9f93304d56a74ff1af620497ff",
      "89b6f38519124b2aa161c4ca7aca43df",
      "1769766f4ba54cb3be1e9add25a40bc3",
      "92fea22b1a264338b4ca737db8470387",
      "1226961056114cefa4b56bf0a4e414db",
      "3b91fb99a0854a5f9832e01bc3f6424e",
      "12e51efddc684b1d9507f13f54ba63cb",
      "154b3268182a449990174cc254b35402",
      "b20c186bfffb46219d1f045d751dc192",
      "05f6283c116e4e5997ece5a98644a672",
      "18cd9301463d4e25ba498fab0fea8e34",
      "12cbe4abfa464a53a185c0e62f0b2678",
      "0f49a4e3798043fea87eed029cfce360",
      "6e114b26a56940679490b7ed31d896fa",
      "fed2985666654c86a8c4dbf74a2b940d",
      "bc385dd4e621478395bcedfdb66515b7",
      "e41c6d3a5ad246968c39c14ac4a080d2",
      "3c92895aa272481988ab69a00fc13573",
      "0210955308714f58bdd2b173aa60fc57",
      "42d4882c319e418f805bff4ed8afda2b",
      "43711bcd19c744689a0f758b864edf20",
      "7bec9473e28842ec9514d6f6b28a670d",
      "9bc7e8a4046846d785746af3c100afd3",
      "b9a9d315f9dc4a7b97ec0d90230d2be1",
      "cdf0825d2010487089390c0aa056b19d",
      "9bc6d02f493e4654bead20dfc89be667",
      "08042f3f4b8e4dca90b21b5063d55735"
     ]
    },
    "id": "hVFAybJTpHh7"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "The following cell trains and validates a two head classifier for a single dataset, shown here using D6, with training and validation only. It uses a frozen Wav2Vec2 feature extractor and reads a single standardized manifest file (`manifests/manifest_all.csv`) from `DX_OUT_ROOT`. Only rows marked as `train` or `val` are used. The dataset name is inferred from the most common `dataset` value in the manifest, and basic counts are printed to confirm that the splits look reasonable. Before training begins, the cell runs safety checks to avoid common Colab problems, such as local files shadowing PyTorch or Transformers, and scans the manifest with a progress bar to confirm that all referenced audio files exist.\n",
    "\n",
    "A new experiment folder is then created under `trainval_runs/` using a fixed tag (`frozen_LNDO`) and a timestamp. Within this folder, one subfolder is created per random seed, and a short summary record is later appended to a running history file so past experiments remain easy to track. Training settings are fixed across datasets to keep results comparable. These include three random seeds, up to 10 epochs, an effective batch size of 64 using gradient accumulation, a learning rate of 1e-3, and early stopping with a patience of 2 epochs. Mixed precision is enabled on the GPU to improve speed, while final head computations are kept in full precision to avoid numerical issues.\n",
    "\n",
    "Data loading is set up to prevent the model from learning from padded silence. Each clip is assigned to one of two task groups using a simple rule: `task == \"vowl\"` is treated as vowel speech, and everything else is treated as other speech. Audio is loaded from disk, converted to mono if needed, and checked to ensure a 16 kHz sample rate. For vowel clips, the code detects trailing near zero samples caused by padding and builds an attention mask that excludes this padded region. When forming batches, shorter clips are padded with zeros, and their attention masks are padded in the same way so padded regions are ignored.\n",
    "\n",
    "The model uses a frozen Wav2Vec2 backbone as a feature extractor, with two separate classification heads on top, one for vowel clips and one for other speech. Each head includes a small task specific preprocessing block made of LayerNorm and Dropout, followed by a linear classifier. Only the head related layers are trained, and the backbone weights are never updated. During training, each sample in a batch is routed to the correct head based on its task group. Loss is computed using standard cross entropy, and optimization is performed with Adam on the head parameters only.\n",
    "\n",
    "For each random seed, the cell builds the training and validation data loaders and runs a short warm up by loading a few batches to catch dataset or padding issues early. Training then runs for several epochs with progress bars, using gradient accumulation to reach the target effective batch size. After each epoch, validation is performed and validation AUROC is computed. The best epoch is tracked based on AUROC, and early stopping is applied if AUROC does not improve for the specified number of epochs.\n",
    "\n",
    "At the end of each seed run, only the best epoch results are saved in that seed’s run folder. These include `best_heads.pt` with the saved weights for the two heads and their preprocessing layers, `roc_curve.png` and `confusion_matrix.png` showing validation results using a fixed threshold of 0.5 for the confusion matrix, and a `metrics.json` file containing key settings, dataset sizes, the best AUROC, the best epoch, and threshold based metrics at 0.5.\n",
    "\n",
    "After all three seeds complete, the cell prints the AUROC for each seed along with the mean AUROC and a 95% confidence interval computed using a t distribution with n=3. A single `summary_trainval.json` file is written to the experiment folder with paths, settings, and per seed results, and the same summary is appended to `trainval_runs/history_index.jsonl` so the experiment is logged. Finally, the Colab runtime is unassigned to stop the GPU instance."
   ],
   "metadata": {
    "id": "Yg99q-uDnk0E"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# =========================\n",
    "# D6 Train + Val — Frozen Backbone, Two Heads, Val AUROC Tracking\n",
    "# Inputs: manifest_all.csv (train/val only) and audio clips referenced by clip_path\n",
    "# Outputs: per-seed run folders with metrics.json, roc_curve.png, confusion_matrix.png, best_heads.pt\n",
    "#          plus one exp-level summary_trainval.json and one appended history_index.jsonl record\n",
    "# =========================\n",
    "\n",
    "import os, json, math, random, time, warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import soundfile as sf\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from transformers import Wav2Vec2Model\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, confusion_matrix, matthews_corrcoef\n",
    "from scipy.stats import fisher_exact\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# -------------------------\n",
    "# Import safety guard\n",
    "# Purpose: avoid importing local files named torch/transformers by mistake\n",
    "# -------------------------\n",
    "if os.path.exists(\"/content/torch.py\") or os.path.exists(\"/content/torch/__init__.py\"):\n",
    "    raise RuntimeError(\"Found local /content/torch.py or /content/torch/ that shadows PyTorch. Rename/remove it and restart runtime.\")\n",
    "if os.path.exists(\"/content/transformers.py\") or os.path.exists(\"/content/transformers/__init__.py\"):\n",
    "    raise RuntimeError(\"Found local /content/transformers.py or /content/transformers/ that shadows Hugging Face Transformers. Rename/remove it and restart runtime.\")\n",
    "\n",
    "# -------------------------\n",
    "# Paths (fallback + runtime override)\n",
    "# Input: DX_OUT_ROOT may already exist from preprocessing\n",
    "# Output: MANIFEST_ALL used for train/val rows only\n",
    "# -------------------------\n",
    "D6_OUT_ROOT_FALLBACK = \"/content/drive/MyDrive/AI_PD_Project/Datasets/D6-Ah Sound (Figshare)/preprocessed_v1\"\n",
    "DX_OUT_ROOT = globals().get(\"DX_OUT_ROOT\", D6_OUT_ROOT_FALLBACK)\n",
    "MANIFEST_ALL = f\"{DX_OUT_ROOT}/manifests/manifest_all.csv\"\n",
    "\n",
    "# -------------------------\n",
    "# Experiment folders\n",
    "# Output: EXP_ROOT for this run, plus global history_index.jsonl under trainval_runs/\n",
    "# -------------------------\n",
    "EXPERIMENT_TAG = \"frozen_LNDO\"\n",
    "RUN_STAMP = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "TRAINVAL_ROOT = Path(DX_OUT_ROOT) / \"trainval_runs\"\n",
    "TRAINVAL_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "EXP_ROOT = TRAINVAL_ROOT / f\"exp_{EXPERIMENT_TAG}_{RUN_STAMP}\"\n",
    "EXP_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "HISTORY_INDEX_PATH = TRAINVAL_ROOT / \"history_index.jsonl\"\n",
    "\n",
    "# -------------------------\n",
    "# Training constants\n",
    "# Purpose: keep settings stable across datasets and runs\n",
    "# -------------------------\n",
    "MAX_EPOCHS     = 10\n",
    "EFFECTIVE_BS   = 64\n",
    "PER_DEVICE_BS  = 16\n",
    "GRAD_ACCUM     = max(1, EFFECTIVE_BS // PER_DEVICE_BS)\n",
    "\n",
    "LR             = 1e-3\n",
    "PATIENCE       = 2\n",
    "SEEDS          = [1337, 2024, 7777]\n",
    "\n",
    "BACKBONE_CKPT  = \"facebook/wav2vec2-base\"\n",
    "SR_EXPECTED    = 16000\n",
    "TINY_THRESH    = 1e-4\n",
    "\n",
    "# Trainable blocks only (heads + small pre-head blocks)\n",
    "DROPOUT_P      = 0.2\n",
    "\n",
    "# Data loader defaults (Drive-friendly)\n",
    "NUM_WORKERS    = 0\n",
    "PIN_MEMORY     = False\n",
    "\n",
    "# Fixed reporting threshold (used only for confusion/threshold metrics on VAL)\n",
    "VAL_THRESHOLD  = 0.5\n",
    "\n",
    "# Mixed precision (GPU only)\n",
    "USE_AMP        = True\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "try:\n",
    "    torch.set_float32_matmul_precision(\"high\")\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", message=\"None of the inputs have requires_grad=True. Gradients will be None\")\n",
    "warnings.filterwarnings(\"ignore\", message=\"Passing `gradient_checkpointing` to a config initialization is deprecated\")\n",
    "\n",
    "print(\"DX_OUT_ROOT:\", DX_OUT_ROOT)\n",
    "print(\"MANIFEST_ALL:\", MANIFEST_ALL)\n",
    "print(\"EXPERIMENT ROOT:\", str(EXP_ROOT))\n",
    "print(\"DEVICE:\", DEVICE)\n",
    "if DEVICE.type == \"cuda\":\n",
    "    print(\"GPU:\", torch.cuda.get_device_name(0))\n",
    "print(\"PER_DEVICE_BS:\", PER_DEVICE_BS, \"| GRAD_ACCUM:\", GRAD_ACCUM, \"| EFFECTIVE_BS:\", PER_DEVICE_BS * GRAD_ACCUM)\n",
    "print(\"NUM_WORKERS:\", NUM_WORKERS, \"| PIN_MEMORY:\", PIN_MEMORY)\n",
    "print(\"EXPERIMENT_TAG:\", EXPERIMENT_TAG, \"| RUN_STAMP:\", RUN_STAMP)\n",
    "print(\"VAL_THRESHOLD:\", VAL_THRESHOLD)\n",
    "print(\"USE_AMP:\", bool(USE_AMP and DEVICE.type == \"cuda\"))\n",
    "\n",
    "# -------------------------\n",
    "# Manifest read + split filtering\n",
    "# Input: manifest_all.csv\n",
    "# Output: train_df and val_df with consistent columns and dataset_id inferred\n",
    "# -------------------------\n",
    "if not os.path.exists(MANIFEST_ALL):\n",
    "    raise FileNotFoundError(f\"Missing manifest_all.csv: {MANIFEST_ALL}\")\n",
    "\n",
    "m = pd.read_csv(MANIFEST_ALL)\n",
    "\n",
    "# Purpose: enforce minimum required columns for training\n",
    "req_cols = {\"split\", \"clip_path\", \"label_num\", \"task\"}\n",
    "missing = [c for c in sorted(req_cols) if c not in m.columns]\n",
    "if missing:\n",
    "    raise ValueError(f\"Manifest missing required columns: {missing}. Found: {list(m.columns)}\")\n",
    "\n",
    "# Purpose: train/val only in this cell\n",
    "m = m[m[\"split\"].isin([\"train\", \"val\"])].copy()\n",
    "if len(m) == 0:\n",
    "    raise RuntimeError(\"After filtering to split in {train,val}, manifest has 0 rows.\")\n",
    "\n",
    "# Purpose: select the active dataset id when multiple datasets are present\n",
    "if \"dataset\" in m.columns and m[\"dataset\"].notna().any():\n",
    "    dataset_id = str(m[\"dataset\"].astype(str).value_counts(dropna=True).idxmax())\n",
    "    m = m[m[\"dataset\"].astype(str) == dataset_id].copy()\n",
    "else:\n",
    "    dataset_id = \"DX\"\n",
    "\n",
    "# Purpose: keep a consistent set of columns even if some are missing\n",
    "keep_cols = [\"clip_path\", \"label_num\", \"task\", \"speaker_id\", \"duration_sec\", \"split\"]\n",
    "for c in keep_cols:\n",
    "    if c not in m.columns:\n",
    "        m[c] = np.nan\n",
    "m = m[keep_cols].copy()\n",
    "\n",
    "train_df = m[m[\"split\"] == \"train\"].copy().reset_index(drop=True)\n",
    "val_df   = m[m[\"split\"] == \"val\"].copy().reset_index(drop=True)\n",
    "\n",
    "print(f\"\\nDataset inferred: {dataset_id}\")\n",
    "print(f\"Train rows: {len(train_df)} | Val rows: {len(val_df)}\")\n",
    "print(\"Train label counts:\", train_df[\"label_num\"].value_counts(dropna=False).to_dict())\n",
    "print(\"Val label counts:\",   val_df[\"label_num\"].value_counts(dropna=False).to_dict())\n",
    "\n",
    "# Purpose: fail early if either split is empty\n",
    "if len(train_df) == 0 or len(val_df) == 0:\n",
    "    raise RuntimeError(\"Train or Val split has 0 rows.\")\n",
    "\n",
    "# -------------------------\n",
    "# Clip existence check\n",
    "# Input: clip_path from train_df/val_df\n",
    "# Output: raises early if any audio files are missing (shows a few examples)\n",
    "# -------------------------\n",
    "def _fail_fast_missing_paths(df: pd.DataFrame, name: str):\n",
    "    missing_paths = []\n",
    "    for p in tqdm(df[\"clip_path\"].astype(str).tolist(), desc=f\"Check {name} clip_path exists\", dynamic_ncols=True):\n",
    "        if not os.path.exists(p):\n",
    "            missing_paths.append(p)\n",
    "            if len(missing_paths) >= 10:\n",
    "                break\n",
    "    if missing_paths:\n",
    "        raise FileNotFoundError(f\"{name}: missing clip_path(s). Examples: {missing_paths}\")\n",
    "\n",
    "_fail_fast_missing_paths(train_df, \"TRAIN\")\n",
    "_fail_fast_missing_paths(val_df, \"VAL\")\n",
    "\n",
    "# -------------------------\n",
    "# Task grouping\n",
    "# Purpose: choose vowel head vs other head per clip\n",
    "# Output: task_group column added to both tables\n",
    "# -------------------------\n",
    "def _task_group(task_val) -> str:\n",
    "    return \"vowel\" if str(task_val) == \"vowl\" else \"other\"\n",
    "\n",
    "train_df[\"task_group\"] = train_df[\"task\"].apply(_task_group)\n",
    "val_df[\"task_group\"]   = val_df[\"task\"].apply(_task_group)\n",
    "\n",
    "# -------------------------\n",
    "# Dataset + collator (padding + attention masks)\n",
    "# Inputs: train_df/val_df clip_path and task_group\n",
    "# Outputs: batches with padded input_values and matching attention_mask\n",
    "# -------------------------\n",
    "class AudioManifestDataset(Dataset):\n",
    "    def __init__(self, df: pd.DataFrame):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        row = self.df.iloc[idx]\n",
    "        clip_path = str(row[\"clip_path\"])\n",
    "        label = int(row[\"label_num\"])\n",
    "        task_group = str(row[\"task_group\"])\n",
    "\n",
    "        if not os.path.exists(clip_path):\n",
    "            raise FileNotFoundError(f\"Missing clip_path: {clip_path}\")\n",
    "\n",
    "        # Purpose: load audio and force mono float32\n",
    "        y, sr = sf.read(clip_path, always_2d=False)\n",
    "        if isinstance(y, np.ndarray) and y.ndim > 1:\n",
    "            y = np.mean(y, axis=1)\n",
    "        y = np.asarray(y, dtype=np.float32)\n",
    "\n",
    "        # Purpose: keep a single sample rate across all training\n",
    "        if int(sr) != SR_EXPECTED:\n",
    "            raise ValueError(f\"Sample rate mismatch (expected {SR_EXPECTED}): {clip_path} has sr={sr}\")\n",
    "\n",
    "        # Purpose: ignore trailing zero padding for vowel clips during pooling\n",
    "        attn = np.ones((len(y),), dtype=np.int64)\n",
    "        if task_group == \"vowel\":\n",
    "            k = -1\n",
    "            for j in range(len(y) - 1, -1, -1):\n",
    "                if abs(float(y[j])) > TINY_THRESH:\n",
    "                    k = j\n",
    "                    break\n",
    "            if k >= 0:\n",
    "                attn[:k+1] = 1\n",
    "                attn[k+1:] = 0\n",
    "            else:\n",
    "                attn[:] = 1\n",
    "        else:\n",
    "            attn[:] = 1\n",
    "\n",
    "        return {\n",
    "            \"input_values\": torch.from_numpy(y),\n",
    "            \"attention_mask\": torch.from_numpy(attn),\n",
    "            \"labels\": torch.tensor(label, dtype=torch.long),\n",
    "            \"task_group\": task_group,\n",
    "        }\n",
    "\n",
    "def collate_fn(batch):\n",
    "    # Purpose: pad to the longest audio in the batch (pads input and mask with zeros)\n",
    "    max_len = int(max(b[\"input_values\"].numel() for b in batch))\n",
    "    input_vals, attn_masks, labels, task_groups = [], [], [], []\n",
    "    for b in batch:\n",
    "        x = b[\"input_values\"]\n",
    "        a = b[\"attention_mask\"]\n",
    "        pad = max_len - x.numel()\n",
    "        if pad > 0:\n",
    "            x = torch.cat([x, torch.zeros(pad, dtype=x.dtype)], dim=0)\n",
    "            a = torch.cat([a, torch.zeros(pad, dtype=a.dtype)], dim=0)\n",
    "        input_vals.append(x)\n",
    "        attn_masks.append(a)\n",
    "        labels.append(b[\"labels\"])\n",
    "        task_groups.append(b[\"task_group\"])\n",
    "    return {\n",
    "        \"input_values\": torch.stack(input_vals, dim=0),\n",
    "        \"attention_mask\": torch.stack(attn_masks, dim=0),\n",
    "        \"labels\": torch.stack(labels, dim=0),\n",
    "        \"task_group\": task_groups,\n",
    "    }\n",
    "\n",
    "# -------------------------\n",
    "# Model: frozen Wav2Vec2 + two task heads\n",
    "# Inputs: backbone checkpoint and dropout probability\n",
    "# Output: loss and logits (PD vs Healthy) using the head selected by task_group\n",
    "# -------------------------\n",
    "class Wav2Vec2TwoHeadClassifier(nn.Module):\n",
    "    def __init__(self, ckpt: str, dropout_p: float):\n",
    "        super().__init__()\n",
    "        self.backbone = Wav2Vec2Model.from_pretrained(ckpt, use_safetensors=True)\n",
    "        for p in self.backbone.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "        H = int(self.backbone.config.hidden_size)\n",
    "\n",
    "        # Purpose: small per-task feature cleanup before each head\n",
    "        self.pre_vowel = nn.Sequential(nn.LayerNorm(H), nn.Dropout(float(dropout_p)))\n",
    "        self.pre_other = nn.Sequential(nn.LayerNorm(H), nn.Dropout(float(dropout_p)))\n",
    "\n",
    "        self.head_vowel = nn.Linear(H, 2)\n",
    "        self.head_other = nn.Linear(H, 2)\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    def masked_mean_pool(self, last_hidden, attn_mask_samples):\n",
    "        # Purpose: pool frame features while ignoring masked samples\n",
    "        feat_mask = self.backbone._get_feature_vector_attention_mask(last_hidden.shape[1], attn_mask_samples)\n",
    "        feat_mask = feat_mask.to(last_hidden.device).unsqueeze(-1).type_as(last_hidden)\n",
    "        masked = last_hidden * feat_mask\n",
    "        denom = feat_mask.sum(dim=1).clamp(min=1.0)\n",
    "        return masked.sum(dim=1) / denom\n",
    "\n",
    "    def _heads_fp32(self, x_fp_any: torch.Tensor, head: nn.Module) -> torch.Tensor:\n",
    "        # Purpose: keep heads in fp32 even when AMP is enabled\n",
    "        x = x_fp_any.float()\n",
    "        if x.is_cuda:\n",
    "            with torch.amp.autocast(device_type=\"cuda\", enabled=False):\n",
    "                return head(x)\n",
    "        return head(x)\n",
    "\n",
    "    def forward(self, input_values, attention_mask, labels, task_group):\n",
    "        # Purpose: backbone is frozen, so gradients only flow through heads\n",
    "        with torch.no_grad():\n",
    "            out = self.backbone(input_values=input_values, attention_mask=attention_mask)\n",
    "            last_hidden = out.last_hidden_state\n",
    "\n",
    "        pooled = self.masked_mean_pool(last_hidden, attention_mask)  # [B,H]\n",
    "\n",
    "        z_v = self.pre_vowel(pooled.float())\n",
    "        z_o = self.pre_other(pooled.float())\n",
    "\n",
    "        logits_v = self._heads_fp32(z_v, self.head_vowel)\n",
    "        logits_o = self._heads_fp32(z_o, self.head_other)\n",
    "\n",
    "        # Purpose: apply the correct head per sample\n",
    "        is_vowel = torch.tensor([tg == \"vowel\" for tg in task_group], device=pooled.device, dtype=torch.bool)\n",
    "        logits = logits_o.clone()\n",
    "        if is_vowel.any():\n",
    "            logits[is_vowel] = logits_v[is_vowel]\n",
    "\n",
    "        loss = self.loss_fn(logits, labels)\n",
    "        return loss, logits\n",
    "\n",
    "# -------------------------\n",
    "# Metrics + plots (VAL only)\n",
    "# Inputs: val labels and predicted PD probabilities\n",
    "# Outputs: AUROC, threshold metrics, ROC curve PNG, confusion matrix PNG\n",
    "# -------------------------\n",
    "def compute_auc(y_true, y_prob):\n",
    "    y_true = np.asarray(y_true, dtype=np.int64)\n",
    "    y_prob = np.asarray(y_prob, dtype=np.float64)\n",
    "    if len(np.unique(y_true)) < 2:\n",
    "        return float(\"nan\")\n",
    "    return float(roc_auc_score(y_true, y_prob))\n",
    "\n",
    "def compute_threshold_metrics(y_true, y_prob, thr=0.5):\n",
    "    y_true = np.asarray(y_true, dtype=np.int64)\n",
    "    y_prob = np.asarray(y_prob, dtype=np.float64)\n",
    "    y_pred = (y_prob >= float(thr)).astype(np.int64)\n",
    "\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=[0, 1])\n",
    "    TN, FP, FN, TP = int(cm[0, 0]), int(cm[0, 1]), int(cm[1, 0]), int(cm[1, 1])\n",
    "\n",
    "    eps = 1e-12\n",
    "    acc = (TP + TN) / max(1, (TP + TN + FP + FN))\n",
    "    prec = TP / (TP + FP + eps)\n",
    "    rec = TP / (TP + FN + eps)\n",
    "    f1 = 2 * prec * rec / (prec + rec + eps)\n",
    "    spec = TN / (TN + FP + eps)\n",
    "\n",
    "    try:\n",
    "        mcc = float(matthews_corrcoef(y_true, y_pred)) if len(np.unique(y_pred)) > 1 else float(\"nan\")\n",
    "    except Exception:\n",
    "        mcc = float(\"nan\")\n",
    "\n",
    "    try:\n",
    "        _, pval = fisher_exact([[TN, FP], [FN, TP]], alternative=\"two-sided\")\n",
    "        pval = float(pval)\n",
    "    except Exception:\n",
    "        pval = float(\"nan\")\n",
    "\n",
    "    return {\n",
    "        \"threshold\": float(thr),\n",
    "        \"confusion_matrix\": {\"TN\": TN, \"FP\": FP, \"FN\": FN, \"TP\": TP},\n",
    "        \"accuracy\": float(acc),\n",
    "        \"precision\": float(prec),\n",
    "        \"recall\": float(rec),\n",
    "        \"f1_score\": float(f1),\n",
    "        \"sensitivity\": float(rec),\n",
    "        \"specificity\": float(spec),\n",
    "        \"mcc\": float(mcc),\n",
    "        \"p_value_fisher_two_sided\": float(pval),\n",
    "    }\n",
    "\n",
    "def save_roc_curve_png(y_true, y_prob, out_png):\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_prob)\n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr)\n",
    "    plt.plot([0, 1], [0, 1], linestyle=\"--\")\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(\"ROC Curve (Val)\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_png, dpi=150)\n",
    "    plt.close()\n",
    "\n",
    "def save_confusion_png(y_true, y_prob, out_png, thr=0.5):\n",
    "    y_pred = (np.asarray(y_prob) >= float(thr)).astype(int)\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=[0, 1])\n",
    "    plt.figure()\n",
    "    plt.imshow(cm)\n",
    "    plt.xticks([0, 1], [\"Healthy(0)\", \"PD(1)\"])\n",
    "    plt.yticks([0, 1], [\"Healthy(0)\", \"PD(1)\"])\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"True\")\n",
    "    plt.title(f\"Confusion Matrix (Val, thr={thr:.2f})\")\n",
    "    for (i, j), v in np.ndenumerate(cm):\n",
    "        plt.text(j, i, str(v), ha=\"center\", va=\"center\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_png, dpi=150)\n",
    "    plt.close()\n",
    "\n",
    "# -------------------------\n",
    "# Seed control\n",
    "# Purpose: stabilize training order and results per seed\n",
    "# -------------------------\n",
    "def set_all_seeds(seed: int):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# -------------------------\n",
    "# One seed run (train + val)\n",
    "# Inputs: seed, train_df, val_df\n",
    "# Outputs: best_heads.pt + plots + metrics.json in run_dir\n",
    "# -------------------------\n",
    "def run_trainval_once(seed: int):\n",
    "    set_all_seeds(seed)\n",
    "\n",
    "    run_dir = EXP_ROOT / f\"run_{dataset_id}_seed{seed}\"\n",
    "    run_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    train_ds = AudioManifestDataset(train_df)\n",
    "    val_ds   = AudioManifestDataset(val_df)\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=PER_DEVICE_BS, shuffle=True,\n",
    "                              num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY, collate_fn=collate_fn)\n",
    "    val_loader   = DataLoader(val_ds, batch_size=PER_DEVICE_BS, shuffle=False,\n",
    "                              num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY, collate_fn=collate_fn)\n",
    "\n",
    "    # Warm-up\n",
    "    # Purpose: ensure the dataloader works before training starts\n",
    "    print(f\"\\n[seed={seed}] Warm-up: loading 3 train batches...\")\n",
    "    t0 = time.time()\n",
    "    it = iter(train_loader)\n",
    "    for i in range(3):\n",
    "        _ = next(it)\n",
    "        print(f\"  loaded warmup batch {i+1}/3\")\n",
    "    print(f\"[seed={seed}] Warm-up done in {time.time()-t0:.2f}s\")\n",
    "\n",
    "    model = Wav2Vec2TwoHeadClassifier(BACKBONE_CKPT, dropout_p=DROPOUT_P).to(DEVICE)\n",
    "\n",
    "    # Purpose: only train pre-head blocks and heads (backbone stays frozen)\n",
    "    trainable_params = (\n",
    "        list(model.pre_vowel.parameters()) + list(model.pre_other.parameters()) +\n",
    "        list(model.head_vowel.parameters()) + list(model.head_other.parameters())\n",
    "    )\n",
    "    opt = torch.optim.Adam(trainable_params, lr=LR)\n",
    "\n",
    "    use_amp = bool(USE_AMP and DEVICE.type == \"cuda\")\n",
    "    scaler = torch.amp.GradScaler(\"cuda\", enabled=use_amp)\n",
    "\n",
    "    # Purpose: keep the best epoch by VAL AUROC\n",
    "    best_auc = -1.0\n",
    "    best_epoch = -1\n",
    "    no_improve = 0\n",
    "    best_state = None\n",
    "    best_val_probs = None\n",
    "    best_val_true = None\n",
    "    best_thr_metrics = None\n",
    "\n",
    "    for epoch in range(1, MAX_EPOCHS + 1):\n",
    "        # Train\n",
    "        model.train()\n",
    "        train_losses = []\n",
    "        opt.zero_grad(set_to_none=True)\n",
    "\n",
    "        pbar = tqdm(train_loader, desc=f\"[seed={seed}] Train epoch {epoch}\", dynamic_ncols=True)\n",
    "        step = 0\n",
    "        for batch in pbar:\n",
    "            step += 1\n",
    "            input_values = batch[\"input_values\"].to(DEVICE, non_blocking=False)\n",
    "            attention_mask = batch[\"attention_mask\"].to(DEVICE, non_blocking=False)\n",
    "            labels = batch[\"labels\"].to(DEVICE, non_blocking=False)\n",
    "            task_group = batch[\"task_group\"]\n",
    "\n",
    "            with torch.amp.autocast(device_type=\"cuda\", enabled=use_amp):\n",
    "                loss, _ = model(input_values, attention_mask, labels, task_group)\n",
    "                loss = loss / GRAD_ACCUM\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            train_losses.append(float(loss.detach().cpu().item()) * GRAD_ACCUM)\n",
    "\n",
    "            # Purpose: update weights every GRAD_ACCUM steps to reach EFFECTIVE_BS\n",
    "            if (step % GRAD_ACCUM) == 0:\n",
    "                scaler.step(opt)\n",
    "                scaler.update()\n",
    "                opt.zero_grad(set_to_none=True)\n",
    "\n",
    "        # Purpose: handle leftover steps when epoch length is not divisible by GRAD_ACCUM\n",
    "        if (step % GRAD_ACCUM) != 0:\n",
    "            scaler.step(opt)\n",
    "            scaler.update()\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "\n",
    "        avg_train_loss = float(np.mean(train_losses)) if train_losses else float(\"nan\")\n",
    "\n",
    "        # Validate\n",
    "        model.eval()\n",
    "        all_probs, all_true = [], []\n",
    "        vpbar = tqdm(val_loader, desc=f\"[seed={seed}] Val epoch {epoch}\", dynamic_ncols=True)\n",
    "        with torch.no_grad():\n",
    "            for batch in vpbar:\n",
    "                input_values = batch[\"input_values\"].to(DEVICE, non_blocking=False)\n",
    "                attention_mask = batch[\"attention_mask\"].to(DEVICE, non_blocking=False)\n",
    "                labels = batch[\"labels\"].to(DEVICE, non_blocking=False)\n",
    "                task_group = batch[\"task_group\"]\n",
    "\n",
    "                with torch.amp.autocast(device_type=\"cuda\", enabled=use_amp):\n",
    "                    _, logits = model(input_values, attention_mask, labels, task_group)\n",
    "\n",
    "                # Purpose: use PD probability from softmax(logits)\n",
    "                probs = torch.softmax(logits.float(), dim=-1)[:, 1].detach().cpu().numpy().astype(np.float64)\n",
    "                all_probs.extend(probs.tolist())\n",
    "                all_true.extend(labels.detach().cpu().numpy().astype(np.int64).tolist())\n",
    "\n",
    "        val_auc = compute_auc(all_true, all_probs)\n",
    "        print(f\"seed={seed} | epoch {epoch:02d}/{MAX_EPOCHS} | train_loss={avg_train_loss:.5f} | val_AUROC={val_auc:.5f}\")\n",
    "\n",
    "        # Purpose: store the best epoch by AUROC and reset early-stop counter\n",
    "        improved = (not math.isnan(val_auc)) and (val_auc > best_auc + 1e-12)\n",
    "        if improved:\n",
    "            best_auc = float(val_auc)\n",
    "            best_epoch = int(epoch)\n",
    "            no_improve = 0\n",
    "            best_state = {\n",
    "                \"pre_vowel\": {k: v.detach().cpu().clone() for k, v in model.pre_vowel.state_dict().items()},\n",
    "                \"pre_other\": {k: v.detach().cpu().clone() for k, v in model.pre_other.state_dict().items()},\n",
    "                \"head_vowel\": {k: v.detach().cpu().clone() for k, v in model.head_vowel.state_dict().items()},\n",
    "                \"head_other\": {k: v.detach().cpu().clone() for k, v in model.head_other.state_dict().items()},\n",
    "            }\n",
    "            best_val_probs = list(all_probs)\n",
    "            best_val_true  = list(all_true)\n",
    "            best_thr_metrics = compute_threshold_metrics(best_val_true, best_val_probs, thr=VAL_THRESHOLD)\n",
    "        else:\n",
    "            no_improve += 1\n",
    "\n",
    "        # Purpose: stop when VAL AUROC has not improved for PATIENCE epochs\n",
    "        if no_improve >= PATIENCE:\n",
    "            break\n",
    "\n",
    "    # Purpose: avoid writing partial artifacts if no valid best epoch exists\n",
    "    if best_state is None or best_val_probs is None or best_val_true is None or best_thr_metrics is None:\n",
    "        raise RuntimeError(\"No best epoch captured. Validation AUROC may be NaN due to single-class validation split.\")\n",
    "\n",
    "    # Save best epoch artifacts\n",
    "    best_heads_path = run_dir / \"best_heads.pt\"\n",
    "    torch.save(best_state, str(best_heads_path))\n",
    "\n",
    "    roc_png = run_dir / \"roc_curve.png\"\n",
    "    cm_png  = run_dir / \"confusion_matrix.png\"\n",
    "    ytrue_np = np.asarray(best_val_true, dtype=np.int64)\n",
    "    yprob_np = np.asarray(best_val_probs, dtype=np.float64)\n",
    "\n",
    "    save_roc_curve_png(ytrue_np, yprob_np, str(roc_png))\n",
    "    save_confusion_png(ytrue_np, yprob_np, str(cm_png), thr=VAL_THRESHOLD)\n",
    "\n",
    "    metrics = {\n",
    "        \"dataset\": dataset_id,\n",
    "        \"seed\": int(seed),\n",
    "        \"best_val_auroc\": float(best_auc),\n",
    "        \"best_epoch\": int(best_epoch),\n",
    "        \"n_train\": int(len(train_df)),\n",
    "        \"n_val\": int(len(val_df)),\n",
    "        \"label_counts_train\": train_df[\"label_num\"].value_counts(dropna=False).to_dict(),\n",
    "        \"label_counts_val\": val_df[\"label_num\"].value_counts(dropna=False).to_dict(),\n",
    "        \"experiment_tag\": EXPERIMENT_TAG,\n",
    "        \"run_stamp\": RUN_STAMP,\n",
    "        \"dropout_p\": float(DROPOUT_P),\n",
    "        \"lr\": float(LR),\n",
    "        \"effective_batch_size\": int(PER_DEVICE_BS * GRAD_ACCUM),\n",
    "        \"per_device_batch_size\": int(PER_DEVICE_BS),\n",
    "        \"grad_accum_steps\": int(GRAD_ACCUM),\n",
    "        \"val_threshold\": float(VAL_THRESHOLD),\n",
    "        \"backbone_ckpt\": BACKBONE_CKPT,\n",
    "        \"threshold_metrics_best_epoch\": best_thr_metrics,\n",
    "    }\n",
    "    metrics_path = run_dir / \"metrics.json\"\n",
    "    with open(metrics_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(metrics, f, indent=2)\n",
    "\n",
    "    print(f\"[seed={seed}] WROTE:\")\n",
    "    print(\" \", str(metrics_path))\n",
    "    print(\" \", str(roc_png))\n",
    "    print(\" \", str(cm_png))\n",
    "    print(\" \", str(best_heads_path))\n",
    "\n",
    "    return float(best_auc), str(run_dir), best_thr_metrics\n",
    "\n",
    "# -------------------------\n",
    "# Run all seeds + write exp summary\n",
    "# Inputs: SEEDS list\n",
    "# Outputs: exp_root/summary_trainval.json and one appended history record\n",
    "# -------------------------\n",
    "aucs = []\n",
    "run_dirs = []\n",
    "per_seed_metrics = []\n",
    "\n",
    "for seed in SEEDS:\n",
    "    a, rd, thrm = run_trainval_once(seed)\n",
    "    aucs.append(a)\n",
    "    run_dirs.append(rd)\n",
    "    per_seed_metrics.append({\"seed\": int(seed), \"best_val_auroc\": float(a), \"threshold_metrics_best_epoch\": thrm})\n",
    "\n",
    "t_crit = 4.302652729911275  # df=2, 95% CI\n",
    "n = len(aucs)\n",
    "mean_auc = float(np.mean(aucs))\n",
    "std_auc = float(np.std(aucs, ddof=1)) if n > 1 else 0.0\n",
    "half_width = float(t_crit * (std_auc / math.sqrt(n))) if n > 1 else 0.0\n",
    "\n",
    "print(\"\\nAUROC by seed:\")\n",
    "for s, a in zip(SEEDS, aucs):\n",
    "    print(f\"  seed {s}: {a:.6f}\")\n",
    "print(f\"\\nMean AUROC: {mean_auc:.6f}\")\n",
    "print(f\"95% CI (t, n=3): [{mean_auc - half_width:.6f}, {mean_auc + half_width:.6f}]\")\n",
    "\n",
    "exp_summary = {\n",
    "    \"dataset\": dataset_id,\n",
    "    \"dx_out_root\": DX_OUT_ROOT,\n",
    "    \"experiment_tag\": EXPERIMENT_TAG,\n",
    "    \"run_stamp\": RUN_STAMP,\n",
    "    \"exp_root\": str(EXP_ROOT),\n",
    "    \"run_dirs\": run_dirs,\n",
    "    \"seeds\": SEEDS,\n",
    "    \"aurocs\": [float(x) for x in aucs],\n",
    "    \"mean_auroc\": float(mean_auc),\n",
    "    \"t_crit_df2_95\": float(t_crit),\n",
    "    \"half_width_95_ci\": float(half_width),\n",
    "    \"ci95\": [float(mean_auc - half_width), float(mean_auc + half_width)],\n",
    "    \"n_train\": int(len(train_df)),\n",
    "    \"n_val\": int(len(val_df)),\n",
    "    \"label_counts_train\": train_df[\"label_num\"].value_counts(dropna=False).to_dict(),\n",
    "    \"label_counts_val\": val_df[\"label_num\"].value_counts(dropna=False).to_dict(),\n",
    "    \"effective_batch_size\": int(PER_DEVICE_BS * GRAD_ACCUM),\n",
    "    \"per_device_batch_size\": int(PER_DEVICE_BS),\n",
    "    \"grad_accum_steps\": int(GRAD_ACCUM),\n",
    "    \"val_threshold\": float(VAL_THRESHOLD),\n",
    "    \"backbone_ckpt\": BACKBONE_CKPT,\n",
    "    \"dropout_p\": float(DROPOUT_P),\n",
    "    \"lr\": float(LR),\n",
    "    \"per_seed_best_epoch_metrics\": per_seed_metrics,\n",
    "}\n",
    "\n",
    "summary_path = EXP_ROOT / \"summary_trainval.json\"\n",
    "with open(summary_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(exp_summary, f, indent=2)\n",
    "\n",
    "with open(HISTORY_INDEX_PATH, \"a\", encoding=\"utf-8\") as f:\n",
    "    f.write(json.dumps(exp_summary) + \"\\n\")\n",
    "\n",
    "print(\"\\nWROTE per-experiment summary:\", str(summary_path))\n",
    "print(\"APPENDED global history index:\", str(HISTORY_INDEX_PATH))\n",
    "print(\"\\nOpen this folder to access artifacts:\", str(EXP_ROOT))\n",
    "\n",
    "# -------------------------\n",
    "# Stop runtime\n",
    "# Purpose: release the GPU machine after outputs are written\n",
    "# -------------------------\n",
    "print(\"\\nAll done. Unassigning the runtime to stop the L4 instance...\")\n",
    "try:\n",
    "    from google.colab import runtime  # type: ignore\n",
    "    print(\"Calling runtime.unassign() now.\")\n",
    "    runtime.unassign()\n",
    "except Exception as e:\n",
    "    print(\"Could not unassign runtime automatically. You can stop the runtime manually in Colab.\")\n",
    "    print(\"Reason:\", repr(e))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 3313,
     "referenced_widgets": [
      "43911b9dff7c4276b0a187659890d915",
      "b821ea20268647c496f0d0e5fae8695a",
      "4a63934c56fd423fbece3688c74ba24b",
      "50bb1a09dd1a4988be093393de7f3017",
      "22c797d2995b4d01adb3c728343c2a08",
      "bfca773072644a3d8f153d2a11aa8b29",
      "412f010c21d2449db60c9036699f7ac2",
      "314c9ff20d624487beb555692bf9f570",
      "871945a3dbf84ec591cce76f655145c5",
      "6f00093eac134704b8e5578534cb0825",
      "8eee484b5b24435fa7526e698a61416e",
      "b5ebe90364fe4ae0a6742566a30f66f5",
      "d568e745d6ae43e4a7466d53adf9cb9d",
      "9eab3fa45e4e4becbf837b3b4890c0e4",
      "d5aa25762551447bb93c65b0137296d6",
      "eee8fb1353f641938c954a95be91e6dc",
      "c4fa84fe726543929725ee28b92eb879",
      "bd1bc5dc1f9d4d4e8be105c3a68f40ae",
      "147be919a81d4b0381aea49298b95212",
      "4d75b41ab8ae40e880b9f5d8e67f9d35",
      "7bd083a2ad8d493bb4b6c6d5f3e3bc35",
      "4f63c6c15010432a82584ab4fef5c9a2",
      "f5272d05d72b4f10b33088bb94c1fd35",
      "f4e29fc4fafc444892abe1285a9fac07",
      "80e7888fa2a44351b7fb321d3a9bbc8d",
      "9d2c276b01ca4cdb925a73cd13cc01cc",
      "d4db57fb060843bfaf89ace74eb76bfd",
      "5c28986451714505b0cd31f29c33496c",
      "38d6d015eec74683a25cfc9facf37df4",
      "60bb089a415b40a0afd01f9fa9c97a14",
      "1ee857ecc2544899bec7ff984eb2449b",
      "e39b6986372c4ac08dff0bb6fe128409",
      "96ab786e3556402c8d4c1b01e3ae4508",
      "fb243a1961e34433809fe192ef9e51f8",
      "fc3f03d3736840f1bdfe4541bf46781a",
      "3f17cd6b0af24e80bc2435c0054991a8",
      "3a4c5b3cca0f4382b08b6c0d1665c699",
      "fce7d110d83b4131af71c2e5b82d0255",
      "d84e6e51e4ab4a109ffb1d2908256677",
      "6d1ddede04e84909b3beaef17d84e0da",
      "63b9011feb5b48aa9ef7219c572c286c",
      "1d4556aba241492daaef4166559d7c90",
      "b17e8a61fb7840ce83a0608ba31c61ff",
      "fc2e791c3b7b4111b2fcad27f46d040a",
      "baf38f47a530470e86843a71fcaa8aa5",
      "5c08909bd964428fb4b96a449b40d3f1",
      "259d170573ab488e8fcf62d17e321e60",
      "7d7757426be94191a8c1efc741c63ecb",
      "6e643a00c2d444a99025849a5dc05412",
      "7c9e2421a82e46aa95890a55eecce049",
      "6ba4ee1761f945f09cf5efbb0c969f07",
      "305e3d2a46564522a479359956738fdf",
      "14ee6cae395748369834547c4d77bc96",
      "234701ee97fb423ba7a029cc4ba7a821",
      "deb188f380a74612a7cf78620d69b51c",
      "0d2303c0ff3b46a59ad18e6eba730a80",
      "b793fe304266428a8be3ee7eaaa61dd6",
      "d7192fe8a7ef4b9884a03f097a9f4a1a",
      "0e83dc92a33b4a589c1c55425e5a33f5",
      "f8808707716b4a939cedfb07f3c2b0ff",
      "8bddc10e08414cfd8e991029d6c053f5",
      "b9f9d47f96914979ac08552d218e3dca",
      "263d634befb74945894459e1051aa777",
      "eb903d8fa4244aafb4ce78e7f80740dc",
      "0537ddabbae84060b8372c69a8de56af",
      "8c6617d92c754cf29b9bae827c4cbec4",
      "15390653ad6448acbf0d2061232ee4e0",
      "096f7fbe8ad1462bb8e0f9edc8cb21c3",
      "6d215cff1fcb41989a7010b620f23e2e",
      "f88f5433c5604970ba474504f96ddcb4",
      "a6bdbcabd7834aea8186b4d1b52b441d",
      "8f333c37adca44ea8844ad7419a60a63",
      "b1d905e8850944288edd8797edde5749",
      "3728d9ede6b4458aac732a65b4108ad3",
      "d1dffd4258b54590a0054555335c3d90",
      "3b9a1f7bed4c47599ccd309d528c84d7",
      "9ac8f142b1ee4a58bf8b46b4d21d3c80",
      "ea163c13b6534588b970cec17bcd2b20",
      "04b0f1317f6d427cbec60317d646e899",
      "39dafc7b44884e27869d2d6f35ddd1da",
      "e2b0b55c07f34b3eb60ee4c8d6d9c4c3",
      "76565ebb7f894c1689c875d3591f174e",
      "9b055f8f211f49969f208f6a8eb2aad2",
      "e99913f3c7684d56b61bd1a01dddd632",
      "a64e9a5725a4403bb05924637b8f7747",
      "86ca1e87a42b46dc924ba9a1278239ba",
      "a26b47c434ec45c7881374d19d616b47",
      "098cfb53e6384aae9e40bc7bf4271e81",
      "510a7b073e184935ba84705a44088e98",
      "174b68f6243048129c1375241c4add8e",
      "6d9f722a02bb4388b7ff2b35f88dbdf6",
      "3647f472e2a94fb1b923ad45302c0d5d",
      "a74a445af9564584ac0be589ff8ebf01",
      "bfa6cd90f65341aa91859e91e1129fa2",
      "f5dc7aca26634e479c2c65b789a1d177",
      "da8b514ad00e48f3b7b2604c0e1bc95b",
      "0f59ce515a9640e3ac48185334886b00",
      "6d82382158774798b1517a3837637f80",
      "e678130b71cf42589a39289164357ddb",
      "94f0b2f96e22440bb324b9827e0c9533",
      "512f11f2b600447493584973e3eb3bb6",
      "21ea4ad4811146198f20bd31825f11f1",
      "8fc2a9df74ad4b94a8ca324f9c1307a6",
      "4b654a6243d2459299fe0b990979af4b",
      "8126ac319ead408296a2c9af926586a2",
      "92f43088133f463a8f80822894fc11fd",
      "f400c94d5ab5410ca14d19feada9bbda",
      "915bdccfaaac4420be1b833aac140d4a",
      "057c27cfcf834ef9b2f12197f7cfeaa5",
      "90d7f0bf0d504c93b0170ff37d016fb2",
      "210576e35a7543b08b0b1c70f3f69ce5",
      "4e670b6538af4ed39511eece805a0fc9",
      "84132322cdd3468e8e42873ea6ac7336",
      "4dc748fa9d4848bbbcde6c9914cc98ab",
      "a7a62a651bf341f38855677105d74f95",
      "5e5cced831b94353810bd23650a87ff1",
      "f96eb272c3354926a38697db477d305a",
      "17d57708574c464fae44d6143995842e",
      "2f9a7bc76f7c4383ae6076721468d374",
      "10a74025e41943af9a7b06281d223f19",
      "2c188c68939a43b49e67bc6165c1869e",
      "137d28f8b8ff46bbb95bdbe898963586",
      "ab59c5fb570c4b88a04306c40db7f986",
      "4b42ebd736d042aa95539dcf9bc9cbca",
      "a60de498cda44a79b285c09cdc3f75fb",
      "b293a16de6854e8bb53895acff4c2059",
      "2f74007681e646f58c71da679510f0c1",
      "b4f7406ee5284a41a3c211c8ba13360a",
      "67f6922f51fa4876836526aaa515d190",
      "583db25e35a048e180ba08312d08f91f",
      "3f8ac150a97b4daabb85b14c1f58894c",
      "13f69c04e6fd40908293797ddffc0634",
      "023ea8e90a8d4b71b1a0ae667ba9dc82",
      "9711b5767de74ba198fab1f0e574a255",
      "27504ccce5b441f9afb0db27c3a17aa4",
      "d217bd9308e54cadbf9774a64d01302b",
      "c890b01eaeb64112a2091ef92e36b6da",
      "f220c12a8d6a48e9b4a7a8bdc3f71fbe",
      "475d049bf96d42c68b7bdb91df8b0b12",
      "014c03279e064bf9bd3701966526589c",
      "7d4b6eeeb5104c1f958b8e3b684918ad",
      "5d4f2320f16a410ba96b515d36991739",
      "f313bec4a0e14544a07d15ed5492bd1a",
      "f6187e5372084b6ca449f5307709f538",
      "b7fbae4b4082451cae262e07434371d6",
      "9843ca0bca894d7ba20bce56af62fb80",
      "1c7e57cc7ab14203a970178f1af2ff4d",
      "0c7bce6dad4e479cae84a09ee66957e8",
      "c31c771c3eb04778a26e32b7aaaecf82",
      "6b0ed883f1f04710a40c971d902f47eb",
      "2beefce1fca44c1ba8fabb352bda148e",
      "5e899f2f9a4a447ab7911116a420bbc9",
      "3eb5efac6b4d47d2b5fca8006b6f38cf",
      "26655ea045ad42099276b2e7923eed36",
      "2faac637a99d451ba50bc431c06d8a9d",
      "39aeda926b5148799806e7a143b2e27c",
      "2d34addd528b4b16bbd2a431fc4473cb",
      "353b77bbb5f44013b4d3ecb0ff48ed12",
      "f2d4f1ecefdf4c43a4ae581eec2b1bc2",
      "480c95f694114825a60ff358423b96be",
      "b07dd1ea6ab64c3093bb40779d52a231",
      "9d62d59cff944ffba2344a3ac0e2c2f0",
      "8d638494913244a1b0aa86b9be4860a7",
      "099acbc2177445d691822213ef98e6ed",
      "cb85c9c1bd1d4089846b0b105c91e848",
      "3154183cbb044fc199794115ab089b47",
      "2c068cc47f90484db707ee0c0fc9b7e1",
      "fbd47ea3cd9f4a99b60a0f8a78747746",
      "0fa483c4ca9b46a88cbbf1c8dab94a8b",
      "966888526fbf4865a4b161d949a88081",
      "0bc6c22c572d4f6ab97036fb68aed0aa",
      "561d2ab5bcfd410d90668a6cb8c23e88",
      "467cfe8775f14d15ae9cd1a537444fd7",
      "bff82aef43c74514b84a11523f4a5bab",
      "4988bc0f59124050b54c1b9584ffa754",
      "41614a358af0418a932b60f6ed79d04b",
      "8938079a12a341a591df16c0612e87ee",
      "14532b3bcfd3495fa76b9c73b7e16460",
      "a882659267044a7abdb457cc35a456ac",
      "e1d38265717549d7945661ce9148cd1c",
      "656a413c08d444a8898c258ab6edffe9",
      "aebd20d1e29146a5aca4bcfb0792f6a7",
      "3a978bed09e04744a8823ca776e6c44f",
      "4cc892f809b44fea86d2c0e2105bd6cc",
      "b5f5a4ce02434cff82205fb47f1563f8",
      "e95f0eef330948f686cf0324485af6aa",
      "0a3fe67407c749589438249006924a58",
      "a948ff8806244aa78d5553dd1a6c9105",
      "a0820cddf0d64959a5b4036f8a151d9f",
      "86c31ec487184a6a9d6445ca8a138562",
      "04d81a682f704622bd64594cf6e892b6",
      "10c23152a44141998add4cf3469063ee",
      "7370cae1d8f34b65af6a3f04fb08692a",
      "2cb15151c70d4260a60329dc5dbe12e4",
      "4cc4bc9142e24f76ae06ed277eb07fb4",
      "850b320e6f5e4ed3aac5e1cd06dbbbc8",
      "a2e1936c8869475d94bd3e22201af425",
      "8f063e9ac43640c39ab56decc085c083",
      "e4479161c46d4a67920387bbdca46040",
      "7d289475fed54bd09dffd2e322c9581b",
      "d7c661ad936f43cfbdf5967991b9d185",
      "d2c64673f14f49bd85b66568fbaceeaa",
      "562f1b364a804d59b31cfa47a9d63aae",
      "88e3d6a68daf4d908160d74daac3924d",
      "02901aa403f3462187d35318bb12cc13",
      "f2bec9de39f844f889224e1474cc7fe0",
      "3481bee2647f49999399dcceb0e933c8",
      "79423eddb92a4caf93b1afafe8b9dbff",
      "a7974f31578f496e9ebcf520e3ffddb6",
      "20c99c8c3ce64293b739c941bf6bba6f",
      "536b023f1dc44afdb9a2822951faa1a6",
      "0e2a9148e2634fd6bbcdda3ca3385f40",
      "856dcbfaa09a470890bae82bcf320123",
      "a96c438f18864826b30befdc77fa9a77",
      "1529d3135825409ba1846a6b7c04aab6",
      "1dacb812243f4c68bdd38b62a658628b",
      "6f337130e76e47afadc4692430913e6f",
      "8d6f075705184a04b390a5f9caa392ce",
      "ee9dae25055e43c79244b2c5a0045ba2",
      "e96905d44b7449a38c409e657ab4cedc",
      "5391aa6ccfbb4cf99470256766b14d2c",
      "c010163a8c9f44ad8713ec8f01bcac4c",
      "b3f3e53d05ec484f85ebf995a26940d9",
      "5d9eb6f4605a44dc868484586423b770",
      "50dd18cb7f7b4e678bd65b6118e73ff0",
      "3652c97f44724400ae093f8338d1e4fb",
      "7e79edb61dbb4c0696b369098c40c593",
      "7736e2d103e4412c8215212032c62ef5",
      "8439a956eedd43c283a5a53802a496de",
      "28310a5cd4c0410ab27ca5a47e89879e",
      "f64356177bcc418f82ebae31909c86a7",
      "92b7aa30561d48d2b79b70e6bf4fcd52",
      "5df8e98218e24f61a9fb8d9bf3435d51",
      "ca2863881d3246a3b3997474f8165869",
      "cff828da0e9a4d8aa6aff0a718c00187",
      "cab2a1e2d5e64ed7877a2f43cc737c57",
      "64a7c429de7a4dfeb113efb32ec46059",
      "f6faaecf7083487fb97bbba5f7c8ca94",
      "52ba1f2d9d5c40dc935d58e89b9d2f1c",
      "d8a41628ff5a40278b86e5a59f1c877e",
      "450d6c6aefb7411e9ea05537665b825c",
      "8c2f7256c6c64035bd18dd55dbbd989e",
      "f328534e68f94f699cbd9c195f00c6a8",
      "f339e7136d75468abe22b12c7555fba2",
      "990470b7a2974b98bf9ff79c974c3a6b",
      "8fe4db865be744c587e947426c885c85",
      "2c5d70af14a045edb3ee87893381ac38",
      "f266428088f84107bd1bb0e6b6871086",
      "3f5e32ac7f4c4a43abec4d6b9901f143",
      "4b9aafdc6a1a4aa19b8e1b960fbbfbdb",
      "d31dc5c760ee466ea721055dac8816ad",
      "86248e0c5e7548be9c691fd0c959f0a4",
      "2df3629c887846249b803534d4d631bd",
      "e0dbfdeef8874859b895988f95fcab39",
      "6a31117c349b49d389118eb8749f70c9",
      "6b23d9148f77489d88559668fa0aedf1",
      "e55f8247320948e2b66ee49d07f8a9ed",
      "81b0483b0a074339918472a68a9a3918",
      "7d79b76af6904cc1ba0d8724bec1f25d",
      "94e4d48a22a64b5c8a6e2a2180a17ab1",
      "ebc9953e124848db9ca43b6a45750693",
      "3af920c4997749e0b4722a966c987c57",
      "949cca3201e54f81aa1f4bbd49381c30",
      "d8aca032ada14fdf8a844d1ad64dba7c",
      "9d4dd2c0ef3c43e98254188b1d73851d",
      "5c58ded3bd914027bca0d5a0de113a18",
      "a472e52a3e8e461892cf5197554c03cd",
      "1a20ed522eea4e0cad1ba00e9ba9fd97",
      "49cc55f651c849e08c68b92156cf9938",
      "744c7e9361ce44c586ce22252b3ddec4",
      "c0396fd91f5d43fd83aa42f56bb1836d",
      "d673fedcba61472192e641494d1b43c1",
      "3cd644b526cb40beb559c6111251fdcb",
      "33e71f82ef7e40dea681b9a0bf3b58f8",
      "db83d39f6c244f6f99662271e61e398c",
      "3039a4de7f2841e49f83b019cd9db1bb",
      "bb89fc3d00b84f70b259f247d33d8107",
      "f8bebf7239fd4aa0a751b23aceb1f50a",
      "e823b51b1ebc47adaa5344ab968e91cd",
      "e59f3fee424942a79903cd47d398dc68",
      "5842943311fc48c483ea2e9ae662555f",
      "e3b1b65499e448798f78409deb56e422",
      "c0dd412f67d04e68a350e3b0b5dbe006",
      "16d66a79e70940fab6acff9a27ea1d36",
      "f4718c2c31c14d3e817591ea7937eeb8",
      "c2353750885a45de907672392a3a3f1a",
      "70664854ef1d466c8e2208bc66a2aeb1",
      "fa9f1db602bc4adcabb30b9c04798c54",
      "45a111ec27b043959ec892bc2643df8e",
      "e210c9d4b7a042508b04069932944443",
      "e55564e9e48644039132e6958493cd4e",
      "a4592ff4356f460f8a28de8d6f829831",
      "836572a797574a10aaf97f1c67ff110d",
      "25893b701d0f46db8e438c7b606d8e3d",
      "642238d20d6149a894649135ee7fe3a0",
      "060e626cae8f43c3a186fe68f9ed5cc3",
      "99473d3745aa424ca5f11fa0993b255d",
      "ff3aafc5ce1a42aea749b7564d1253b6",
      "5d69bb898dc34bbb9162e1f1adcdaa8d",
      "db195cc17d1546e094777a1f61f24c23",
      "6eba806533e04d5091add3b2d5e0752b",
      "2f07407c85794178905ff17b7c978538",
      "b154150c8277492bbffd7e2797614859",
      "5baa67f8ad8443c2adad803416bdbecb",
      "a06cde88bc6b47278dc39c4fb03c42e6",
      "d8aad01c9eaa41e3bbf18c7cdee3d626",
      "a92c9da44d1f464a8fa6fef62ac81387",
      "9a7103ab215f44aca287212e75b00cc5",
      "5ee49ba3a1e34e9e8537e5220a83be54",
      "8e3f0a8731254b7da5ed0a8b5b3e2017",
      "726ef9d1adb64ab7bb4485cd5294840b",
      "3c9ef1ecfeb44715aeef1aded20e4b6d",
      "5971655670f644aeb058d507c2db8264",
      "7e5db381ee4c475eafef830446b53c00",
      "7e48026e295c435ca0a6a4fd79b159a0",
      "b99e573b331249ea999f97da42c99072",
      "177ab40011d14819bb30988bd13c1cac",
      "97bf465f00894b64a24d818947a8c9b2",
      "b0f29c17087444f5b419e8715db0c87b",
      "cbc4f8012eb141e2923b25c6d7a2a795",
      "1f2def5f01d145a88225b095b6970775",
      "d42f8fe3b8604f1c881a541b6d4cf49d",
      "0790bc9a0eb8413f82482132ada9ff80",
      "07f6b249889f4e5db5eb45b5d1fbba34",
      "80e943cd41584e26848649b91a7eccbe",
      "101bc8bd2d614c159617f2499acf9147",
      "500840887d904e61bea180629dd875d5",
      "56c835865c4441489090b7a0e0008ef0",
      "c808ab0a8c4d4610923afbb9e1bad012",
      "f5321eda5f9f4c7f801458ab362f3284",
      "561aece100684a139b86d41a17688bd2",
      "ec067babd9b243c091bc816059ecf3a0",
      "5fc27b6ca2664ad8bdedddac1d938465",
      "5a8296fbabdf4fa8af6ac095bd95fc44",
      "fc36ea07afde48b9b25f5967fc770973",
      "a1703cb668f642d28ac14dbe06d2feb5",
      "0e2aa0c1a3dd4dec957876c540065eec",
      "de44b0655fb94d99a86c40f169553b22",
      "fdbac05dd5e44c6da10151720b2c2a3b",
      "1060612b35f54bfe8032ea41b13a7e78",
      "fe082f7a38a347b59b1460dafa1cab5c",
      "937da519a0a149c1960173f797c93d39",
      "2b912946a4e24fadba672f8c29e46686",
      "952844aa598f494db449efad32a9073f",
      "9f1d14f2610e414f9e33fc1e3345e07e",
      "2d9bf201aa8e460baf920a467cee6e9b",
      "d9dfbaf9540c44de999a34289ccab277",
      "fd797c8986d84de7affce123d4ca83a8",
      "9cde4a4f4bcd481caf134e46172fccf2",
      "5255bd669b20493b8bff4c09f73efc0e",
      "ed45fd7d92de4636bf7f10aae73dad3b",
      "1f234f9f5a34495383e80c8ef9a51824",
      "4498aa64261c4131ba34c98ec5bdd436",
      "b704b48624894897afc015e8d73e8e27",
      "65bca8d2eae74622b512a1142c8816cf",
      "f8dbfba0f7424addaf0205cb01c4f31d",
      "5e60ae337d1941a287432e385c25f238",
      "d40789f59fda44ee8c3ad72554875d37",
      "2e607d9fa8354a9484f5ae402af21fa8",
      "ff6f4b5050de40d0bc136b1f21a95dd7",
      "5c479d3f1caf4a11a28cc0165882f1f6",
      "18b22482ff1f4cea81fd3c073323944a",
      "591b0ac7993743d4a9d2d543d3a2f68f",
      "3957bc36b3de4995894c23bbd0e441a9",
      "0cd08e82e0184cb58cab91ec206e5ca3",
      "748561d1bbda43a1b8d2c06effc237d0",
      "a1dc14dd46374bafae2dff726c2abd34",
      "801335697ae944d9883fdbba997acaa5",
      "7c1cc13f670f41cf8b1c31c58cda1e58",
      "7733bd16673c45e092b5fc635ec761db",
      "d3a5c89772ee4d479d7ad99201191434",
      "eb3cf5fc6bbb40e2b78f6ba811cbbe44",
      "7dbab7629d8041b88e7ebf5a65f2e96f",
      "e06aadc607724192abeefcf054871381",
      "0abda875159144a7872e7e6bbd4fe75b",
      "7fe69126f6d444bdb017cb282f01fe47",
      "88e555dcba5b46c8941d3cd52cd2fa45",
      "27c1b546827248bfaf3d57dede238935",
      "de7657ffe8ed417e844ed16be2ee4bfd",
      "2583aff620a04eac93905714e9564432",
      "98330016f15546daaf3deaf0f74cd321",
      "ec55286ebfcf4f2e9b4b38e4b3ef49ea",
      "c15547109dde481cafeee199af8b395d",
      "58f956edc62d4df79ad08db0dbce2787",
      "98b49d0eea1649c4bb988e7cd0c43ac9",
      "d46e9090cbe74592afce2cca52c8e783",
      "22282139f3204604abec319ea5f461f9",
      "270c91af79974d8c87345d784df6c007",
      "4bbd4a7290954701a4ee16340a1145c1",
      "239b998318af4460af37f99a4b6dc96f",
      "79c14cf46a3848929e715a137bf67859",
      "f1789977201d4c81ab868436a80afb30",
      "1f33fdfeed3e496ca2c427d9a9bff81d",
      "74d58f9836af48a487e46e55900e057e",
      "06c899a2dd6a42bfa972b8d31c1e4b9f",
      "2d6a2f54998a449094b0d899f572c0c5",
      "c4c771195795456eb7e89c6863113796",
      "7bb025dcf41c451fa9fe1c74dcb121ee",
      "c5378dc5e5fa4fa487000a37774aa720",
      "cc0b7e7e97474c159ab6273d7ef20262",
      "1702e7b8cf394955b0f1adeafbc2d564",
      "41246eedf73e472eb248b81db2bd8eb8",
      "9df33baa4d7a4c04b661d06688b59403",
      "a0f2731aedac46879fdaddc13e27071c",
      "7cb433141d934c5ea9bdf66882e23af0",
      "d6a6636bf0594eeca749360ab5b50244",
      "b1dc51d888164939be20c57d49e8059a",
      "a64ef85d408d4e02a982c97805e24d9b",
      "a1cd17578aa544bca899b1a48640ec06",
      "02391dd8ebf14e22a3acb759ecdbdf5c",
      "d74decf534124f08a7aecfde13e10b74",
      "7fb127aad9904a26ac045ffa9ecf02ee",
      "51b1b88deb1d4a48bbc91cf5952a6ac5",
      "7607784689f9479fbf29d4f3dab958ae",
      "7c92691e835441f68e9c3595d1ccb7c1",
      "0a1795534f9b4559ac20c09ab9657d2c",
      "9bfa18ae43c54eee9dc63ec4451788cf",
      "af13da3d677f4c11915c3853a490b5e7",
      "3db216128d6f479ca9cc186ba3b44d23",
      "d7e3efa016d84bc0839c5535e533cc40",
      "6b563ac91b5a4f5dad442c7170d3802f",
      "e76dcab0ceb0479ea7ebed8c1cacda99",
      "ef41226034e74f938cf5c3a14b6bf86f",
      "be43ffc4b59746a1b9af682df051e2e5",
      "5b7a88ed04b04af496c91ef472eedf5b",
      "258a84f8b3c74051a7038d7b468267c1",
      "ac6963017770488d9fac99a34e09fb7f",
      "6df5d7d7c1fb47c781f502c8047fb2ae",
      "baf91408cbc146eaabe73fac7db1a405",
      "f810bf20b69b4ced9341ee61ed8924df",
      "cd0988f06736455fbd348e61ef4b567a",
      "b3d9cd0ac9ff470d979e04d9ad874dc9",
      "7ec49eda970542c4815c2261989321ff",
      "04f06b6393d749d7b4f12ec1aa1e4bc8",
      "2a08820f068c492e828812d7be62e669",
      "84c4beac1ccd4cef90e8d2ec71571c5a",
      "0ced012ff8bd433eab1a8dceb486d4cf",
      "daad5102d54847a49d9eae4da5f420e8",
      "dfd0ed29ec4a4d76af32ee656e037991",
      "55d915c6c08448eaa0303103b2062092",
      "c00f623724c34787af8fcc8e20a1fdcc",
      "f495377e45b84dcfbb953b6f176a18ef",
      "98761f9c79e240cbaf1f65a8778d43e5",
      "77f7ad5f4721473d8d2e832bb91542be",
      "d3a9c3718ebe4356b2a1f841b8521347",
      "e03d0515d37b4c91a36121e49e6e0b49",
      "ac2b79a6b6e4479b83514c0eea8f18b6",
      "9ed1842d02134ab6a36fb161c4ae8c43",
      "e7c02b8bcd8945c581ecb36e3e3787ae",
      "77b3460a81ad43b399837cc5e9612ad8",
      "618e8e2523ad4ace841d0e52930d39c2",
      "6214ddcca01644deb241d0dc9a11b9c0",
      "02c2daf2e8634c1895f0a520ab5f9443",
      "f433fe9063614902b17025b66838ff48",
      "b631de8051ef49beaad3f8700aeee50c",
      "a67487af2b5f42b9b7b1996b15c04d21",
      "dab508903fb54425ba9a26a3850f4747",
      "2a33c4db52594ce6be1f886a4364fe8f",
      "49d51d824f69486289d74f1096f4a23a",
      "34993c5ab66f48a6bea2c811d991ea92",
      "0ccb90d8231343b99e4ba2a733d144dd",
      "40e3fdab32ac49eebe8bc17e69c54e49",
      "723194242caa4c4aa1e08ad9fdcf5d81",
      "14039aa1f9f84c8eafc315345752c791",
      "45deec189241477db99cdcdc0ec273a4",
      "61913f7d643546048383e8f3fe810f45",
      "5c3f68f011164b37a2be58a8fa507eee",
      "f84a0146144345de9c1cb2d574c0b6d2",
      "4c52782b654041b7b4d36090a9a56b0b",
      "4bf645f6c07f465b8797dc18ac244a96",
      "bdb4c5814a4c49e5a7325179c0a1e328",
      "a1a85eac0c124375ac02ca2741f29ea3",
      "c6165c9b362d48fa8ff0327472fa69ec",
      "039d953fbd9e49d3acd55c03aad189ef",
      "883cf9820b264d698c8620fade44c25c",
      "1229223377b3421482e9e9d38d82fb0f",
      "cdc3b724d5ae44d393d4da3b1af2556c",
      "d75328d574c746e8a1546a0fd162b626",
      "a082dc4543964d2c87fb4ae7104e266c",
      "63e48f5e40ef489db7ccb658c3c58e74",
      "4042c2f54c3a4610b1bb43b4fa9d5129",
      "d47772fd51804e9ca888e41fa27f302b",
      "b936c89b77aa423ea1e755b9a7fd22f1",
      "f8c1c9dbaea84289b1e3c7f12d1859a9",
      "6b4d5d56d9ce4862a2584b86ddb80f56",
      "653c28ec48ab44e9b1a4115960495f86",
      "e181e52db4a74906b70493cebfa8b5e4",
      "ee99f1d461b7423ca0d36ccf12737250",
      "b5871adb693d4e7d8073de4bf7bc2ea6",
      "fbbd81d9247c4e2b9f62573e57c5f516",
      "93547c021b17426c8cc2b58eda39d55d",
      "17d3ef162d3c4f4bbe6433447474ff38",
      "8e4dbe009a554654ae7b06949c08f988",
      "56fd72799ba9471b9a8d7df58bd20605",
      "86ae35665c7f4d3991a89feef7c63298",
      "192529b10f474c6b9a793fab2a872176",
      "42cbb0a3861140a0ac5a1daa3ffbfa6d",
      "d2034e6891aa45dea4c984f29f520b03",
      "5e96dc0faca04d379ca708b0bc3618f8",
      "1ce39538649747dda70566b2f43ff728",
      "3d174a9cfa87470ab1166957e30aaf85",
      "65c836b34b9f4971b6adc72abfda4eea",
      "f092a80b9eb149c6bce068e5e97549a6",
      "9f7b01bf4b014fbaa48c789e6f941518",
      "90ab36068e8b41b3b1f554b0f3bb2955",
      "24427acca080475997762a969e40d246",
      "b05fefacab4c46d882a7464c65a2e5e8",
      "ad03a088405c4554a1a4a24f57794d60",
      "9f842748df3044e18e004adca0be062d",
      "72f20e2e2d0042ff88dd80638ccfc213",
      "8ccca42981bf431bb28ceb6287521d8c",
      "60dff95ec2bb47e081b28b8e91b0374f",
      "5fd380973f604eb1acaaf137051081f1",
      "65a93a0ff13f4f2cbddece935ff28f48",
      "01c0f580a4f445a39e2d30a4bb0a53da",
      "4a4f58682d8841979fa39587695eb047",
      "3516948d91324177b09129aebf3836cb",
      "20616ab9663d44d5beecd1ffd1a08cc6",
      "849501951b4b45eda13df3c9e404693d",
      "f2d6e4a530594fbba9b6960ce9be9085",
      "4e0ef7bd419e40619689f74538a459a4",
      "49bd5e9784e94fb08e2ac216f2bb853f",
      "f561e74e52134c2bbf91ad8d7b24eb67",
      "bc978412901645deaf9008f8c6e6fbaa",
      "348a99ded2134c218e8f491d6194f22c",
      "579b1fb7ba4148868a846f08ce35c125",
      "5ff46c35184f48319c9d0499caf05e08",
      "ecfb43552a2d465aba7cfe6b94d225ae",
      "9c81dfc9a92f44a4962e92486366300c",
      "84ef23b9f40a4e669c79f870cf171d01",
      "310936c784134edc91283a89724093bb",
      "43c69142a3ca4d38ba20c62286477d92",
      "4e1e59f99b6c41768a1a2513e391d61c",
      "2d7d95fd709243ba86331bdb21c2e99e",
      "e4d0e7f20e1b4a07988833e8e46cf1d9",
      "f70de09d7ca64f9faf4d325f24fa6c7f",
      "4ee4d81121814b979e63076844409a48",
      "888b38687e974c9f8be0868ae2a04632",
      "1fef976ee39d4b03bb6849c19f4b2eb1",
      "1168242588ef486fbb0bfb385b4e9fbb",
      "a1cda899b3b44b77bee5c68445456bb6",
      "1f05de3091c4459383cc975296f1f979",
      "65a35181b4ac44e5820f6f2a5a7e243b",
      "dbb6b7aee3f14694a8c76c2e6525567e",
      "3d29116590aa4d37b1cc60222b1fa4c4",
      "782be784d2b44724bea2976fc8cb0bf8",
      "9e7ce82f4d6e458ebd09a9db20003fbc",
      "00d102c6ca674e7fa9a9f334095fe971",
      "93013b582de34e90be1b07697c5e291b",
      "f2c80a905472431986274d6408f38065",
      "cbc14101fb8f4d51a03dea1ae90e445e",
      "28b6845fd91849fa91c9c2064f0624b6",
      "71c72fdebed94482a69b768eb13b6181",
      "efb04fbba4d949f094c1bcc748c9a2b4",
      "0ea1f102be464e61a92daba7f257ded8",
      "8828adbdef3a4c4f95f4d80998bb788f",
      "8c3792d68b7a482c959d31171975f97f",
      "919d17f22c33432e8e68a88e0bcfca5f",
      "385e9583f2ab4402a0d0bd3cb6f123ea",
      "7894adcb486d4cbc88a6225a380f6b69",
      "ecfee319153b4660a4d5960ae7fcc3f6",
      "e9c51047c5354602a581652d75444f6c",
      "de864cd1d2cf412fa1b0d8f7badbd068",
      "56d69d3e7c434e60a1fa0f3ec5d03579",
      "9acec8ba048b4d81b34b79f51fc73267",
      "0cb4937cb4454eea992ebd8bbb2726f2",
      "e4473c9a3d2748078c01855c79f713ab",
      "448db8ae592548a8a367eff79dd6e935",
      "85f5988bff3d49bfb9a40137a755085f",
      "0d32ba5638fa45f580828e1c77931c57",
      "de593542592d4afbaed3657a3bf7cda0",
      "2abdfc5f8df343a0ab65729058fcf1a0"
     ]
    },
    "id": "YLYvFV-ru0_A"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "#Monolingual Testing of the D1, D2, D4, D5 and D6 Models on Their Own Datasets"
   ],
   "metadata": {
    "id": "O5LTQ41Op3X8"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The following cell evaluates the D1 (NeuroVoz Spanish) model on the D1 test split in a test only setting, without any retraining. It reads `manifests/manifest_all.csv` from the D1 `preprocessed_v1` folder, using `DX_OUT_ROOT` if it is already defined at runtime, otherwise falling back to the default D1 path. Only the validation and test splits are used: validation is used to choose a decision threshold, and test is used to report final results. Two safety checks are enforced. First, the manifest must clearly indicate that the dataset is D1, otherwise the cell stops to avoid evaluating the wrong dataset. Second, after locating the most recent training run, the cell confirms that `best_heads.pt` exists for all three seeds before continuing.\n",
    "\n",
    "Before evaluation starts, the cell checks the runtime environment to avoid issues such as local files shadowing PyTorch or Transformers. It mounts Google Drive if needed, sets consistent evaluation settings including a 16 kHz sample rate, three seeds, batch size, and mixed precision on the GPU, and prints key paths and hardware details. The manifest schema is validated to ensure required columns are present, basic counts for the test split are printed, and a fail fast scan confirms that all audio files referenced by the manifest exist on disk.\n",
    "\n",
    "The cell then prepares inputs for inference. Each clip is assigned to a task group using a strict rule: `task == \"vowl\"` is treated as vowel speech, and everything else is treated as other speech. Sex values are normalized to M, F, or UNK. For D1, sex is encoded numerically in the manifest, so 0 is mapped to F and 1 is mapped to M, with common text labels also supported. A dataset loader reads each audio file, converts stereo audio to mono if needed, enforces a 16 kHz sample rate, and builds an attention mask. For vowel clips, the mask attempts to ignore trailing near silence so padded or silent regions contribute less to the pooled features. A collator pads audio in each batch to the same length and pads the attention masks in the same way. A short warm up step loads a few batches from both the validation and test splits to catch data issues early.\n",
    "\n",
    "The model architecture matches the D1 training setup. It uses a frozen Wav2Vec2 backbone with two separate heads, one for vowel clips and one for other speech. Each head includes a small preprocessing block made of LayerNorm and Dropout followed by a linear classifier, and each clip is routed to the correct head based on its task group. The cell automatically finds the most recent train and validation experiment that contains `best_heads.pt` for all three seeds and loads the saved head weights for each seed.\n",
    "\n",
    "For each seed, evaluation is performed in two stages. First, a validation pass runs inference on the validation split, computes validation AUROC, and selects a validation optimal threshold using Youden’s J statistic, which maximizes the difference between true positive rate and false positive rate. Second, a test pass runs inference on the test split and computes test AUROC and threshold based metrics using the threshold selected from validation, not a fixed value of 0.5. If the validation split cannot produce a threshold, for example if it contains only one class, the seed falls back to a threshold of 0.5 and records a note explaining the reason.\n",
    "\n",
    "For each seed, the cell writes a full set of results under\n",
    "`<DX_OUT_ROOT>/monolingual_test_runs/run_D1_seed<seed>/`.\n",
    "This includes `metrics.json`, a ROC curve image, an overall confusion matrix at the selected threshold, and additional confusion matrices split by sex for M and F when available. Fairness is computed on the test split at the same threshold using the project’s H3 definition. This is based on the false negative rate for male and female Parkinson’s cases only, with ΔFNR defined as FNR(F) minus FNR(M). The absolute difference is also stored. Confusion counts by sex group, including UNK, are recorded as well.\n",
    "\n",
    "After all three seeds finish, the cell aggregates results across seeds and prints them. This includes the mean test AUROC with a 95 percent confidence interval computed using a t distribution with n equal to 3, the mean and standard deviation of the validation selected thresholds, the mean and standard deviation of threshold based test metrics, and the mean and standard deviation of the fairness values. A combined `summary_test.json` file is written to `monolingual_test_runs/`, and the same summary is appended to `history_index.jsonl` so multiple runs can be tracked over time. Finally, the Colab runtime is unassigned to stop the GPU instance."
   ],
   "metadata": {
    "id": "kIfx5uJKoBpJ"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# =========================\n",
    "# D1 Test Only — Val-Optimal Threshold + Fairness (Youden J)\n",
    "# Inputs: manifest_all.csv (VAL for threshold, TEST for reporting), best_heads.pt from latest trainval run\n",
    "# Outputs: per-seed metrics.json + ROC/confusion PNGs, plus monolingual_test_runs/summary_test.json and history_index.jsonl\n",
    "# =========================\n",
    "\n",
    "import os, json, math, random, time, warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import soundfile as sf\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from transformers import Wav2Vec2Model\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, confusion_matrix, matthews_corrcoef\n",
    "from scipy.stats import fisher_exact\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# -------------------------\n",
    "# Runtime sanity checks\n",
    "# Purpose: avoid accidental imports from local files that shadow installed libraries\n",
    "# -------------------------\n",
    "if os.path.exists(\"/content/torch.py\") or os.path.exists(\"/content/torch/__init__.py\"):\n",
    "    raise RuntimeError(\"Found local /content/torch.py or /content/torch/ that shadows PyTorch. Rename/remove it and restart runtime.\")\n",
    "if os.path.exists(\"/content/transformers.py\") or os.path.exists(\"/content/transformers/__init__.py\"):\n",
    "    raise RuntimeError(\"Found local /content/transformers.py or /content/transformers/ that shadows Hugging Face Transformers. Rename/remove it and restart runtime.\")\n",
    "\n",
    "# -------------------------\n",
    "# Drive mount\n",
    "# Purpose: ensure files under MyDrive can be read and written\n",
    "# -------------------------\n",
    "if not os.path.isdir(\"/content/drive/MyDrive\"):\n",
    "    from google.colab import drive  # type: ignore\n",
    "    drive.mount(\"/content/drive\")\n",
    "\n",
    "# -------------------------\n",
    "# Paths\n",
    "# Inputs: DX_OUT_ROOT (preferred if already defined), otherwise D1 fallback\n",
    "# Output: MANIFEST_ALL path used throughout the cell\n",
    "# -------------------------\n",
    "D1_OUT_ROOT_FALLBACK = \"/content/drive/MyDrive/AI_PD_Project/Datasets/D1-NeuroVoz-Castillan Spanish/preprocessed_v1\"\n",
    "DX_OUT_ROOT = globals().get(\"DX_OUT_ROOT\", D1_OUT_ROOT_FALLBACK)\n",
    "MANIFEST_ALL = f\"{DX_OUT_ROOT}/manifests/manifest_all.csv\"\n",
    "\n",
    "# -------------------------\n",
    "# Fixed run settings\n",
    "# Purpose: match trainval assumptions and keep reporting consistent across seeds\n",
    "# -------------------------\n",
    "SEEDS          = [1337, 2024, 7777]\n",
    "BACKBONE_CKPT  = \"facebook/wav2vec2-base\"\n",
    "SR_EXPECTED    = 16000\n",
    "TINY_THRESH    = 1e-4\n",
    "\n",
    "EFFECTIVE_BS   = 64\n",
    "PER_DEVICE_BS  = 16\n",
    "GRAD_ACCUM     = max(1, EFFECTIVE_BS // PER_DEVICE_BS)\n",
    "\n",
    "DROPOUT_P      = 0.2\n",
    "\n",
    "NUM_WORKERS    = 0\n",
    "PIN_MEMORY     = False\n",
    "\n",
    "USE_AMP        = True\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "try:\n",
    "    torch.set_float32_matmul_precision(\"high\")\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", message=\"None of the inputs have requires_grad=True. Gradients will be None\")\n",
    "warnings.filterwarnings(\"ignore\", message=\"Passing `gradient_checkpointing` to a config initialization is deprecated\")\n",
    "\n",
    "print(\"DX_OUT_ROOT:\", DX_OUT_ROOT)\n",
    "print(\"MANIFEST_ALL:\", MANIFEST_ALL)\n",
    "print(\"DEVICE:\", DEVICE)\n",
    "if DEVICE.type == \"cuda\":\n",
    "    print(\"GPU:\", torch.cuda.get_device_name(0))\n",
    "print(\"PER_DEVICE_BS:\", PER_DEVICE_BS, \"| EFFECTIVE_BS:\", PER_DEVICE_BS * GRAD_ACCUM)\n",
    "print(\"NUM_WORKERS:\", NUM_WORKERS, \"| PIN_MEMORY:\", PIN_MEMORY)\n",
    "print(\"USE_AMP:\", bool(USE_AMP and DEVICE.type == \"cuda\"))\n",
    "\n",
    "# -------------------------\n",
    "# Read manifest and build VAL/TEST tables\n",
    "# Inputs: manifest_all.csv\n",
    "# Outputs: val_df and test_df with a standardized set of columns\n",
    "# -------------------------\n",
    "if not os.path.exists(MANIFEST_ALL):\n",
    "    raise FileNotFoundError(f\"Missing manifest_all.csv: {MANIFEST_ALL}\")\n",
    "\n",
    "m_all = pd.read_csv(MANIFEST_ALL)\n",
    "\n",
    "# Purpose: enforce the minimum schema needed for metrics + fairness reporting\n",
    "req_cols = {\"split\", \"clip_path\", \"label_num\", \"task\", \"sex\", \"age\"}\n",
    "missing = [c for c in sorted(req_cols) if c not in m_all.columns]\n",
    "if missing:\n",
    "    raise ValueError(f\"Manifest missing required columns: {missing}. Found: {list(m_all.columns)}\")\n",
    "\n",
    "# Purpose: identify dataset id from the manifest, then filter to that dataset\n",
    "if \"dataset\" in m_all.columns and m_all[\"dataset\"].notna().any():\n",
    "    dataset_id = str(m_all[\"dataset\"].astype(str).value_counts(dropna=True).idxmax())\n",
    "    m_all = m_all[m_all[\"dataset\"].astype(str) == dataset_id].copy()\n",
    "else:\n",
    "    dataset_id = \"DX\"\n",
    "\n",
    "# Guard A\n",
    "# Purpose: prevent running D1 evaluation on the wrong dataset by accident\n",
    "if dataset_id != \"D1\":\n",
    "    raise RuntimeError(\n",
    "        f\"Expected dataset_id=='D1' but got {dataset_id!r}. \"\n",
    "        \"This usually means DX_OUT_ROOT was inherited from a previous cell or the manifest is not D1. \"\n",
    "        f\"DX_OUT_ROOT={DX_OUT_ROOT}\"\n",
    "    )\n",
    "\n",
    "# Purpose: keep a consistent column set even if some optional columns are missing\n",
    "keep_cols = [\"clip_path\", \"label_num\", \"task\", \"speaker_id\", \"sex\", \"age\", \"duration_sec\", \"split\"]\n",
    "for c in keep_cols:\n",
    "    if c not in m_all.columns:\n",
    "        m_all[c] = np.nan\n",
    "m_all = m_all[keep_cols].copy()\n",
    "\n",
    "val_df  = m_all[m_all[\"split\"].isin([\"val\"])].reset_index(drop=True)\n",
    "test_df = m_all[m_all[\"split\"].isin([\"test\"])].reset_index(drop=True)\n",
    "\n",
    "print(f\"\\nDataset inferred: {dataset_id}\")\n",
    "print(f\"VAL rows:  {len(val_df)}\")\n",
    "print(f\"TEST rows: {len(test_df)}\")\n",
    "print(\"TEST label counts:\", test_df[\"label_num\"].value_counts(dropna=False).to_dict())\n",
    "print(\"TEST sex counts (raw):\", test_df[\"sex\"].value_counts(dropna=False).to_dict())\n",
    "\n",
    "# Purpose: fail early if required splits are missing\n",
    "if len(val_df) == 0:\n",
    "    raise RuntimeError(\"After filtering to split=='val', manifest has 0 rows (VAL is required for Youden-J threshold).\")\n",
    "if len(test_df) == 0:\n",
    "    raise RuntimeError(\"After filtering to split=='test', manifest has 0 rows.\")\n",
    "\n",
    "# -------------------------\n",
    "# Clip existence check\n",
    "# Inputs: val_df/test_df clip_path\n",
    "# Output: raises early if any required audio files are missing (shows a few examples)\n",
    "# -------------------------\n",
    "def _fail_fast_missing_paths(df: pd.DataFrame, name: str):\n",
    "    missing_paths = []\n",
    "    for p in tqdm(df[\"clip_path\"].astype(str).tolist(), desc=f\"Check {name} clip_path exists\", dynamic_ncols=True):\n",
    "        if not os.path.exists(p):\n",
    "            missing_paths.append(p)\n",
    "            if len(missing_paths) >= 10:\n",
    "                break\n",
    "    if missing_paths:\n",
    "        raise FileNotFoundError(f\"{name}: missing clip_path(s). Examples: {missing_paths}\")\n",
    "\n",
    "_fail_fast_missing_paths(val_df, \"VAL\")\n",
    "_fail_fast_missing_paths(test_df, \"TEST\")\n",
    "\n",
    "# -------------------------\n",
    "# Task grouping\n",
    "# Purpose: decide which head to use (vowel head vs other head)\n",
    "# Output: task_group column in both VAL and TEST\n",
    "# -------------------------\n",
    "def _task_group(task_val) -> str:\n",
    "    return \"vowel\" if str(task_val) == \"vowl\" else \"other\"\n",
    "\n",
    "val_df[\"task_group\"]  = val_df[\"task\"].apply(_task_group)\n",
    "test_df[\"task_group\"] = test_df[\"task\"].apply(_task_group)\n",
    "\n",
    "# -------------------------\n",
    "# Sex normalization (D1 encoding)\n",
    "# Inputs: manifest sex column (0/1 in D1)\n",
    "# Outputs: sex_norm in {M, F, UNK} for fairness and sex-split charts\n",
    "# -------------------------\n",
    "def normalize_sex(val) -> str:\n",
    "    \"\"\"\n",
    "    Returns 'M', 'F', or 'UNK'\n",
    "\n",
    "    D1 manifest encoding:\n",
    "      0 -> F\n",
    "      1 -> M\n",
    "\n",
    "    Also handles common string encodings: M/F, Male/Female, etc.\n",
    "    \"\"\"\n",
    "    if pd.isna(val):\n",
    "        return \"UNK\"\n",
    "\n",
    "    # numeric (float/int-like)\n",
    "    try:\n",
    "        fv = float(val)\n",
    "        if np.isfinite(fv) and abs(fv - round(fv)) < 1e-9:\n",
    "            iv = int(round(fv))\n",
    "            if iv == 0:\n",
    "                return \"F\"\n",
    "            if iv == 1:\n",
    "                return \"M\"\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # string\n",
    "    s = str(val).strip().lower()\n",
    "    if s in {\"m\", \"male\", \"man\", \"masc\", \"masculine\"}:\n",
    "        return \"M\"\n",
    "    if s in {\"f\", \"female\", \"woman\", \"fem\", \"feminine\"}:\n",
    "        return \"F\"\n",
    "\n",
    "    return \"UNK\"\n",
    "\n",
    "val_df[\"sex_norm\"]  = val_df[\"sex\"].apply(normalize_sex)\n",
    "test_df[\"sex_norm\"] = test_df[\"sex\"].apply(normalize_sex)\n",
    "\n",
    "print(\"VAL sex counts (normalized): \", val_df[\"sex_norm\"].value_counts(dropna=False).to_dict())\n",
    "print(\"TEST sex counts (normalized):\", test_df[\"sex_norm\"].value_counts(dropna=False).to_dict())\n",
    "if (val_df[\"sex_norm\"] == \"UNK\").any() or (test_df[\"sex_norm\"] == \"UNK\").any():\n",
    "    print(\"NOTE: Some 'sex' values could not be normalized to M/F and were counted as 'UNK' for fairness and sex charts.\")\n",
    "\n",
    "# -------------------------\n",
    "# Audio dataset + padding collator\n",
    "# Inputs: val_df/test_df\n",
    "# Outputs: batches with input_values, attention_mask, labels, task_group, sex_norm\n",
    "# -------------------------\n",
    "class AudioManifestDataset(Dataset):\n",
    "    def __init__(self, df: pd.DataFrame):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        row = self.df.iloc[idx]\n",
    "        clip_path = str(row[\"clip_path\"])\n",
    "        label = int(row[\"label_num\"])\n",
    "        task_group = str(row[\"task_group\"])\n",
    "        sex_norm = str(row[\"sex_norm\"])\n",
    "\n",
    "        if not os.path.exists(clip_path):\n",
    "            raise FileNotFoundError(f\"Missing clip_path: {clip_path}\")\n",
    "\n",
    "        # Purpose: load audio, force mono, enforce expected sample rate\n",
    "        y, sr = sf.read(clip_path, always_2d=False)\n",
    "        if isinstance(y, np.ndarray) and y.ndim > 1:\n",
    "            y = np.mean(y, axis=1)\n",
    "        y = np.asarray(y, dtype=np.float32)\n",
    "\n",
    "        if int(sr) != SR_EXPECTED:\n",
    "            raise ValueError(f\"Sample rate mismatch (expected {SR_EXPECTED}): {clip_path} has sr={sr}\")\n",
    "\n",
    "        # Purpose: for vowels, ignore padded trailing silence when pooling\n",
    "        attn = np.ones((len(y),), dtype=np.int64)\n",
    "        if task_group == \"vowel\":\n",
    "            k = -1\n",
    "            for j in range(len(y) - 1, -1, -1):\n",
    "                if abs(float(y[j])) > TINY_THRESH:\n",
    "                    k = j\n",
    "                    break\n",
    "            if k >= 0:\n",
    "                attn[:k+1] = 1\n",
    "                attn[k+1:] = 0\n",
    "            else:\n",
    "                attn[:] = 1\n",
    "        else:\n",
    "            attn[:] = 1\n",
    "\n",
    "        return {\n",
    "            \"input_values\": torch.from_numpy(y),\n",
    "            \"attention_mask\": torch.from_numpy(attn),\n",
    "            \"labels\": torch.tensor(label, dtype=torch.long),\n",
    "            \"task_group\": task_group,\n",
    "            \"sex_norm\": sex_norm,\n",
    "        }\n",
    "\n",
    "def collate_fn(batch):\n",
    "    # Purpose: pad variable-length audio to the max length in the batch\n",
    "    max_len = int(max(b[\"input_values\"].numel() for b in batch))\n",
    "    input_vals, attn_masks, labels, task_groups, sex_norms = [], [], [], [], []\n",
    "    for b in batch:\n",
    "        x = b[\"input_values\"]\n",
    "        a = b[\"attention_mask\"]\n",
    "        pad = max_len - x.numel()\n",
    "        if pad > 0:\n",
    "            x = torch.cat([x, torch.zeros(pad, dtype=x.dtype)], dim=0)\n",
    "            a = torch.cat([a, torch.zeros(pad, dtype=a.dtype)], dim=0)\n",
    "        input_vals.append(x)\n",
    "        attn_masks.append(a)\n",
    "        labels.append(b[\"labels\"])\n",
    "        task_groups.append(b[\"task_group\"])\n",
    "        sex_norms.append(b[\"sex_norm\"])\n",
    "    return {\n",
    "        \"input_values\": torch.stack(input_vals, dim=0),\n",
    "        \"attention_mask\": torch.stack(attn_masks, dim=0),\n",
    "        \"labels\": torch.stack(labels, dim=0),\n",
    "        \"task_group\": task_groups,\n",
    "        \"sex_norm\": sex_norms,\n",
    "    }\n",
    "\n",
    "# -------------------------\n",
    "# Model (frozen backbone + two heads)\n",
    "# Inputs: wav2vec2 checkpoint and dropout\n",
    "# Output: logits for PD vs Healthy using the head chosen by task_group\n",
    "# -------------------------\n",
    "class Wav2Vec2TwoHeadClassifier(nn.Module):\n",
    "    def __init__(self, ckpt: str, dropout_p: float):\n",
    "        super().__init__()\n",
    "        self.backbone = Wav2Vec2Model.from_pretrained(ckpt, use_safetensors=True)\n",
    "        for p in self.backbone.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "        H = int(self.backbone.config.hidden_size)\n",
    "        self.pre_vowel = nn.Sequential(nn.LayerNorm(H), nn.Dropout(float(dropout_p)))\n",
    "        self.pre_other = nn.Sequential(nn.LayerNorm(H), nn.Dropout(float(dropout_p)))\n",
    "        self.head_vowel = nn.Linear(H, 2)\n",
    "        self.head_other = nn.Linear(H, 2)\n",
    "\n",
    "    def masked_mean_pool(self, last_hidden, attn_mask_samples):\n",
    "        # Purpose: pool frame features using a mask derived from the sample-level attention mask\n",
    "        feat_mask = self.backbone._get_feature_vector_attention_mask(last_hidden.shape[1], attn_mask_samples)\n",
    "        feat_mask = feat_mask.to(last_hidden.device).unsqueeze(-1).type_as(last_hidden)\n",
    "        masked = last_hidden * feat_mask\n",
    "        denom = feat_mask.sum(dim=1).clamp(min=1.0)\n",
    "        return masked.sum(dim=1) / denom\n",
    "\n",
    "    def _heads_fp32(self, x_fp_any: torch.Tensor, head: nn.Module) -> torch.Tensor:\n",
    "        # Purpose: run small heads in fp32 even when AMP is enabled\n",
    "        x = x_fp_any.float()\n",
    "        if x.is_cuda:\n",
    "            with torch.amp.autocast(device_type=\"cuda\", enabled=False):\n",
    "                return head(x)\n",
    "        return head(x)\n",
    "\n",
    "    def forward_logits(self, input_values, attention_mask, task_group):\n",
    "        with torch.no_grad():\n",
    "            out = self.backbone(input_values=input_values, attention_mask=attention_mask)\n",
    "            last_hidden = out.last_hidden_state\n",
    "\n",
    "        pooled = self.masked_mean_pool(last_hidden, attention_mask)  # [B,H]\n",
    "\n",
    "        z_v = self.pre_vowel(pooled.float())\n",
    "        z_o = self.pre_other(pooled.float())\n",
    "\n",
    "        logits_v = self._heads_fp32(z_v, self.head_vowel)\n",
    "        logits_o = self._heads_fp32(z_o, self.head_other)\n",
    "\n",
    "        # Purpose: choose the correct head per sample based on task_group\n",
    "        is_vowel = torch.tensor([tg == \"vowel\" for tg in task_group], device=pooled.device, dtype=torch.bool)\n",
    "        logits = logits_o.clone()\n",
    "        if is_vowel.any():\n",
    "            logits[is_vowel] = logits_v[is_vowel]\n",
    "        return logits\n",
    "\n",
    "# -------------------------\n",
    "# Metrics and plots\n",
    "# Inputs: y_true and y_prob (PD probability)\n",
    "# Outputs: AUROC, threshold metrics, ROC/confusion PNG files\n",
    "# -------------------------\n",
    "def compute_auc(y_true, y_prob):\n",
    "    y_true = np.asarray(y_true, dtype=np.int64)\n",
    "    y_prob = np.asarray(y_prob, dtype=np.float64)\n",
    "    if len(np.unique(y_true)) < 2:\n",
    "        return float(\"nan\")\n",
    "    return float(roc_auc_score(y_true, y_prob))\n",
    "\n",
    "def compute_threshold_metrics(y_true, y_prob, thr=0.5):\n",
    "    # Purpose: compute threshold-based metrics and confusion counts\n",
    "    y_true = np.asarray(y_true, dtype=np.int64)\n",
    "    y_prob = np.asarray(y_prob, dtype=np.float64)\n",
    "    y_pred = (y_prob >= float(thr)).astype(np.int64)\n",
    "\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=[0, 1])\n",
    "    TN, FP, FN, TP = int(cm[0, 0]), int(cm[0, 1]), int(cm[1, 0]), int(cm[1, 1])\n",
    "\n",
    "    eps = 1e-12\n",
    "    acc = (TP + TN) / max(1, (TP + TN + FP + FN))\n",
    "    prec = TP / (TP + FP + eps)\n",
    "    rec = TP / (TP + FN + eps)     # sensitivity\n",
    "    f1 = 2 * prec * rec / (prec + rec + eps)\n",
    "    spec = TN / (TN + FP + eps)\n",
    "\n",
    "    try:\n",
    "        mcc = float(matthews_corrcoef(y_true, y_pred)) if len(np.unique(y_pred)) > 1 else float(\"nan\")\n",
    "    except Exception:\n",
    "        mcc = float(\"nan\")\n",
    "\n",
    "    try:\n",
    "        _, pval = fisher_exact([[TN, FP], [FN, TP]], alternative=\"two-sided\")\n",
    "        pval = float(pval)\n",
    "    except Exception:\n",
    "        pval = float(\"nan\")\n",
    "\n",
    "    return {\n",
    "        \"threshold\": float(thr),\n",
    "        \"confusion_matrix\": {\"TN\": TN, \"FP\": FP, \"FN\": FN, \"TP\": TP},\n",
    "        \"accuracy\": float(acc),\n",
    "        \"precision\": float(prec),\n",
    "        \"recall\": float(rec),\n",
    "        \"f1_score\": float(f1),\n",
    "        \"sensitivity\": float(rec),\n",
    "        \"specificity\": float(spec),\n",
    "        \"mcc\": float(mcc),\n",
    "        \"p_value_fisher_two_sided\": float(pval),\n",
    "    }\n",
    "\n",
    "def save_roc_curve_png(y_true, y_prob, out_png, title_suffix=\"Test\"):\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_prob)\n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr)\n",
    "    plt.plot([0, 1], [0, 1], linestyle=\"--\")\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(f\"ROC Curve ({title_suffix})\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_png, dpi=150)\n",
    "    plt.close()\n",
    "\n",
    "def save_confusion_png(y_true, y_prob, out_png, thr=0.5, title_suffix=\"Test\"):\n",
    "    y_true = np.asarray(y_true, dtype=np.int64)\n",
    "    y_prob = np.asarray(y_prob, dtype=np.float64)\n",
    "    y_pred = (y_prob >= float(thr)).astype(int)\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=[0, 1])\n",
    "\n",
    "    plt.figure()\n",
    "    plt.imshow(cm)\n",
    "    plt.xticks([0, 1], [\"Healthy(0)\", \"PD(1)\"])\n",
    "    plt.yticks([0, 1], [\"Healthy(0)\", \"PD(1)\"])\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"True\")\n",
    "    plt.title(f\"Confusion Matrix ({title_suffix}, thr={thr:.2f})\")\n",
    "    for (i, j), v in np.ndenumerate(cm):\n",
    "        plt.text(j, i, str(v), ha=\"center\", va=\"center\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_png, dpi=150)\n",
    "    plt.close()\n",
    "\n",
    "# -------------------------\n",
    "# Val-optimal threshold (Youden J)\n",
    "# Input: VAL labels and probabilities\n",
    "# Output: threshold that maximizes (TPR - FPR) on the VAL ROC curve\n",
    "# -------------------------\n",
    "def youden_j_optimal_threshold(y_true, y_prob):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      thr_opt, j_opt, tpr_opt, fpr_opt\n",
    "    If undefined (single-class), returns (nan, nan, nan, nan).\n",
    "    \"\"\"\n",
    "    y_true = np.asarray(y_true, dtype=np.int64)\n",
    "    y_prob = np.asarray(y_prob, dtype=np.float64)\n",
    "    if len(np.unique(y_true)) < 2:\n",
    "        return float(\"nan\"), float(\"nan\"), float(\"nan\"), float(\"nan\")\n",
    "\n",
    "    fpr, tpr, thr = roc_curve(y_true, y_prob)\n",
    "    j = tpr - fpr\n",
    "    if len(j) == 0:\n",
    "        return float(\"nan\"), float(\"nan\"), float(\"nan\"), float(\"nan\")\n",
    "\n",
    "    best_idx = int(np.nanargmax(j))\n",
    "    return float(thr[best_idx]), float(j[best_idx]), float(tpr[best_idx]), float(fpr[best_idx])\n",
    "\n",
    "# -------------------------\n",
    "# Fairness (H3): FNR by sex and ΔFNR\n",
    "# Inputs: TEST labels, probabilities, sex_norm, and chosen threshold\n",
    "# Outputs: per-group FNR and ΔFNR = FNR(F) - FNR(M)\n",
    "# -------------------------\n",
    "def compute_fnr_by_group_signed(y_true, y_prob, groups, thr=0.5):\n",
    "    \"\"\"\n",
    "    FNR = FN/(FN+TP) computed on POSITIVE ground-truth samples only.\n",
    "    Returns:\n",
    "      - per-group: n_total, n_pos, tp, fn, fnr\n",
    "      - delta_f_minus_m (signed): FNR(F) - FNR(M) when both defined\n",
    "      - delta_abs_m_f: |FNR(F) - FNR(M)|\n",
    "    \"\"\"\n",
    "    y_true = np.asarray(y_true, dtype=np.int64)\n",
    "    y_prob = np.asarray(y_prob, dtype=np.float64)\n",
    "    groups = np.asarray(list(groups), dtype=object)\n",
    "    y_pred = (y_prob >= float(thr)).astype(np.int64)\n",
    "\n",
    "    out = {}\n",
    "    for g in sorted(set(groups.tolist())):\n",
    "        mask_g = (groups == g)\n",
    "        if mask_g.sum() == 0:\n",
    "            continue\n",
    "\n",
    "        pos_mask = mask_g & (y_true == 1)\n",
    "        n_pos = int(pos_mask.sum())\n",
    "        if n_pos == 0:\n",
    "            out[g] = {\"n_total\": int(mask_g.sum()), \"n_pos\": 0, \"tp\": 0, \"fn\": 0, \"fnr\": float(\"nan\")}\n",
    "            continue\n",
    "\n",
    "        tp = int(((y_pred == 1) & pos_mask).sum())\n",
    "        fn = int(((y_pred == 0) & pos_mask).sum())\n",
    "        fnr = float(fn / max(1, (fn + tp)))\n",
    "\n",
    "        out[g] = {\"n_total\": int(mask_g.sum()), \"n_pos\": int(n_pos), \"tp\": int(tp), \"fn\": int(fn), \"fnr\": float(fnr)}\n",
    "\n",
    "    fnr_m = out.get(\"M\", {}).get(\"fnr\", float(\"nan\"))\n",
    "    fnr_f = out.get(\"F\", {}).get(\"fnr\", float(\"nan\"))\n",
    "\n",
    "    if (not np.isnan(fnr_m)) and (not np.isnan(fnr_f)):\n",
    "        delta_signed = float(fnr_f - fnr_m)         # H3: FNR(F) - FNR(M)\n",
    "        delta_abs = float(abs(delta_signed))\n",
    "    else:\n",
    "        delta_signed = float(\"nan\")\n",
    "        delta_abs = float(\"nan\")\n",
    "\n",
    "    return out, delta_signed, delta_abs\n",
    "\n",
    "# -------------------------\n",
    "# Confusion counts by sex\n",
    "# Inputs: TEST labels, probabilities, sex_norm, threshold\n",
    "# Output: TN/FP/FN/TP counts per sex group\n",
    "# -------------------------\n",
    "def compute_confusion_counts(y_true, y_prob, thr=0.5):\n",
    "    y_true = np.asarray(y_true, dtype=np.int64)\n",
    "    y_prob = np.asarray(y_prob, dtype=np.float64)\n",
    "    y_pred = (y_prob >= float(thr)).astype(np.int64)\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=[0, 1])\n",
    "    TN, FP, FN, TP = int(cm[0, 0]), int(cm[0, 1]), int(cm[1, 0]), int(cm[1, 1])\n",
    "    return {\"TN\": TN, \"FP\": FP, \"FN\": FN, \"TP\": TP}\n",
    "\n",
    "def compute_confusion_by_group(y_true, y_prob, groups, thr=0.5):\n",
    "    y_true = np.asarray(y_true, dtype=np.int64)\n",
    "    y_prob = np.asarray(y_prob, dtype=np.float64)\n",
    "    groups = np.asarray(list(groups), dtype=object)\n",
    "    out = {}\n",
    "    for g in sorted(set(groups.tolist())):\n",
    "        mask = (groups == g)\n",
    "        if int(mask.sum()) == 0:\n",
    "            continue\n",
    "        out[g] = {\n",
    "            \"n\": int(mask.sum()),\n",
    "            \"confusion\": compute_confusion_counts(y_true[mask], y_prob[mask], thr=thr),\n",
    "        }\n",
    "    return out\n",
    "\n",
    "# -------------------------\n",
    "# Reproducibility seeding\n",
    "# Purpose: stabilize dataloader order and any stochastic ops\n",
    "# -------------------------\n",
    "def set_all_seeds(seed: int):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# -------------------------\n",
    "# Pick latest trainval experiment with all required head checkpoints\n",
    "# Inputs: trainval_runs/exp_* folders\n",
    "# Output: chosen_exp path\n",
    "# -------------------------\n",
    "TRAINVAL_ROOT = Path(DX_OUT_ROOT) / \"trainval_runs\"\n",
    "if not TRAINVAL_ROOT.exists():\n",
    "    raise FileNotFoundError(f\"Missing trainval_runs folder: {str(TRAINVAL_ROOT)}\")\n",
    "\n",
    "exp_dirs = sorted([p for p in TRAINVAL_ROOT.glob(\"exp_*\") if p.is_dir()], key=lambda p: p.stat().st_mtime, reverse=True)\n",
    "if not exp_dirs:\n",
    "    raise FileNotFoundError(f\"No exp_* folders found under: {str(TRAINVAL_ROOT)}\")\n",
    "\n",
    "def _has_all_seeds(exp_path: Path, dataset_id: str, seeds: list):\n",
    "    for s in seeds:\n",
    "        p = exp_path / f\"run_{dataset_id}_seed{s}\" / \"best_heads.pt\"\n",
    "        if not p.exists():\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "chosen_exp = None\n",
    "for ed in exp_dirs:\n",
    "    if _has_all_seeds(ed, dataset_id, SEEDS):\n",
    "        chosen_exp = ed\n",
    "        break\n",
    "\n",
    "if chosen_exp is None:\n",
    "    sample = exp_dirs[0]\n",
    "    raise FileNotFoundError(\n",
    "        \"Could not find a recent trainval experiment with all 3 best_heads.pt files.\\n\"\n",
    "        f\"Expected under: {str(TRAINVAL_ROOT)}/exp_*/run_{dataset_id}_seedXXXX/best_heads.pt\\n\"\n",
    "        f\"Most recent exp checked: {str(sample)}\"\n",
    "    )\n",
    "\n",
    "print(\"\\nUsing Train+Val experiment folder:\")\n",
    "print(\" \", str(chosen_exp))\n",
    "\n",
    "# Guard B\n",
    "# Purpose: confirm artifacts exist right after selecting the experiment folder\n",
    "for s in SEEDS:\n",
    "    p = chosen_exp / f\"run_{dataset_id}_seed{s}\" / \"best_heads.pt\"\n",
    "    if not p.exists():\n",
    "        raise RuntimeError(f\"Trainval artifact missing after choosing exp. Missing: {str(p)}\")\n",
    "\n",
    "# -------------------------\n",
    "# Output folder for this evaluation\n",
    "# Output: per-seed run_... folders and aggregated summary files\n",
    "# -------------------------\n",
    "TEST_ROOT = Path(DX_OUT_ROOT) / \"monolingual_test_runs\"\n",
    "TEST_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# -------------------------\n",
    "# Dataloaders for VAL and TEST\n",
    "# Inputs: val_df/test_df\n",
    "# Output: val_loader/test_loader used by inference\n",
    "# -------------------------\n",
    "val_ds = AudioManifestDataset(val_df)\n",
    "val_loader = DataLoader(\n",
    "    val_ds,\n",
    "    batch_size=PER_DEVICE_BS,\n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=PIN_MEMORY,\n",
    "    collate_fn=collate_fn,\n",
    ")\n",
    "\n",
    "test_ds = AudioManifestDataset(test_df)\n",
    "test_loader = DataLoader(\n",
    "    test_ds,\n",
    "    batch_size=PER_DEVICE_BS,\n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=PIN_MEMORY,\n",
    "    collate_fn=collate_fn,\n",
    ")\n",
    "\n",
    "# -------------------------\n",
    "# Loader warm-up\n",
    "# Purpose: catch dataloader issues early (empty batches, IO problems)\n",
    "# -------------------------\n",
    "print(\"\\nWarm-up: loading up to 3 VAL + TEST batches...\")\n",
    "t0 = time.time()\n",
    "\n",
    "def _warmup(loader, name):\n",
    "    nb = len(loader)\n",
    "    wb = min(3, nb)\n",
    "    if wb == 0:\n",
    "        raise RuntimeError(f\"{name} DataLoader has 0 batches. Check df length and PER_DEVICE_BS.\")\n",
    "    it = iter(loader)\n",
    "    for i in range(wb):\n",
    "        _ = next(it)\n",
    "        print(f\"  loaded warmup {name} batch {i+1}/{wb}\")\n",
    "\n",
    "_warmup(val_loader, \"VAL\")\n",
    "_warmup(test_loader, \"TEST\")\n",
    "print(f\"Warm-up done in {time.time()-t0:.2f}s\")\n",
    "\n",
    "# -------------------------\n",
    "# Load trained heads into the model\n",
    "# Inputs: best_heads.pt written by D1 trainval\n",
    "# Output: model ready for inference\n",
    "# -------------------------\n",
    "def load_heads_into_model(model: Wav2Vec2TwoHeadClassifier, best_heads_path: Path):\n",
    "    if not best_heads_path.exists():\n",
    "        raise FileNotFoundError(f\"Missing best_heads.pt: {str(best_heads_path)}\")\n",
    "    state = torch.load(str(best_heads_path), map_location=\"cpu\")\n",
    "    model.pre_vowel.load_state_dict(state[\"pre_vowel\"], strict=True)\n",
    "    model.pre_other.load_state_dict(state[\"pre_other\"], strict=True)\n",
    "    model.head_vowel.load_state_dict(state[\"head_vowel\"], strict=True)\n",
    "    model.head_other.load_state_dict(state[\"head_other\"], strict=True)\n",
    "    return model\n",
    "\n",
    "# -------------------------\n",
    "# Inference helper\n",
    "# Inputs: loader and model\n",
    "# Outputs: y_true, y_prob(PD), and sex_norm for each clip\n",
    "# -------------------------\n",
    "def _infer_probs(loader, model, desc):\n",
    "    use_amp = bool(USE_AMP and DEVICE.type == \"cuda\")\n",
    "    all_probs, all_true, all_sex = [], [], []\n",
    "\n",
    "    pbar = tqdm(loader, desc=desc, dynamic_ncols=True)\n",
    "    with torch.inference_mode():\n",
    "        for batch in pbar:\n",
    "            input_values = batch[\"input_values\"].to(DEVICE, non_blocking=False)\n",
    "            attention_mask = batch[\"attention_mask\"].to(DEVICE, non_blocking=False)\n",
    "            labels = batch[\"labels\"].to(DEVICE, non_blocking=False)\n",
    "            task_group = batch[\"task_group\"]\n",
    "            sex_norm = batch[\"sex_norm\"]\n",
    "\n",
    "            with torch.amp.autocast(device_type=\"cuda\", enabled=use_amp):\n",
    "                logits = model.forward_logits(input_values, attention_mask, task_group)\n",
    "\n",
    "            probs = torch.softmax(logits.float(), dim=-1)[:, 1].detach().cpu().numpy().astype(np.float64)\n",
    "            all_probs.extend(probs.tolist())\n",
    "            all_true.extend(labels.detach().cpu().numpy().astype(np.int64).tolist())\n",
    "            all_sex.extend(list(sex_norm))\n",
    "\n",
    "    return np.asarray(all_true, dtype=np.int64), np.asarray(all_probs, dtype=np.float64), np.asarray(all_sex, dtype=object)\n",
    "\n",
    "# -------------------------\n",
    "# One seed run: VAL threshold, then TEST reporting\n",
    "# Inputs: seed, val_loader, test_loader, chosen_exp/best_heads.pt\n",
    "# Outputs: run_dir artifacts + metrics.json, and a compact console summary\n",
    "# -------------------------\n",
    "def run_test_once(seed: int):\n",
    "    set_all_seeds(seed)\n",
    "\n",
    "    run_dir = TEST_ROOT / f\"run_{dataset_id}_seed{seed}\"\n",
    "    run_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    best_heads_path = chosen_exp / f\"run_{dataset_id}_seed{seed}\" / \"best_heads.pt\"\n",
    "\n",
    "    print(f\"\\n[seed={seed}] Loading model + heads from:\")\n",
    "    print(\" \", str(best_heads_path))\n",
    "\n",
    "    model = Wav2Vec2TwoHeadClassifier(BACKBONE_CKPT, dropout_p=DROPOUT_P).to(DEVICE)\n",
    "    model = load_heads_into_model(model, best_heads_path)\n",
    "    model.eval()\n",
    "\n",
    "    # VAL: select threshold only\n",
    "    yv_true, yv_prob, _ = _infer_probs(val_loader, model, desc=f\"[seed={seed}] Val\")\n",
    "    val_auc = compute_auc(yv_true, yv_prob)\n",
    "    val_thr, val_j, val_tpr, val_fpr = youden_j_optimal_threshold(yv_true, yv_prob)\n",
    "\n",
    "    print(f\"[seed={seed}] VAL Youden-J threshold: {val_thr:.6f}\" if not np.isnan(val_thr) else f\"[seed={seed}] VAL Youden-J threshold: nan\")\n",
    "\n",
    "    # TEST: report at fixed val-selected threshold\n",
    "    yt_true, yt_prob, yt_sex = _infer_probs(test_loader, model, desc=f\"[seed={seed}] Test\")\n",
    "    test_auc = compute_auc(yt_true, yt_prob)\n",
    "\n",
    "    # Purpose: avoid crashing if VAL ROC cannot be formed (single-class VAL)\n",
    "    if np.isnan(val_thr):\n",
    "        thr_used = 0.5\n",
    "        thr_note = \"VAL Youden-J threshold was NaN (single-class VAL). Fallback thr_used=0.5 for this seed only.\"\n",
    "    else:\n",
    "        thr_used = float(val_thr)\n",
    "        thr_note = None\n",
    "\n",
    "    thr_metrics_test = compute_threshold_metrics(yt_true, yt_prob, thr=thr_used)\n",
    "\n",
    "    fnr_by_sex, delta_f_minus_m, delta_abs = compute_fnr_by_group_signed(\n",
    "        yt_true, yt_prob, yt_sex, thr=thr_used\n",
    "    )\n",
    "\n",
    "    confusion_by_sex = compute_confusion_by_group(yt_true, yt_prob, yt_sex, thr=thr_used)\n",
    "\n",
    "    # Plots: overall and sex-split confusion matrices\n",
    "    roc_png = run_dir / \"roc_curve.png\"\n",
    "    cm_png  = run_dir / \"confusion_matrix.png\"\n",
    "    save_roc_curve_png(yt_true, yt_prob, str(roc_png), title_suffix=f\"Test (seed={seed})\")\n",
    "    save_confusion_png(yt_true, yt_prob, str(cm_png), thr=thr_used, title_suffix=f\"Test (seed={seed})\")\n",
    "\n",
    "    cm_m_png = None\n",
    "    cm_f_png = None\n",
    "    mask_m = (yt_sex == \"M\")\n",
    "    mask_f = (yt_sex == \"F\")\n",
    "\n",
    "    if int(mask_m.sum()) > 0:\n",
    "        cm_m_png = run_dir / \"confusion_matrix_M.png\"\n",
    "        save_confusion_png(\n",
    "            yt_true[mask_m], yt_prob[mask_m], str(cm_m_png),\n",
    "            thr=thr_used, title_suffix=f\"Test SEX=M (seed={seed})\"\n",
    "        )\n",
    "\n",
    "    if int(mask_f.sum()) > 0:\n",
    "        cm_f_png = run_dir / \"confusion_matrix_F.png\"\n",
    "        save_confusion_png(\n",
    "            yt_true[mask_f], yt_prob[mask_f], str(cm_f_png),\n",
    "            thr=thr_used, title_suffix=f\"Test SEX=F (seed={seed})\"\n",
    "        )\n",
    "\n",
    "    # Output: one JSON per seed with the key values needed for later analysis and paper writing\n",
    "    metrics = {\n",
    "        \"dataset\": dataset_id,\n",
    "        \"seed\": int(seed),\n",
    "\n",
    "        \"n_val\": int(len(val_df)),\n",
    "        \"label_counts_val\": val_df[\"label_num\"].value_counts(dropna=False).to_dict(),\n",
    "        \"sex_counts_val_norm\": val_df[\"sex_norm\"].value_counts(dropna=False).to_dict(),\n",
    "        \"val_auroc\": float(val_auc),\n",
    "\n",
    "        \"val_optimal_threshold\": {\n",
    "            \"method\": \"Youden J (argmax(TPR - FPR) on VAL ROC)\",\n",
    "            \"threshold\": float(val_thr),\n",
    "            \"youden_j\": float(val_j),\n",
    "            \"tpr_at_threshold\": float(val_tpr),\n",
    "            \"fpr_at_threshold\": float(val_fpr),\n",
    "        },\n",
    "\n",
    "        \"n_test\": int(len(test_df)),\n",
    "        \"label_counts_test\": test_df[\"label_num\"].value_counts(dropna=False).to_dict(),\n",
    "        \"sex_counts_test_norm\": test_df[\"sex_norm\"].value_counts(dropna=False).to_dict(),\n",
    "        \"test_auroc\": float(test_auc),\n",
    "\n",
    "        \"test_threshold_used\": float(thr_used),\n",
    "        \"test_threshold_note\": thr_note,\n",
    "        \"threshold_metrics_test_at_val_opt\": thr_metrics_test,\n",
    "\n",
    "        \"fairness_test_at_val_opt\": {\n",
    "            \"definition\": \"H3: ΔFNR = FNR(F) - FNR(M), where FNR(sex)=FN/(FN+TP) computed on PD-only true labels at test_threshold_used.\",\n",
    "            \"threshold_used\": float(thr_used),\n",
    "            \"fnr_by_sex_norm\": fnr_by_sex,\n",
    "            \"delta_fnr_F_minus_M\": float(delta_f_minus_m),\n",
    "            \"delta_fnr_abs\": float(delta_abs),\n",
    "            \"note\": \"If n_PD for a sex is 0, its FNR is NaN and ΔFNR is NaN.\",\n",
    "            \"sex_normalization_note\": \"D1 mapping: 0->F, 1->M; otherwise UNK.\",\n",
    "        },\n",
    "\n",
    "        \"confusion_by_sex_norm_at_val_opt\": confusion_by_sex,\n",
    "\n",
    "        \"artifacts\": {\n",
    "            \"roc_curve_png\": str(roc_png),\n",
    "            \"confusion_matrix_png\": str(cm_png),\n",
    "            \"confusion_matrix_M_png\": str(cm_m_png) if cm_m_png is not None else None,\n",
    "            \"confusion_matrix_F_png\": str(cm_f_png) if cm_f_png is not None else None,\n",
    "        },\n",
    "\n",
    "        \"dx_out_root\": DX_OUT_ROOT,\n",
    "        \"trainval_experiment_used\": str(chosen_exp),\n",
    "        \"best_heads_path\": str(best_heads_path),\n",
    "        \"backbone_ckpt\": BACKBONE_CKPT,\n",
    "        \"dropout_p\": float(DROPOUT_P),\n",
    "        \"timestamp\": time.strftime(\"%Y%m%d_%H%M%S\"),\n",
    "    }\n",
    "\n",
    "    metrics_path = run_dir / \"metrics.json\"\n",
    "    with open(metrics_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(metrics, f, indent=2)\n",
    "\n",
    "    print(f\"[seed={seed}] DONE | test_AUROC={test_auc:.6f}\")\n",
    "\n",
    "    def _fmt_fnr(dct, g):\n",
    "        d = (dct or {}).get(g, None)\n",
    "        if d is None:\n",
    "            return \"n/a\"\n",
    "        return f\"fnr={d['fnr']:.6f} (n_PD={d['n_pos']}, fn={d['fn']}, tp={d['tp']})\"\n",
    "\n",
    "    print(f\"[seed={seed}] TEST @ thr_used={thr_used:.6f}:\")\n",
    "    if thr_note is not None:\n",
    "        print(\"  NOTE:\", thr_note)\n",
    "\n",
    "    print(f\"[seed={seed}] FAIRNESS (H3) @ thr_used={thr_used:.6f}:\")\n",
    "    print(\"  M:\", _fmt_fnr(fnr_by_sex, \"M\"))\n",
    "    print(\"  F:\", _fmt_fnr(fnr_by_sex, \"F\"))\n",
    "    if fnr_by_sex is not None and \"UNK\" in fnr_by_sex:\n",
    "        print(\"  UNK:\", _fmt_fnr(fnr_by_sex, \"UNK\"))\n",
    "    print(\"  ΔFNR (F-M):\", f\"{delta_f_minus_m:.6f}\" if not np.isnan(delta_f_minus_m) else \"nan\")\n",
    "    print(\"  |ΔFNR|:\", f\"{delta_abs:.6f}\" if not np.isnan(delta_abs) else \"nan\")\n",
    "\n",
    "    print(f\"[seed={seed}] WROTE:\")\n",
    "    print(\" \", str(metrics_path))\n",
    "    print(\" \", str(roc_png))\n",
    "    print(\" \", str(cm_png))\n",
    "    if cm_m_png is not None:\n",
    "        print(\" \", str(cm_m_png))\n",
    "    if cm_f_png is not None:\n",
    "        print(\" \", str(cm_f_png))\n",
    "\n",
    "    return {\n",
    "        \"seed\": int(seed),\n",
    "        \"val_thr\": float(val_thr),\n",
    "        \"thr_used\": float(thr_used),\n",
    "        \"thr_note\": thr_note,\n",
    "        \"val_auc\": float(val_auc),\n",
    "        \"test_auc\": float(test_auc),\n",
    "        \"thr_metrics_test\": thr_metrics_test,\n",
    "        \"fnr_by_sex\": fnr_by_sex,\n",
    "        \"delta_signed\": float(delta_f_minus_m),\n",
    "        \"delta_abs\": float(delta_abs),\n",
    "        \"run_dir\": str(run_dir),\n",
    "    }\n",
    "\n",
    "# -------------------------\n",
    "# Run all seeds and aggregate results\n",
    "# Outputs: printed summary + summary_test.json + history_index.jsonl\n",
    "# -------------------------\n",
    "results = []\n",
    "for seed in SEEDS:\n",
    "    results.append(run_test_once(seed))\n",
    "\n",
    "# Purpose: report AUROC mean and 95% CI across seeds\n",
    "aurocs = [r[\"test_auc\"] for r in results]\n",
    "t_crit = 4.302652729911275  # df=2, 95% CI\n",
    "n = len(aurocs)\n",
    "mean_auc = float(np.mean(aurocs))\n",
    "std_auc = float(np.std(aurocs, ddof=1)) if n > 1 else 0.0\n",
    "half_width = float(t_crit * (std_auc / math.sqrt(n))) if n > 1 else 0.0\n",
    "ci95 = [float(mean_auc - half_width), float(mean_auc + half_width)]\n",
    "\n",
    "def _mean_sd(vals):\n",
    "    vals = np.asarray(vals, dtype=np.float64)\n",
    "    return float(np.nanmean(vals)), float(np.nanstd(vals, ddof=1)) if np.sum(~np.isnan(vals)) > 1 else 0.0\n",
    "\n",
    "# Purpose: summarize thresholds and threshold-based metrics across seeds\n",
    "val_thrs = [float(r[\"val_thr\"]) for r in results]\n",
    "val_thr_mean, val_thr_sd = _mean_sd(val_thrs)\n",
    "\n",
    "thr_used_list = [float(r[\"thr_used\"]) for r in results]\n",
    "thr_used_mean, thr_used_sd = _mean_sd(thr_used_list)\n",
    "\n",
    "thr_list = [r[\"thr_metrics_test\"] for r in results]\n",
    "keys = [\"accuracy\",\"precision\",\"recall\",\"f1_score\",\"sensitivity\",\"specificity\",\"mcc\",\"p_value_fisher_two_sided\"]\n",
    "agg = {}\n",
    "for k in keys:\n",
    "    v = [float(tm.get(k, float(\"nan\"))) for tm in thr_list]\n",
    "    mu, sd = _mean_sd(v)\n",
    "    agg[k] = {\"mean\": mu, \"sd\": sd, \"values_by_seed\": {str(s): float(tm.get(k, float(\"nan\"))) for s, tm in zip(SEEDS, thr_list)}}\n",
    "cm_by_seed = {str(s): thr_list[i][\"confusion_matrix\"] for i, s in enumerate(SEEDS)}\n",
    "\n",
    "# Purpose: summarize fairness (H3) across seeds\n",
    "fnr_by_seed = {str(r[\"seed\"]): r[\"fnr_by_sex\"] for r in results}\n",
    "delta_signed_by_seed = {str(r[\"seed\"]): float(r[\"delta_signed\"]) for r in results}\n",
    "delta_abs_by_seed = {str(r[\"seed\"]): float(r[\"delta_abs\"]) for r in results}\n",
    "\n",
    "fnr_m_vals, fnr_f_vals, n_pd_m_vals, n_pd_f_vals = [], [], [], []\n",
    "d_signed_vals, d_abs_vals = [], []\n",
    "for r in results:\n",
    "    d = r[\"fnr_by_sex\"] or {}\n",
    "    fnr_m_vals.append(float(d.get(\"M\", {}).get(\"fnr\", float(\"nan\"))))\n",
    "    fnr_f_vals.append(float(d.get(\"F\", {}).get(\"fnr\", float(\"nan\"))))\n",
    "    n_pd_m_vals.append(float(d.get(\"M\", {}).get(\"n_pos\", float(\"nan\"))))\n",
    "    n_pd_f_vals.append(float(d.get(\"F\", {}).get(\"n_pos\", float(\"nan\"))))\n",
    "    d_signed_vals.append(float(r[\"delta_signed\"]))\n",
    "    d_abs_vals.append(float(r[\"delta_abs\"]))\n",
    "\n",
    "fnr_m_mean, fnr_m_sd = _mean_sd(fnr_m_vals)\n",
    "fnr_f_mean, fnr_f_sd = _mean_sd(fnr_f_vals)\n",
    "d_signed_mean, d_signed_sd = _mean_sd(d_signed_vals)\n",
    "d_abs_mean, d_abs_sd = _mean_sd(d_abs_vals)\n",
    "\n",
    "print(\"\\nTest AUROC by seed:\")\n",
    "for r in results:\n",
    "    print(f\"  seed {r['seed']}: {r['test_auc']:.6f}\")\n",
    "print(f\"\\nMean Test AUROC: {mean_auc:.6f}\")\n",
    "print(f\"95% CI (t, n=3): [{ci95[0]:.6f}, {ci95[1]:.6f}]\")\n",
    "\n",
    "print(\"\\nVAL Youden-J threshold by seed:\")\n",
    "for r in results:\n",
    "    vthr = r[\"val_thr\"]\n",
    "    if np.isnan(vthr):\n",
    "        print(f\"  seed {r['seed']}: nan\")\n",
    "    else:\n",
    "        print(f\"  seed {r['seed']}: {vthr:.6f}\")\n",
    "print(f\"VAL Youden-J threshold (mean ± SD): {val_thr_mean:.6f} ± {val_thr_sd:.6f}\")\n",
    "\n",
    "print(\"\\nTEST threshold used (should equal VAL Youden-J threshold unless VAL threshold is NaN):\")\n",
    "for r in results:\n",
    "    if r[\"thr_note\"] is None:\n",
    "        print(f\"  seed {r['seed']}: {r['thr_used']:.6f}\")\n",
    "    else:\n",
    "        print(f\"  seed {r['seed']}: {r['thr_used']:.6f}  (NOTE: fallback used)\")\n",
    "print(f\"TEST threshold used (mean ± SD): {thr_used_mean:.6f} ± {thr_used_sd:.6f}\")\n",
    "\n",
    "print(\"\\nThreshold metrics on TEST @ VAL Youden-J threshold (mean ± SD across seeds):\")\n",
    "for k in [\"accuracy\",\"precision\",\"sensitivity\",\"specificity\",\"f1_score\",\"mcc\"]:\n",
    "    mu = agg[k][\"mean\"]\n",
    "    sd = agg[k][\"sd\"]\n",
    "    print(f\"  {k}: {mu:.6f} ± {sd:.6f}\")\n",
    "print(\"  fisher_p_value_two_sided:\", f\"{agg['p_value_fisher_two_sided']['mean']:.6g} ± {agg['p_value_fisher_two_sided']['sd']:.6g}\")\n",
    "\n",
    "print(\"\\nFAIRNESS (H3) on TEST @ VAL Youden-J threshold across seeds (mean ± SD):\")\n",
    "print(f\"  FNR_M: {fnr_m_mean:.6f} ± {fnr_m_sd:.6f}\")\n",
    "print(f\"  FNR_F: {fnr_f_mean:.6f} ± {fnr_f_sd:.6f}\")\n",
    "print(f\"  ΔFNR (F-M): {d_signed_mean:.6f} ± {d_signed_sd:.6f}\")\n",
    "print(f\"  |ΔFNR|: {d_abs_mean:.6f} ± {d_abs_sd:.6f}\")\n",
    "print(\"  Per-seed:\", {\n",
    "    str(SEEDS[i]): {\n",
    "        \"thr_used\": thr_used_list[i],\n",
    "        \"FNR_M\": fnr_m_vals[i],\n",
    "        \"n_PD_M\": n_pd_m_vals[i],\n",
    "        \"FNR_F\": fnr_f_vals[i],\n",
    "        \"n_PD_F\": n_pd_f_vals[i],\n",
    "        \"delta_F_minus_M\": d_signed_vals[i],\n",
    "        \"abs_delta\": d_abs_vals[i],\n",
    "    } for i in range(len(SEEDS))\n",
    "})\n",
    "\n",
    "# -------------------------\n",
    "# Write aggregated summary files\n",
    "# Inputs: per-seed results collected above\n",
    "# Outputs: summary_test.json (overwrite) and history_index.jsonl (append)\n",
    "# -------------------------\n",
    "summary = {\n",
    "    \"dataset\": dataset_id,\n",
    "    \"dx_out_root\": DX_OUT_ROOT,\n",
    "    \"manifest_all\": MANIFEST_ALL,\n",
    "    \"trainval_experiment_used\": str(chosen_exp),\n",
    "    \"seeds\": SEEDS,\n",
    "\n",
    "    \"n_val\": int(len(val_df)),\n",
    "    \"label_counts_val\": val_df[\"label_num\"].value_counts(dropna=False).to_dict(),\n",
    "    \"sex_counts_val_norm\": val_df[\"sex_norm\"].value_counts(dropna=False).to_dict(),\n",
    "\n",
    "    \"n_test\": int(len(test_df)),\n",
    "    \"label_counts_test\": test_df[\"label_num\"].value_counts(dropna=False).to_dict(),\n",
    "    \"sex_counts_test_norm\": test_df[\"sex_norm\"].value_counts(dropna=False).to_dict(),\n",
    "\n",
    "    \"val_optimal_threshold\": {\n",
    "        \"method\": \"Youden J (argmax(TPR - FPR) on VAL ROC)\",\n",
    "        \"thresholds_by_seed\": {str(r[\"seed\"]): float(r[\"val_thr\"]) for r in results},\n",
    "        \"mean_sd\": {\"mean\": float(val_thr_mean), \"sd\": float(val_thr_sd)},\n",
    "    },\n",
    "\n",
    "    \"test_threshold_used\": {\n",
    "        \"threshold_used_by_seed\": {str(r[\"seed\"]): float(r[\"thr_used\"]) for r in results},\n",
    "        \"mean_sd\": {\"mean\": float(thr_used_mean), \"sd\": float(thr_used_sd)},\n",
    "        \"notes_by_seed\": {str(r[\"seed\"]): (r[\"thr_note\"] if r[\"thr_note\"] is not None else \"\") for r in results},\n",
    "    },\n",
    "\n",
    "    \"test_aurocs_by_seed\": {str(r[\"seed\"]): float(r[\"test_auc\"]) for r in results},\n",
    "    \"mean_test_auroc\": float(mean_auc),\n",
    "    \"t_crit_df2_95\": float(t_crit),\n",
    "    \"half_width_95_ci\": float(half_width),\n",
    "    \"ci95_test_auroc\": ci95,\n",
    "\n",
    "    \"threshold_metrics_mean_sd_test_at_val_opt\": agg,\n",
    "    \"confusion_matrix_by_seed_test_at_val_opt\": cm_by_seed,\n",
    "\n",
    "    \"fairness_test_at_val_opt\": {\n",
    "        \"definition\": \"H3: ΔFNR = FNR(F) - FNR(M), where FNR(sex)=FN/(FN+TP) computed on PD-only true labels at test_threshold_used.\",\n",
    "        \"fnr_by_sex_norm_by_seed\": fnr_by_seed,\n",
    "        \"delta_fnr_F_minus_M_by_seed\": delta_signed_by_seed,\n",
    "        \"delta_fnr_abs_by_seed\": delta_abs_by_seed,\n",
    "        \"fnr_M_mean_sd\": {\"mean\": float(fnr_m_mean), \"sd\": float(fnr_m_sd), \"values_by_seed\": {str(s): float(v) for s, v in zip(SEEDS, fnr_m_vals)}},\n",
    "        \"fnr_F_mean_sd\": {\"mean\": float(fnr_f_mean), \"sd\": float(fnr_f_sd), \"values_by_seed\": {str(s): float(v) for s, v in zip(SEEDS, fnr_f_vals)}},\n",
    "        \"delta_fnr_F_minus_M_mean_sd\": {\"mean\": float(d_signed_mean), \"sd\": float(d_signed_sd), \"values_by_seed\": {str(s): float(v) for s, v in zip(SEEDS, d_signed_vals)}},\n",
    "        \"delta_fnr_abs_mean_sd\": {\"mean\": float(d_abs_mean), \"sd\": float(d_abs_sd), \"values_by_seed\": {str(s): float(v) for s, v in zip(SEEDS, d_abs_vals)}},\n",
    "        \"denominators_PD_by_seed\": {str(s): {\"n_PD_M\": float(n_pd_m_vals[i]), \"n_PD_F\": float(n_pd_f_vals[i])} for i, s in enumerate(SEEDS)},\n",
    "        \"sex_normalization_note\": \"D1 mapping: 0->F, 1->M; otherwise UNK. ΔFNR computed only when both M and F have defined FNR.\",\n",
    "    },\n",
    "\n",
    "    \"run_dirs\": [r[\"run_dir\"] for r in results],\n",
    "    \"timestamp\": time.strftime(\"%Y%m%d_%H%M%S\"),\n",
    "}\n",
    "\n",
    "summary_path = TEST_ROOT / \"summary_test.json\"\n",
    "with open(summary_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "history_path = TEST_ROOT / \"history_index.jsonl\"\n",
    "with open(history_path, \"a\", encoding=\"utf-8\") as f:\n",
    "    f.write(json.dumps(summary) + \"\\n\")\n",
    "\n",
    "print(\"\\nWROTE summary:\", str(summary_path))\n",
    "print(\"APPENDED history index:\", str(history_path))\n",
    "print(\"Open this folder to access artifacts:\", str(TEST_ROOT))\n",
    "\n",
    "# -------------------------\n",
    "# Stop runtime\n",
    "# Purpose: release the GPU machine after outputs are written\n",
    "# -------------------------\n",
    "print(\"\\nAll done. Unassigning the runtime to stop the L4 instance...\")\n",
    "try:\n",
    "    from google.colab import runtime  # type: ignore\n",
    "    print(\"Calling runtime.unassign() now.\")\n",
    "    runtime.unassign()\n",
    "except Exception as e:\n",
    "    print(\"Could not unassign runtime automatically. You can stop the runtime manually in Colab.\")\n",
    "    print(\"Reason:\", repr(e))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "34758bdd36c444dda1ed7ea7617e5d43",
      "c37bed3b05764e5b9baae5606a5d48ad",
      "b3041d35058f498bb49f89578aaa5faf",
      "21134ea5fdd048c6b2ecd452b9a0dcd1",
      "28d26516f7e94ba5b7be1ac6efc9270a",
      "d915fdc5161b4da2adc6179f41cd4c08",
      "c74d21d2e0f4456f8b1d3f7629ff0b27",
      "c57b83e5be0842a18982fb2287500c01",
      "9bda1083a1e64788814c91bcbc88a303",
      "ca687c44bfb046ceab046f7fb0b7e9ac",
      "5eb2ecc4d350464fb68f54e1d8ddd9e0",
      "bc1edeb4bc11445b981353dc72d7c091",
      "b787686b53ce46c5898766046cd22cde",
      "4e99dc76d7f842efbc5dd593a323718d",
      "e99ea3b92590453386225753d5962e0e",
      "c9503303cf3a4897a69bc13f142a9166",
      "96b5df8928fb4f5abd92038d3b5326d1",
      "c8265a9fd941456abca0a793127fa06a",
      "1e5c056d4c414f46b4c6af7cee5acff6",
      "2ade6155d33b486289677124e2232fa5",
      "4d951134d7944da7ac077788b37e6c68",
      "9ee91ea0429c4416bd18878cdc620689",
      "927e40fa73de45858eb1a9a8915f1036",
      "845b4afd541e4846905c8ed47e74d6bb",
      "1bd970ac2e5d4b43983d650519063320",
      "c85b7130cbab4f0d85d5f15732883006",
      "6b2c106f5ac0406e87b9198d620cfc44",
      "f773c3c1b22a4d44adaac0cdcd7afc59",
      "ce8f15e5526c4f96b42539ccb5c41313",
      "93adea3541fa4b2fa8549fa8bad94d55",
      "19993431e23546589a3131e7f9be96cb",
      "44d4018893fb4a17930046b219b9232c",
      "756da603a5694dd98a9445886f4bf43d",
      "5da26fe7bf334700a61aa6e170afd781",
      "c7efa647bce54ebfbddd91eabf03a9f7",
      "30473b72d20740f2aa3e4bd72dda8804",
      "5a87405edb224348bc4263b12c240061",
      "674bccdbcf8840bfb262ff17c1037ca2",
      "6e20331cbd084b088383b161b64a26cf",
      "155302c43f69484882fd1f6d5a9960f0",
      "82207799a49d4205b91479486ad93000",
      "f03b19872e7e4184bf605650b2977830",
      "6598b8cdd2fa4bef95328bbc5d182661",
      "326035944d0f435d8283c107f0b2ee83",
      "658fca13d7444e2688df36f9febce1bd",
      "5310323e272d4204945a60aecbd2d5cd",
      "817850eddddd4b64916d67fe21ca8f6d",
      "b18e2ae977254c19a153f10870a38ffe",
      "2d7c06d09eb0400081f52febb33ff726",
      "994e7ef170b24333ba0bf2822703b4e9",
      "c55b1721bc2641efa5bc310aab02e4fa",
      "48d9cc7e5493480cae690f0064f80d08",
      "c27992f8d9924ba69fa11af8fda572ec",
      "1e9412019a4e44ce936513e5d078acad",
      "57f80f6a08454c47856c800ec488c1f6",
      "ecf9020643d6482ea126f12bf3f3c06a",
      "e5f4a6e4963d45859c1dbd0fc262b661",
      "b0880551e2b64aa5adaf595bc2b8bdc9",
      "ab33212a27464437b4afae2eabb80bec",
      "2d12cfabe4a94f39a22637722020c51f",
      "3f11da9a0f504bdfb8075573805aab76",
      "13c006bfeabb4712b22dabf5da13f522",
      "9384c64892ef4a2b94b9f927bb6cadf4",
      "ccd14c1195c843f8bb7f593e8f040e43",
      "c7d01b1989c04818873f34c63bf5f6fb",
      "f5eb656df3784e8ba8e93addfecc6939",
      "e2fd001ccfaa4210bd36f6a24cdb9415",
      "313659c6075b46d882c52acb12925b35",
      "e64bea7b74e943b9a4e21127c1538281",
      "f67b34009fc442dc8f86c7a43c9d0019",
      "ff27c174030f4b02ab642ec24ad6f4cb",
      "9721eaea271c4792a6ab8cfe3e3a7d70",
      "a14faaa72eda4419afb2e6c13d927e85",
      "5490250de2af45c2a7f0c64cba3bbdd0",
      "2285a01b72ed4d5bbfcaa110f814f348",
      "26ec289f53dd417283775c9aefbaf0f7",
      "ef090712b8a34f228f25a7f2ed19f49b",
      "c23b9b1a6d294a6c89cc49dd9a3ad915",
      "920feb2c69314bf9a8eac3cff9b79fee",
      "131f43105bdb49808907736f4ca0e846",
      "5d8ee1e0a9794f5ab33e3703430925ca",
      "4928533e227c4e2eb4ca2e072c86df89",
      "bc9b5e433b1d44f0a01a82abe36c6393",
      "2dbc777831af41ddae6d93b1db3c2a39",
      "557751b64bd646e797d74270235bd833",
      "1fb734cbc6f94d1d82df80703f9ae460",
      "acc204e0f7c7409e95f3b0ede0b48a94",
      "6c59a90dbb5244df935874832edb386f",
      "b6fe6be41b5a484f9aafcb80eed9a880",
      "97ec41f076a049feb26b3e958d085ad0",
      "a45c008ab76b4349b8cc3960753ed6ae",
      "9eaf80b3a8bb41fa80bdfe5920abfcf7",
      "87ad284f88094fa2b617817e4120a94e",
      "5bd6ba6acfa844d8a3f6bd5cd6c0648b",
      "eb2f0dbf30974b64a7d26d425d6100aa",
      "266426eb2fb6441da4fa90b7d8e917d7",
      "32ef49418cb0409aade0443ef51bf9d6",
      "507e294d09014210ab0c5cfe40f19d27",
      "da5fedd3d26849d7b9e16a8976414466",
      "ec93cdd8bb7446aa9112863636b31d7a",
      "efa60b1b956c426085ad9868f4ef5ef3",
      "67254179720347e58a378803f3503e65",
      "431c452e66154118a6d71d8760d4910b",
      "9714b85655c44610b4ea75b411936725",
      "4febd4a4666d474789ce584303b6b5ec",
      "6b349c9576cb48c4b2c2b383da7e08f8",
      "dcc4f62826e64574aca96cd4ce25d772",
      "7c86afa38db94f649f7fb2c8adf853f4",
      "3567a6d9808c44ae968cd61e68f39993",
      "8622cd8a019b41a1a5ac2f7f63467598"
     ]
    },
    "id": "DFPS2d2oy2cF"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "The following cell runs the final D2 (Slovak, EWA-DB) test evaluation in a safe and repeatable way, without retraining the model. It reads the combined manifest file (`manifests/manifest_all.csv`) from the D2 `preprocessed_v1` folder and keeps only the validation and test rows. It checks that all required fields are present, including split, audio path, label, task type, sex, and age. Before continuing, it performs quick fail fast checks to confirm that both splits are not empty and that all referenced audio files exist on disk, showing progress while the paths are checked.\n",
    "\n",
    "The cell then prepares the data for inference. Each clip is assigned to one of two task groups using a strict rule: `task == \"vowl\"` is treated as vowel speech, and all other tasks are treated as other speech. Sex labels are normalized to M, F, or UNK using common text formats, without guessing numeric encodings. A dataset class loads each audio clip, converts stereo audio to mono if needed, enforces a 16 kHz sample rate, and builds an attention mask. For vowel clips, the attention mask attempts to ignore trailing near silence so padded or silent regions do not influence the model. A custom collator pads audio clips in each batch to a common length and pads the attention masks in the same way. A short warm up step loads a few batches from both validation and test splits to catch data loading issues early.\n",
    "\n",
    "The model used for evaluation matches the training setup. It consists of a frozen Wav2Vec2 backbone and two small classification heads, one for vowel clips and one for other speech, each built as LayerNorm followed by Dropout and a Linear layer. The cell automatically finds the most recent training and validation experiment folder that contains `best_heads.pt` for all three seeds (1337, 2024, 7777). For each seed, it loads the saved head weights and, when available, reads the backbone checkpoint name and dropout value from the checkpoint so the test run remains consistent with training.\n",
    "\n",
    "For each seed, evaluation is performed in two stages. First, during the validation stage, inference is run on the validation split and a decision threshold is selected using Youden’s J statistic, which chooses the threshold that maximizes the difference between true positive rate and false positive rate. Second, during the test stage, inference is run on the test split and all threshold based metrics are computed using the threshold chosen from validation, not a fixed value of 0.5, so the test set is not used to tune the threshold.\n",
    "\n",
    "For each seed, results are saved under\n",
    "`<DX_OUT_ROOT>/monolingual_test_runs/run_<DATASET>_seed<seed>/`.\n",
    "Saved files include `metrics.json` with dataset counts, the chosen validation threshold, validation and test AUROC, test metrics at the validation selected threshold, and fairness results. A ROC curve image for the test split is saved, along with a confusion matrix for the full test set at the chosen threshold. Additional confusion matrices split by M and F are also saved when those groups are present.\n",
    "\n",
    "Fairness is computed on the test split at the same chosen threshold using the project’s H3 definition. This measures the false negative rate by sex on Parkinson’s positive cases only, and computes ΔFNR as FNR(F) minus FNR(M), along with the absolute difference. The script also records confusion counts by sex group, including M, F, and UNK, for the full label set.\n",
    "\n",
    "After all three seeds finish, the cell prints and writes a combined summary. This includes the mean test AUROC with a 95% confidence interval using a t distribution with n equal to 3, as well as the mean and standard deviation of the validation selected thresholds, threshold based test metrics, and fairness statistics including FNR for M and F, ΔFNR, and its absolute value. A `summary_test.json` file is written and the same summary is appended to `history_index.jsonl` under `monolingual_test_runs/`. The cell then prints the output folder location and unassigns the Colab runtime to stop the GPU instance."
   ],
   "metadata": {
    "id": "jCl4-w-robmP"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# D2 Test Only: VAL-Selected Threshold Evaluation (Youden J)\n",
    "#\n",
    "# What this cell does\n",
    "# - Reads one manifest (VAL + TEST rows only)\n",
    "# - For each seed:\n",
    "#   1) Runs VAL inference to pick a threshold using Youden J (max TPR − FPR)\n",
    "#   2) Runs TEST inference and reports metrics at that fixed VAL-selected threshold\n",
    "# - Aggregates results across 3 seeds (AUROC + threshold metrics + fairness)\n",
    "#\n",
    "# Inputs\n",
    "# - manifest_all.csv (must include split, clip_path, label_num, task, sex, age)\n",
    "# - best_heads.pt for each seed from the most recent trainval experiment\n",
    "#\n",
    "# Outputs\n",
    "# - Per-seed folder: metrics.json + ROC and confusion PNGs (overall + by sex when present)\n",
    "# - Summary JSON (summary_test.json) + append-only history (history_index.jsonl)\n",
    "# - Stops the Colab runtime at the end\n",
    "#\n",
    "# Notes\n",
    "# - Threshold is selected on VAL only to avoid test-tuning\n",
    "# - Fairness metric (H3): ΔFNR = FNR(F) − FNR(M) at the selected threshold\n",
    "# =========================\n",
    "\n",
    "import os, json, math, random, time, warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import soundfile as sf\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from transformers import Wav2Vec2Model\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, confusion_matrix, matthews_corrcoef\n",
    "from scipy.stats import fisher_exact\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# -------------------------\n",
    "# Drive mount (Colab)\n",
    "# -------------------------\n",
    "if not os.path.isdir(\"/content/drive/MyDrive\"):\n",
    "    from google.colab import drive  # type: ignore\n",
    "    drive.mount(\"/content/drive\")\n",
    "\n",
    "# -------------------------\n",
    "# Paths: dataset root + manifest\n",
    "# -------------------------\n",
    "DX_OUT_ROOT = \"/content/drive/MyDrive/AI_PD_Project/Datasets/D2-Slovak (EWA-DB)/EWA-DB/preprocessed_v1\"\n",
    "MANIFEST_ALL = f\"{DX_OUT_ROOT}/manifests/manifest_all.csv\"\n",
    "\n",
    "# -------------------------\n",
    "# Fixed run settings (match Train+Val patterns)\n",
    "# -------------------------\n",
    "SEEDS          = [1337, 2024, 7777]\n",
    "BACKBONE_CKPT  = \"facebook/wav2vec2-base\"  # default fallback; best_heads.pt may override\n",
    "SR_EXPECTED    = 16000\n",
    "TINY_THRESH    = 1e-4\n",
    "\n",
    "# Batch sizing (same pattern as Train+Val)\n",
    "EFFECTIVE_BS   = 64\n",
    "PER_DEVICE_BS  = 16\n",
    "GRAD_ACCUM     = max(1, EFFECTIVE_BS // PER_DEVICE_BS)  # printed only\n",
    "\n",
    "# Threshold selection rule\n",
    "# - Youden J on VAL: maximize (TPR - FPR)\n",
    "THRESHOLD_SELECTION = \"youden_j_on_val\"\n",
    "\n",
    "# Drive stability defaults\n",
    "NUM_WORKERS    = 0\n",
    "PIN_MEMORY     = False\n",
    "\n",
    "# AMP (safe on L4)\n",
    "USE_AMP        = True\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "try:\n",
    "    torch.set_float32_matmul_precision(\"high\")\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", message=\"None of the inputs have requires_grad=True. Gradients will be None\")\n",
    "\n",
    "print(\"DX_OUT_ROOT:\", DX_OUT_ROOT)\n",
    "print(\"MANIFEST_ALL:\", MANIFEST_ALL)\n",
    "print(\"DEVICE:\", DEVICE)\n",
    "if DEVICE.type == \"cuda\":\n",
    "    print(\"GPU:\", torch.cuda.get_device_name(0))\n",
    "print(\"PER_DEVICE_BS:\", PER_DEVICE_BS, \"| EFFECTIVE_BS:\", PER_DEVICE_BS * GRAD_ACCUM)\n",
    "print(\"NUM_WORKERS:\", NUM_WORKERS, \"| PIN_MEMORY:\", PIN_MEMORY)\n",
    "print(\"THRESHOLD_SELECTION:\", THRESHOLD_SELECTION)\n",
    "print(\"USE_AMP:\", bool(USE_AMP and DEVICE.type == \"cuda\"))\n",
    "\n",
    "# -------------------------\n",
    "# Load manifest + keep only VAL/TEST rows\n",
    "# -------------------------\n",
    "if not os.path.exists(MANIFEST_ALL):\n",
    "    raise FileNotFoundError(f\"Missing manifest_all.csv: {MANIFEST_ALL}\")\n",
    "\n",
    "m = pd.read_csv(MANIFEST_ALL)\n",
    "\n",
    "# Required columns for metrics + fairness + schema checks\n",
    "req_cols = {\"split\", \"clip_path\", \"label_num\", \"task\", \"sex\", \"age\"}\n",
    "missing = [c for c in sorted(req_cols) if c not in m.columns]\n",
    "if missing:\n",
    "    raise ValueError(f\"Manifest missing required columns: {missing}. Found: {list(m.columns)}\")\n",
    "\n",
    "# Keep only val/test for this script (val selects threshold, test reports results)\n",
    "m = m[m[\"split\"].isin([\"val\", \"test\"])].copy()\n",
    "if len(m) == 0:\n",
    "    raise RuntimeError(\"After filtering to split in {'val','test'}, manifest has 0 rows.\")\n",
    "\n",
    "# Infer dataset_id (most frequent non-null dataset value, if present)\n",
    "if \"dataset\" in m.columns and m[\"dataset\"].notna().any():\n",
    "    dataset_id = str(m[\"dataset\"].value_counts(dropna=True).idxmax())\n",
    "    m = m[m[\"dataset\"].astype(str) == dataset_id].copy()\n",
    "else:\n",
    "    dataset_id = \"DX\"\n",
    "\n",
    "# Keep a standard subset of columns (create as NaN if missing)\n",
    "keep_cols = [\"clip_path\", \"label_num\", \"task\", \"speaker_id\", \"sex\", \"age\", \"duration_sec\", \"split\"]\n",
    "for c in keep_cols:\n",
    "    if c not in m.columns:\n",
    "        m[c] = np.nan\n",
    "m = m[keep_cols].copy()\n",
    "\n",
    "val_df  = m[m[\"split\"] == \"val\"].reset_index(drop=True)\n",
    "test_df = m[m[\"split\"] == \"test\"].reset_index(drop=True)\n",
    "\n",
    "print(f\"\\nDataset inferred: {dataset_id}\")\n",
    "print(f\"Val rows:  {len(val_df)}\")\n",
    "print(f\"Test rows: {len(test_df)}\")\n",
    "print(\"Test label counts:\", test_df[\"label_num\"].value_counts(dropna=False).to_dict())\n",
    "print(\"Test sex counts (raw):\", test_df[\"sex\"].value_counts(dropna=False).to_dict())\n",
    "\n",
    "if len(test_df) == 0:\n",
    "    raise RuntimeError(\"Test split has 0 rows.\")\n",
    "if len(val_df) == 0:\n",
    "    raise RuntimeError(\"Validation split has 0 rows. Cannot select a val-optimal threshold.\")\n",
    "\n",
    "# -------------------------\n",
    "# Clip existence check (stop early if paths are broken)\n",
    "# -------------------------\n",
    "def _fail_fast_missing_paths(df: pd.DataFrame, name: str):\n",
    "    missing_paths = []\n",
    "    for p in tqdm(df[\"clip_path\"].astype(str).tolist(), desc=f\"Check {name} clip_path exists\", dynamic_ncols=True):\n",
    "        if not os.path.exists(p):\n",
    "            missing_paths.append(p)\n",
    "            if len(missing_paths) >= 10:\n",
    "                break\n",
    "    if missing_paths:\n",
    "        raise FileNotFoundError(f\"{name}: missing clip_path(s). Examples: {missing_paths}\")\n",
    "\n",
    "_fail_fast_missing_paths(val_df, \"VAL\")\n",
    "_fail_fast_missing_paths(test_df, \"TEST\")\n",
    "\n",
    "# -------------------------\n",
    "# Task grouping (exact mapping used by the project)\n",
    "# -------------------------\n",
    "def _task_group(task_val) -> str:\n",
    "    return \"vowel\" if str(task_val) == \"vowl\" else \"other\"\n",
    "\n",
    "val_df[\"task_group\"]  = val_df[\"task\"].apply(_task_group)\n",
    "test_df[\"task_group\"] = test_df[\"task\"].apply(_task_group)\n",
    "\n",
    "# -------------------------\n",
    "# Sex normalization for fairness + sex-split confusion charts\n",
    "# -------------------------\n",
    "def normalize_sex(val) -> str:\n",
    "    \"\"\"\n",
    "    Returns 'M', 'F', or 'UNK'\n",
    "    Handles common strings: M/F, Male/Female, etc.\n",
    "    Numeric encodings are treated as UNK to avoid silent mis-mapping.\n",
    "    \"\"\"\n",
    "    if pd.isna(val):\n",
    "        return \"UNK\"\n",
    "    s = str(val).strip().lower()\n",
    "\n",
    "    if s in {\"m\", \"male\", \"man\", \"masc\", \"masculine\"}:\n",
    "        return \"M\"\n",
    "    if s in {\"f\", \"female\", \"woman\", \"fem\", \"feminine\"}:\n",
    "        return \"F\"\n",
    "\n",
    "    return \"UNK\"\n",
    "\n",
    "val_df[\"sex_norm\"]  = val_df[\"sex\"].apply(normalize_sex)\n",
    "test_df[\"sex_norm\"] = test_df[\"sex\"].apply(normalize_sex)\n",
    "print(\"Test sex counts (normalized):\", test_df[\"sex_norm\"].value_counts(dropna=False).to_dict())\n",
    "if (test_df[\"sex_norm\"] == \"UNK\").any():\n",
    "    print(\"NOTE: Some 'sex' values could not be normalized to M/F and were counted as 'UNK' for fairness and sex charts.\")\n",
    "\n",
    "# -------------------------\n",
    "# Audio dataset + collator (pads to batch max length)\n",
    "# -------------------------\n",
    "class AudioManifestDataset(Dataset):\n",
    "    def __init__(self, df: pd.DataFrame):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        row = self.df.iloc[idx]\n",
    "        clip_path = str(row[\"clip_path\"])\n",
    "        label = int(row[\"label_num\"])\n",
    "        task_group = str(row[\"task_group\"])\n",
    "        sex_norm = str(row[\"sex_norm\"])\n",
    "\n",
    "        if not os.path.exists(clip_path):\n",
    "            raise FileNotFoundError(f\"Missing clip_path: {clip_path}\")\n",
    "\n",
    "        y, sr = sf.read(clip_path, always_2d=False)\n",
    "        if isinstance(y, np.ndarray) and y.ndim > 1:\n",
    "            y = np.mean(y, axis=1)\n",
    "        y = np.asarray(y, dtype=np.float32)\n",
    "\n",
    "        if int(sr) != SR_EXPECTED:\n",
    "            raise ValueError(f\"Sample rate mismatch (expected {SR_EXPECTED}): {clip_path} has sr={sr}\")\n",
    "\n",
    "        # Attention mask blocks padded tail for vowel clips (based on tiny amplitude cutoff)\n",
    "        attn = np.ones((len(y),), dtype=np.int64)\n",
    "        if task_group == \"vowel\":\n",
    "            k = -1\n",
    "            for j in range(len(y) - 1, -1, -1):\n",
    "                if abs(float(y[j])) > TINY_THRESH:\n",
    "                    k = j\n",
    "                    break\n",
    "            if k >= 0:\n",
    "                attn[:k+1] = 1\n",
    "                attn[k+1:] = 0\n",
    "            else:\n",
    "                attn[:] = 1\n",
    "        else:\n",
    "            attn[:] = 1\n",
    "\n",
    "        return {\n",
    "            \"input_values\": torch.from_numpy(y),\n",
    "            \"attention_mask\": torch.from_numpy(attn),\n",
    "            \"labels\": torch.tensor(label, dtype=torch.long),\n",
    "            \"task_group\": task_group,\n",
    "            \"sex_norm\": sex_norm,\n",
    "        }\n",
    "\n",
    "def collate_fn(batch):\n",
    "    max_len = int(max(b[\"input_values\"].numel() for b in batch))\n",
    "    input_vals, attn_masks, labels, task_groups, sex_norms = [], [], [], [], []\n",
    "    for b in batch:\n",
    "        x = b[\"input_values\"]\n",
    "        a = b[\"attention_mask\"]\n",
    "        pad = max_len - x.numel()\n",
    "        if pad > 0:\n",
    "            x = torch.cat([x, torch.zeros(pad, dtype=x.dtype)], dim=0)\n",
    "            a = torch.cat([a, torch.zeros(pad, dtype=a.dtype)], dim=0)\n",
    "        input_vals.append(x)\n",
    "        attn_masks.append(a)\n",
    "        labels.append(b[\"labels\"])\n",
    "        task_groups.append(b[\"task_group\"])\n",
    "        sex_norms.append(b[\"sex_norm\"])\n",
    "    return {\n",
    "        \"input_values\": torch.stack(input_vals, dim=0),\n",
    "        \"attention_mask\": torch.stack(attn_masks, dim=0),\n",
    "        \"labels\": torch.stack(labels, dim=0),\n",
    "        \"task_group\": task_groups,\n",
    "        \"sex_norm\": sex_norms,\n",
    "    }\n",
    "\n",
    "# -------------------------\n",
    "# Model (test-time): frozen Wav2Vec2 + two task heads\n",
    "# -------------------------\n",
    "class FrozenW2V2TwoHeadTest(nn.Module):\n",
    "    def __init__(self, ckpt: str, dropout_p: float):\n",
    "        super().__init__()\n",
    "        self.backbone = Wav2Vec2Model.from_pretrained(ckpt)\n",
    "        for p in self.backbone.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "        H = int(self.backbone.config.hidden_size)\n",
    "        self.head_vowel = nn.Sequential(\n",
    "            nn.LayerNorm(H),\n",
    "            nn.Dropout(float(dropout_p)),\n",
    "            nn.Linear(H, 2),\n",
    "        )\n",
    "        self.head_other = nn.Sequential(\n",
    "            nn.LayerNorm(H),\n",
    "            nn.Dropout(float(dropout_p)),\n",
    "            nn.Linear(H, 2),\n",
    "        )\n",
    "\n",
    "    def masked_mean_pool(self, last_hidden, attn_mask_samples):\n",
    "        feat_mask = self.backbone._get_feature_vector_attention_mask(last_hidden.shape[1], attn_mask_samples)\n",
    "        feat_mask = feat_mask.to(last_hidden.device).unsqueeze(-1).type_as(last_hidden)\n",
    "        masked = last_hidden * feat_mask\n",
    "        denom = feat_mask.sum(dim=1).clamp(min=1.0)\n",
    "        return masked.sum(dim=1) / denom  # [B, H]\n",
    "\n",
    "    def _heads_fp32(self, x_any: torch.Tensor, head: nn.Module) -> torch.Tensor:\n",
    "        # Keep heads in fp32 even under AMP for numeric stability\n",
    "        x = x_any.float()\n",
    "        if x.is_cuda:\n",
    "            with torch.amp.autocast(device_type=\"cuda\", enabled=False):\n",
    "                return head(x)\n",
    "        return head(x)\n",
    "\n",
    "    def forward_logits(self, input_values, attention_mask, task_group):\n",
    "        with torch.no_grad():\n",
    "            out = self.backbone(input_values=input_values, attention_mask=attention_mask)\n",
    "            last_hidden = out.last_hidden_state\n",
    "\n",
    "        pooled = self.masked_mean_pool(last_hidden, attention_mask)  # [B, H]\n",
    "\n",
    "        logits_v = self._heads_fp32(pooled, self.head_vowel)\n",
    "        logits_o = self._heads_fp32(pooled, self.head_other)\n",
    "\n",
    "        # Route each sample to the matching head based on task_group\n",
    "        is_vowel = torch.tensor([tg == \"vowel\" for tg in task_group], device=pooled.device, dtype=torch.bool)\n",
    "        logits = logits_o.clone()\n",
    "        if is_vowel.any():\n",
    "            logits[is_vowel] = logits_v[is_vowel]\n",
    "        return logits\n",
    "\n",
    "# -------------------------\n",
    "# Metrics + plotting helpers\n",
    "# -------------------------\n",
    "def compute_auc(y_true, y_prob):\n",
    "    y_true = np.asarray(y_true, dtype=np.int64)\n",
    "    y_prob = np.asarray(y_prob, dtype=np.float64)\n",
    "    if len(np.unique(y_true)) < 2:\n",
    "        return float(\"nan\")\n",
    "    return float(roc_auc_score(y_true, y_prob))\n",
    "\n",
    "def compute_threshold_metrics(y_true, y_prob, thr):\n",
    "    y_true = np.asarray(y_true, dtype=np.int64)\n",
    "    y_prob = np.asarray(y_prob, dtype=np.float64)\n",
    "    y_pred = (y_prob >= float(thr)).astype(np.int64)\n",
    "\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=[0, 1])\n",
    "    TN, FP, FN, TP = int(cm[0, 0]), int(cm[0, 1]), int(cm[1, 0]), int(cm[1, 1])\n",
    "\n",
    "    eps = 1e-12\n",
    "    acc = (TP + TN) / max(1, (TP + TN + FP + FN))\n",
    "    prec = TP / (TP + FP + eps)\n",
    "    rec = TP / (TP + FN + eps)     # sensitivity\n",
    "    f1 = 2 * prec * rec / (prec + rec + eps)\n",
    "    spec = TN / (TN + FP + eps)\n",
    "\n",
    "    try:\n",
    "        mcc = float(matthews_corrcoef(y_true, y_pred)) if len(np.unique(y_pred)) > 1 else float(\"nan\")\n",
    "    except Exception:\n",
    "        mcc = float(\"nan\")\n",
    "\n",
    "    try:\n",
    "        _, pval = fisher_exact([[TN, FP], [FN, TP]], alternative=\"two-sided\")\n",
    "        pval = float(pval)\n",
    "    except Exception:\n",
    "        pval = float(\"nan\")\n",
    "\n",
    "    return {\n",
    "        \"threshold\": float(thr),\n",
    "        \"confusion_matrix\": {\"TN\": TN, \"FP\": FP, \"FN\": FN, \"TP\": TP},\n",
    "        \"accuracy\": float(acc),\n",
    "        \"precision\": float(prec),\n",
    "        \"recall\": float(rec),\n",
    "        \"f1_score\": float(f1),\n",
    "        \"sensitivity\": float(rec),\n",
    "        \"specificity\": float(spec),\n",
    "        \"mcc\": float(mcc),\n",
    "        \"p_value_fisher_two_sided\": float(pval),\n",
    "    }\n",
    "\n",
    "def save_roc_curve_png(y_true, y_prob, out_png, title_suffix=\"Set\"):\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_prob)\n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr)\n",
    "    plt.plot([0, 1], [0, 1], linestyle=\"--\")\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(f\"ROC Curve ({title_suffix})\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_png, dpi=150)\n",
    "    plt.close()\n",
    "\n",
    "def save_confusion_png(y_true, y_prob, out_png, thr, title_suffix=\"Set\"):\n",
    "    y_true = np.asarray(y_true, dtype=np.int64)\n",
    "    y_prob = np.asarray(y_prob, dtype=np.float64)\n",
    "    y_pred = (y_prob >= float(thr)).astype(int)\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=[0, 1])\n",
    "    plt.figure()\n",
    "    plt.imshow(cm)\n",
    "    plt.xticks([0, 1], [\"Healthy(0)\", \"PD(1)\"])\n",
    "    plt.yticks([0, 1], [\"Healthy(0)\", \"PD(1)\"])\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"True\")\n",
    "    plt.title(f\"Confusion Matrix ({title_suffix}, thr={thr:.4f})\")\n",
    "    for (i, j), v in np.ndenumerate(cm):\n",
    "        plt.text(j, i, str(v), ha=\"center\", va=\"center\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_png, dpi=150)\n",
    "    plt.close()\n",
    "\n",
    "# -------------------------\n",
    "# Fairness (H3): FNR by sex + ΔFNR = FNR(F) − FNR(M)\n",
    "# -------------------------\n",
    "def compute_fnr_by_group_signed(y_true, y_prob, groups, thr):\n",
    "    \"\"\"\n",
    "    FNR = FN/(FN+TP) computed on PD-only true labels.\n",
    "    Returns:\n",
    "      - per group: counts + fnr\n",
    "      - delta_signed: FNR(F) - FNR(M)\n",
    "      - delta_abs: |FNR(F) - FNR(M)|\n",
    "    \"\"\"\n",
    "    y_true = np.asarray(y_true, dtype=np.int64)\n",
    "    y_prob = np.asarray(y_prob, dtype=np.float64)\n",
    "    groups = np.asarray(list(groups), dtype=object)\n",
    "    y_pred = (y_prob >= float(thr)).astype(np.int64)\n",
    "\n",
    "    out = {}\n",
    "    for g in sorted(set(groups.tolist())):\n",
    "        mask_g = (groups == g)\n",
    "        if int(mask_g.sum()) == 0:\n",
    "            continue\n",
    "\n",
    "        pos_mask = mask_g & (y_true == 1)\n",
    "        n_pos = int(pos_mask.sum())\n",
    "        if n_pos == 0:\n",
    "            out[g] = {\"n_total\": int(mask_g.sum()), \"n_pos\": 0, \"tp\": 0, \"fn\": 0, \"fnr\": float(\"nan\")}\n",
    "            continue\n",
    "\n",
    "        tp = int(((y_pred == 1) & pos_mask).sum())\n",
    "        fn = int(((y_pred == 0) & pos_mask).sum())\n",
    "        fnr = float(fn / max(1, (fn + tp)))\n",
    "\n",
    "        out[g] = {\"n_total\": int(mask_g.sum()), \"n_pos\": int(n_pos), \"tp\": int(tp), \"fn\": int(fn), \"fnr\": float(fnr)}\n",
    "\n",
    "    fnr_m = out.get(\"M\", {}).get(\"fnr\", float(\"nan\"))\n",
    "    fnr_f = out.get(\"F\", {}).get(\"fnr\", float(\"nan\"))\n",
    "\n",
    "    if (not np.isnan(fnr_m)) and (not np.isnan(fnr_f)):\n",
    "        delta_signed = float(fnr_f - fnr_m)   # H3\n",
    "        delta_abs = float(abs(delta_signed))\n",
    "    else:\n",
    "        delta_signed = float(\"nan\")\n",
    "        delta_abs = float(\"nan\")\n",
    "\n",
    "    return out, delta_signed, delta_abs\n",
    "\n",
    "# -------------------------\n",
    "# Confusion counts by group (all labels)\n",
    "# -------------------------\n",
    "def compute_confusion_counts(y_true, y_prob, thr):\n",
    "    y_true = np.asarray(y_true, dtype=np.int64)\n",
    "    y_prob = np.asarray(y_prob, dtype=np.float64)\n",
    "    y_pred = (y_prob >= float(thr)).astype(np.int64)\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=[0, 1])\n",
    "    TN, FP, FN, TP = int(cm[0, 0]), int(cm[0, 1]), int(cm[1, 0]), int(cm[1, 1])\n",
    "    return {\"TN\": TN, \"FP\": FP, \"FN\": FN, \"TP\": TP}\n",
    "\n",
    "def compute_confusion_by_group(y_true, y_prob, groups, thr):\n",
    "    y_true = np.asarray(y_true, dtype=np.int64)\n",
    "    y_prob = np.asarray(y_prob, dtype=np.float64)\n",
    "    groups = np.asarray(list(groups), dtype=object)\n",
    "    out = {}\n",
    "    for g in sorted(set(groups.tolist())):\n",
    "        mask = (groups == g)\n",
    "        if int(mask.sum()) == 0:\n",
    "            continue\n",
    "        out[g] = {\n",
    "            \"n\": int(mask.sum()),\n",
    "            \"confusion\": compute_confusion_counts(y_true[mask], y_prob[mask], thr=thr),\n",
    "        }\n",
    "    return out\n",
    "\n",
    "# -------------------------\n",
    "# Reproducible seeding (per run)\n",
    "# -------------------------\n",
    "def set_all_seeds(seed: int):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# -------------------------\n",
    "# Find the most recent trainval experiment with all 3 seeds present\n",
    "# -------------------------\n",
    "TRAINVAL_ROOT = Path(DX_OUT_ROOT) / \"trainval_runs\"\n",
    "if not TRAINVAL_ROOT.exists():\n",
    "    raise FileNotFoundError(f\"Missing trainval_runs folder: {str(TRAINVAL_ROOT)}\")\n",
    "\n",
    "exp_dirs = sorted([p for p in TRAINVAL_ROOT.glob(\"exp_*\") if p.is_dir()], key=lambda p: p.stat().st_mtime, reverse=True)\n",
    "if not exp_dirs:\n",
    "    raise FileNotFoundError(f\"No exp_* folders found under: {str(TRAINVAL_ROOT)}\")\n",
    "\n",
    "def _has_all_seeds(exp_path: Path, dataset_id: str, seeds: list):\n",
    "    for s in seeds:\n",
    "        p = exp_path / f\"run_{dataset_id}_seed{s}\" / \"best_heads.pt\"\n",
    "        if not p.exists():\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "chosen_exp = None\n",
    "for ed in exp_dirs:\n",
    "    if _has_all_seeds(ed, dataset_id, SEEDS):\n",
    "        chosen_exp = ed\n",
    "        break\n",
    "\n",
    "if chosen_exp is None:\n",
    "    sample = exp_dirs[0]\n",
    "    raise FileNotFoundError(\n",
    "        \"Could not find a trainval experiment with all 3 best_heads.pt files.\\n\"\n",
    "        f\"Expected under: {str(TRAINVAL_ROOT)}/exp_*/run_{dataset_id}_seedXXXX/best_heads.pt\\n\"\n",
    "        f\"Most recent exp checked: {str(sample)}\"\n",
    "    )\n",
    "\n",
    "print(\"\\nUsing Train+Val experiment folder:\")\n",
    "print(\" \", str(chosen_exp))\n",
    "\n",
    "# -------------------------\n",
    "# Output root for this evaluation\n",
    "# -------------------------\n",
    "TEST_ROOT = Path(DX_OUT_ROOT) / \"monolingual_test_runs\"\n",
    "TEST_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# -------------------------\n",
    "# DataLoaders (VAL selects threshold, TEST reports metrics)\n",
    "# -------------------------\n",
    "val_ds = AudioManifestDataset(val_df)\n",
    "val_loader = DataLoader(\n",
    "    val_ds,\n",
    "    batch_size=PER_DEVICE_BS,\n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=PIN_MEMORY,\n",
    "    collate_fn=collate_fn,\n",
    ")\n",
    "\n",
    "test_ds = AudioManifestDataset(test_df)\n",
    "test_loader = DataLoader(\n",
    "    test_ds,\n",
    "    batch_size=PER_DEVICE_BS,\n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=PIN_MEMORY,\n",
    "    collate_fn=collate_fn,\n",
    ")\n",
    "\n",
    "# -------------------------\n",
    "# Warm-up: catch loader issues early (no training)\n",
    "# -------------------------\n",
    "def warmup_loader(loader, name: str, max_batches: int = 3):\n",
    "    print(f\"\\nWarm-up: loading up to {max_batches} {name} batches...\")\n",
    "    t0 = time.time()\n",
    "    nb = len(loader)\n",
    "    wb = min(max_batches, nb)\n",
    "    if wb == 0:\n",
    "        raise RuntimeError(f\"{name} DataLoader has 0 batches. Check df length and PER_DEVICE_BS.\")\n",
    "    it = iter(loader)\n",
    "    for i in range(wb):\n",
    "        _ = next(it)\n",
    "        print(f\"  loaded warmup batch {i+1}/{wb}\")\n",
    "    print(f\"Warm-up done in {time.time()-t0:.2f}s\")\n",
    "\n",
    "warmup_loader(val_loader, \"VAL\", max_batches=2)\n",
    "warmup_loader(test_loader, \"TEST\", max_batches=2)\n",
    "\n",
    "# -------------------------\n",
    "# Load heads from best_heads.pt (supports current format + common wrappers)\n",
    "# -------------------------\n",
    "def load_heads_into_model(model: FrozenW2V2TwoHeadTest, best_heads_path: Path, state: dict):\n",
    "    \"\"\"\n",
    "    Expected trainval format:\n",
    "      state[\"head_vowel\"] = state_dict for nn.Sequential(LN, Dropout, Linear)\n",
    "      state[\"head_other\"] = state_dict for nn.Sequential(LN, Dropout, Linear)\n",
    "\n",
    "    Fallback: wrapper dict with a full model state_dict.\n",
    "    \"\"\"\n",
    "    if not best_heads_path.exists():\n",
    "        raise FileNotFoundError(f\"Missing best_heads.pt: {str(best_heads_path)}\")\n",
    "\n",
    "    if not isinstance(state, dict):\n",
    "        raise ValueError(f\"Unexpected best_heads.pt type: {type(state)}\")\n",
    "\n",
    "    # Preferred (current trainval format)\n",
    "    if \"head_vowel\" in state and \"head_other\" in state and isinstance(state[\"head_vowel\"], dict) and isinstance(state[\"head_other\"], dict):\n",
    "        model.head_vowel.load_state_dict(state[\"head_vowel\"], strict=True)\n",
    "        model.head_other.load_state_dict(state[\"head_other\"], strict=True)\n",
    "        return model\n",
    "\n",
    "    # Wrapper fallback\n",
    "    for wrap_key in [\"state_dict\", \"model_state_dict\", \"model\"]:\n",
    "        if wrap_key in state and isinstance(state[wrap_key], dict):\n",
    "            sd = state[wrap_key]\n",
    "            _missing, _unexpected = model.load_state_dict(sd, strict=False)\n",
    "            has_head = any(k.startswith(\"head_vowel.\") or k.startswith(\"head_other.\") for k in sd.keys())\n",
    "            if not has_head:\n",
    "                raise KeyError(f\"Checkpoint wrapper '{wrap_key}' did not contain head keys. First keys: {list(sd.keys())[:25]}\")\n",
    "            return model\n",
    "\n",
    "    raise KeyError(\n",
    "        \"best_heads.pt did not match expected formats.\\n\"\n",
    "        f\"Top-level keys: {list(state.keys())[:50]}\"\n",
    "    )\n",
    "\n",
    "# -------------------------\n",
    "# Inference helper (returns y_true, y_score, sex_norm)\n",
    "# -------------------------\n",
    "def run_inference(model: FrozenW2V2TwoHeadTest, loader: DataLoader, desc: str, use_amp: bool):\n",
    "    all_probs, all_true, all_sex = [], [], []\n",
    "    pbar = tqdm(loader, desc=desc, dynamic_ncols=True)\n",
    "    with torch.inference_mode():\n",
    "        for batch in pbar:\n",
    "            input_values = batch[\"input_values\"].to(DEVICE, non_blocking=False)\n",
    "            attention_mask = batch[\"attention_mask\"].to(DEVICE, non_blocking=False)\n",
    "            labels = batch[\"labels\"].to(DEVICE, non_blocking=False)\n",
    "            task_group = batch[\"task_group\"]\n",
    "            sex_norm = batch[\"sex_norm\"]\n",
    "\n",
    "            with torch.amp.autocast(device_type=\"cuda\", enabled=use_amp):\n",
    "                logits = model.forward_logits(input_values, attention_mask, task_group)\n",
    "\n",
    "            probs = torch.softmax(logits.float(), dim=-1)[:, 1].detach().cpu().numpy().astype(np.float64)\n",
    "            all_probs.extend(probs.tolist())\n",
    "            all_true.extend(labels.detach().cpu().numpy().astype(np.int64).tolist())\n",
    "            all_sex.extend(list(sex_norm))\n",
    "\n",
    "    return np.asarray(all_true, dtype=np.int64), np.asarray(all_probs, dtype=np.float64), np.asarray(all_sex, dtype=object)\n",
    "\n",
    "# -------------------------\n",
    "# VAL threshold selection (Youden J on ROC curve)\n",
    "# -------------------------\n",
    "def select_threshold_youden_j(y_true_val: np.ndarray, y_prob_val: np.ndarray) -> float:\n",
    "    y_true_val = np.asarray(y_true_val, dtype=np.int64)\n",
    "    y_prob_val = np.asarray(y_prob_val, dtype=np.float64)\n",
    "\n",
    "    if len(np.unique(y_true_val)) < 2:\n",
    "        # ROC curve is undefined with only one class\n",
    "        return 0.5\n",
    "\n",
    "    fpr, tpr, thresholds = roc_curve(y_true_val, y_prob_val)\n",
    "\n",
    "    # thresholds may include inf; ignore non-finite values\n",
    "    finite = np.isfinite(thresholds)\n",
    "    fpr = fpr[finite]\n",
    "    tpr = tpr[finite]\n",
    "    thresholds = thresholds[finite]\n",
    "\n",
    "    if thresholds.size == 0:\n",
    "        return 0.5\n",
    "\n",
    "    j = tpr - fpr\n",
    "    best_idx = int(np.argmax(j))\n",
    "    thr = float(thresholds[best_idx])\n",
    "\n",
    "    # Probabilities are in [0,1]; clamp for safety\n",
    "    thr = float(min(1.0, max(0.0, thr)))\n",
    "    return thr\n",
    "\n",
    "# -------------------------\n",
    "# One seed: VAL threshold selection -> TEST evaluation\n",
    "# -------------------------\n",
    "def run_seed_once(seed: int):\n",
    "    set_all_seeds(seed)\n",
    "\n",
    "    run_dir = TEST_ROOT / f\"run_{dataset_id}_seed{seed}\"\n",
    "    run_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    best_heads_path = chosen_exp / f\"run_{dataset_id}_seed{seed}\" / \"best_heads.pt\"\n",
    "\n",
    "    print(f\"\\n[seed={seed}] Loading model + heads from:\")\n",
    "    print(\" \", str(best_heads_path))\n",
    "\n",
    "    state = torch.load(str(best_heads_path), map_location=\"cpu\")\n",
    "\n",
    "    # Read config from checkpoint when available (keeps test aligned to trainval)\n",
    "    ckpt_backbone = str(state.get(\"backbone_ckpt\", BACKBONE_CKPT)) if isinstance(state, dict) else BACKBONE_CKPT\n",
    "    ckpt_dropout  = float(state.get(\"dropout_p\", 0.10)) if isinstance(state, dict) else 0.10  # trainval uses 0.10\n",
    "\n",
    "    model = FrozenW2V2TwoHeadTest(ckpt_backbone, dropout_p=ckpt_dropout).to(DEVICE)\n",
    "    model = load_heads_into_model(model, best_heads_path, state)\n",
    "    model.eval()\n",
    "\n",
    "    use_amp = bool(USE_AMP and DEVICE.type == \"cuda\")\n",
    "\n",
    "    # ---- VAL inference (select threshold)\n",
    "    yv, pv, sv = run_inference(model, val_loader, desc=f\"[seed={seed}] Val (for threshold)\", use_amp=use_amp)\n",
    "    val_auc = compute_auc(yv, pv)\n",
    "    val_thr = select_threshold_youden_j(yv, pv)\n",
    "\n",
    "    val_thr_metrics = compute_threshold_metrics(yv, pv, thr=val_thr)\n",
    "\n",
    "    print(f\"[seed={seed}] VAL: auroc={val_auc:.6f} | selected_thr={val_thr:.6f} (Youden J)\")\n",
    "\n",
    "    # ---- TEST inference (fixed threshold from VAL)\n",
    "    yt, pt, st = run_inference(model, test_loader, desc=f\"[seed={seed}] Test\", use_amp=use_amp)\n",
    "    test_auc = compute_auc(yt, pt)\n",
    "    test_thr_metrics = compute_threshold_metrics(yt, pt, thr=val_thr)\n",
    "\n",
    "    # ---- fairness on TEST at val_thr\n",
    "    fnr_by_sex, delta_f_minus_m, delta_abs = compute_fnr_by_group_signed(\n",
    "        yt, pt, st, thr=val_thr\n",
    "    )\n",
    "\n",
    "    # ---- confusion by sex (all labels) on TEST at val_thr\n",
    "    confusion_by_sex = compute_confusion_by_group(yt, pt, st, thr=val_thr)\n",
    "\n",
    "    # ---- plots (TEST overall)\n",
    "    roc_png = run_dir / \"roc_curve.png\"\n",
    "    cm_png  = run_dir / \"confusion_matrix.png\"\n",
    "    save_roc_curve_png(yt, pt, str(roc_png), title_suffix=f\"Test (seed={seed})\")\n",
    "    save_confusion_png(yt, pt, str(cm_png), thr=val_thr, title_suffix=f\"Test (seed={seed})\")\n",
    "\n",
    "    # ---- plots (TEST by sex) at val_thr\n",
    "    cm_m_png = None\n",
    "    cm_f_png = None\n",
    "\n",
    "    mask_m = (st == \"M\")\n",
    "    mask_f = (st == \"F\")\n",
    "\n",
    "    if int(mask_m.sum()) > 0:\n",
    "        cm_m_png = run_dir / \"confusion_matrix_M.png\"\n",
    "        save_confusion_png(\n",
    "            yt[mask_m], pt[mask_m], str(cm_m_png),\n",
    "            thr=val_thr, title_suffix=f\"Test SEX=M (seed={seed})\"\n",
    "        )\n",
    "\n",
    "    if int(mask_f.sum()) > 0:\n",
    "        cm_f_png = run_dir / \"confusion_matrix_F.png\"\n",
    "        save_confusion_png(\n",
    "            yt[mask_f], pt[mask_f], str(cm_f_png),\n",
    "            thr=val_thr, title_suffix=f\"Test SEX=F (seed={seed})\"\n",
    "        )\n",
    "\n",
    "    metrics = {\n",
    "        \"dataset\": dataset_id,\n",
    "        \"seed\": int(seed),\n",
    "\n",
    "        \"n_val\": int(len(val_df)),\n",
    "        \"label_counts_val\": val_df[\"label_num\"].value_counts(dropna=False).to_dict(),\n",
    "        \"sex_counts_val_norm\": val_df[\"sex_norm\"].value_counts(dropna=False).to_dict(),\n",
    "\n",
    "        \"n_test\": int(len(test_df)),\n",
    "        \"label_counts_test\": test_df[\"label_num\"].value_counts(dropna=False).to_dict(),\n",
    "        \"sex_counts_test_norm\": test_df[\"sex_norm\"].value_counts(dropna=False).to_dict(),\n",
    "\n",
    "        \"threshold_selection\": {\n",
    "            \"method\": \"youden_j_on_val\",\n",
    "            \"selected_threshold\": float(val_thr),\n",
    "            \"val_auroc\": float(val_auc),\n",
    "            \"val_threshold_metrics\": val_thr_metrics,\n",
    "            \"note\": \"Threshold selected on VAL only (Youden J). TEST metrics computed at this fixed threshold to avoid test-tuning.\",\n",
    "        },\n",
    "\n",
    "        \"test_auroc\": float(test_auc),\n",
    "        \"threshold_metrics_test\": test_thr_metrics,\n",
    "        \"test_threshold\": float(val_thr),\n",
    "\n",
    "        \"fairness_test\": {\n",
    "            \"definition\": \"H3: ΔFNR = FNR(F) - FNR(M), where FNR(sex)=FN/(FN+TP) computed on PD-only true labels at test_threshold.\",\n",
    "            \"fnr_by_sex_norm\": fnr_by_sex,\n",
    "            \"delta_fnr_F_minus_M\": float(delta_f_minus_m),\n",
    "            \"delta_fnr_abs\": float(delta_abs),\n",
    "            \"note\": \"If n_PD for a sex is 0, its FNR is NaN and ΔFNR is NaN.\",\n",
    "            \"sex_normalization_note\": \"sex_norm in {M,F,UNK}. Values not mapped to M/F counted as UNK.\",\n",
    "        },\n",
    "\n",
    "        \"confusion_by_sex_norm\": confusion_by_sex,\n",
    "\n",
    "        \"artifacts\": {\n",
    "            \"roc_curve_png\": str(roc_png),\n",
    "            \"confusion_matrix_png\": str(cm_png),\n",
    "            \"confusion_matrix_M_png\": str(cm_m_png) if cm_m_png is not None else None,\n",
    "            \"confusion_matrix_F_png\": str(cm_f_png) if cm_f_png is not None else None,\n",
    "        },\n",
    "\n",
    "        \"dx_out_root\": DX_OUT_ROOT,\n",
    "        \"trainval_experiment_used\": str(chosen_exp),\n",
    "        \"best_heads_path\": str(best_heads_path),\n",
    "        \"backbone_ckpt_used\": ckpt_backbone,\n",
    "        \"dropout_p_used\": float(ckpt_dropout),\n",
    "        \"timestamp\": time.strftime(\"%Y%m%d_%H%M%S\"),\n",
    "    }\n",
    "\n",
    "    metrics_path = run_dir / \"metrics.json\"\n",
    "    with open(metrics_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(metrics, f, indent=2)\n",
    "\n",
    "    print(f\"[seed={seed}] DONE | test_AUROC={test_auc:.6f} | thr(val-opt)={val_thr:.6f}\")\n",
    "\n",
    "    def _fmt_fnr(g):\n",
    "        d = fnr_by_sex.get(g, None)\n",
    "        if d is None:\n",
    "            return \"n/a\"\n",
    "        return f\"fnr={d['fnr']:.6f} (n_PD={d['n_pos']}, fn={d['fn']}, tp={d['tp']})\"\n",
    "\n",
    "    print(f\"[seed={seed}] FAIRNESS (H3) @ thr(val-opt)={val_thr:.6f}:\")\n",
    "    print(\"  M:\", _fmt_fnr(\"M\"))\n",
    "    print(\"  F:\", _fmt_fnr(\"F\"))\n",
    "    if \"UNK\" in fnr_by_sex:\n",
    "        print(\"  UNK:\", _fmt_fnr(\"UNK\"))\n",
    "    print(\"  ΔFNR (F-M):\", f\"{delta_f_minus_m:.6f}\" if not np.isnan(delta_f_minus_m) else \"nan\")\n",
    "    print(\"  |ΔFNR|:\", f\"{delta_abs:.6f}\" if not np.isnan(delta_abs) else \"nan\")\n",
    "\n",
    "    print(f\"[seed={seed}] WROTE:\")\n",
    "    print(\" \", str(metrics_path))\n",
    "    print(\" \", str(roc_png))\n",
    "    print(\" \", str(cm_png))\n",
    "    if cm_m_png is not None:\n",
    "        print(\" \", str(cm_m_png))\n",
    "    if cm_f_png is not None:\n",
    "        print(\" \", str(cm_f_png))\n",
    "\n",
    "    return {\n",
    "        \"seed\": int(seed),\n",
    "        \"val_selected_threshold\": float(val_thr),\n",
    "        \"val_auc\": float(val_auc),\n",
    "        \"test_auc\": float(test_auc),\n",
    "        \"test_thr_metrics\": test_thr_metrics,\n",
    "        \"fnr_by_sex\": fnr_by_sex,\n",
    "        \"delta_signed\": float(delta_f_minus_m),\n",
    "        \"delta_abs\": float(delta_abs),\n",
    "        \"confusion_by_sex\": confusion_by_sex,\n",
    "        \"run_dir\": str(run_dir),\n",
    "    }\n",
    "\n",
    "# -------------------------\n",
    "# Run all seeds + aggregate summary\n",
    "# -------------------------\n",
    "results = []\n",
    "for seed in SEEDS:\n",
    "    results.append(run_seed_once(seed))\n",
    "\n",
    "# AUROC mean ± 95% CI (t, n=3)\n",
    "aurocs = [r[\"test_auc\"] for r in results]\n",
    "t_crit = 4.302652729911275  # df=2, 95% CI\n",
    "n = len(aurocs)\n",
    "mean_auc = float(np.mean(aurocs))\n",
    "std_auc = float(np.std(aurocs, ddof=1)) if n > 1 else 0.0\n",
    "half_width = float(t_crit * (std_auc / math.sqrt(n))) if n > 1 else 0.0\n",
    "ci95 = [float(mean_auc - half_width), float(mean_auc + half_width)]\n",
    "\n",
    "def _mean_sd(vals):\n",
    "    vals = np.asarray(vals, dtype=np.float64)\n",
    "    return float(np.nanmean(vals)), float(np.nanstd(vals, ddof=1)) if np.sum(~np.isnan(vals)) > 1 else 0.0\n",
    "\n",
    "# Aggregate threshold metrics at val-opt threshold\n",
    "keys = [\"accuracy\",\"precision\",\"recall\",\"f1_score\",\"sensitivity\",\"specificity\",\"mcc\",\"p_value_fisher_two_sided\"]\n",
    "agg = {}\n",
    "for k in keys:\n",
    "    v = [float(r[\"test_thr_metrics\"].get(k, float(\"nan\"))) for r in results]\n",
    "    mu, sd = _mean_sd(v)\n",
    "    agg[k] = {\n",
    "        \"mean\": mu,\n",
    "        \"sd\": sd,\n",
    "        \"values_by_seed\": {str(r[\"seed\"]): float(r[\"test_thr_metrics\"].get(k, float(\"nan\"))) for r in results},\n",
    "    }\n",
    "\n",
    "cm_by_seed = {str(r[\"seed\"]): r[\"test_thr_metrics\"][\"confusion_matrix\"] for r in results}\n",
    "\n",
    "# Aggregate thresholds selected on VAL\n",
    "thr_vals = [float(r[\"val_selected_threshold\"]) for r in results]\n",
    "thr_mean, thr_sd = _mean_sd(thr_vals)\n",
    "\n",
    "# FAIRNESS aggregation (H3) on TEST at val-opt threshold\n",
    "fnr_m_vals, fnr_f_vals, n_pd_m_vals, n_pd_f_vals = [], [], [], []\n",
    "d_signed_vals, d_abs_vals = [], []\n",
    "\n",
    "fnr_by_seed = {}\n",
    "delta_signed_by_seed = {}\n",
    "delta_abs_by_seed = {}\n",
    "confusion_by_sex_by_seed = {}\n",
    "run_dirs = []\n",
    "\n",
    "for r in results:\n",
    "    s = str(r[\"seed\"])\n",
    "    fnr_by_seed[s] = r[\"fnr_by_sex\"]\n",
    "    delta_signed_by_seed[s] = float(r[\"delta_signed\"])\n",
    "    delta_abs_by_seed[s] = float(r[\"delta_abs\"])\n",
    "    confusion_by_sex_by_seed[s] = r[\"confusion_by_sex\"]\n",
    "    run_dirs.append(r[\"run_dir\"])\n",
    "\n",
    "    d = r[\"fnr_by_sex\"]\n",
    "    fnr_m_vals.append(float(d.get(\"M\", {}).get(\"fnr\", float(\"nan\"))))\n",
    "    fnr_f_vals.append(float(d.get(\"F\", {}).get(\"fnr\", float(\"nan\"))))\n",
    "    n_pd_m_vals.append(float(d.get(\"M\", {}).get(\"n_pos\", float(\"nan\"))))\n",
    "    n_pd_f_vals.append(float(d.get(\"F\", {}).get(\"n_pos\", float(\"nan\"))))\n",
    "    d_signed_vals.append(float(r[\"delta_signed\"]))\n",
    "    d_abs_vals.append(float(r[\"delta_abs\"]))\n",
    "\n",
    "fnr_m_mean, fnr_m_sd = _mean_sd(fnr_m_vals)\n",
    "fnr_f_mean, fnr_f_sd = _mean_sd(fnr_f_vals)\n",
    "d_signed_mean, d_signed_sd = _mean_sd(d_signed_vals)\n",
    "d_abs_mean, d_abs_sd = _mean_sd(d_abs_vals)\n",
    "\n",
    "print(\"\\nTest AUROC by seed:\")\n",
    "for r in results:\n",
    "    print(f\"  seed {r['seed']}: {r['test_auc']:.6f}\")\n",
    "print(f\"\\nMean Test AUROC: {mean_auc:.6f}\")\n",
    "print(f\"95% CI (t, n=3): [{ci95[0]:.6f}, {ci95[1]:.6f}]\")\n",
    "\n",
    "print(\"\\nVAL-selected threshold (Youden J) across seeds (mean ± SD):\")\n",
    "print(f\"  thr_val_opt: {thr_mean:.6f} ± {thr_sd:.6f}\")\n",
    "print(\"  values_by_seed:\", {str(r[\"seed\"]): float(r[\"val_selected_threshold\"]) for r in results})\n",
    "\n",
    "print(\"\\nThreshold metrics at VAL-OPT threshold on TEST (mean ± SD across seeds):\")\n",
    "for k in [\"accuracy\",\"precision\",\"sensitivity\",\"specificity\",\"f1_score\",\"mcc\"]:\n",
    "    mu = agg[k][\"mean\"]\n",
    "    sd = agg[k][\"sd\"]\n",
    "    print(f\"  {k}: {mu:.6f} ± {sd:.6f}\")\n",
    "print(\"  fisher_p_value_two_sided:\", f\"{agg['p_value_fisher_two_sided']['mean']:.6g} ± {agg['p_value_fisher_two_sided']['sd']:.6g}\")\n",
    "\n",
    "print(\"\\nFAIRNESS (H3) on TEST @ VAL-OPT threshold (mean ± SD):\")\n",
    "print(f\"  FNR_M: {fnr_m_mean:.6f} ± {fnr_m_sd:.6f}\")\n",
    "print(f\"  FNR_F: {fnr_f_mean:.6f} ± {fnr_f_sd:.6f}\")\n",
    "print(f\"  ΔFNR (F-M): {d_signed_mean:.6f} ± {d_signed_sd:.6f}\")\n",
    "print(f\"  |ΔFNR|: {d_abs_mean:.6f} ± {d_abs_sd:.6f}\")\n",
    "print(\"  Per-seed:\", {\n",
    "    str(r[\"seed\"]): {\n",
    "        \"thr_val_opt\": float(r[\"val_selected_threshold\"]),\n",
    "        \"FNR_M\": fnr_m_vals[i],\n",
    "        \"n_PD_M\": n_pd_m_vals[i],\n",
    "        \"FNR_F\": fnr_f_vals[i],\n",
    "        \"n_PD_F\": n_pd_f_vals[i],\n",
    "        \"delta_F_minus_M\": d_signed_vals[i],\n",
    "        \"abs_delta\": d_abs_vals[i],\n",
    "    } for i, r in enumerate(results)\n",
    "})\n",
    "\n",
    "summary = {\n",
    "    \"dataset\": dataset_id,\n",
    "    \"dx_out_root\": DX_OUT_ROOT,\n",
    "    \"manifest_all\": MANIFEST_ALL,\n",
    "    \"trainval_experiment_used\": str(chosen_exp),\n",
    "    \"seeds\": SEEDS,\n",
    "\n",
    "    \"n_val\": int(len(val_df)),\n",
    "    \"label_counts_val\": val_df[\"label_num\"].value_counts(dropna=False).to_dict(),\n",
    "    \"sex_counts_val_norm\": val_df[\"sex_norm\"].value_counts(dropna=False).to_dict(),\n",
    "\n",
    "    \"n_test\": int(len(test_df)),\n",
    "    \"label_counts_test\": test_df[\"label_num\"].value_counts(dropna=False).to_dict(),\n",
    "    \"sex_counts_test_norm\": test_df[\"sex_norm\"].value_counts(dropna=False).to_dict(),\n",
    "\n",
    "    \"threshold_selection\": {\n",
    "        \"method\": \"youden_j_on_val\",\n",
    "        \"val_selected_threshold_by_seed\": {str(r[\"seed\"]): float(r[\"val_selected_threshold\"]) for r in results},\n",
    "        \"val_selected_threshold_mean_sd\": {\"mean\": float(thr_mean), \"sd\": float(thr_sd)},\n",
    "        \"note\": \"Threshold selected on VAL only (Youden J). TEST metrics computed at this fixed threshold to avoid test-tuning.\",\n",
    "    },\n",
    "\n",
    "    \"test_aurocs_by_seed\": {str(r[\"seed\"]): float(r[\"test_auc\"]) for r in results},\n",
    "    \"mean_test_auroc\": float(mean_auc),\n",
    "    \"t_crit_df2_95\": float(t_crit),\n",
    "    \"half_width_95_ci\": float(half_width),\n",
    "    \"ci95_test_auroc\": ci95,\n",
    "\n",
    "    \"threshold_metrics_mean_sd\": agg,\n",
    "    \"confusion_matrix_by_seed\": cm_by_seed,\n",
    "    \"run_dirs\": run_dirs,\n",
    "\n",
    "    \"fairness_test\": {\n",
    "        \"definition\": \"H3: ΔFNR = FNR(F) - FNR(M), where FNR(sex)=FN/(FN+TP) computed on PD-only true labels at test_threshold.\",\n",
    "        \"fnr_by_sex_norm_by_seed\": fnr_by_seed,\n",
    "        \"delta_fnr_F_minus_M_by_seed\": delta_signed_by_seed,\n",
    "        \"delta_fnr_abs_by_seed\": delta_abs_by_seed,\n",
    "        \"fnr_M_mean_sd\": {\"mean\": float(fnr_m_mean), \"sd\": float(fnr_m_sd), \"values_by_seed\": {str(s): float(v) for s, v in zip(SEEDS, fnr_m_vals)}},\n",
    "        \"fnr_F_mean_sd\": {\"mean\": float(fnr_f_mean), \"sd\": float(fnr_f_sd), \"values_by_seed\": {str(s): float(v) for s, v in zip(SEEDS, fnr_f_vals)}},\n",
    "        \"delta_fnr_F_minus_M_mean_sd\": {\"mean\": float(d_signed_mean), \"sd\": float(d_signed_sd), \"values_by_seed\": {str(s): float(v) for s, v in zip(SEEDS, d_signed_vals)}},\n",
    "        \"delta_fnr_abs_mean_sd\": {\"mean\": float(d_abs_mean), \"sd\": float(d_abs_sd), \"values_by_seed\": {str(s): float(v) for s, v in zip(SEEDS, d_abs_vals)}},\n",
    "        \"denominators_PD_by_seed\": {str(s): {\"n_PD_M\": float(n_pd_m_vals[i]), \"n_PD_F\": float(n_pd_f_vals[i])} for i, s in enumerate(SEEDS)},\n",
    "        \"sex_normalization_note\": \"sex_norm in {M,F,UNK}. Values not mapped to M/F counted as UNK. ΔFNR computed only when both M and F have defined FNR.\",\n",
    "    },\n",
    "\n",
    "    \"confusion_by_sex_norm_by_seed\": confusion_by_sex_by_seed,\n",
    "\n",
    "    \"timestamp\": time.strftime(\"%Y%m%d_%H%M%S\"),\n",
    "}\n",
    "\n",
    "summary_path = TEST_ROOT / \"summary_test.json\"\n",
    "with open(summary_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "history_path = TEST_ROOT / \"history_index.jsonl\"\n",
    "with open(history_path, \"a\", encoding=\"utf-8\") as f:\n",
    "    f.write(json.dumps(summary) + \"\\n\")\n",
    "\n",
    "print(\"\\nWROTE summary:\", str(summary_path))\n",
    "print(\"APPENDED history index:\", str(history_path))\n",
    "print(\"Open this folder to access artifacts:\", str(TEST_ROOT))\n",
    "\n",
    "# -------------------------\n",
    "# Stop runtime (release L4)\n",
    "# -------------------------\n",
    "print(\"\\nAll done. Unassigning the runtime to stop the L4 instance...\")\n",
    "try:\n",
    "    from google.colab import runtime  # type: ignore\n",
    "    print(\"Calling runtime.unassign() now.\")\n",
    "    runtime.unassign()\n",
    "except Exception as e:\n",
    "    print(\"Could not unassign runtime automatically. You can stop the runtime manually in Colab.\")\n",
    "    print(\"Reason:\", repr(e))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "d87e5281688c49d4b2977f2d65c8c25c",
      "0a87e1d2b4c84b659e3ad49f637eb045",
      "5ddfb28d7cd944548c6e54030b18e219",
      "29cdb1f61f5d47e5a0f8d3383821af5a",
      "83b8193e595f4e5e879844a0469f4f6b",
      "dfe2189f46764a4aa93d0fb26ef03fe4",
      "b29722b673b44430a5021acac87e9af0",
      "8e5ebd575504414890a61495e754ced6",
      "863c4e7ba8184ef98d3936736141dcd6",
      "09539a400cc74f1c93942b95acb7bc35",
      "6bca193ca9c74b00a09a08f1ed19d018",
      "a534c46667f542739e5587490c790eb3",
      "6200e8958eb14bdba599cfe256068abd",
      "d60db4f87422412aa44bb254807369cc",
      "b461ddf689354888b9bbf6d3582763c8",
      "a3b7421e02514c0dbc35496a3068eef2",
      "d2f53d58315b4bcab7309407ae2ded9a",
      "fc7cd18495f649e9a3955fbe166d47ea",
      "81a4c4b1799446f1ab994a68cd01de44",
      "5fcb8c1ef5aa4640853a43791cbef3d3",
      "61b5f946084b4db8aed17876ef39fcbb",
      "a5acb9453aac4a8b8ebe73df56ac9263",
      "ea61a052d1a44450947d4d25bcf08154",
      "5529a5ecb65f45a2aa7ca7c68a617cfe",
      "20661e1cd0fc4dd5aab13afd46449f7e",
      "aa19c6d9072841e7ab2056fc91837f7c",
      "a2a2e38dc649409b9a3b54322a4b2673",
      "fc721394a82b484a9453edbb600f20be",
      "e36483660f284ab0ad63cf38e411f7b8",
      "22a3d35bd3344fd98cacba5359702481",
      "02cc4d9ece6c4d1f8afe0629e625664e",
      "50a71f44f82440b593797884152a5732",
      "f17ce012feb24a22817f0a561e7ce2ef",
      "ecff6a7674544297846eef9e5949449a",
      "f009855174aa4713ba355ef278cc68ed",
      "d1d2a9d4f794445a951649b2c77d52d9",
      "6bee0d071780457cb5996bd3eb158c32",
      "3bfe14c1ffd845ef89520ee6f14ab819",
      "4ba72cc3c2b74ec58abe088a0ac02783",
      "a7e29f7b252546f2945df281fab7338a",
      "d1e53ce3f29749be820b380aa67ab8a3",
      "468f773e7e664f579a4d1050b5bfd86f",
      "5f4fedde71b343a69e3cf54f93873bc0",
      "7aaededbdd944fc9823491ca7712abf8",
      "395fe73edbd24e3f8f31e2cfc20f4d9f",
      "4006f6306c0e47518282cc5273fe46c4",
      "5909534448ac4d33b0f27ab66910a3dd",
      "40c519c506f94015b178b9fffdafdc51",
      "fb9876d97c4340aeb66ebe4ca7ab6220",
      "1a9c00b3acbf47cd88534b6e1501ffe8",
      "b078b58842d64437a8d41d9e752fc1cb",
      "82a584fb044d482cbb3ffd9fe0b79b18",
      "7cb2d37cc1da4daaa55cb3e20cd006b7",
      "d82572fe354f47829550cc2953746948",
      "29db587a67454dea81f14a501d3ed71b",
      "525cf65125fe42318da1c7b4e9ffa99f",
      "912a2000858243eeaa391d88ebf1809d",
      "39cabb34dd96429b9c91b3dd6b26cde3",
      "8e04d7152c724b26a8522ae01e842fd7",
      "2805307142904ea39c71bb3f5e9cd3f8",
      "c4fbb90c72e448bc84fe9476e795a2ef",
      "5553291405874b2593554659162cdf4d",
      "b46e2904c947415bb59bf7ba5c836051",
      "0a12865bbc164e1b92f307101db26d41",
      "697ce4b458b9403c9237f819fb99f2c1",
      "4a2206079a934e35ab0dcc5b5355f66b",
      "c66f92ecb98243178f7385ea21121473",
      "0d3de8964bc24ab2853dd43d3c8eefdb",
      "4fe6278c199844aa80d3a70bdf084cd9",
      "946fd7544dc14c48bb86dc3962d47f2b",
      "cc5faaf6aeb748199bc0abf718dac410",
      "314247e35e494dce85e49aba8e847883",
      "1943047198454fd5bbd9409808cdfd88",
      "08d50fa8c20e4625b6da438cd4b2bc77",
      "be6a5933f1204b3289550a8582be5fc4",
      "97c396d952d847d3838eba0b5a85f223",
      "fabff9c9e8c84192ac8dfa4d7e250d8b",
      "a64384b3c6a34a53bb42eb6f7cecbc7e",
      "9338c7f8d938491e906c8cc41a397f08",
      "8aa6fb89d103434bb955163391d8486e",
      "9e0276d5f7114dfdb5e8e6d80b3dcad5",
      "37cba8b4b593456b9b052d1d73f8e672",
      "ff81e2061bff41a59386773f3b43ffe3",
      "539d5d180d1a4f04a179cca307b6c4f7",
      "e7dbf034ac1644b5b5b53c218d202a7e",
      "8916ed55f8474ecc8346498b061fa51e",
      "c8f8b8a48f29441abec5670276f08d3f",
      "ee96fafc942d4a399167c90ddcc1aa44",
      "c86777a9798e4c02b9c40788285f701e",
      "1654a668c193466b929543793cfd694c",
      "ca90f7b39ad64473b63c31d9470ec7c8",
      "57cf5abcecca48dc941c841279c22ddc",
      "847c5f67e2154a54a655aa4bfc590851",
      "7d278fae9d6c48cbbc77684b083fb726",
      "6ad54f00e4f44c2c9f482cd275608026",
      "01dcc93792ed40a9b59d1444dd398cfb",
      "c22acf2cde844a55a165adaf8e1b327b",
      "915c9edd2c7141029ec7faae4f7a7af7",
      "c738d66c2ae246338d2f1cd2e7521679",
      "66758eb1832f44e3aaac05d44314405e",
      "f90afdaac892423da68d253aeabd40ac",
      "62e36aca7c8d4cbd967abdb5587c04af",
      "444453f49eaa47949703814f011b33f0",
      "14b7ce9933c4481aaf97238a71c78c84",
      "eab10a19685a4e5db49f11cc2ad065e0",
      "5e68207f3b3a4764a8d09528a8afe1bd",
      "6d7bc80592224cbb951a65b2a154ff59",
      "801f99baa302495a8d30432f5d5418f4",
      "e3379075d0b540e8a563c1b482e10c66",
      "c039960d9bdb489cbb1a4ef3c9b50115",
      "a58a64a3e1b745c68f9f187107b1d6b0",
      "209f94ca153d4e8da02e86bd1a6b1776",
      "d3d0785384114cfc94542efdc9562f1d",
      "0688d43195644a018f032492701c630b",
      "1aec83bdeb4441e384308c803a2f1590",
      "6c450086980348f3a55c1a4e573c27df",
      "a015529d0c1645998d3dfc94821451b0",
      "34ed1185b234487e8669b4d4e318aeb0",
      "529b2f073fdb4ae18ac19c1b5fb21f20",
      "fdc07a33af994ffcb322b71380b86614",
      "a99b886c315747bd9805fa4ba26f6ac4"
     ]
    },
    "id": "3GiKK2jwfi5Y"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "The following cell runs the final D4 (Italian, IPVS) test evaluation in a safe and repeatable way, using the trained model heads from the most recent train and validation experiment. It reads `manifests/manifest_all.csv` from the D4 output folder and keeps only the validation split, which is used to choose a decision threshold, and the test split, which is used to report performance. It checks that all required columns are present, with special attention to the sex column since it is needed for fairness checks and sex specific confusion matrices. Before any model work begins, the cell performs basic fail fast checks to confirm that the manifest exists, that the validation and test splits are not empty, and that all audio files listed in `clip_path` are present on disk.\n",
    "\n",
    "After setup, including fixed random seeds, batch settings, GPU mixed precision settings, and a required 16 kHz sample rate, the cell prepares the data for inference. Each clip is assigned to one of two task groups using a strict rule: `task == \"vowl\"` is treated as vowel speech and everything else is treated as other speech. Sex labels are normalized to M, F, or UNK using common text formats, without guessing numeric encodings. A dataset class loads each audio file, converts it to mono, verifies the sample rate, and builds an attention mask to help the model ignore trailing silence in short vowel clips. A collator pads audio and attention masks in each batch to a common length. A short warm up step loads up to two batches from both validation and test to catch data loading or file issues early, including edge cases with very small splits.\n",
    "\n",
    "The model used for evaluation matches the training setup. It consists of a frozen Wav2Vec2 backbone with two small classification heads, one for vowel clips and one for other clips, each preceded by LayerNorm and Dropout. The cell then finds the most recent train and validation experiment folder that contains the required `best_heads.pt` files for all three seeds. For each seed, 1337, 2024, and 7777, it loads the saved head weights, runs inference on the validation split to compute a validation optimal threshold using Youden’s J statistic, which maximizes true positive rate minus false positive rate, and then runs inference on the test split using that same threshold rather than a fixed value of 0.5.\n",
    "\n",
    "For each seed, results are saved under\n",
    "`<DX_OUT_ROOT>/monolingual_test_runs/run_<DATASET>_seed<seed>/`.\n",
    "Saved outputs include a ROC curve image for the test split, a confusion matrix for the test split at the validation chosen threshold, additional confusion matrices split by M and F when those groups are present, and a `metrics.json` file containing dataset counts, the selected threshold and its details, AUROC values, threshold based metrics, fairness results, and paths to the saved plots.\n",
    "\n",
    "After all three seeds finish, the cell combines results across seeds and prints a short summary. This includes the mean test AUROC with a 95% confidence interval computed using a t distribution with n equal to 3, along with the mean and standard deviation of the validation optimal thresholds, threshold based test metrics, and the H3 fairness measure. The fairness measure reports false negative rates by sex on Parkinson’s positive cases only and computes ΔFNR as FNR(F) minus FNR(M). A combined `summary_test.json` file is written, the same summary is appended to `history_index.jsonl`, the output locations are printed, and the Colab runtime is unassigned to stop the GPU instance."
   ],
   "metadata": {
    "id": "h4lwBvAbonrm"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# =========================\n",
    "# D4 Monolingual TEST (VAL-OPT THRESHOLD via Youden J) + Fairness by Sex\n",
    "# What this cell does:\n",
    "# - Reads one manifest (VAL + TEST) for a single dataset\n",
    "# - For each seed:\n",
    "#   1) Runs inference on VAL to pick a threshold using Youden J (TPR − FPR)\n",
    "#   2) Runs inference on TEST and reports metrics at that VAL-picked threshold\n",
    "# - Also saves ROC and confusion matrix plots (overall + by sex when available)\n",
    "# Inputs:\n",
    "# - manifest_all.csv (must include VAL and TEST rows)\n",
    "# - best_heads.pt for each seed from the most recent trainval experiment\n",
    "# Outputs:\n",
    "# - Per-seed run folder with metrics.json and plots\n",
    "# - summary_test.json (aggregated across seeds) + history_index.jsonl\n",
    "# =========================\n",
    "#\n",
    "# NOTE FOR D4:\n",
    "# - Manifest sex is typically \"M\" and \"F\".\n",
    "# - Sex is normalized to M/F/UNK. Numeric encodings are not auto-mapped.\n",
    "# =========================\n",
    "\n",
    "import os, json, math, random, time, warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import soundfile as sf\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from transformers import Wav2Vec2Model\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, confusion_matrix, matthews_corrcoef\n",
    "from scipy.stats import fisher_exact\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# -------------------------\n",
    "# Safety check: avoid local files that override real libraries\n",
    "# -------------------------\n",
    "if os.path.exists(\"/content/torch.py\") or os.path.exists(\"/content/torch/__init__.py\"):\n",
    "    raise RuntimeError(\"Found local /content/torch.py or /content/torch/ that shadows PyTorch. Rename/remove it and restart runtime.\")\n",
    "if os.path.exists(\"/content/transformers.py\") or os.path.exists(\"/content/transformers/__init__.py\"):\n",
    "    raise RuntimeError(\"Found local /content/transformers.py or /content/transformers/ that shadows Hugging Face Transformers. Rename/remove it and restart runtime.\")\n",
    "\n",
    "# -------------------------\n",
    "# Colab Drive: mount if missing\n",
    "# -------------------------\n",
    "if not os.path.isdir(\"/content/drive/MyDrive\"):\n",
    "    from google.colab import drive  # type: ignore\n",
    "    drive.mount(\"/content/drive\")\n",
    "\n",
    "# -------------------------\n",
    "# Paths: use runtime DX_OUT_ROOT if present, otherwise fallback to D4\n",
    "# Inputs:\n",
    "# - DX_OUT_ROOT/manifests/manifest_all.csv\n",
    "# -------------------------\n",
    "D4_OUT_ROOT_FALLBACK = \"/content/drive/MyDrive/AI_PD_Project/Datasets/D4-Italian (IPVS)/preprocessed_v1\"\n",
    "DX_OUT_ROOT = globals().get(\"DX_OUT_ROOT\", D4_OUT_ROOT_FALLBACK)\n",
    "MANIFEST_ALL = f\"{DX_OUT_ROOT}/manifests/manifest_all.csv\"\n",
    "\n",
    "# -------------------------\n",
    "# Settings (kept consistent with trainval where relevant)\n",
    "# -------------------------\n",
    "SEEDS          = [1337, 2024, 7777]\n",
    "BACKBONE_CKPT  = \"facebook/wav2vec2-base\"\n",
    "SR_EXPECTED    = 16000\n",
    "TINY_THRESH    = 1e-4\n",
    "\n",
    "EFFECTIVE_BS   = 64\n",
    "PER_DEVICE_BS  = 16\n",
    "GRAD_ACCUM     = max(1, EFFECTIVE_BS // PER_DEVICE_BS)  # printed only\n",
    "\n",
    "DROPOUT_P      = 0.2\n",
    "\n",
    "NUM_WORKERS    = 0\n",
    "PIN_MEMORY     = False\n",
    "\n",
    "USE_AMP        = True\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "try:\n",
    "    torch.set_float32_matmul_precision(\"high\")\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# Quiet down known, non-actionable warnings for this test cell\n",
    "warnings.filterwarnings(\"ignore\", message=\"None of the inputs have requires_grad=True. Gradients will be None\")\n",
    "warnings.filterwarnings(\"ignore\", message=\"Passing `gradient_checkpointing` to a config initialization is deprecated\")\n",
    "\n",
    "print(\"DX_OUT_ROOT:\", DX_OUT_ROOT)\n",
    "print(\"MANIFEST_ALL:\", MANIFEST_ALL)\n",
    "print(\"DEVICE:\", DEVICE)\n",
    "if DEVICE.type == \"cuda\":\n",
    "    print(\"GPU:\", torch.cuda.get_device_name(0))\n",
    "print(\"PER_DEVICE_BS:\", PER_DEVICE_BS, \"| EFFECTIVE_BS:\", PER_DEVICE_BS * GRAD_ACCUM)\n",
    "print(\"NUM_WORKERS:\", NUM_WORKERS, \"| PIN_MEMORY:\", PIN_MEMORY)\n",
    "print(\"USE_AMP:\", bool(USE_AMP and DEVICE.type == \"cuda\"))\n",
    "\n",
    "# -------------------------\n",
    "# Load manifest and prepare VAL/TEST tables\n",
    "# Inputs:\n",
    "# - manifest_all.csv with required columns\n",
    "# Outputs:\n",
    "# - val_df, test_df filtered to split == val/test\n",
    "# - dataset_id inferred from manifest (if available)\n",
    "# -------------------------\n",
    "if not os.path.exists(MANIFEST_ALL):\n",
    "    raise FileNotFoundError(f\"Missing manifest_all.csv: {MANIFEST_ALL}\")\n",
    "\n",
    "m_all = pd.read_csv(MANIFEST_ALL)\n",
    "\n",
    "# Required for: splitting, audio loading, labels, fairness and sex plots\n",
    "req_cols = {\"split\", \"clip_path\", \"label_num\", \"task\", \"sex\"}\n",
    "missing = [c for c in sorted(req_cols) if c not in m_all.columns]\n",
    "if missing:\n",
    "    raise ValueError(f\"Manifest missing required columns: {missing}. Found: {list(m_all.columns)}\")\n",
    "\n",
    "# Infer dataset id from the dominant value in 'dataset' (if present)\n",
    "if \"dataset\" in m_all.columns and m_all[\"dataset\"].notna().any():\n",
    "    dataset_id = str(m_all[\"dataset\"].astype(str).value_counts(dropna=True).idxmax())\n",
    "    m_all = m_all[m_all[\"dataset\"].astype(str) == dataset_id].copy()\n",
    "else:\n",
    "    dataset_id = \"DX\"\n",
    "\n",
    "# Keep a compact set of columns (adds placeholders if missing)\n",
    "keep_cols = [\"clip_path\", \"label_num\", \"task\", \"speaker_id\", \"sex\", \"duration_sec\", \"split\"]\n",
    "for c in keep_cols:\n",
    "    if c not in m_all.columns:\n",
    "        m_all[c] = np.nan\n",
    "m_all = m_all[keep_cols].copy()\n",
    "\n",
    "val_df  = m_all[m_all[\"split\"].isin([\"val\"])].copy().reset_index(drop=True)\n",
    "test_df = m_all[m_all[\"split\"].isin([\"test\"])].copy().reset_index(drop=True)\n",
    "\n",
    "print(f\"\\nDataset inferred: {dataset_id}\")\n",
    "print(f\"VAL rows:  {len(val_df)}\")\n",
    "print(f\"TEST rows: {len(test_df)}\")\n",
    "\n",
    "# VAL is required here because the threshold is chosen on VAL (Youden J)\n",
    "if len(val_df) == 0:\n",
    "    raise RuntimeError(\"After filtering to split=='val', manifest has 0 rows. VAL is required to compute VAL-opt threshold (Youden J).\")\n",
    "if len(test_df) == 0:\n",
    "    raise RuntimeError(\"After filtering to split=='test', manifest has 0 rows.\")\n",
    "\n",
    "print(\"VAL label counts:\",  val_df[\"label_num\"].value_counts(dropna=False).to_dict())\n",
    "print(\"TEST label counts:\", test_df[\"label_num\"].value_counts(dropna=False).to_dict())\n",
    "print(\"VAL sex counts (raw):\",  val_df[\"sex\"].value_counts(dropna=False).to_dict())\n",
    "print(\"TEST sex counts (raw):\", test_df[\"sex\"].value_counts(dropna=False).to_dict())\n",
    "\n",
    "# -------------------------\n",
    "# Fast failure: verify clip_path files exist (shows progress)\n",
    "# Output: raises early if missing audio is found\n",
    "# -------------------------\n",
    "def _fail_fast_missing_paths(df: pd.DataFrame, name: str):\n",
    "    missing_paths = []\n",
    "    for p in tqdm(df[\"clip_path\"].astype(str).tolist(), desc=f\"Check {name} clip_path exists\", dynamic_ncols=True):\n",
    "        if not os.path.exists(p):\n",
    "            missing_paths.append(p)\n",
    "            if len(missing_paths) >= 10:\n",
    "                break\n",
    "    if missing_paths:\n",
    "        raise FileNotFoundError(f\"{name}: missing clip_path(s). Examples: {missing_paths}\")\n",
    "\n",
    "_fail_fast_missing_paths(val_df, \"VAL\")\n",
    "_fail_fast_missing_paths(test_df, \"TEST\")\n",
    "\n",
    "# -------------------------\n",
    "# Task grouping (exact rule used by this project)\n",
    "# Output: task_group in {\"vowel\",\"other\"}\n",
    "# -------------------------\n",
    "def _task_group(task_val) -> str:\n",
    "    return \"vowel\" if str(task_val) == \"vowl\" else \"other\"\n",
    "\n",
    "val_df[\"task_group\"]  = val_df[\"task\"].apply(_task_group)\n",
    "test_df[\"task_group\"] = test_df[\"task\"].apply(_task_group)\n",
    "\n",
    "# -------------------------\n",
    "# Sex normalization for fairness + sex-specific confusion charts\n",
    "# Output: sex_norm in {\"M\",\"F\",\"UNK\"}\n",
    "# -------------------------\n",
    "def normalize_sex(val) -> str:\n",
    "    \"\"\"\n",
    "    Returns 'M', 'F', or 'UNK'.\n",
    "    Maps common text forms (M/F, Male/Female, etc).\n",
    "    Numeric encodings are not auto-mapped to avoid silent mix-ups.\n",
    "    \"\"\"\n",
    "    if pd.isna(val):\n",
    "        return \"UNK\"\n",
    "    s = str(val).strip().lower()\n",
    "    if s in {\"m\", \"male\", \"man\", \"masc\", \"masculine\"}:\n",
    "        return \"M\"\n",
    "    if s in {\"f\", \"female\", \"woman\", \"fem\", \"feminine\"}:\n",
    "        return \"F\"\n",
    "    return \"UNK\"\n",
    "\n",
    "val_df[\"sex_norm\"]  = val_df[\"sex\"].apply(normalize_sex)\n",
    "test_df[\"sex_norm\"] = test_df[\"sex\"].apply(normalize_sex)\n",
    "\n",
    "print(\"VAL sex counts (normalized):\",  val_df[\"sex_norm\"].value_counts(dropna=False).to_dict())\n",
    "print(\"TEST sex counts (normalized):\", test_df[\"sex_norm\"].value_counts(dropna=False).to_dict())\n",
    "if (val_df[\"sex_norm\"] == \"UNK\").any() or (test_df[\"sex_norm\"] == \"UNK\").any():\n",
    "    print(\"NOTE: Some 'sex' values could not be normalized to M/F and were counted as 'UNK' for fairness and sex charts.\")\n",
    "\n",
    "# -------------------------\n",
    "# Dataset + collator\n",
    "# Input: df with clip_path, label_num, task_group, sex_norm\n",
    "# Output: padded batches with attention_mask\n",
    "# Notes:\n",
    "# - For vowel clips, attention_mask trims trailing near-silence using TINY_THRESH\n",
    "# - For other clips, attention_mask is all ones\n",
    "# -------------------------\n",
    "class AudioManifestDataset(Dataset):\n",
    "    def __init__(self, df: pd.DataFrame):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        row = self.df.iloc[idx]\n",
    "        clip_path = str(row[\"clip_path\"])\n",
    "        label = int(row[\"label_num\"])\n",
    "        task_group = str(row[\"task_group\"])\n",
    "        sex_norm = str(row[\"sex_norm\"])\n",
    "\n",
    "        if not os.path.exists(clip_path):\n",
    "            raise FileNotFoundError(f\"Missing clip_path: {clip_path}\")\n",
    "\n",
    "        y, sr = sf.read(clip_path, always_2d=False)\n",
    "        if isinstance(y, np.ndarray) and y.ndim > 1:\n",
    "            y = np.mean(y, axis=1)\n",
    "        y = np.asarray(y, dtype=np.float32)\n",
    "\n",
    "        if int(sr) != SR_EXPECTED:\n",
    "            raise ValueError(f\"Sample rate mismatch (expected {SR_EXPECTED}): {clip_path} has sr={sr}\")\n",
    "\n",
    "        # attention_mask is in sample space, then converted inside Wav2Vec2 to feature-mask\n",
    "        attn = np.ones((len(y),), dtype=np.int64)\n",
    "        if task_group == \"vowel\":\n",
    "            # mark trailing near-zero tail as padding so it does not affect pooling\n",
    "            k = -1\n",
    "            for j in range(len(y) - 1, -1, -1):\n",
    "                if abs(float(y[j])) > TINY_THRESH:\n",
    "                    k = j\n",
    "                    break\n",
    "            if k >= 0:\n",
    "                attn[:k+1] = 1\n",
    "                attn[k+1:] = 0\n",
    "            else:\n",
    "                attn[:] = 1\n",
    "        else:\n",
    "            attn[:] = 1\n",
    "\n",
    "        return {\n",
    "            \"input_values\": torch.from_numpy(y),\n",
    "            \"attention_mask\": torch.from_numpy(attn),\n",
    "            \"labels\": torch.tensor(label, dtype=torch.long),\n",
    "            \"task_group\": task_group,\n",
    "            \"sex_norm\": sex_norm,\n",
    "        }\n",
    "\n",
    "def collate_fn(batch):\n",
    "    # Pads to the max length in the batch (zeros for audio and attention)\n",
    "    max_len = int(max(b[\"input_values\"].numel() for b in batch))\n",
    "    input_vals, attn_masks, labels, task_groups, sex_norms = [], [], [], [], []\n",
    "    for b in batch:\n",
    "        x = b[\"input_values\"]\n",
    "        a = b[\"attention_mask\"]\n",
    "        pad = max_len - x.numel()\n",
    "        if pad > 0:\n",
    "            x = torch.cat([x, torch.zeros(pad, dtype=x.dtype)], dim=0)\n",
    "            a = torch.cat([a, torch.zeros(pad, dtype=a.dtype)], dim=0)\n",
    "        input_vals.append(x)\n",
    "        attn_masks.append(a)\n",
    "        labels.append(b[\"labels\"])\n",
    "        task_groups.append(b[\"task_group\"])\n",
    "        sex_norms.append(b[\"sex_norm\"])\n",
    "    return {\n",
    "        \"input_values\": torch.stack(input_vals, dim=0),\n",
    "        \"attention_mask\": torch.stack(attn_masks, dim=0),\n",
    "        \"labels\": torch.stack(labels, dim=0),\n",
    "        \"task_group\": task_groups,\n",
    "        \"sex_norm\": sex_norms,\n",
    "    }\n",
    "\n",
    "# -------------------------\n",
    "# Model: frozen wav2vec2 backbone + two task-specific heads\n",
    "# Output: logits for PD vs Healthy, chosen per item by task_group\n",
    "# -------------------------\n",
    "class Wav2Vec2TwoHeadClassifier(nn.Module):\n",
    "    def __init__(self, ckpt: str, dropout_p: float):\n",
    "        super().__init__()\n",
    "        self.backbone = Wav2Vec2Model.from_pretrained(ckpt, use_safetensors=True)\n",
    "        for p in self.backbone.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "        H = int(self.backbone.config.hidden_size)\n",
    "        self.pre_vowel = nn.Sequential(nn.LayerNorm(H), nn.Dropout(float(dropout_p)))\n",
    "        self.pre_other = nn.Sequential(nn.LayerNorm(H), nn.Dropout(float(dropout_p)))\n",
    "        self.head_vowel = nn.Linear(H, 2)\n",
    "        self.head_other = nn.Linear(H, 2)\n",
    "\n",
    "    def masked_mean_pool(self, last_hidden, attn_mask_samples):\n",
    "        # Converts sample-level mask to feature-level mask, then mean-pools masked features\n",
    "        feat_mask = self.backbone._get_feature_vector_attention_mask(last_hidden.shape[1], attn_mask_samples)\n",
    "        feat_mask = feat_mask.to(last_hidden.device).unsqueeze(-1).type_as(last_hidden)\n",
    "        masked = last_hidden * feat_mask\n",
    "        denom = feat_mask.sum(dim=1).clamp(min=1.0)\n",
    "        return masked.sum(dim=1) / denom\n",
    "\n",
    "    def _heads_fp32(self, x_fp_any: torch.Tensor, head: nn.Module) -> torch.Tensor:\n",
    "        # Keeps heads in float32 even when AMP is enabled\n",
    "        x = x_fp_any.float()\n",
    "        if x.is_cuda:\n",
    "            with torch.amp.autocast(device_type=\"cuda\", enabled=False):\n",
    "                return head(x)\n",
    "        return head(x)\n",
    "\n",
    "    def forward_logits(self, input_values, attention_mask, task_group):\n",
    "        # Backbone is frozen: inference-only for features\n",
    "        with torch.no_grad():\n",
    "            out = self.backbone(input_values=input_values, attention_mask=attention_mask)\n",
    "            last_hidden = out.last_hidden_state\n",
    "\n",
    "        pooled = self.masked_mean_pool(last_hidden, attention_mask)  # [B,H]\n",
    "\n",
    "        z_v = self.pre_vowel(pooled.float())\n",
    "        z_o = self.pre_other(pooled.float())\n",
    "\n",
    "        logits_v = self._heads_fp32(z_v, self.head_vowel)\n",
    "        logits_o = self._heads_fp32(z_o, self.head_other)\n",
    "\n",
    "        # Pick the matching head for each item in the batch\n",
    "        is_vowel = torch.tensor([tg == \"vowel\" for tg in task_group], device=pooled.device, dtype=torch.bool)\n",
    "        logits = logits_o.clone()\n",
    "        if is_vowel.any():\n",
    "            logits[is_vowel] = logits_v[is_vowel]\n",
    "        return logits\n",
    "\n",
    "# -------------------------\n",
    "# Metrics and plots\n",
    "# Output:\n",
    "# - AUROC (threshold-free)\n",
    "# - Threshold metrics (confusion-based) at chosen threshold\n",
    "# - ROC and confusion matrix PNGs\n",
    "# -------------------------\n",
    "def compute_auc(y_true, y_prob):\n",
    "    y_true = np.asarray(y_true, dtype=np.int64)\n",
    "    y_prob = np.asarray(y_prob, dtype=np.float64)\n",
    "    if len(np.unique(y_true)) < 2:\n",
    "        return float(\"nan\")\n",
    "    return float(roc_auc_score(y_true, y_prob))\n",
    "\n",
    "def compute_threshold_metrics(y_true, y_prob, thr=0.5):\n",
    "    y_true = np.asarray(y_true, dtype=np.int64)\n",
    "    y_prob = np.asarray(y_prob, dtype=np.float64)\n",
    "    y_pred = (y_prob >= float(thr)).astype(np.int64)\n",
    "\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=[0, 1])\n",
    "    TN, FP, FN, TP = int(cm[0, 0]), int(cm[0, 1]), int(cm[1, 0]), int(cm[1, 1])\n",
    "\n",
    "    eps = 1e-12\n",
    "    acc = (TP + TN) / max(1, (TP + TN + FP + FN))\n",
    "    prec = TP / (TP + FP + eps)\n",
    "    rec = TP / (TP + FN + eps)     # sensitivity\n",
    "    f1 = 2 * prec * rec / (prec + rec + eps)\n",
    "    spec = TN / (TN + FP + eps)\n",
    "\n",
    "    try:\n",
    "        mcc = float(matthews_corrcoef(y_true, y_pred)) if len(np.unique(y_pred)) > 1 else float(\"nan\")\n",
    "    except Exception:\n",
    "        mcc = float(\"nan\")\n",
    "\n",
    "    try:\n",
    "        _, pval = fisher_exact([[TN, FP], [FN, TP]], alternative=\"two-sided\")\n",
    "        pval = float(pval)\n",
    "    except Exception:\n",
    "        pval = float(\"nan\")\n",
    "\n",
    "    return {\n",
    "        \"threshold\": float(thr),\n",
    "        \"confusion_matrix\": {\"TN\": TN, \"FP\": FP, \"FN\": FN, \"TP\": TP},\n",
    "        \"accuracy\": float(acc),\n",
    "        \"precision\": float(prec),\n",
    "        \"recall\": float(rec),\n",
    "        \"f1_score\": float(f1),\n",
    "        \"sensitivity\": float(rec),\n",
    "        \"specificity\": float(spec),\n",
    "        \"mcc\": float(mcc),\n",
    "        \"p_value_fisher_two_sided\": float(pval),\n",
    "    }\n",
    "\n",
    "def save_roc_curve_png(y_true, y_prob, out_png, title_suffix=\"Test\"):\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_prob)\n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr)\n",
    "    plt.plot([0, 1], [0, 1], linestyle=\"--\")\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(f\"ROC Curve ({title_suffix})\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_png, dpi=150)\n",
    "    plt.close()\n",
    "\n",
    "def save_confusion_png(y_true, y_prob, out_png, thr=0.5, title_suffix=\"Test\"):\n",
    "    y_true = np.asarray(y_true, dtype=np.int64)\n",
    "    y_prob = np.asarray(y_prob, dtype=np.float64)\n",
    "    y_pred = (y_prob >= float(thr)).astype(int)\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=[0, 1])\n",
    "\n",
    "    plt.figure()\n",
    "    plt.imshow(cm)\n",
    "    plt.xticks([0, 1], [\"Healthy(0)\", \"PD(1)\"])\n",
    "    plt.yticks([0, 1], [\"Healthy(0)\", \"PD(1)\"])\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"True\")\n",
    "    plt.title(f\"Confusion Matrix ({title_suffix}, thr={thr:.2f})\")\n",
    "    for (i, j), v in np.ndenumerate(cm):\n",
    "        plt.text(j, i, str(v), ha=\"center\", va=\"center\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_png, dpi=150)\n",
    "    plt.close()\n",
    "\n",
    "# -------------------------\n",
    "# VAL-opt threshold (Youden J): maximize (TPR - FPR) on VAL\n",
    "# Output: thr_opt plus details for logging\n",
    "# -------------------------\n",
    "def compute_val_opt_threshold_youden_j(y_true, y_prob):\n",
    "    \"\"\"\n",
    "    Picks the ROC threshold that maximizes J = TPR - FPR on VAL.\n",
    "    Returns: thr_opt, J_opt, tpr_opt, fpr_opt\n",
    "    \"\"\"\n",
    "    y_true = np.asarray(y_true, dtype=np.int64)\n",
    "    y_prob = np.asarray(y_prob, dtype=np.float64)\n",
    "\n",
    "    if len(np.unique(y_true)) < 2:\n",
    "        return float(\"nan\"), float(\"nan\"), float(\"nan\"), float(\"nan\")\n",
    "\n",
    "    fpr, tpr, thr = roc_curve(y_true, y_prob)\n",
    "    J = tpr - fpr\n",
    "    idx = int(np.argmax(J))\n",
    "    return float(thr[idx]), float(J[idx]), float(tpr[idx]), float(fpr[idx])\n",
    "\n",
    "# -------------------------\n",
    "# Fairness metric (H3): FNR by sex and ΔFNR = FNR(F) - FNR(M)\n",
    "# Notes:\n",
    "# - FNR is computed only on true PD samples (label == 1)\n",
    "# - UNK is tracked but ΔFNR is only defined when both M and F exist\n",
    "# -------------------------\n",
    "def compute_fnr_by_group_signed(y_true, y_prob, groups, thr=0.5):\n",
    "    \"\"\"\n",
    "    FNR = FN/(FN+TP) on true PD samples only.\n",
    "    Returns:\n",
    "    - per-group stats (n_total, n_pos, tp, fn, fnr)\n",
    "    - delta_f_minus_m: FNR(F) - FNR(M) when both defined\n",
    "    - delta_abs_m_f: |FNR(F) - FNR(M)|\n",
    "    \"\"\"\n",
    "    y_true = np.asarray(y_true, dtype=np.int64)\n",
    "    y_prob = np.asarray(y_prob, dtype=np.float64)\n",
    "    groups = np.asarray(list(groups), dtype=object)\n",
    "    y_pred = (y_prob >= float(thr)).astype(np.int64)\n",
    "\n",
    "    out = {}\n",
    "    for g in sorted(set(groups.tolist())):\n",
    "        mask_g = (groups == g)\n",
    "        if int(mask_g.sum()) == 0:\n",
    "            continue\n",
    "\n",
    "        pos_mask = mask_g & (y_true == 1)\n",
    "        n_pos = int(pos_mask.sum())\n",
    "        if n_pos == 0:\n",
    "            out[g] = {\"n_total\": int(mask_g.sum()), \"n_pos\": 0, \"tp\": 0, \"fn\": 0, \"fnr\": float(\"nan\")}\n",
    "            continue\n",
    "\n",
    "        tp = int(((y_pred == 1) & pos_mask).sum())\n",
    "        fn = int(((y_pred == 0) & pos_mask).sum())\n",
    "        fnr = float(fn / max(1, (fn + tp)))\n",
    "\n",
    "        out[g] = {\"n_total\": int(mask_g.sum()), \"n_pos\": int(n_pos), \"tp\": int(tp), \"fn\": int(fn), \"fnr\": float(fnr)}\n",
    "\n",
    "    fnr_m = out.get(\"M\", {}).get(\"fnr\", float(\"nan\"))\n",
    "    fnr_f = out.get(\"F\", {}).get(\"fnr\", float(\"nan\"))\n",
    "\n",
    "    if (not np.isnan(fnr_m)) and (not np.isnan(fnr_f)):\n",
    "        delta_signed = float(fnr_f - fnr_m)   # H3\n",
    "        delta_abs = float(abs(delta_signed))\n",
    "    else:\n",
    "        delta_signed = float(\"nan\")\n",
    "        delta_abs = float(\"nan\")\n",
    "\n",
    "    return out, delta_signed, delta_abs\n",
    "\n",
    "# -------------------------\n",
    "# Confusion counts by group (M/F/UNK), useful for deeper debugging\n",
    "# -------------------------\n",
    "def compute_confusion_counts(y_true, y_prob, thr=0.5):\n",
    "    y_true = np.asarray(y_true, dtype=np.int64)\n",
    "    y_prob = np.asarray(y_prob, dtype=np.float64)\n",
    "    y_pred = (y_prob >= float(thr)).astype(np.int64)\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=[0, 1])\n",
    "    TN, FP, FN, TP = int(cm[0, 0]), int(cm[0, 1]), int(cm[1, 0]), int(cm[1, 1])\n",
    "    return {\"TN\": TN, \"FP\": FP, \"FN\": FN, \"TP\": TP}\n",
    "\n",
    "def compute_confusion_by_group(y_true, y_prob, groups, thr=0.5):\n",
    "    y_true = np.asarray(y_true, dtype=np.int64)\n",
    "    y_prob = np.asarray(y_prob, dtype=np.float64)\n",
    "    groups = np.asarray(list(groups), dtype=object)\n",
    "    out = {}\n",
    "    for g in sorted(set(groups.tolist())):\n",
    "        mask = (groups == g)\n",
    "        if int(mask.sum()) == 0:\n",
    "            continue\n",
    "        out[g] = {\n",
    "            \"n\": int(mask.sum()),\n",
    "            \"confusion\": compute_confusion_counts(y_true[mask], y_prob[mask], thr=thr),\n",
    "        }\n",
    "    return out\n",
    "\n",
    "# -------------------------\n",
    "# Seed control for repeatable inference\n",
    "# -------------------------\n",
    "def set_all_seeds(seed: int):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# -------------------------\n",
    "# Locate the most recent trainval experiment with all 3 best_heads.pt files\n",
    "# Input: DX_OUT_ROOT/trainval_runs/exp_*\n",
    "# Output: chosen_exp (folder path)\n",
    "# -------------------------\n",
    "TRAINVAL_ROOT = Path(DX_OUT_ROOT) / \"trainval_runs\"\n",
    "if not TRAINVAL_ROOT.exists():\n",
    "    raise FileNotFoundError(f\"Missing trainval_runs folder: {str(TRAINVAL_ROOT)}\")\n",
    "\n",
    "exp_dirs = sorted([p for p in TRAINVAL_ROOT.glob(\"exp_*\") if p.is_dir()], key=lambda p: p.stat().st_mtime, reverse=True)\n",
    "if not exp_dirs:\n",
    "    raise FileNotFoundError(f\"No exp_* folders found under: {str(TRAINVAL_ROOT)}\")\n",
    "\n",
    "def _has_all_seeds(exp_path: Path, dataset_id: str, seeds: list):\n",
    "    for s in seeds:\n",
    "        p = exp_path / f\"run_{dataset_id}_seed{s}\" / \"best_heads.pt\"\n",
    "        if not p.exists():\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "chosen_exp = None\n",
    "for ed in exp_dirs:\n",
    "    if _has_all_seeds(ed, dataset_id, SEEDS):\n",
    "        chosen_exp = ed\n",
    "        break\n",
    "\n",
    "if chosen_exp is None:\n",
    "    sample = exp_dirs[0]\n",
    "    raise FileNotFoundError(\n",
    "        \"Could not find a trainval experiment with all 3 best_heads.pt files.\\n\"\n",
    "        f\"Expected under: {str(TRAINVAL_ROOT)}/exp_*/run_{dataset_id}_seedXXXX/best_heads.pt\\n\"\n",
    "        f\"Most recent exp checked: {str(sample)}\"\n",
    "    )\n",
    "\n",
    "print(\"\\nUsing Train+Val experiment folder:\")\n",
    "print(\" \", str(chosen_exp))\n",
    "\n",
    "# -------------------------\n",
    "# Output root for this test run\n",
    "# Output: DX_OUT_ROOT/monolingual_test_runs/...\n",
    "# -------------------------\n",
    "TEST_ROOT = Path(DX_OUT_ROOT) / \"monolingual_test_runs\"\n",
    "TEST_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# -------------------------\n",
    "# Build VAL and TEST loaders and do a small warm-up read\n",
    "# Purpose: catches empty loaders and common I/O issues early\n",
    "# -------------------------\n",
    "val_ds = AudioManifestDataset(val_df)\n",
    "val_loader = DataLoader(\n",
    "    val_ds,\n",
    "    batch_size=PER_DEVICE_BS,\n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=PIN_MEMORY,\n",
    "    collate_fn=collate_fn,\n",
    ")\n",
    "\n",
    "test_ds = AudioManifestDataset(test_df)\n",
    "test_loader = DataLoader(\n",
    "    test_ds,\n",
    "    batch_size=PER_DEVICE_BS,\n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=PIN_MEMORY,\n",
    "    collate_fn=collate_fn,\n",
    ")\n",
    "\n",
    "print(\"\\nWarm-up: loading up to 2 VAL batches and 2 TEST batches...\")\n",
    "\n",
    "def _warmup_loader(loader, name):\n",
    "    t0 = time.time()\n",
    "    num_batches = len(loader)\n",
    "    warmup_batches = min(2, num_batches)\n",
    "    if warmup_batches == 0:\n",
    "        raise RuntimeError(f\"{name} DataLoader has 0 batches. Check df length and PER_DEVICE_BS.\")\n",
    "    it = iter(loader)\n",
    "    for i in range(warmup_batches):\n",
    "        _ = next(it)\n",
    "        print(f\"  loaded {name} warmup batch {i+1}/{warmup_batches}\")\n",
    "    print(f\"{name} warm-up done in {time.time()-t0:.2f}s\")\n",
    "\n",
    "_warmup_loader(val_loader, \"VAL\")\n",
    "_warmup_loader(test_loader, \"TEST\")\n",
    "\n",
    "# -------------------------\n",
    "# Helpers: load heads and run inference\n",
    "# Inputs:\n",
    "# - best_heads.pt (trainval output)\n",
    "# - loader (VAL or TEST)\n",
    "# Outputs:\n",
    "# - arrays: y_true, y_prob (PD probability), sex_norm\n",
    "# -------------------------\n",
    "def load_heads_into_model(model: Wav2Vec2TwoHeadClassifier, best_heads_path: Path):\n",
    "    if not best_heads_path.exists():\n",
    "        raise FileNotFoundError(f\"Missing best_heads.pt: {str(best_heads_path)}\")\n",
    "    state = torch.load(str(best_heads_path), map_location=\"cpu\")\n",
    "    needed = [\"pre_vowel\", \"pre_other\", \"head_vowel\", \"head_other\"]\n",
    "    missing = [k for k in needed if k not in state]\n",
    "    if missing:\n",
    "        raise KeyError(\n",
    "            f\"best_heads.pt missing keys {missing}. Found keys: {list(state.keys())}. \"\n",
    "            \"This test code expects the trainval save format.\"\n",
    "        )\n",
    "    model.pre_vowel.load_state_dict(state[\"pre_vowel\"], strict=True)\n",
    "    model.pre_other.load_state_dict(state[\"pre_other\"], strict=True)\n",
    "    model.head_vowel.load_state_dict(state[\"head_vowel\"], strict=True)\n",
    "    model.head_other.load_state_dict(state[\"head_other\"], strict=True)\n",
    "    return model\n",
    "\n",
    "def run_inference(loader, model, desc):\n",
    "    # Returns PD probability (class 1) for each clip\n",
    "    use_amp = bool(USE_AMP and DEVICE.type == \"cuda\")\n",
    "    all_probs, all_true, all_sex = [], [], []\n",
    "\n",
    "    pbar = tqdm(loader, desc=desc, dynamic_ncols=True)\n",
    "    with torch.inference_mode():\n",
    "        for batch in pbar:\n",
    "            input_values = batch[\"input_values\"].to(DEVICE, non_blocking=False)\n",
    "            attention_mask = batch[\"attention_mask\"].to(DEVICE, non_blocking=False)\n",
    "            labels = batch[\"labels\"].to(DEVICE, non_blocking=False)\n",
    "            task_group = batch[\"task_group\"]\n",
    "            sex_norm = batch[\"sex_norm\"]\n",
    "\n",
    "            with torch.amp.autocast(device_type=\"cuda\", enabled=use_amp):\n",
    "                logits = model.forward_logits(input_values, attention_mask, task_group)\n",
    "\n",
    "            probs = torch.softmax(logits.float(), dim=-1)[:, 1].detach().cpu().numpy().astype(np.float64)\n",
    "            all_probs.extend(probs.tolist())\n",
    "            all_true.extend(labels.detach().cpu().numpy().astype(np.int64).tolist())\n",
    "            all_sex.extend(list(sex_norm))\n",
    "\n",
    "    return np.asarray(all_true, dtype=np.int64), np.asarray(all_probs, dtype=np.float64), np.asarray(all_sex, dtype=object)\n",
    "\n",
    "# -------------------------\n",
    "# One seed run:\n",
    "# 1) VAL inference -> choose threshold (Youden J)\n",
    "# 2) TEST inference -> metrics at that threshold + plots + metrics.json\n",
    "# Output: compact dict used later for aggregation\n",
    "# -------------------------\n",
    "def run_seed(seed: int):\n",
    "    set_all_seeds(seed)\n",
    "\n",
    "    run_dir = TEST_ROOT / f\"run_{dataset_id}_seed{seed}\"\n",
    "    run_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    best_heads_path = chosen_exp / f\"run_{dataset_id}_seed{seed}\" / \"best_heads.pt\"\n",
    "\n",
    "    print(f\"\\n[seed={seed}] Loading model + heads from:\")\n",
    "    print(\" \", str(best_heads_path))\n",
    "\n",
    "    model = Wav2Vec2TwoHeadClassifier(BACKBONE_CKPT, dropout_p=DROPOUT_P).to(DEVICE)\n",
    "    model = load_heads_into_model(model, best_heads_path)\n",
    "    model.eval()\n",
    "\n",
    "    # ----- VAL -> threshold via Youden J\n",
    "    yv, pv, _ = run_inference(val_loader, model, desc=f\"[seed={seed}] VAL (for thr)\")\n",
    "    val_auc = compute_auc(yv, pv)\n",
    "    thr_opt, j_opt, tpr_opt, fpr_opt = compute_val_opt_threshold_youden_j(yv, pv)\n",
    "\n",
    "    # Display format (keep tight + numeric)\n",
    "    print(f\"[seed={seed}] VAL AUROC: {val_auc:.6f}\")\n",
    "    print(f\"[seed={seed}] VAL-opt threshold (Youden J): {thr_opt:.6f}\")\n",
    "\n",
    "    # ----- TEST -> metrics and plots at VAL-picked threshold\n",
    "    yt, pt, st = run_inference(test_loader, model, desc=f\"[seed={seed}] TEST\")\n",
    "\n",
    "    test_auc = compute_auc(yt, pt)  # threshold-free\n",
    "    thr_metrics = compute_threshold_metrics(yt, pt, thr=thr_opt)\n",
    "\n",
    "    fnr_by_sex, delta_f_minus_m, delta_abs = compute_fnr_by_group_signed(\n",
    "        yt, pt, st, thr=thr_opt\n",
    "    )\n",
    "\n",
    "    confusion_by_sex = compute_confusion_by_group(yt, pt, st, thr=thr_opt)\n",
    "\n",
    "    # ----- plots (overall)\n",
    "    roc_png = run_dir / \"roc_curve.png\"\n",
    "    cm_png  = run_dir / \"confusion_matrix.png\"\n",
    "    save_roc_curve_png(yt, pt, str(roc_png), title_suffix=f\"Test (seed={seed})\")\n",
    "    save_confusion_png(yt, pt, str(cm_png), thr=thr_opt, title_suffix=f\"Test (seed={seed})\")\n",
    "\n",
    "    # ----- plots (by sex): separate confusion charts for M and F when present\n",
    "    cm_m_png = None\n",
    "    cm_f_png = None\n",
    "    mask_m = (st == \"M\")\n",
    "    mask_f = (st == \"F\")\n",
    "\n",
    "    if int(mask_m.sum()) > 0:\n",
    "        cm_m_png = run_dir / \"confusion_matrix_M.png\"\n",
    "        save_confusion_png(\n",
    "            yt[mask_m], pt[mask_m], str(cm_m_png),\n",
    "            thr=thr_opt, title_suffix=f\"Test SEX=M (seed={seed})\"\n",
    "        )\n",
    "\n",
    "    if int(mask_f.sum()) > 0:\n",
    "        cm_f_png = run_dir / \"confusion_matrix_F.png\"\n",
    "        save_confusion_png(\n",
    "            yt[mask_f], pt[mask_f], str(cm_f_png),\n",
    "            thr=thr_opt, title_suffix=f\"Test SEX=F (seed={seed})\"\n",
    "        )\n",
    "\n",
    "    # Full per-seed record written to metrics.json\n",
    "    metrics = {\n",
    "        \"dataset\": dataset_id,\n",
    "        \"seed\": int(seed),\n",
    "\n",
    "        \"n_val\": int(len(val_df)),\n",
    "        \"label_counts_val\": val_df[\"label_num\"].value_counts(dropna=False).to_dict(),\n",
    "        \"sex_counts_val_norm\": val_df[\"sex_norm\"].value_counts(dropna=False).to_dict(),\n",
    "\n",
    "        \"n_test\": int(len(test_df)),\n",
    "        \"label_counts_test\": test_df[\"label_num\"].value_counts(dropna=False).to_dict(),\n",
    "        \"sex_counts_test_norm\": test_df[\"sex_norm\"].value_counts(dropna=False).to_dict(),\n",
    "\n",
    "        \"val_auroc\": float(val_auc),\n",
    "        \"val_opt_threshold_method\": \"Youden J (maximize TPR - FPR on VAL ROC curve)\",\n",
    "        \"val_opt_threshold\": float(thr_opt),\n",
    "        \"val_opt_details\": {\n",
    "            \"youden_j\": float(j_opt),\n",
    "            \"tpr_at_opt\": float(tpr_opt),\n",
    "            \"fpr_at_opt\": float(fpr_opt),\n",
    "        },\n",
    "\n",
    "        \"test_auroc\": float(test_auc),\n",
    "\n",
    "        # TEST evaluated at VAL-opt threshold\n",
    "        \"threshold_metrics_test\": thr_metrics,\n",
    "        \"test_threshold_used\": float(thr_opt),\n",
    "\n",
    "        \"fairness_test\": {\n",
    "            \"definition\": \"H3: ΔFNR = FNR(F) - FNR(M), where FNR(sex)=FN/(FN+TP) computed on PD-only true labels at test_threshold_used.\",\n",
    "            \"fnr_by_sex_norm\": fnr_by_sex,\n",
    "            \"delta_fnr_F_minus_M\": float(delta_f_minus_m),\n",
    "            \"delta_fnr_abs\": float(delta_abs),\n",
    "            \"note\": \"If n_PD for a sex is 0, its FNR is NaN and ΔFNR is NaN.\",\n",
    "            \"sex_normalization_note\": \"sex_norm in {M,F,UNK}. Values not mapped to M/F counted as UNK.\",\n",
    "        },\n",
    "\n",
    "        \"confusion_by_sex_norm\": confusion_by_sex,\n",
    "\n",
    "        \"artifacts\": {\n",
    "            \"roc_curve_png\": str(roc_png),\n",
    "            \"confusion_matrix_png\": str(cm_png),\n",
    "            \"confusion_matrix_M_png\": str(cm_m_png) if cm_m_png is not None else None,\n",
    "            \"confusion_matrix_F_png\": str(cm_f_png) if cm_f_png is not None else None,\n",
    "        },\n",
    "\n",
    "        \"dx_out_root\": DX_OUT_ROOT,\n",
    "        \"trainval_experiment_used\": str(chosen_exp),\n",
    "        \"best_heads_path\": str(best_heads_path),\n",
    "        \"backbone_ckpt\": BACKBONE_CKPT,\n",
    "        \"dropout_p\": float(DROPOUT_P),\n",
    "        \"timestamp\": time.strftime(\"%Y%m%d_%H%M%S\"),\n",
    "    }\n",
    "\n",
    "    metrics_path = run_dir / \"metrics.json\"\n",
    "    with open(metrics_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(metrics, f, indent=2)\n",
    "\n",
    "    print(f\"[seed={seed}] DONE | test_AUROC={test_auc:.6f}\")\n",
    "\n",
    "    # Small, readable fairness printout\n",
    "    def _fmt_fnr(g):\n",
    "        d = fnr_by_sex.get(g, None)\n",
    "        if d is None:\n",
    "            return \"n/a\"\n",
    "        return f\"fnr={d['fnr']:.6f} (n_PD={d['n_pos']}, fn={d['fn']}, tp={d['tp']})\"\n",
    "\n",
    "    print(f\"[seed={seed}] TEST metrics @ VAL-opt thr={thr_opt:.6f}\")\n",
    "    print(f\"[seed={seed}] FAIRNESS (H3) @ thr={thr_opt:.6f}:\")\n",
    "    print(\"  M:\", _fmt_fnr(\"M\"))\n",
    "    print(\"  F:\", _fmt_fnr(\"F\"))\n",
    "    if \"UNK\" in fnr_by_sex:\n",
    "        print(\"  UNK:\", _fmt_fnr(\"UNK\"))\n",
    "    print(\"  ΔFNR (F-M):\", f\"{delta_f_minus_m:.6f}\" if not np.isnan(delta_f_minus_m) else \"nan\")\n",
    "    print(\"  |ΔFNR|:\", f\"{delta_abs:.6f}\" if not np.isnan(delta_abs) else \"nan\")\n",
    "\n",
    "    print(f\"[seed={seed}] WROTE:\")\n",
    "    print(\" \", str(metrics_path))\n",
    "    print(\" \", str(roc_png))\n",
    "    print(\" \", str(cm_png))\n",
    "    if cm_m_png is not None:\n",
    "        print(\" \", str(cm_m_png))\n",
    "    if cm_f_png is not None:\n",
    "        print(\" \", str(cm_f_png))\n",
    "\n",
    "    return {\n",
    "        \"seed\": int(seed),\n",
    "        \"val_opt_threshold\": float(thr_opt),\n",
    "        \"test_auroc\": float(test_auc),\n",
    "        \"thr_metrics_test\": thr_metrics,\n",
    "        \"fnr_by_sex_test\": fnr_by_sex,\n",
    "        \"delta_f_minus_m_test\": float(delta_f_minus_m),\n",
    "        \"delta_abs_test\": float(delta_abs),\n",
    "        \"run_dir\": str(run_dir),\n",
    "    }\n",
    "\n",
    "# -------------------------\n",
    "# Run all seeds and aggregate results\n",
    "# Outputs:\n",
    "# - Printed per-seed thresholds and AUROC\n",
    "# - summary_test.json and history_index.jsonl\n",
    "# -------------------------\n",
    "seed_results = []\n",
    "for seed in SEEDS:\n",
    "    seed_results.append(run_seed(seed))\n",
    "\n",
    "# AUROC aggregation (threshold-free)\n",
    "aurocs = [r[\"test_auroc\"] for r in seed_results]\n",
    "t_crit = 4.302652729911275  # df=2, 95% CI\n",
    "n = len(aurocs)\n",
    "mean_auc = float(np.mean(aurocs))\n",
    "std_auc = float(np.std(aurocs, ddof=1)) if n > 1 else 0.0\n",
    "half_width = float(t_crit * (std_auc / math.sqrt(n))) if n > 1 else 0.0\n",
    "ci95 = [float(mean_auc - half_width), float(mean_auc + half_width)]\n",
    "\n",
    "def _mean_sd(vals):\n",
    "    vals = np.asarray(vals, dtype=np.float64)\n",
    "    return float(np.nanmean(vals)), float(np.nanstd(vals, ddof=1)) if np.sum(~np.isnan(vals)) > 1 else 0.0\n",
    "\n",
    "# Threshold aggregation (VAL-opt)\n",
    "thr_vals = [r[\"val_opt_threshold\"] for r in seed_results]\n",
    "thr_mean, thr_sd = _mean_sd(thr_vals)\n",
    "\n",
    "# Threshold metrics on TEST @ per-seed VAL-opt threshold\n",
    "thr_list = [r[\"thr_metrics_test\"] for r in seed_results]\n",
    "keys = [\"accuracy\",\"precision\",\"recall\",\"f1_score\",\"sensitivity\",\"specificity\",\"mcc\",\"p_value_fisher_two_sided\"]\n",
    "\n",
    "agg = {}\n",
    "for k in keys:\n",
    "    v = [float(tm.get(k, float(\"nan\"))) for tm in thr_list]\n",
    "    mu, sd = _mean_sd(v)\n",
    "    agg[k] = {\"mean\": mu, \"sd\": sd, \"values_by_seed\": {str(r[\"seed\"]): float(tm.get(k, float(\"nan\"))) for r, tm in zip(seed_results, thr_list)}}\n",
    "\n",
    "cm_by_seed = {str(r[\"seed\"]): r[\"thr_metrics_test\"][\"confusion_matrix\"] for r in seed_results}\n",
    "\n",
    "# FAIRNESS aggregation (H3) on TEST @ per-seed VAL-opt threshold\n",
    "fnr_by_seed = {str(r[\"seed\"]): r[\"fnr_by_sex_test\"] for r in seed_results}\n",
    "delta_signed_by_seed = {str(r[\"seed\"]): float(r[\"delta_f_minus_m_test\"]) for r in seed_results}\n",
    "delta_abs_by_seed = {str(r[\"seed\"]): float(r[\"delta_abs_test\"]) for r in seed_results}\n",
    "\n",
    "fnr_m_vals, fnr_f_vals, n_pd_m_vals, n_pd_f_vals = [], [], [], []\n",
    "d_signed_vals, d_abs_vals = [], []\n",
    "\n",
    "for r in seed_results:\n",
    "    s = str(r[\"seed\"])\n",
    "    d = fnr_by_seed.get(s, {})\n",
    "    fnr_m_vals.append(float(d.get(\"M\", {}).get(\"fnr\", float(\"nan\"))))\n",
    "    fnr_f_vals.append(float(d.get(\"F\", {}).get(\"fnr\", float(\"nan\"))))\n",
    "    n_pd_m_vals.append(float(d.get(\"M\", {}).get(\"n_pos\", float(\"nan\"))))\n",
    "    n_pd_f_vals.append(float(d.get(\"F\", {}).get(\"n_pos\", float(\"nan\"))))\n",
    "    d_signed_vals.append(float(delta_signed_by_seed.get(s, float(\"nan\"))))\n",
    "    d_abs_vals.append(float(delta_abs_by_seed.get(s, float(\"nan\"))))\n",
    "\n",
    "fnr_m_mean, fnr_m_sd = _mean_sd(fnr_m_vals)\n",
    "fnr_f_mean, fnr_f_sd = _mean_sd(fnr_f_vals)\n",
    "d_signed_mean, d_signed_sd = _mean_sd(d_signed_vals)\n",
    "d_abs_mean, d_abs_sd = _mean_sd(d_abs_vals)\n",
    "\n",
    "print(\"\\nVAL-opt thresholds (Youden J) by seed:\")\n",
    "for r in seed_results:\n",
    "    print(f\"  seed {r['seed']}: {r['val_opt_threshold']:.6f}\")\n",
    "print(f\"  mean ± SD: {thr_mean:.6f} ± {thr_sd:.6f}\")\n",
    "\n",
    "print(\"\\nTest AUROC by seed:\")\n",
    "for r in seed_results:\n",
    "    print(f\"  seed {r['seed']}: {r['test_auroc']:.6f}\")\n",
    "print(f\"\\nMean Test AUROC: {mean_auc:.6f}\")\n",
    "print(f\"95% CI (t, n=3): [{ci95[0]:.6f}, {ci95[1]:.6f}]\")\n",
    "\n",
    "print(\"\\nThreshold metrics on TEST @ VAL-opt threshold (mean ± SD across seeds):\")\n",
    "for k in [\"accuracy\",\"precision\",\"sensitivity\",\"specificity\",\"f1_score\",\"mcc\"]:\n",
    "    mu = agg[k][\"mean\"]\n",
    "    sd = agg[k][\"sd\"]\n",
    "    print(f\"  {k}: {mu:.6f} ± {sd:.6f}\")\n",
    "print(\"  fisher_p_value_two_sided:\", f\"{agg['p_value_fisher_two_sided']['mean']:.6g} ± {agg['p_value_fisher_two_sided']['sd']:.6g}\")\n",
    "\n",
    "print(\"\\nFAIRNESS (H3) on TEST @ VAL-opt threshold (mean ± SD across seeds):\")\n",
    "print(f\"  FNR_M: {fnr_m_mean:.6f} ± {fnr_m_sd:.6f}\")\n",
    "print(f\"  FNR_F: {fnr_f_mean:.6f} ± {fnr_f_sd:.6f}\")\n",
    "print(f\"  ΔFNR (F-M): {d_signed_mean:.6f} ± {d_signed_sd:.6f}\")\n",
    "print(f\"  |ΔFNR|: {d_abs_mean:.6f} ± {d_abs_sd:.6f}\")\n",
    "print(\"  Per-seed:\", {\n",
    "    str(r[\"seed\"]): {\n",
    "        \"val_opt_thr\": float(r[\"val_opt_threshold\"]),\n",
    "        \"FNR_M\": fnr_m_vals[i],\n",
    "        \"n_PD_M\": n_pd_m_vals[i],\n",
    "        \"FNR_F\": fnr_f_vals[i],\n",
    "        \"n_PD_F\": n_pd_f_vals[i],\n",
    "        \"delta_F_minus_M\": d_signed_vals[i],\n",
    "        \"abs_delta\": d_abs_vals[i],\n",
    "    } for i, r in enumerate(seed_results)\n",
    "})\n",
    "\n",
    "summary = {\n",
    "    \"dataset\": dataset_id,\n",
    "    \"dx_out_root\": DX_OUT_ROOT,\n",
    "    \"manifest_all\": MANIFEST_ALL,\n",
    "    \"trainval_experiment_used\": str(chosen_exp),\n",
    "    \"seeds\": SEEDS,\n",
    "\n",
    "    \"n_val\": int(len(val_df)),\n",
    "    \"label_counts_val\": val_df[\"label_num\"].value_counts(dropna=False).to_dict(),\n",
    "    \"sex_counts_val_norm\": val_df[\"sex_norm\"].value_counts(dropna=False).to_dict(),\n",
    "\n",
    "    \"n_test\": int(len(test_df)),\n",
    "    \"label_counts_test\": test_df[\"label_num\"].value_counts(dropna=False).to_dict(),\n",
    "    \"sex_counts_test_norm\": test_df[\"sex_norm\"].value_counts(dropna=False).to_dict(),\n",
    "\n",
    "    \"val_opt_threshold_method\": \"Youden J (maximize TPR - FPR on VAL ROC curve)\",\n",
    "    \"val_opt_threshold_by_seed\": {str(r[\"seed\"]): float(r[\"val_opt_threshold\"]) for r in seed_results},\n",
    "    \"val_opt_threshold_mean_sd\": {\"mean\": float(thr_mean), \"sd\": float(thr_sd)},\n",
    "\n",
    "    \"test_aurocs_by_seed\": {str(r[\"seed\"]): float(r[\"test_auroc\"]) for r in seed_results},\n",
    "    \"mean_test_auroc\": float(mean_auc),\n",
    "    \"t_crit_df2_95\": float(t_crit),\n",
    "    \"half_width_95_ci\": float(half_width),\n",
    "    \"ci95_test_auroc\": ci95,\n",
    "\n",
    "    \"threshold_metrics_test_mean_sd\": agg,\n",
    "    \"confusion_matrix_by_seed\": cm_by_seed,\n",
    "    \"run_dirs\": [r[\"run_dir\"] for r in seed_results],\n",
    "\n",
    "    \"fairness_test\": {\n",
    "        \"definition\": \"H3: ΔFNR = FNR(F) - FNR(M), where FNR(sex)=FN/(FN+TP) computed on PD-only true labels at per-seed VAL-opt threshold used on TEST.\",\n",
    "        \"fnr_by_sex_norm_by_seed\": fnr_by_seed,\n",
    "        \"delta_fnr_F_minus_M_by_seed\": delta_signed_by_seed,\n",
    "        \"delta_fnr_abs_by_seed\": delta_abs_by_seed,\n",
    "        \"fnr_M_mean_sd\": {\"mean\": float(fnr_m_mean), \"sd\": float(fnr_m_sd), \"values_by_seed\": {str(s): float(v) for s, v in zip(SEEDS, fnr_m_vals)}},\n",
    "        \"fnr_F_mean_sd\": {\"mean\": float(fnr_f_mean), \"sd\": float(fnr_f_sd), \"values_by_seed\": {str(s): float(v) for s, v in zip(SEEDS, fnr_f_vals)}},\n",
    "        \"delta_fnr_F_minus_M_mean_sd\": {\"mean\": float(d_signed_mean), \"sd\": float(d_signed_sd), \"values_by_seed\": {str(s): float(v) for s, v in zip(SEEDS, d_signed_vals)}},\n",
    "        \"delta_fnr_abs_mean_sd\": {\"mean\": float(d_abs_mean), \"sd\": float(d_abs_sd), \"values_by_seed\": {str(s): float(v) for s, v in zip(SEEDS, d_abs_vals)}},\n",
    "        \"denominators_PD_by_seed\": {str(s): {\"n_PD_M\": float(n_pd_m_vals[i]), \"n_PD_F\": float(n_pd_f_vals[i])} for i, s in enumerate(SEEDS)},\n",
    "        \"sex_normalization_note\": \"sex_norm in {M,F,UNK}. Values not mapped to M/F counted as UNK. ΔFNR computed only when both M and F have defined FNR.\",\n",
    "    },\n",
    "\n",
    "    \"timestamp\": time.strftime(\"%Y%m%d_%H%M%S\"),\n",
    "}\n",
    "\n",
    "summary_path = TEST_ROOT / \"summary_test.json\"\n",
    "with open(summary_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "history_path = TEST_ROOT / \"history_index.jsonl\"\n",
    "with open(history_path, \"a\", encoding=\"utf-8\") as f:\n",
    "    f.write(json.dumps(summary) + \"\\n\")\n",
    "\n",
    "print(\"\\nWROTE summary:\", str(summary_path))\n",
    "print(\"APPENDED history index:\", str(history_path))\n",
    "print(\"Open this folder to access artifacts:\", str(TEST_ROOT))\n",
    "\n",
    "# -------------------------\n",
    "# Runtime release (stop paid GPU)\n",
    "# -------------------------\n",
    "print(\"\\nAll done. Unassigning the runtime to stop the L4 instance...\")\n",
    "try:\n",
    "    from google.colab import runtime  # type: ignore\n",
    "    print(\"Calling runtime.unassign() now.\")\n",
    "    runtime.unassign()\n",
    "except Exception as e:\n",
    "    print(\"Could not unassign runtime automatically. You can stop the runtime manually in Colab.\")\n",
    "    print(\"Reason:\", repr(e))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 2562,
     "referenced_widgets": [
      "2b384979d05a4e09adf0cf5e431c2d7f",
      "6341e72f750d45df810839083ac33600",
      "42a8fc96c55d4c2cb2a9d6951f6ab716",
      "e7debf36cea043218ed30fcc4b44f295",
      "1dd0e881320247c18a3323126141b31b",
      "c1e68b2039954d53b901e26eab8b998d",
      "1b167ed67ccb4ae2b2c5c1484c59d7fe",
      "4fb6743fe2114f08b1855717634e032c",
      "c1479abca5a346bb90f0893b18b7c3b3",
      "aa7a5d23bb8941e08f14fab5ffd4d554",
      "43783fdb88114ea7a7fb5f0485fbdba5",
      "d793a56d5dbb42019ba1dd53d797f670",
      "667f9b7b4a6a48b689a2856596d80ce7",
      "8a66332fd5ef4cc2ab4076275da90ecf",
      "129581593f9a4978b7609be28a9ce21f",
      "a2fb5423ec6a4d6ab6feaf993ef80818",
      "306e9b809f0d42fbac326f54d5b36f43",
      "0e8d57e9898141b3b55370c9d71ce9e5",
      "208b99b09d4b4291b6c831397caf887b",
      "d5897394e16042509a7373941c016e15",
      "69b64ae76a8544238f1d1b7e4ea7eba4",
      "c14fc42f48c24777970c3dff4601c810",
      "441e1ff6e6224649a584aa2726909e84",
      "c5fe91a9cbc548928e425b763767d5df",
      "0bc140659a504c9299489ddac1234c70",
      "43616fd867f5435895f84aa8f8b8bb5f",
      "8ff3b5923c614a39a09cde3a6f9d53b6",
      "5bcb1c48194345df88ecfe38f6cd266e",
      "c06d0759bb8d44a0a75ee00c0262408c",
      "6c1d58f55362473ba0c7063df7bced56",
      "73927fe416c44794911dce2d071f162d",
      "678c22501a1a49b49925c4612ff61aae",
      "b27c45a2d45f4585b73982a1d9aee241",
      "301b6403d5fc4b17a0df2cc9fe102243",
      "ae2d59007b6c42ec8cf03ae923f00d33",
      "13fa5c63c0bd4aefb928cee4ddce5a72",
      "626b51bed10b45e89fcbebc344c68d05",
      "69c0dde1cdc24a6e9659fd3bee5c0ced",
      "c8df37a6dc29479bb723274e58c14086",
      "96b6a73f6fd342fe892f0f108faaec1b",
      "53525e255a8349c0abedfe891b3a0432",
      "dc6e18acb0784bba99faecefd176d192",
      "9aef60a9c58a4fe9b7d907e4a86486ad",
      "050223b19d844ff18ee1652da95386ed",
      "338e7fa368aa4699a9ef5ac7337db7f1",
      "ae3dc90381154a85bf4cf79fc33075f8",
      "a902f0ff058a43a393d4e5803f5b4a60",
      "c4db6c0bca864003aa7bea9dbf3d7250",
      "58e7a80365274eed876c048411136b85",
      "8a157107ee1743de8c654d9082474154",
      "16353011c64543abb868ac05d3730991",
      "6ba4bec5d596409387a11c897506ba21",
      "381dde413b1745c49980aee1d06cc9d6",
      "6af3c6c40d7f4a198a248403c1f4e252",
      "95ea397d838e48a1b28b2fcdfd9bf47f",
      "a9a107e65ead4fe78caf1d238c04d76e",
      "00bca62f947f495a889c2660daaca322",
      "754dcadf37e343fbb8c2a5f07e316650",
      "4ef3a9b779764200abb14d6c2c710d78",
      "aff55208e56a4617b5b8872066a0a72a",
      "63e62e08eff4402f97da3ffa67c62d72",
      "8104f360eb1e4daa99ea2cd7b155a8ba",
      "0b6de64b3a2c480880a9e5f9b864cf24",
      "a84bd051de864f74b05439d4d0040012",
      "8e00caff2a2c462fac6ee2257169d648",
      "47cd8ed173994337baece7fa7a1e8571",
      "5cd9663e72704278a497fed9ea7ee345",
      "689fa869f75c47a69c32b729da92efdd",
      "0529b22da3f34d66809ef9449a22fb1a",
      "a6c9d34e3d4e4a13a697bdb106ba69bf",
      "cc92188d903b42fd9828bcd74c739243",
      "815ee2bd19f04d30ba802c1ad3a344fc",
      "a80af7ced5a147eaaf306c0a8d5c1207",
      "27e35a3cd46a454aa0060845e570bbfa",
      "fc766ba2effd4035b23a72b1e5ed4fab",
      "12c408bbd8084054a38ab0b52d6c30e5",
      "e48d2a2b12df4adaa37f6e877e3325c8",
      "4e408fb5224644a7999cccd8616b58a1",
      "8057be37694f43c0ba86fb0e0eb62e59",
      "4d97543e1a5e49f28d711e4b306b5dba",
      "0ba10e5616a0478cbd1d9608c1d0c78e",
      "5c1d0bcb08e342d98c19a9a489d22716",
      "17bd8fa9ab2342c7bf87ab5c739e73e5",
      "6cd04f405f15421c938e7f8246b69e12",
      "d549c7b653d44fc9a07ce46797324484",
      "005c81a3a9e645f09a7ae4e4f2ed97b3",
      "733970bf2fa242749c9c2f5f72db8ce0",
      "4a86a3f32f2f4a10a56ccd16909501db",
      "79dda013b9da455aa45d2ec53bd6fc07",
      "4dd1985dce3b414d8cc78673b18e685c",
      "8149401e3a774c29956977b184f4d152",
      "e23cc32150054d87bdab9ab62eb6f189",
      "209fc10c1895467d85ba8804aae3b424",
      "dac7d313bc124b48975db441b4d214ec",
      "18b5687d7a61448fb65d8304f470c95a",
      "011fea04972b4ea8b2af8e19373d91b5",
      "b9677be12c6846408cc6b5d10cbb3af8",
      "a7f5343c5aac4a8aa8334ae70a9ff9b7",
      "5bedf4b464024e3d82bdb7a892766a94",
      "81f9a882e6414f26916422d567d89390",
      "bc33b5738a2441779623b9a377209317",
      "d3119da0293143a2b24a9ab29156b24f",
      "6a44544360a64ad8b31bf98464559c31",
      "c4105f31fe784e8a92ff182fd6c9994f",
      "edb44309ecd041c3a54be153e4c882a0",
      "2a4dae2c3bf74db18fd2e12cc11a9473",
      "d89c49fa02514f5da1d19b49efe35ff2",
      "e8aa7a067e9f4562b809720d4edd5865",
      "ab58ebeb4a354385b50ee09cc85f099e",
      "5a40fa78372d4d22972c7a74d8c6f154"
     ]
    },
    "id": "hjGwh3guCE66"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "The following cell evaluates the D5 (English, MDVR-KCL) model on the test split, using a decision threshold that is chosen only from the validation split. It reads `manifests/manifest_all.csv` from the D5 `preprocessed_v2` folder and keeps only the validation and test rows. It requires that the sex and age columns are present so fairness can be evaluated, and it checks that the dataset is clearly identified as D5 to avoid accidentally using files from another dataset. Before running any model code, it confirms that all referenced audio clip files exist on disk.\n",
    "\n",
    "The cell then sets fixed run settings, including three random seeds, the Wav2Vec2 backbone, a 16 kHz sample rate, batch sizes, dropout, and mixed precision on the GPU. Input data is prepared for inference by assigning each clip to either a vowel group (`task == \"vowl\"`) or an other group. Sex values are normalized to M, F, or UNK for consistent reporting. A dataset and collator load audio from disk, check that the sample rate is correct, pad clips in each batch to a common length, and create an attention mask so padded silence in vowel clips does not affect the model. A short warm up step loads a few batches from each split to catch data loading or file issues early, even when the splits are small.\n",
    "\n",
    "The model structure matches the training setup. It uses a frozen Wav2Vec2 backbone with two small classification heads, one for vowel clips and one for other clips, each preceded by LayerNorm and Dropout. For each seed, 1337, 2024, and 7777, the cell finds the most recent train and validation experiment folder that contains all required `best_heads.pt` files, reloads the saved head weights, and runs inference in two stages.\n",
    "\n",
    "In the validation stage, prediction scores are computed on the validation split and a validation optimal threshold is selected using Youden’s J statistic, which maximizes the difference between true positive rate and false positive rate. In the test stage, this validation chosen threshold is applied to the test split to compute test AUROC, threshold based metrics including the confusion matrix, accuracy, precision, sensitivity, specificity, F1 score, MCC, and Fisher exact test p value, and fairness metrics. Fairness is computed using the H3 definition as the false negative rate for males and females separately on Parkinson’s positive cases only, along with ΔFNR defined as FNR(F) minus FNR(M).\n",
    "\n",
    "For each seed, results are saved under\n",
    "`<DX_OUT_ROOT>/monolingual_test_runs/run_D5_seed<seed>/`.\n",
    "Saved outputs include a ROC curve image, an overall confusion matrix, additional confusion matrices split by M and F when those groups are present in the test data, and a per seed `metrics.json` file containing all settings, counts, thresholds, metrics, fairness results, and paths to the saved artifacts.\n",
    "\n",
    "After all three seeds complete, the cell aggregates results across seeds. It reports the mean test AUROC with a 95% confidence interval using a t distribution with n equal to 3, along with the mean and standard deviation of the validation chosen thresholds, threshold based test metrics, and fairness values. A combined `summary_test.json` file is written, the same summary is appended to `history_index.jsonl`, the output locations are printed, and the Colab runtime is unassigned to stop the GPU instance."
   ],
   "metadata": {
    "id": "-dHDwi3wpM8V"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# =========================\n",
    "# D5 Test Only — Val Optimal Threshold + Fairness\n",
    "# Purpose: Evaluate the D5 model on VAL and TEST, pick a per seed VAL optimal threshold (Youden J),\n",
    "#          then report TEST metrics and fairness at that threshold.\n",
    "# Inputs:  manifest_all.csv (VAL and TEST rows), best_heads.pt from latest trainval exp (3 seeds)\n",
    "# Outputs: Per seed run folder with metrics.json + plots, plus summary_test.json + history_index.jsonl\n",
    "# =========================\n",
    "\n",
    "import os, json, math, random, time, warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import soundfile as sf\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from transformers import Wav2Vec2Model\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, confusion_matrix, matthews_corrcoef\n",
    "from scipy.stats import fisher_exact\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# -------------------------\n",
    "# Safety checks: avoid importing local files named torch/transformers\n",
    "# -------------------------\n",
    "if os.path.exists(\"/content/torch.py\") or os.path.exists(\"/content/torch/__init__.py\"):\n",
    "    raise RuntimeError(\"Found local /content/torch.py or /content/torch/ that shadows PyTorch. Rename/remove it and restart runtime.\")\n",
    "if os.path.exists(\"/content/transformers.py\") or os.path.exists(\"/content/transformers/__init__.py\"):\n",
    "    raise RuntimeError(\"Found local /content/transformers.py or /content/transformers/ that shadows Hugging Face Transformers. Rename/remove it and restart runtime.\")\n",
    "\n",
    "# -------------------------\n",
    "# Drive access\n",
    "# -------------------------\n",
    "if not os.path.isdir(\"/content/drive/MyDrive\"):\n",
    "    from google.colab import drive  # type: ignore\n",
    "    drive.mount(\"/content/drive\")\n",
    "\n",
    "# -------------------------\n",
    "# Paths: output root and the single manifest used for VAL and TEST\n",
    "# -------------------------\n",
    "D5_OUT_ROOT_FALLBACK = \"/content/drive/MyDrive/AI_PD_Project/Datasets/D5-English (MDVR-KCL)/preprocessed_v2\"\n",
    "# DX_OUT_ROOT = globals().get(\"DX_OUT_ROOT\", D5_OUT_ROOT_FALLBACK)\n",
    "# modified to consider the new 50/20/30 splits path\n",
    "DX_OUT_ROOT = \"/content/drive/MyDrive/AI_PD_Project/Datasets/D5-English (MDVR-KCL)/preprocessed_v2\"  # modified to consider the new 50/20/30 splits path\n",
    "MANIFEST_ALL = f\"{DX_OUT_ROOT}/manifests/manifest_all.csv\"\n",
    "\n",
    "# -------------------------\n",
    "# Run settings: must match Train+Val behavior where relevant\n",
    "# -------------------------\n",
    "SEEDS          = [1337, 2024, 7777]\n",
    "BACKBONE_CKPT  = \"facebook/wav2vec2-base\"\n",
    "SR_EXPECTED    = 16000\n",
    "TINY_THRESH    = 1e-4\n",
    "\n",
    "# Batch sizing (used only for inference here; printed for traceability)\n",
    "EFFECTIVE_BS   = 64\n",
    "PER_DEVICE_BS  = 16\n",
    "GRAD_ACCUM     = max(1, EFFECTIVE_BS // PER_DEVICE_BS)  # not used in test, but printed\n",
    "\n",
    "# Must match Train+Val\n",
    "DROPOUT_P      = 0.2\n",
    "\n",
    "# Drive friendly defaults\n",
    "NUM_WORKERS    = 0\n",
    "PIN_MEMORY     = False\n",
    "\n",
    "# AMP speeds up inference on GPU; heads are still computed in FP32\n",
    "USE_AMP        = True\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "try:\n",
    "    torch.set_float32_matmul_precision(\"high\")\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# Keep notebook output cleaner (no change to logic)\n",
    "warnings.filterwarnings(\"ignore\", message=\"None of the inputs have requires_grad=True. Gradients will be None\")\n",
    "warnings.filterwarnings(\"ignore\", message=\"Passing `gradient_checkpointing` to a config initialization is deprecated\")\n",
    "\n",
    "print(\"DX_OUT_ROOT:\", DX_OUT_ROOT)\n",
    "print(\"MANIFEST_ALL:\", MANIFEST_ALL)\n",
    "print(\"DEVICE:\", DEVICE)\n",
    "if DEVICE.type == \"cuda\":\n",
    "    print(\"GPU:\", torch.cuda.get_device_name(0))\n",
    "print(\"PER_DEVICE_BS:\", PER_DEVICE_BS, \"| EFFECTIVE_BS:\", PER_DEVICE_BS * GRAD_ACCUM)\n",
    "print(\"NUM_WORKERS:\", NUM_WORKERS, \"| PIN_MEMORY:\", PIN_MEMORY)\n",
    "print(\"USE_AMP:\", bool(USE_AMP and DEVICE.type == \"cuda\"))\n",
    "\n",
    "# -------------------------\n",
    "# Load manifest, keep VAL and TEST only, and lock to dataset_id (guarded)\n",
    "# -------------------------\n",
    "if not os.path.exists(MANIFEST_ALL):\n",
    "    raise FileNotFoundError(f\"Missing manifest_all.csv: {MANIFEST_ALL}\")\n",
    "\n",
    "m = pd.read_csv(MANIFEST_ALL)\n",
    "\n",
    "# Required columns for labels, task routing, and fairness reporting\n",
    "req_cols = {\"split\", \"clip_path\", \"label_num\", \"task\", \"sex\", \"age\"}\n",
    "missing = [c for c in sorted(req_cols) if c not in m.columns]\n",
    "if missing:\n",
    "    raise ValueError(f\"Manifest missing required columns: {missing}. Found: {list(m.columns)}\")\n",
    "\n",
    "m = m[m[\"split\"].isin([\"val\", \"test\"])].copy()\n",
    "if len(m) == 0:\n",
    "    raise RuntimeError(\"After filtering to split in {'val','test'}, manifest has 0 rows.\")\n",
    "\n",
    "# Prefer the manifest dataset column when present, otherwise treat as generic \"DX\"\n",
    "if \"dataset\" in m.columns and m[\"dataset\"].notna().any():\n",
    "    dataset_id = str(m[\"dataset\"].astype(str).value_counts(dropna=True).idxmax())\n",
    "    m = m[m[\"dataset\"].astype(str) == dataset_id].copy()\n",
    "else:\n",
    "    dataset_id = \"DX\"\n",
    "\n",
    "# Guard: prevents accidental reuse of the wrong DX_OUT_ROOT or manifest\n",
    "if dataset_id != \"D5\":\n",
    "    raise RuntimeError(\n",
    "        f\"Expected dataset_id=='D5' but got {dataset_id!r}. \"\n",
    "        \"This usually means DX_OUT_ROOT was inherited from a previous cell or the manifest is not D5. \"\n",
    "        f\"DX_OUT_ROOT={DX_OUT_ROOT}\"\n",
    "    )\n",
    "\n",
    "# Keep only the columns this cell needs (missing ones are created as NaN)\n",
    "keep_cols = [\"clip_path\", \"label_num\", \"task\", \"speaker_id\", \"sex\", \"age\", \"duration_sec\", \"split\"]\n",
    "for c in keep_cols:\n",
    "    if c not in m.columns:\n",
    "        m[c] = np.nan\n",
    "m = m[keep_cols].copy()\n",
    "\n",
    "val_df  = m[m[\"split\"] == \"val\"].reset_index(drop=True)\n",
    "test_df = m[m[\"split\"] == \"test\"].reset_index(drop=True)\n",
    "\n",
    "print(f\"\\nDataset inferred: {dataset_id}\")\n",
    "print(f\"Val rows:  {len(val_df)}\")\n",
    "print(f\"Test rows: {len(test_df)}\")\n",
    "print(\"Val label counts:\",  val_df[\"label_num\"].value_counts(dropna=False).to_dict())\n",
    "print(\"Test label counts:\", test_df[\"label_num\"].value_counts(dropna=False).to_dict())\n",
    "print(\"Val sex counts (raw):\",  val_df[\"sex\"].value_counts(dropna=False).to_dict())\n",
    "print(\"Test sex counts (raw):\", test_df[\"sex\"].value_counts(dropna=False).to_dict())\n",
    "\n",
    "# VAL is required because the threshold is picked from VAL\n",
    "if len(val_df) == 0:\n",
    "    raise RuntimeError(\"Val split has 0 rows. VAL-opt threshold cannot be computed.\")\n",
    "if len(test_df) == 0:\n",
    "    raise RuntimeError(\"Test split has 0 rows.\")\n",
    "\n",
    "# -------------------------\n",
    "# Quick data integrity: ensure referenced audio clips exist (stop early if not)\n",
    "# -------------------------\n",
    "def _fail_fast_missing_paths(df: pd.DataFrame, name: str):\n",
    "    missing_paths = []\n",
    "    for p in tqdm(df[\"clip_path\"].astype(str).tolist(), desc=f\"Check {name} clip_path exists\", dynamic_ncols=True):\n",
    "        if not os.path.exists(p):\n",
    "            missing_paths.append(p)\n",
    "            if len(missing_paths) >= 10:\n",
    "                break\n",
    "    if missing_paths:\n",
    "        raise FileNotFoundError(f\"{name}: missing clip_path(s). Examples: {missing_paths}\")\n",
    "\n",
    "_fail_fast_missing_paths(val_df, \"VAL\")\n",
    "_fail_fast_missing_paths(test_df, \"TEST\")\n",
    "\n",
    "# -------------------------\n",
    "# Task routing: map each clip into the head it should use\n",
    "# -------------------------\n",
    "def _task_group(task_val) -> str:\n",
    "    return \"vowel\" if str(task_val) == \"vowl\" else \"other\"\n",
    "\n",
    "val_df[\"task_group\"]  = val_df[\"task\"].apply(_task_group)\n",
    "test_df[\"task_group\"] = test_df[\"task\"].apply(_task_group)\n",
    "\n",
    "# -------------------------\n",
    "# Sex normalization: standardize for fairness reporting and sex specific confusion charts\n",
    "# -------------------------\n",
    "def normalize_sex(val) -> str:\n",
    "    \"\"\"\n",
    "    Returns 'M', 'F', or 'UNK'.\n",
    "    Leaves unknown formats as UNK to avoid silent remapping mistakes.\n",
    "    \"\"\"\n",
    "    if pd.isna(val):\n",
    "        return \"UNK\"\n",
    "    s = str(val).strip().lower()\n",
    "    if s in {\"m\", \"male\", \"man\", \"masc\", \"masculine\"}:\n",
    "        return \"M\"\n",
    "    if s in {\"f\", \"female\", \"woman\", \"fem\", \"feminine\"}:\n",
    "        return \"F\"\n",
    "    return \"UNK\"\n",
    "\n",
    "val_df[\"sex_norm\"]  = val_df[\"sex\"].apply(normalize_sex)\n",
    "test_df[\"sex_norm\"] = test_df[\"sex\"].apply(normalize_sex)\n",
    "\n",
    "print(\"Val sex counts (normalized):\",  val_df[\"sex_norm\"].value_counts(dropna=False).to_dict())\n",
    "print(\"Test sex counts (normalized):\", test_df[\"sex_norm\"].value_counts(dropna=False).to_dict())\n",
    "if (val_df[\"sex_norm\"] == \"UNK\").any() or (test_df[\"sex_norm\"] == \"UNK\").any():\n",
    "    print(\"NOTE: Some 'sex' values could not be normalized to M/F and were counted as 'UNK' for fairness and sex charts.\")\n",
    "\n",
    "# -------------------------\n",
    "# Data pipeline: read waveform + build attention mask + pad batches\n",
    "# -------------------------\n",
    "class AudioManifestDataset(Dataset):\n",
    "    def __init__(self, df: pd.DataFrame):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        row = self.df.iloc[idx]\n",
    "        clip_path = str(row[\"clip_path\"])\n",
    "        label = int(row[\"label_num\"])\n",
    "        task_group = str(row[\"task_group\"])\n",
    "        sex_norm = str(row[\"sex_norm\"])\n",
    "\n",
    "        if not os.path.exists(clip_path):\n",
    "            raise FileNotFoundError(f\"Missing clip_path: {clip_path}\")\n",
    "\n",
    "        # Input: audio clip on disk. Output: mono float32 waveform at 16 kHz.\n",
    "        y, sr = sf.read(clip_path, always_2d=False)\n",
    "        if isinstance(y, np.ndarray) and y.ndim > 1:\n",
    "            y = np.mean(y, axis=1)\n",
    "        y = np.asarray(y, dtype=np.float32)\n",
    "\n",
    "        if int(sr) != SR_EXPECTED:\n",
    "            raise ValueError(f\"Sample rate mismatch (expected {SR_EXPECTED}): {clip_path} has sr={sr}\")\n",
    "\n",
    "        # Attention mask: for vowel clips, ignore trailing near zero samples (padding like silence)\n",
    "        attn = np.ones((len(y),), dtype=np.int64)\n",
    "        if task_group == \"vowel\":\n",
    "            k = -1\n",
    "            for j in range(len(y) - 1, -1, -1):\n",
    "                if abs(float(y[j])) > TINY_THRESH:\n",
    "                    k = j\n",
    "                    break\n",
    "            if k >= 0:\n",
    "                attn[:k+1] = 1\n",
    "                attn[k+1:] = 0\n",
    "            else:\n",
    "                attn[:] = 1\n",
    "        else:\n",
    "            attn[:] = 1\n",
    "\n",
    "        return {\n",
    "            \"input_values\": torch.from_numpy(y),\n",
    "            \"attention_mask\": torch.from_numpy(attn),\n",
    "            \"labels\": torch.tensor(label, dtype=torch.long),\n",
    "            \"task_group\": task_group,\n",
    "            \"sex_norm\": sex_norm,\n",
    "        }\n",
    "\n",
    "def collate_fn(batch):\n",
    "    # Pads a batch to the longest waveform length and pads attention mask to match\n",
    "    max_len = int(max(b[\"input_values\"].numel() for b in batch))\n",
    "    input_vals, attn_masks, labels, task_groups, sex_norms = [], [], [], [], []\n",
    "    for b in batch:\n",
    "        x = b[\"input_values\"]\n",
    "        a = b[\"attention_mask\"]\n",
    "        pad = max_len - x.numel()\n",
    "        if pad > 0:\n",
    "            x = torch.cat([x, torch.zeros(pad, dtype=x.dtype)], dim=0)\n",
    "            a = torch.cat([a, torch.zeros(pad, dtype=a.dtype)], dim=0)\n",
    "        input_vals.append(x)\n",
    "        attn_masks.append(a)\n",
    "        labels.append(b[\"labels\"])\n",
    "        task_groups.append(b[\"task_group\"])\n",
    "        sex_norms.append(b[\"sex_norm\"])\n",
    "    return {\n",
    "        \"input_values\": torch.stack(input_vals, dim=0),\n",
    "        \"attention_mask\": torch.stack(attn_masks, dim=0),\n",
    "        \"labels\": torch.stack(labels, dim=0),\n",
    "        \"task_group\": task_groups,\n",
    "        \"sex_norm\": sex_norms,\n",
    "    }\n",
    "\n",
    "# -------------------------\n",
    "# Model: frozen Wav2Vec2 backbone + 2 heads (task routed) like Train+Val\n",
    "# -------------------------\n",
    "class Wav2Vec2TwoHeadClassifier(nn.Module):\n",
    "    def __init__(self, ckpt: str, dropout_p: float):\n",
    "        super().__init__()\n",
    "        # Input: padded waveforms + attention mask. Output: pooled embedding -> head logits.\n",
    "        self.backbone = Wav2Vec2Model.from_pretrained(ckpt, use_safetensors=True)\n",
    "        for p in self.backbone.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "        H = int(self.backbone.config.hidden_size)\n",
    "\n",
    "        self.pre_vowel = nn.Sequential(nn.LayerNorm(H), nn.Dropout(float(dropout_p)))\n",
    "        self.pre_other = nn.Sequential(nn.LayerNorm(H), nn.Dropout(float(dropout_p)))\n",
    "\n",
    "        self.head_vowel = nn.Linear(H, 2)\n",
    "        self.head_other = nn.Linear(H, 2)\n",
    "\n",
    "    def masked_mean_pool(self, last_hidden, attn_mask_samples):\n",
    "        # Converts sample mask -> feature mask (Wav2Vec2 downsampling) and mean pools\n",
    "        feat_mask = self.backbone._get_feature_vector_attention_mask(last_hidden.shape[1], attn_mask_samples)\n",
    "        feat_mask = feat_mask.to(last_hidden.device).unsqueeze(-1).type_as(last_hidden)\n",
    "        masked = last_hidden * feat_mask\n",
    "        denom = feat_mask.sum(dim=1).clamp(min=1.0)\n",
    "        return masked.sum(dim=1) / denom\n",
    "\n",
    "    def _heads_fp32(self, x_fp_any: torch.Tensor, head: nn.Module) -> torch.Tensor:\n",
    "        # Ensures head logits are computed in FP32 even when AMP is enabled\n",
    "        x = x_fp_any.float()\n",
    "        if x.is_cuda:\n",
    "            with torch.amp.autocast(device_type=\"cuda\", enabled=False):\n",
    "                return head(x)\n",
    "        return head(x)\n",
    "\n",
    "    def forward_logits(self, input_values, attention_mask, task_group):\n",
    "        with torch.no_grad():\n",
    "            out = self.backbone(input_values=input_values, attention_mask=attention_mask)\n",
    "            last_hidden = out.last_hidden_state\n",
    "\n",
    "        pooled = self.masked_mean_pool(last_hidden, attention_mask)  # [B,H]\n",
    "\n",
    "        z_v = self.pre_vowel(pooled.float())\n",
    "        z_o = self.pre_other(pooled.float())\n",
    "\n",
    "        logits_v = self._heads_fp32(z_v, self.head_vowel)\n",
    "        logits_o = self._heads_fp32(z_o, self.head_other)\n",
    "\n",
    "        # Route each sample to the correct head based on task_group\n",
    "        is_vowel = torch.tensor([tg == \"vowel\" for tg in task_group], device=pooled.device, dtype=torch.bool)\n",
    "        logits = logits_o.clone()\n",
    "        if is_vowel.any():\n",
    "            logits[is_vowel] = logits_v[is_vowel]\n",
    "        return logits\n",
    "\n",
    "# -------------------------\n",
    "# Metrics + plots: AUROC, threshold metrics, ROC and confusion charts\n",
    "# -------------------------\n",
    "def compute_auc(y_true, y_prob):\n",
    "    # Output: AUROC (NaN if split has only one class)\n",
    "    y_true = np.asarray(y_true, dtype=np.int64)\n",
    "    y_prob = np.asarray(y_prob, dtype=np.float64)\n",
    "    if len(np.unique(y_true)) < 2:\n",
    "        return float(\"nan\")\n",
    "    return float(roc_auc_score(y_true, y_prob))\n",
    "\n",
    "def compute_threshold_metrics(y_true, y_prob, thr=0.5):\n",
    "    # Output: confusion matrix + accuracy/precision/recall/F1/specificity/MCC + Fisher p value\n",
    "    y_true = np.asarray(y_true, dtype=np.int64)\n",
    "    y_prob = np.asarray(y_prob, dtype=np.float64)\n",
    "    y_pred = (y_prob >= float(thr)).astype(np.int64)\n",
    "\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=[0, 1])\n",
    "    TN, FP, FN, TP = int(cm[0, 0]), int(cm[0, 1]), int(cm[1, 0]), int(cm[1, 1])\n",
    "\n",
    "    eps = 1e-12\n",
    "    acc = (TP + TN) / max(1, (TP + TN + FP + FN))\n",
    "    prec = TP / (TP + FP + eps)\n",
    "    rec = TP / (TP + FN + eps)     # sensitivity\n",
    "    f1 = 2 * prec * rec / (prec + rec + eps)\n",
    "    spec = TN / (TN + FP + eps)\n",
    "\n",
    "    try:\n",
    "        mcc = float(matthews_corrcoef(y_true, y_pred)) if len(np.unique(y_pred)) > 1 else float(\"nan\")\n",
    "    except Exception:\n",
    "        mcc = float(\"nan\")\n",
    "\n",
    "    try:\n",
    "        _, pval = fisher_exact([[TN, FP], [FN, TP]], alternative=\"two-sided\")\n",
    "        pval = float(pval)\n",
    "    except Exception:\n",
    "        pval = float(\"nan\")\n",
    "\n",
    "    return {\n",
    "        \"threshold\": float(thr),\n",
    "        \"confusion_matrix\": {\"TN\": TN, \"FP\": FP, \"FN\": FN, \"TP\": TP},\n",
    "        \"accuracy\": float(acc),\n",
    "        \"precision\": float(prec),\n",
    "        \"recall\": float(rec),\n",
    "        \"f1_score\": float(f1),\n",
    "        \"sensitivity\": float(rec),\n",
    "        \"specificity\": float(spec),\n",
    "        \"mcc\": float(mcc),\n",
    "        \"p_value_fisher_two_sided\": float(pval),\n",
    "    }\n",
    "\n",
    "def save_roc_curve_png(y_true, y_prob, out_png, title_suffix=\"Test\"):\n",
    "    # Output: ROC curve PNG (threshold free)\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_prob)\n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr)\n",
    "    plt.plot([0, 1], [0, 1], linestyle=\"--\")\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(f\"ROC Curve ({title_suffix})\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_png, dpi=150)\n",
    "    plt.close()\n",
    "\n",
    "def save_confusion_png(y_true, y_prob, out_png, thr=0.5, title_suffix=\"Test\"):\n",
    "    # Output: confusion matrix PNG at a fixed threshold\n",
    "    y_true = np.asarray(y_true, dtype=np.int64)\n",
    "    y_prob = np.asarray(y_prob, dtype=np.float64)\n",
    "    y_pred = (y_prob >= float(thr)).astype(int)\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=[0, 1])\n",
    "    plt.figure()\n",
    "    plt.imshow(cm)\n",
    "    plt.xticks([0, 1], [\"Healthy(0)\", \"PD(1)\"])\n",
    "    plt.yticks([0, 1], [\"Healthy(0)\", \"PD(1)\"])\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"True\")\n",
    "    plt.title(f\"Confusion Matrix ({title_suffix}, thr={thr:.2f})\")\n",
    "    for (i, j), v in np.ndenumerate(cm):\n",
    "        plt.text(j, i, str(v), ha=\"center\", va=\"center\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_png, dpi=150)\n",
    "    plt.close()\n",
    "\n",
    "# -------------------------\n",
    "# Val optimal threshold (Youden J): pick thr that maximizes TPR - FPR on VAL ROC\n",
    "# -------------------------\n",
    "def compute_val_opt_threshold_youden(y_true, y_prob):\n",
    "    \"\"\"\n",
    "    Output: (thr_opt, j_opt, tpr_opt, fpr_opt)\n",
    "    Fallback: if VAL has one class, returns thr_opt=0.5 and NaNs for others.\n",
    "    \"\"\"\n",
    "    y_true = np.asarray(y_true, dtype=np.int64)\n",
    "    y_prob = np.asarray(y_prob, dtype=np.float64)\n",
    "\n",
    "    if len(np.unique(y_true)) < 2:\n",
    "        return 0.5, float(\"nan\"), float(\"nan\"), float(\"nan\")\n",
    "\n",
    "    fpr, tpr, thr = roc_curve(y_true, y_prob)\n",
    "    thr = np.asarray(thr, dtype=np.float64)\n",
    "    fpr = np.asarray(fpr, dtype=np.float64)\n",
    "    tpr = np.asarray(tpr, dtype=np.float64)\n",
    "\n",
    "    finite = np.isfinite(thr)\n",
    "    if finite.any():\n",
    "        thr_f = thr[finite]\n",
    "        fpr_f = fpr[finite]\n",
    "        tpr_f = tpr[finite]\n",
    "    else:\n",
    "        thr_f, fpr_f, tpr_f = thr, fpr, tpr\n",
    "\n",
    "    j = tpr_f - fpr_f\n",
    "    idx = int(np.nanargmax(j)) if len(j) else 0\n",
    "\n",
    "    thr_opt = float(thr_f[idx]) if len(thr_f) else 0.5\n",
    "    j_opt   = float(j[idx]) if len(j) else float(\"nan\")\n",
    "    tpr_opt = float(tpr_f[idx]) if len(tpr_f) else float(\"nan\")\n",
    "    fpr_opt = float(fpr_f[idx]) if len(fpr_f) else float(\"nan\")\n",
    "\n",
    "    return thr_opt, j_opt, tpr_opt, fpr_opt\n",
    "\n",
    "# -------------------------\n",
    "# Fairness (H3): FNR by sex on PD only, plus ΔFNR = FNR(F) - FNR(M)\n",
    "# -------------------------\n",
    "def compute_fnr_by_group_signed(y_true, y_prob, groups, thr=0.5):\n",
    "    \"\"\"\n",
    "    Output:\n",
    "      - per group: n_total, n_pos (PD), tp, fn, fnr\n",
    "      - delta_f_minus_m: FNR(F) - FNR(M) when both exist\n",
    "      - delta_abs: absolute gap\n",
    "    \"\"\"\n",
    "    y_true = np.asarray(y_true, dtype=np.int64)\n",
    "    y_prob = np.asarray(y_prob, dtype=np.float64)\n",
    "    groups = np.asarray(list(groups), dtype=object)\n",
    "\n",
    "    y_pred = (y_prob >= float(thr)).astype(np.int64)\n",
    "\n",
    "    out = {}\n",
    "    for g in sorted(set(groups.tolist())):\n",
    "        mask_g = (groups == g)\n",
    "        if int(mask_g.sum()) == 0:\n",
    "            continue\n",
    "\n",
    "        pos_mask = mask_g & (y_true == 1)\n",
    "        n_pos = int(pos_mask.sum())\n",
    "\n",
    "        if n_pos == 0:\n",
    "            out[g] = {\"n_total\": int(mask_g.sum()), \"n_pos\": 0, \"tp\": 0, \"fn\": 0, \"fnr\": float(\"nan\")}\n",
    "            continue\n",
    "\n",
    "        tp = int(((y_pred == 1) & pos_mask).sum())\n",
    "        fn = int(((y_pred == 0) & pos_mask).sum())\n",
    "        fnr = float(fn / max(1, (fn + tp)))\n",
    "\n",
    "        out[g] = {\"n_total\": int(mask_g.sum()), \"n_pos\": int(n_pos), \"tp\": int(tp), \"fn\": int(fn), \"fnr\": float(fnr)}\n",
    "\n",
    "    fnr_m = out.get(\"M\", {}).get(\"fnr\", float(\"nan\"))\n",
    "    fnr_f = out.get(\"F\", {}).get(\"fnr\", float(\"nan\"))\n",
    "    if (not np.isnan(fnr_m)) and (not np.isnan(fnr_f)):\n",
    "        delta_signed = float(fnr_f - fnr_m)   # H3\n",
    "        delta_abs = float(abs(delta_signed))\n",
    "    else:\n",
    "        delta_signed = float(\"nan\")\n",
    "        delta_abs = float(\"nan\")\n",
    "\n",
    "    return out, delta_signed, delta_abs\n",
    "\n",
    "# -------------------------\n",
    "# Confusion breakdown by group: full TN/FP/FN/TP for each sex category\n",
    "# -------------------------\n",
    "def compute_confusion_counts(y_true, y_prob, thr=0.5):\n",
    "    y_true = np.asarray(y_true, dtype=np.int64)\n",
    "    y_prob = np.asarray(y_prob, dtype=np.float64)\n",
    "    y_pred = (y_prob >= float(thr)).astype(np.int64)\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=[0, 1])\n",
    "    TN, FP, FN, TP = int(cm[0, 0]), int(cm[0, 1]), int(cm[1, 0]), int(cm[1, 1])\n",
    "    return {\"TN\": TN, \"FP\": FP, \"FN\": FN, \"TP\": TP}\n",
    "\n",
    "def compute_confusion_by_group(y_true, y_prob, groups, thr=0.5):\n",
    "    y_true = np.asarray(y_true, dtype=np.int64)\n",
    "    y_prob = np.asarray(y_prob, dtype=np.float64)\n",
    "    groups = np.asarray(list(groups), dtype=object)\n",
    "    out = {}\n",
    "    for g in sorted(set(groups.tolist())):\n",
    "        mask = (groups == g)\n",
    "        if int(mask.sum()) == 0:\n",
    "            continue\n",
    "        out[g] = {\"n\": int(mask.sum()), \"confusion\": compute_confusion_counts(y_true[mask], y_prob[mask], thr=thr)}\n",
    "    return out\n",
    "\n",
    "# -------------------------\n",
    "# Reproducibility: set all RNG seeds per run\n",
    "# -------------------------\n",
    "def set_all_seeds(seed: int):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# -------------------------\n",
    "# Find the most recent trainval experiment that has best_heads.pt for all 3 seeds\n",
    "# -------------------------\n",
    "TRAINVAL_ROOT = Path(DX_OUT_ROOT) / \"trainval_runs\"\n",
    "if not TRAINVAL_ROOT.exists():\n",
    "    raise FileNotFoundError(f\"Missing trainval_runs folder: {str(TRAINVAL_ROOT)}\")\n",
    "\n",
    "exp_dirs = sorted([p for p in TRAINVAL_ROOT.glob(\"exp_*\") if p.is_dir()], key=lambda p: p.stat().st_mtime, reverse=True)\n",
    "if not exp_dirs:\n",
    "    raise FileNotFoundError(f\"No exp_* folders found under: {str(TRAINVAL_ROOT)}\")\n",
    "\n",
    "def _has_all_seeds(exp_path: Path, dataset_id: str, seeds: list):\n",
    "    for s in seeds:\n",
    "        p = exp_path / f\"run_{dataset_id}_seed{s}\" / \"best_heads.pt\"\n",
    "        if not p.exists():\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "chosen_exp = None\n",
    "for ed in exp_dirs:\n",
    "    if _has_all_seeds(ed, dataset_id, SEEDS):\n",
    "        chosen_exp = ed\n",
    "        break\n",
    "\n",
    "if chosen_exp is None:\n",
    "    sample = exp_dirs[0]\n",
    "    raise FileNotFoundError(\n",
    "        \"Could not find a trainval experiment with all 3 best_heads.pt files.\\n\"\n",
    "        f\"Expected under: {str(TRAINVAL_ROOT)}/exp_*/run_{dataset_id}_seedXXXX/best_heads.pt\\n\"\n",
    "        f\"Most recent exp checked: {str(sample)}\"\n",
    "    )\n",
    "\n",
    "print(\"\\nUsing Train+Val experiment folder:\")\n",
    "print(\" \", str(chosen_exp))\n",
    "\n",
    "# Guard: recheck expected artifacts right after choosing the exp folder\n",
    "for s in SEEDS:\n",
    "    p = chosen_exp / f\"run_{dataset_id}_seed{s}\" / \"best_heads.pt\"\n",
    "    if not p.exists():\n",
    "        raise RuntimeError(f\"Trainval artifact missing after choosing exp. Missing: {str(p)}\")\n",
    "\n",
    "# -------------------------\n",
    "# Output folder for test runs\n",
    "# -------------------------\n",
    "TEST_ROOT = Path(DX_OUT_ROOT) / \"monolingual_test_runs\"\n",
    "TEST_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# -------------------------\n",
    "# Build DataLoaders and warm up a few batches to catch issues early\n",
    "# -------------------------\n",
    "def _num_batches(n_items: int, bs: int) -> int:\n",
    "    if bs <= 0:\n",
    "        return 0\n",
    "    return int((n_items + bs - 1) // bs)\n",
    "\n",
    "val_ds = AudioManifestDataset(val_df)\n",
    "test_ds = AudioManifestDataset(test_df)\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_ds,\n",
    "    batch_size=PER_DEVICE_BS,\n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=PIN_MEMORY,\n",
    "    collate_fn=collate_fn,\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_ds,\n",
    "    batch_size=PER_DEVICE_BS,\n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=PIN_MEMORY,\n",
    "    collate_fn=collate_fn,\n",
    ")\n",
    "\n",
    "n_val_items = int(len(val_ds))\n",
    "n_test_items = int(len(test_ds))\n",
    "n_val_batches = _num_batches(n_val_items, int(PER_DEVICE_BS))\n",
    "n_test_batches = _num_batches(n_test_items, int(PER_DEVICE_BS))\n",
    "\n",
    "print(f\"\\nVAL  items: {n_val_items}  | batch_size: {PER_DEVICE_BS} | num_batches: {n_val_batches}\")\n",
    "print(f\"TEST items: {n_test_items} | batch_size: {PER_DEVICE_BS} | num_batches: {n_test_batches}\")\n",
    "\n",
    "def _warmup(loader, name: str):\n",
    "    # Loads up to 3 batches to verify reading, padding, and shapes\n",
    "    n_items = len(loader.dataset)\n",
    "    n_batches = _num_batches(int(n_items), int(PER_DEVICE_BS))\n",
    "    n_warm = int(min(3, n_batches))\n",
    "    print(f\"\\nWarm-up ({name}): loading {n_warm} batch(es)...\")\n",
    "    t0 = time.time()\n",
    "    if n_warm == 0:\n",
    "        raise RuntimeError(f\"{name} DataLoader has 0 batches. Check that {name.lower()}_df is non-empty and paths exist.\")\n",
    "    it = iter(loader)\n",
    "    for i in range(n_warm):\n",
    "        _ = next(it)\n",
    "        print(f\"  loaded warmup batch {i+1}/{n_warm}\")\n",
    "    print(f\"Warm-up ({name}) done in {time.time()-t0:.2f}s\")\n",
    "\n",
    "_warmup(val_loader, \"VAL\")\n",
    "_warmup(test_loader, \"TEST\")\n",
    "\n",
    "# -------------------------\n",
    "# Load only the saved heads into a fresh model (backbone stays frozen)\n",
    "# -------------------------\n",
    "def load_heads_into_model(model: Wav2Vec2TwoHeadClassifier, best_heads_path: Path):\n",
    "    if not best_heads_path.exists():\n",
    "        raise FileNotFoundError(f\"Missing best_heads.pt: {str(best_heads_path)}\")\n",
    "    state = torch.load(str(best_heads_path), map_location=\"cpu\")\n",
    "\n",
    "    needed = [\"pre_vowel\", \"pre_other\", \"head_vowel\", \"head_other\"]\n",
    "    missing = [k for k in needed if k not in state]\n",
    "    if missing:\n",
    "        raise KeyError(\n",
    "            f\"best_heads.pt missing keys {missing}. Found keys: {list(state.keys())}. \"\n",
    "            \"This test code expects the D5 trainval save format.\"\n",
    "        )\n",
    "\n",
    "    model.pre_vowel.load_state_dict(state[\"pre_vowel\"], strict=True)\n",
    "    model.pre_other.load_state_dict(state[\"pre_other\"], strict=True)\n",
    "    model.head_vowel.load_state_dict(state[\"head_vowel\"], strict=True)\n",
    "    model.head_other.load_state_dict(state[\"head_other\"], strict=True)\n",
    "    return model\n",
    "\n",
    "# -------------------------\n",
    "# Inference helper: run a loader and return labels, PD probabilities, and sex labels\n",
    "# -------------------------\n",
    "def infer_probs(model, loader, seed: int, split_name: str):\n",
    "    use_amp = bool(USE_AMP and DEVICE.type == \"cuda\")\n",
    "    all_probs, all_true, all_sex = [], [], []\n",
    "\n",
    "    pbar = tqdm(loader, desc=f\"[seed={seed}] {split_name}\", dynamic_ncols=True)\n",
    "    with torch.inference_mode():\n",
    "        for batch in pbar:\n",
    "            input_values = batch[\"input_values\"].to(DEVICE, non_blocking=False)\n",
    "            attention_mask = batch[\"attention_mask\"].to(DEVICE, non_blocking=False)\n",
    "            labels = batch[\"labels\"].to(DEVICE, non_blocking=False)\n",
    "            task_group = batch[\"task_group\"]\n",
    "            sex_norm = batch[\"sex_norm\"]\n",
    "\n",
    "            with torch.amp.autocast(device_type=\"cuda\", enabled=use_amp):\n",
    "                logits = model.forward_logits(input_values, attention_mask, task_group)\n",
    "\n",
    "            # Output probability: PD class probability (class index 1)\n",
    "            probs = torch.softmax(logits.float(), dim=-1)[:, 1].detach().cpu().numpy().astype(np.float64)\n",
    "            all_probs.extend(probs.tolist())\n",
    "            all_true.extend(labels.detach().cpu().numpy().astype(np.int64).tolist())\n",
    "            all_sex.extend(list(sex_norm))\n",
    "\n",
    "    return np.asarray(all_true, dtype=np.int64), np.asarray(all_probs, dtype=np.float64), np.asarray(all_sex, dtype=object)\n",
    "\n",
    "# -------------------------\n",
    "# Single seed evaluation:\n",
    "# 1) infer on VAL -> choose threshold\n",
    "# 2) infer on TEST -> report metrics and fairness at that threshold\n",
    "# 3) write metrics.json and plots under run_<DATASET>_seedXXXX\n",
    "# -------------------------\n",
    "def run_test_once(seed: int):\n",
    "    set_all_seeds(seed)\n",
    "\n",
    "    run_dir = TEST_ROOT / f\"run_{dataset_id}_seed{seed}\"\n",
    "    run_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    best_heads_path = chosen_exp / f\"run_{dataset_id}_seed{seed}\" / \"best_heads.pt\"\n",
    "\n",
    "    print(f\"\\n[seed={seed}] Loading model + heads from:\")\n",
    "    print(\" \", str(best_heads_path))\n",
    "\n",
    "    model = Wav2Vec2TwoHeadClassifier(BACKBONE_CKPT, dropout_p=DROPOUT_P).to(DEVICE)\n",
    "    model = load_heads_into_model(model, best_heads_path)\n",
    "    model.eval()\n",
    "\n",
    "    # VAL: used only to pick a threshold (Youden J)\n",
    "    y_val, p_val, _sex_val = infer_probs(model, val_loader, seed=seed, split_name=\"VAL\")\n",
    "    val_auc = compute_auc(y_val, p_val)\n",
    "\n",
    "    val_thr_opt, val_j_opt, val_tpr_opt, val_fpr_opt = compute_val_opt_threshold_youden(y_val, p_val)\n",
    "\n",
    "    print(f\"[seed={seed}] VAL-opt threshold (Youden J): thr={val_thr_opt:.6f} | J={val_j_opt:.6f} | TPR={val_tpr_opt:.6f} | FPR={val_fpr_opt:.6f}\")\n",
    "\n",
    "    # TEST: final reporting split\n",
    "    y_test, p_test, sex_test = infer_probs(model, test_loader, seed=seed, split_name=\"TEST\")\n",
    "\n",
    "    test_auc = compute_auc(y_test, p_test)\n",
    "    thr_metrics = compute_threshold_metrics(y_test, p_test, thr=val_thr_opt)\n",
    "\n",
    "    # Fairness on TEST using PD only denominators\n",
    "    fnr_by_sex, delta_f_minus_m, delta_abs = compute_fnr_by_group_signed(\n",
    "        y_test, p_test, sex_test, thr=val_thr_opt\n",
    "    )\n",
    "\n",
    "    # Confusion matrices per sex on TEST (all labels)\n",
    "    confusion_by_sex = compute_confusion_by_group(y_test, p_test, sex_test, thr=val_thr_opt)\n",
    "\n",
    "    # Plots (overall + sex specific confusion)\n",
    "    roc_png = run_dir / \"roc_curve.png\"\n",
    "    cm_png  = run_dir / \"confusion_matrix.png\"\n",
    "    save_roc_curve_png(y_test, p_test, str(roc_png), title_suffix=f\"Test (seed={seed})\")\n",
    "    save_confusion_png(y_test, p_test, str(cm_png), thr=val_thr_opt, title_suffix=f\"Test (seed={seed})\")\n",
    "\n",
    "    cm_m_png = None\n",
    "    cm_f_png = None\n",
    "    mask_m = (sex_test == \"M\")\n",
    "    mask_f = (sex_test == \"F\")\n",
    "\n",
    "    if int(mask_m.sum()) > 0:\n",
    "        cm_m_png = run_dir / \"confusion_matrix_M.png\"\n",
    "        save_confusion_png(\n",
    "            y_test[mask_m], p_test[mask_m], str(cm_m_png),\n",
    "            thr=val_thr_opt, title_suffix=f\"Test SEX=M (seed={seed})\"\n",
    "        )\n",
    "\n",
    "    if int(mask_f.sum()) > 0:\n",
    "        cm_f_png = run_dir / \"confusion_matrix_F.png\"\n",
    "        save_confusion_png(\n",
    "            y_test[mask_f], p_test[mask_f], str(cm_f_png),\n",
    "            thr=val_thr_opt, title_suffix=f\"Test SEX=F (seed={seed})\"\n",
    "        )\n",
    "\n",
    "    # Save: per seed JSON metrics + artifact paths\n",
    "    metrics = {\n",
    "        \"dataset\": dataset_id,\n",
    "        \"seed\": int(seed),\n",
    "\n",
    "        \"n_val\": int(len(val_df)),\n",
    "        \"label_counts_val\": val_df[\"label_num\"].value_counts(dropna=False).to_dict(),\n",
    "        \"sex_counts_val_norm\": val_df[\"sex_norm\"].value_counts(dropna=False).to_dict(),\n",
    "\n",
    "        \"n_test\": int(len(test_df)),\n",
    "        \"label_counts_test\": test_df[\"label_num\"].value_counts(dropna=False).to_dict(),\n",
    "        \"sex_counts_test_norm\": test_df[\"sex_norm\"].value_counts(dropna=False).to_dict(),\n",
    "\n",
    "        \"val_auroc\": float(val_auc),\n",
    "        \"val_opt_threshold\": {\n",
    "            \"method\": \"Youden J on VAL (maximize TPR - FPR)\",\n",
    "            \"threshold\": float(val_thr_opt),\n",
    "            \"youden_j\": float(val_j_opt),\n",
    "            \"tpr_at_opt\": float(val_tpr_opt),\n",
    "            \"fpr_at_opt\": float(val_fpr_opt),\n",
    "            \"note\": \"If VAL has only one class, threshold defaults to 0.5 and J/TPR/FPR are NaN.\",\n",
    "        },\n",
    "\n",
    "        \"test_auroc\": float(test_auc),\n",
    "        \"threshold_metrics_test_at_val_opt\": thr_metrics,\n",
    "\n",
    "        \"fairness_test_at_val_opt\": {\n",
    "            \"definition\": \"H3: ΔFNR = FNR(F) - FNR(M), where FNR(sex)=FN/(FN+TP) computed on PD-only true labels at VAL-opt threshold.\",\n",
    "            \"fnr_by_sex_norm\": fnr_by_sex,\n",
    "            \"delta_fnr_F_minus_M\": float(delta_f_minus_m),\n",
    "            \"delta_fnr_abs\": float(delta_abs),\n",
    "            \"sex_normalization_note\": \"sex_norm in {M,F,UNK}. Values not mapped to M/F counted as UNK. ΔFNR computed only when both M and F have defined FNR.\",\n",
    "        },\n",
    "\n",
    "        \"confusion_by_sex_norm_at_val_opt\": confusion_by_sex,\n",
    "\n",
    "        \"artifacts\": {\n",
    "            \"roc_curve_png\": str(roc_png),\n",
    "            \"confusion_matrix_png\": str(cm_png),\n",
    "            \"confusion_matrix_M_png\": (str(cm_m_png) if cm_m_png is not None else None),\n",
    "            \"confusion_matrix_F_png\": (str(cm_f_png) if cm_f_png is not None else None),\n",
    "        },\n",
    "\n",
    "        \"dx_out_root\": DX_OUT_ROOT,\n",
    "        \"trainval_experiment_used\": str(chosen_exp),\n",
    "        \"best_heads_path\": str(best_heads_path),\n",
    "        \"backbone_ckpt\": BACKBONE_CKPT,\n",
    "        \"dropout_p\": float(DROPOUT_P),\n",
    "\n",
    "        \"batching\": {\n",
    "            \"per_device_bs\": int(PER_DEVICE_BS),\n",
    "            \"effective_bs\": int(PER_DEVICE_BS * GRAD_ACCUM),\n",
    "            \"num_val_batches\": int(n_val_batches),\n",
    "            \"num_test_batches\": int(n_test_batches),\n",
    "            \"n_val_items\": int(n_val_items),\n",
    "            \"n_test_items\": int(n_test_items),\n",
    "        },\n",
    "        \"amp\": bool(USE_AMP and DEVICE.type == \"cuda\"),\n",
    "        \"device\": str(DEVICE),\n",
    "        \"gpu\": (torch.cuda.get_device_name(0) if DEVICE.type == \"cuda\" else \"CPU\"),\n",
    "        \"timestamp\": time.strftime(\"%Y%m%d_%H%M%S\"),\n",
    "    }\n",
    "\n",
    "    metrics_path = run_dir / \"metrics.json\"\n",
    "    with open(metrics_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(metrics, f, indent=2)\n",
    "\n",
    "    # Console summary (kept as existing prints)\n",
    "    def _fmt_fnr(g):\n",
    "        d = fnr_by_sex.get(g, None)\n",
    "        if d is None:\n",
    "            return \"n/a\"\n",
    "        return f\"fnr={d['fnr']:.6f} (n_PD={d['n_pos']}, fn={d['fn']}, tp={d['tp']})\"\n",
    "\n",
    "    print(f\"[seed={seed}] DONE | val_AUROC={val_auc:.6f} | test_AUROC={test_auc:.6f}\")\n",
    "    print(f\"[seed={seed}] TEST METRICS @ VAL-OPT thr={val_thr_opt:.6f} written to metrics.json\")\n",
    "    print(f\"[seed={seed}] FAIRNESS (H3) @ VAL-OPT thr={val_thr_opt:.6f}:\")\n",
    "    print(\"  M:\", _fmt_fnr(\"M\"))\n",
    "    print(\"  F:\", _fmt_fnr(\"F\"))\n",
    "    if \"UNK\" in fnr_by_sex:\n",
    "        print(\"  UNK:\", _fmt_fnr(\"UNK\"))\n",
    "    print(\"  ΔFNR (F-M):\", f\"{delta_f_minus_m:.6f}\" if not np.isnan(delta_f_minus_m) else \"nan\")\n",
    "    print(\"  |ΔFNR|:\", f\"{delta_abs:.6f}\" if not np.isnan(delta_abs) else \"nan\")\n",
    "\n",
    "    print(f\"[seed={seed}] WROTE:\")\n",
    "    print(\" \", str(metrics_path))\n",
    "    print(\" \", str(roc_png))\n",
    "    print(\" \", str(cm_png))\n",
    "    if cm_m_png is not None:\n",
    "        print(\" \", str(cm_m_png))\n",
    "    if cm_f_png is not None:\n",
    "        print(\" \", str(cm_f_png))\n",
    "\n",
    "    return {\n",
    "        \"seed\": int(seed),\n",
    "        \"val_auc\": float(val_auc),\n",
    "        \"val_opt_thr\": float(val_thr_opt),\n",
    "        \"val_youj\": float(val_j_opt),\n",
    "        \"test_auc\": float(test_auc),\n",
    "        \"thr_metrics_test\": thr_metrics,\n",
    "        \"fnr_by_sex\": fnr_by_sex,\n",
    "        \"delta_signed\": float(delta_f_minus_m),\n",
    "        \"delta_abs\": float(delta_abs),\n",
    "        \"confusion_by_sex\": confusion_by_sex,\n",
    "        \"run_dir\": str(run_dir),\n",
    "    }\n",
    "\n",
    "# -------------------------\n",
    "# Run all seeds and summarize:\n",
    "# - mean TEST AUROC with 95% CI\n",
    "# - mean ± SD threshold metrics on TEST\n",
    "# - mean ± SD fairness gaps on TEST\n",
    "# -------------------------\n",
    "results = []\n",
    "for seed in SEEDS:\n",
    "    results.append(run_test_once(seed))\n",
    "\n",
    "test_aurocs = [r[\"test_auc\"] for r in results]\n",
    "\n",
    "t_crit = 4.302652729911275  # df=2, 95% CI\n",
    "n = len(test_aurocs)\n",
    "mean_auc = float(np.mean(test_aurocs))\n",
    "std_auc = float(np.std(test_aurocs, ddof=1)) if n > 1 else 0.0\n",
    "half_width = float(t_crit * (std_auc / math.sqrt(n))) if n > 1 else 0.0\n",
    "ci95 = [float(mean_auc - half_width), float(mean_auc + half_width)]\n",
    "\n",
    "def _mean_sd_nan(vals):\n",
    "    # Aggregates across seeds while safely ignoring NaNs\n",
    "    vals = np.asarray(vals, dtype=np.float64)\n",
    "    return float(np.nanmean(vals)), float(np.nanstd(vals, ddof=1)) if np.sum(~np.isnan(vals)) > 1 else 0.0\n",
    "\n",
    "# Aggregate threshold metrics (TEST @ VAL-opt thr)\n",
    "keys = [\"accuracy\",\"precision\",\"recall\",\"f1_score\",\"sensitivity\",\"specificity\",\"mcc\",\"p_value_fisher_two_sided\"]\n",
    "agg = {}\n",
    "for k in keys:\n",
    "    v = [float(r[\"thr_metrics_test\"].get(k, float(\"nan\"))) for r in results]\n",
    "    mu, sd = _mean_sd_nan(v)\n",
    "    agg[k] = {\n",
    "        \"mean\": mu,\n",
    "        \"sd\": sd,\n",
    "        \"values_by_seed\": {str(r[\"seed\"]): float(r[\"thr_metrics_test\"].get(k, float(\"nan\"))) for r in results},\n",
    "    }\n",
    "\n",
    "cm_by_seed = {str(r[\"seed\"]): r[\"thr_metrics_test\"][\"confusion_matrix\"] for r in results}\n",
    "\n",
    "# Aggregate thresholds picked on VAL (informational)\n",
    "val_thr_vals = [float(r[\"val_opt_thr\"]) for r in results]\n",
    "val_thr_mean, val_thr_sd = _mean_sd_nan(val_thr_vals)\n",
    "\n",
    "# Aggregate fairness (TEST @ VAL-opt thr)\n",
    "fnr_m_vals, fnr_f_vals, n_pd_m_vals, n_pd_f_vals = [], [], [], []\n",
    "d_signed_vals, d_abs_vals = [], []\n",
    "\n",
    "for r in results:\n",
    "    d = r[\"fnr_by_sex\"]\n",
    "    fnr_m_vals.append(float(d.get(\"M\", {}).get(\"fnr\", float(\"nan\"))))\n",
    "    fnr_f_vals.append(float(d.get(\"F\", {}).get(\"fnr\", float(\"nan\"))))\n",
    "    n_pd_m_vals.append(float(d.get(\"M\", {}).get(\"n_pos\", float(\"nan\"))))\n",
    "    n_pd_f_vals.append(float(d.get(\"F\", {}).get(\"n_pos\", float(\"nan\"))))\n",
    "    d_signed_vals.append(float(r[\"delta_signed\"]))\n",
    "    d_abs_vals.append(float(r[\"delta_abs\"]))\n",
    "\n",
    "fnr_m_mean, fnr_m_sd = _mean_sd_nan(fnr_m_vals)\n",
    "fnr_f_mean, fnr_f_sd = _mean_sd_nan(fnr_f_vals)\n",
    "d_signed_mean, d_signed_sd = _mean_sd_nan(d_signed_vals)\n",
    "d_abs_mean, d_abs_sd = _mean_sd_nan(d_abs_vals)\n",
    "\n",
    "print(\"\\nTest AUROC by seed:\")\n",
    "for r in results:\n",
    "    print(f\"  seed {r['seed']}: {r['test_auc']:.6f}\")\n",
    "print(f\"\\nMean Test AUROC: {mean_auc:.6f}\")\n",
    "print(f\"95% CI (t, n=3): [{ci95[0]:.6f}, {ci95[1]:.6f}]\")\n",
    "\n",
    "print(\"\\nVAL-opt thresholds (Youden J) by seed:\")\n",
    "for r in results:\n",
    "    print(f\"  seed {r['seed']}: thr={r['val_opt_thr']:.6f}\")\n",
    "print(f\"  mean ± SD: {val_thr_mean:.6f} ± {val_thr_sd:.6f}\")\n",
    "\n",
    "print(\"\\nThreshold metrics on TEST @ VAL-OPT threshold (mean ± SD across seeds):\")\n",
    "for k in [\"accuracy\",\"precision\",\"sensitivity\",\"specificity\",\"f1_score\",\"mcc\"]:\n",
    "    mu = agg[k][\"mean\"]\n",
    "    sd = agg[k][\"sd\"]\n",
    "    print(f\"  {k}: {mu:.6f} ± {sd:.6f}\")\n",
    "print(\"  fisher_p_value_two_sided:\", f\"{agg['p_value_fisher_two_sided']['mean']:.6g} ± {agg['p_value_fisher_two_sided']['sd']:.6g}\")\n",
    "\n",
    "print(\"\\nFAIRNESS (H3) on TEST @ VAL-OPT threshold (mean ± SD):\")\n",
    "print(f\"  FNR_M: {fnr_m_mean:.6f} ± {fnr_m_sd:.6f}\")\n",
    "print(f\"  FNR_F: {fnr_f_mean:.6f} ± {fnr_f_sd:.6f}\")\n",
    "print(f\"  ΔFNR (F-M): {d_signed_mean:.6f} ± {d_signed_sd:.6f}\")\n",
    "print(f\"  |ΔFNR|: {d_abs_mean:.6f} ± {d_abs_sd:.6f}\")\n",
    "print(\"  Per-seed:\", {\n",
    "    str(r[\"seed\"]): {\n",
    "        \"val_opt_thr\": r[\"val_opt_thr\"],\n",
    "        \"FNR_M\": fnr_m_vals[i],\n",
    "        \"n_PD_M\": n_pd_m_vals[i],\n",
    "        \"FNR_F\": fnr_f_vals[i],\n",
    "        \"n_PD_F\": n_pd_f_vals[i],\n",
    "        \"delta_F_minus_M\": d_signed_vals[i],\n",
    "        \"abs_delta\": d_abs_vals[i],\n",
    "    } for i, r in enumerate(results)\n",
    "})\n",
    "\n",
    "# Summary JSON: single place to read all aggregated results + pointers to per-seed runs\n",
    "summary = {\n",
    "    \"dataset\": dataset_id,\n",
    "    \"dx_out_root\": DX_OUT_ROOT,\n",
    "    \"manifest_all\": MANIFEST_ALL,\n",
    "    \"trainval_experiment_used\": str(chosen_exp),\n",
    "    \"seeds\": SEEDS,\n",
    "\n",
    "    \"n_val\": int(len(val_df)),\n",
    "    \"label_counts_val\": val_df[\"label_num\"].value_counts(dropna=False).to_dict(),\n",
    "    \"sex_counts_val_norm\": val_df[\"sex_norm\"].value_counts(dropna=False).to_dict(),\n",
    "\n",
    "    \"n_test\": int(len(test_df)),\n",
    "    \"label_counts_test\": test_df[\"label_num\"].value_counts(dropna=False).to_dict(),\n",
    "    \"sex_counts_test_norm\": test_df[\"sex_norm\"].value_counts(dropna=False).to_dict(),\n",
    "\n",
    "    \"val_opt_threshold_by_seed\": {str(r[\"seed\"]): float(r[\"val_opt_thr\"]) for r in results},\n",
    "    \"val_opt_threshold_mean_sd\": {\"mean\": float(val_thr_mean), \"sd\": float(val_thr_sd)},\n",
    "    \"val_you_dens_j_by_seed\": {str(r[\"seed\"]): float(r[\"val_youj\"]) for r in results},\n",
    "\n",
    "    \"test_aurocs_by_seed\": {str(r[\"seed\"]): float(r[\"test_auc\"]) for r in results},\n",
    "    \"mean_test_auroc\": float(mean_auc),\n",
    "    \"t_crit_df2_95\": float(t_crit),\n",
    "    \"half_width_95_ci\": float(half_width),\n",
    "    \"ci95_test_auroc\": ci95,\n",
    "\n",
    "    \"threshold_metrics_test_at_val_opt_mean_sd\": agg,\n",
    "    \"confusion_matrix_test_at_val_opt_by_seed\": cm_by_seed,\n",
    "    \"run_dirs\": [r[\"run_dir\"] for r in results],\n",
    "\n",
    "    \"fairness_test_at_val_opt\": {\n",
    "        \"definition\": \"H3: ΔFNR = FNR(F) - FNR(M), where FNR(sex)=FN/(FN+TP) computed on PD-only true labels at VAL-opt threshold.\",\n",
    "        \"fnr_by_sex_norm_by_seed\": {str(r[\"seed\"]): r[\"fnr_by_sex\"] for r in results},\n",
    "        \"delta_fnr_F_minus_M_by_seed\": {str(r[\"seed\"]): float(r[\"delta_signed\"]) for r in results},\n",
    "        \"delta_fnr_abs_by_seed\": {str(r[\"seed\"]): float(r[\"delta_abs\"]) for r in results},\n",
    "        \"fnr_M_mean_sd\": {\"mean\": float(fnr_m_mean), \"sd\": float(fnr_m_sd), \"values_by_seed\": {str(r[\"seed\"]): float(v) for r, v in zip(results, fnr_m_vals)}},\n",
    "        \"fnr_F_mean_sd\": {\"mean\": float(fnr_f_mean), \"sd\": float(fnr_f_sd), \"values_by_seed\": {str(r[\"seed\"]): float(v) for r, v in zip(results, fnr_f_vals)}},\n",
    "        \"delta_fnr_F_minus_M_mean_sd\": {\"mean\": float(d_signed_mean), \"sd\": float(d_signed_sd), \"values_by_seed\": {str(r[\"seed\"]): float(v) for r, v in zip(results, d_signed_vals)}},\n",
    "        \"delta_fnr_abs_mean_sd\": {\"mean\": float(d_abs_mean), \"sd\": float(d_abs_sd), \"values_by_seed\": {str(r[\"seed\"]): float(v) for r, v in zip(results, d_abs_vals)}},\n",
    "        \"denominators_PD_by_seed\": {str(r[\"seed\"]): {\"n_PD_M\": float(n_pd_m_vals[i]), \"n_PD_F\": float(n_pd_f_vals[i])} for i, r in enumerate(results)},\n",
    "        \"sex_normalization_note\": \"sex_norm in {M,F,UNK}. Values not mapped to M/F counted as UNK. ΔFNR computed only when both M and F have defined FNR.\",\n",
    "    },\n",
    "\n",
    "    \"batching\": {\n",
    "        \"per_device_bs\": int(PER_DEVICE_BS),\n",
    "        \"effective_bs\": int(PER_DEVICE_BS * GRAD_ACCUM),\n",
    "        \"num_val_batches\": int(n_val_batches),\n",
    "        \"num_test_batches\": int(n_test_batches),\n",
    "        \"n_val_items\": int(n_val_items),\n",
    "        \"n_test_items\": int(n_test_items),\n",
    "    },\n",
    "    \"device\": str(DEVICE),\n",
    "    \"gpu\": (torch.cuda.get_device_name(0) if DEVICE.type == \"cuda\" else \"CPU\"),\n",
    "    \"timestamp\": time.strftime(\"%Y%m%d_%H%M%S\"),\n",
    "}\n",
    "\n",
    "summary_path = TEST_ROOT / \"summary_test.json\"\n",
    "with open(summary_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "history_path = TEST_ROOT / \"history_index.jsonl\"\n",
    "with open(history_path, \"a\", encoding=\"utf-8\") as f:\n",
    "    f.write(json.dumps(summary) + \"\\n\")\n",
    "\n",
    "print(\"\\nWROTE summary:\", str(summary_path))\n",
    "print(\"APPENDED history index:\", str(history_path))\n",
    "print(\"Open this folder to access artifacts:\", str(TEST_ROOT))\n",
    "\n",
    "# -------------------------\n",
    "# Stop the runtime to avoid leaving the GPU instance running\n",
    "# -------------------------\n",
    "print(\"\\nAll done. Unassigning the runtime to stop the L4 instance...\")\n",
    "try:\n",
    "    from google.colab import runtime  # type: ignore\n",
    "    print(\"Calling runtime.unassign() now.\")\n",
    "    runtime.unassign()\n",
    "except Exception as e:\n",
    "    print(\"Could not unassign runtime automatically. You can stop the runtime manually in Colab.\")\n",
    "    print(\"Reason:\", repr(e))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "d2b8120142ef4bbfb417a95313ef81ce",
      "140416a41dd74f419992f727beffb555",
      "565ce02bb6c94288be94bd835a5e8b5b",
      "c9218d2536a149b081ee9056ffb7d69f",
      "ecaad6aac0464e3c9e8e583764e1055b",
      "04cead1697f049e6b121f229fefea2a6",
      "8c9fd1bc97184e3a991abbeb36a0a14c",
      "215f33680f534845ae2bcd56a32beb0e",
      "9c411281c0c741aa862356d84068e8b6",
      "96f53ace7d1c4d0fa6e901baefc9d72d",
      "1331f8ca45764f55883c27adc75aa927",
      "f33618d7d941493a8079a4e340b28ae4",
      "41a6da4461d94c368a1201b2b3328bf7",
      "a50fa14f6c09418eafe067441656649a",
      "f970223391a8405cb5736ac6e1b4ca93",
      "d5abc95498a642e18567b16b37066595",
      "302c7ff83f024118b6aef723a9a1dd06",
      "ff61d2305b31439da2d7ccefcf3a91d9",
      "6f3e89eb1f444b1193dab2648ea6ad51",
      "b432a0ed3f8e423fb2f3deffc829fe15",
      "33de4096a7e74badb013d001cafdfc9a",
      "65d73a91196d48ec849b17c7c52a763f",
      "923f126cd2074249981299db5082b2c0",
      "01d635d8dfe1430e87b5243cd5347aa1",
      "5d9669328d714b05a86c8d2d7c1c834a",
      "c6db2d13d3264a3098ee7024640a80cc",
      "579fce76cd314fe98397d84ba3c2b4bd",
      "229d7415b517445683b81e4673786da8",
      "b41cc67f1157415884427424a4a9e4ee",
      "6a0de160b0874011ae6b3a8932fde71e",
      "9f3e55a95c5240da97db531e77a0a29f",
      "f289864065bb4779a4541bd148b1f29c",
      "cf296fa75bb04f8192ce02c26f0e1455",
      "552166f3c0d242e0b119ecd66b9dc025",
      "ba5b67bfafe643bd920ade350f11f3c7",
      "a9f60db305a5426d9693daffce331c18",
      "14f917e2744d47de995f5d8668806f33",
      "31f8a18d2935404bb1fcccf0f5dac087",
      "b1c07e178ff544d29d1ad9a079a32a3e",
      "9b6d27d3884e478c89c5b3b77b7fa632",
      "10016208d0864d6eadb671816050fc7d",
      "df820b719e4c46f8bba798d81567a23e",
      "5afd0541533c42ff902707732e43fd13",
      "1fa6be5b49bb433eb3923eae0da36c5b",
      "cda1762209dd4bd1bc727d7e7dc0dc06",
      "fd73b882dca8453aa424e8df3456f74a",
      "2bd17ba9168743f6863a29290df4e95c",
      "5d1c549cee9b4bfc854303d3f24dfaa3",
      "8a8dc8d9bc524598bec24bda48beca2b",
      "a84805d2f586477bb108877e4ddd2408",
      "8cd01b635c3244ce9bf879aad2477b33",
      "151b27c31e7e4930a90dccce1bb8c9d1",
      "9bd934723edb4bada264e51f8f1e5914",
      "7f5b998070b948cab7a2bb500df1fcc3",
      "3c75bed87c07434aa17036aa6578bf59",
      "82e45977af82483ebf8ce7d12699c972",
      "ee8c4be574044ade951d0713c29739e8",
      "d1c26f4fd8e74108aacff9acb982ca14",
      "b942dbcea87c428fb9e07d33ce61a77f",
      "dc3f2f12ba0a4acabe60e655679b8395",
      "4790fae807a9465aba562bfc2166c998",
      "1edd62a3fc424dc89a9037cdd52b014c",
      "a6bae8ad3bb44e178de0645f41f5cd66",
      "916414d504264ca89909bb67bde6648b",
      "e6d9de9856114adc80bed51a1eeb60b6",
      "c458f2015c4f46089e36a012786f1664",
      "e25dc8446ae546e791646ae58839bde7",
      "9ca1fbc96a914b4c8b742a241294c596",
      "fc85e114e76f473cbee960b609c5d8e7",
      "7df4b704c6c84c13bbd32f4098a217d1",
      "c7a2e164c9bd4a2d880f0e994993a76d",
      "a1ef093bf4a64a5e9716c938fc706cef",
      "71ef4dff937d4f8f800aa7aaf1adf74d",
      "a996572900d74106b96f14617fd6d502",
      "fa06666d298e456d9d03ce4198b59445",
      "3bb440fe3f394872b3a2b34982468c4f",
      "2afd0c5d21b3467682bc753e2cdd96ea",
      "94593146835b4e86b18164712596ef76",
      "e03170ee67a14eb286543661f2b2a62b",
      "71df98ee25fd42c0be5cf2e7fb1e06a1",
      "34cdf93163554597931ba57d92d6d575",
      "9b63e28442a849589f88c9a23e5c81e7",
      "c26349360ccc4f25bb91ed82615f5433",
      "fe4fb20e6bbc49198fe304f8829d208e",
      "6bcf6105c1a84140a31993392b9a828b",
      "a35498df0e3044628ea3d7853f7bcf8f",
      "57c3d58dbb2246b593f90423b8f6754f",
      "4e0a834d026d48ac99e0e7ceded53bc6",
      "921ca731124b47539670543f442672b7",
      "21929c81819a498f9ff62905a203f5de",
      "d70390f883ea43cbbb765e2189a8aaff",
      "79fbee86662540dead9c9a2c18c7839f",
      "1aa221a91d7145b99d84a878901a724e",
      "5bcfb8d324854726bc023600480381e4",
      "449beb903d1a47298ab3b9cbc222226b",
      "e70276e6ce2d4d0da883cf4abc3a1eec",
      "4823f9025dec49b5b7fb56d3c37f4fce",
      "d281df4cee764c6380a27f2e0895c7f2",
      "c51e51eb56394338a97fa32a13ec2b04",
      "a5e941e3bd59490f8b184aa3dd72bf27",
      "67b2c14b2745431b9493beccca54e73f",
      "e5d88d618fd8470c971c95adcfa4dfb8",
      "b85a73bc7cd14cb398f226fb82e0cd5e",
      "7b21556ae48d4b50b52601d845177f07",
      "fd0f5174ec694b5dae20e6968fd5836f",
      "7c8311f2146f4806a93bc53970030274",
      "91d2a149e24d48cdbfdbda147e3ba6d8",
      "80595649064249469139974a9615112c",
      "aae43c67af2047dc897085d7ee752bb2",
      "2f71e2df7f804bb69d9c673288759431"
     ]
    },
    "id": "jJWIaaciEFJK"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "The following cell runs a monolingual test evaluation for D6 (AhSound) using the model heads from the most recent D6 training and validation run. It reads `manifest_all.csv`, keeps only the validation and test rows, and checks that the dataset is clearly D6 so the wrong dataset is not evaluated by mistake. It also confirms that the expected `best_heads.pt` files exist for all three seeds (1337, 2024, 7777) before any evaluation starts.\n",
    "\n",
    "The cell then prepares the data and model in a consistent and repeatable way. It checks that all audio files listed in the manifest exist on disk, assigns each clip to a simple task group (“vowel” when `task == \"vowl\"`, otherwise “other”), and normalizes sex labels to M, F, or UNK so sex based results are reported consistently. Audio is loaded from disk, verified to be at a 16 kHz sample rate, and formatted for use with the Wav2Vec2 backbone. For vowel clips, the attention mask is adjusted so the model ignores trailing padded silence. For non vowel clips, the full signal is used.\n",
    "\n",
    "For each seed, the cell runs two linked steps. First, a validation pass is used only to choose a decision threshold. Inference is run on the D6 validation split and a validation optimal threshold is selected using Youden J, which maximizes TPR minus FPR on the validation ROC curve. Second, a test pass is run on the D6 test split using that same threshold. During this step, the cell reports test AUROC, threshold based metrics at the chosen threshold (including the confusion matrix, accuracy, precision, sensitivity, specificity, F1, MCC, and Fisher exact test p value), and fairness results using the H3 definition. Fairness is computed as the false negative rate for males and females separately, along with ΔFNR defined as FNR(F) minus FNR(M). Confusion counts by sex are also recorded.\n",
    "\n",
    "For each seed, the cell saves standard plots such as the ROC curve and confusion matrix, along with sex specific confusion matrices when possible. It also writes a detailed `metrics.json` file under\n",
    "`<DX_OUT_ROOT>/monolingual_test_runs/run_D6_seed<seed>/`.\n",
    "\n",
    "After all three seeds complete, the cell combines results across seeds. It reports the mean test AUROC with a 95 percent confidence interval using a t distribution with n equal to 3, as well as the mean and standard deviation of the validation chosen thresholds, test threshold metrics, and fairness values. The combined summary is saved to `monolingual_test_runs/summary_test.json`, the same record is appended to `monolingual_test_runs/history_index.jsonl`, the output locations are printed, and the Colab runtime is unassigned to stop the GPU instance."
   ],
   "metadata": {
    "id": "KFvTWJjPpenk"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# =========================\n",
    "# D6 Monolingual Test: VAL threshold (Youden J) + TEST metrics + fairness\n",
    "# - Computes a VAL-optimal threshold per seed (maximize TPR - FPR)\n",
    "# - Evaluates TEST metrics and fairness at that VAL-opt threshold\n",
    "# - Saves per-seed artifacts and a cross-seed summary under monolingual_test_runs/\n",
    "# =========================\n",
    "\n",
    "import os, json, math, random, time, warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import soundfile as sf\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from transformers import Wav2Vec2Model\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, confusion_matrix, matthews_corrcoef\n",
    "from scipy.stats import fisher_exact\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# -------------------------\n",
    "# Import safety checks\n",
    "# Inputs: /content directory\n",
    "# Output: raises early if a local file/folder would override torch/transformers\n",
    "# -------------------------\n",
    "if os.path.exists(\"/content/torch.py\") or os.path.exists(\"/content/torch/__init__.py\"):\n",
    "    raise RuntimeError(\"Found local /content/torch.py or /content/torch/ that shadows PyTorch. Rename/remove it and restart runtime.\")\n",
    "if os.path.exists(\"/content/transformers.py\") or os.path.exists(\"/content/transformers/__init__.py\"):\n",
    "    raise RuntimeError(\"Found local /content/transformers.py or /content/transformers/ that shadows Hugging Face Transformers. Rename/remove it and restart runtime.\")\n",
    "\n",
    "# -------------------------\n",
    "# Drive mount (Colab)\n",
    "# Inputs: Google Drive\n",
    "# Output: /content/drive mounted for reading inputs and writing results\n",
    "# -------------------------\n",
    "if not os.path.isdir(\"/content/drive/MyDrive\"):\n",
    "    from google.colab import drive  # type: ignore\n",
    "    drive.mount(\"/content/drive\")\n",
    "\n",
    "# -------------------------\n",
    "# Roots and manifest path\n",
    "# Inputs: optional DX_OUT_ROOT global, else fallback\n",
    "# Output: MANIFEST_ALL path for loading val/test rows\n",
    "# -------------------------\n",
    "D6_OUT_ROOT_FALLBACK = \"/content/drive/MyDrive/AI_PD_Project/Datasets/D6-Ah Sound (Figshare)/preprocessed_v1\"\n",
    "DX_OUT_ROOT = globals().get(\"DX_OUT_ROOT\", D6_OUT_ROOT_FALLBACK)\n",
    "MANIFEST_ALL = f\"{DX_OUT_ROOT}/manifests/manifest_all.csv\"\n",
    "\n",
    "# -------------------------\n",
    "# Runtime settings (kept consistent with other test cells)\n",
    "# Inputs: constants below\n",
    "# Output: printed config and device selection\n",
    "# -------------------------\n",
    "SEEDS          = [1337, 2024, 7777]\n",
    "BACKBONE_CKPT  = \"facebook/wav2vec2-base\"\n",
    "SR_EXPECTED    = 16000\n",
    "TINY_THRESH    = 1e-4\n",
    "\n",
    "EFFECTIVE_BS   = 64\n",
    "PER_DEVICE_BS  = 16\n",
    "GRAD_ACCUM     = max(1, EFFECTIVE_BS // PER_DEVICE_BS)  # printed only\n",
    "\n",
    "DROPOUT_P      = 0.2\n",
    "\n",
    "NUM_WORKERS    = 0\n",
    "PIN_MEMORY     = False\n",
    "\n",
    "USE_AMP        = True\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "try:\n",
    "    torch.set_float32_matmul_precision(\"high\")\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", message=\"None of the inputs have requires_grad=True. Gradients will be None\")\n",
    "warnings.filterwarnings(\"ignore\", message=\"Passing `gradient_checkpointing` to a config initialization is deprecated\")\n",
    "\n",
    "print(\"DX_OUT_ROOT:\", DX_OUT_ROOT)\n",
    "print(\"MANIFEST_ALL:\", MANIFEST_ALL)\n",
    "print(\"DEVICE:\", DEVICE)\n",
    "if DEVICE.type == \"cuda\":\n",
    "    print(\"GPU:\", torch.cuda.get_device_name(0))\n",
    "print(\"PER_DEVICE_BS:\", PER_DEVICE_BS, \"| EFFECTIVE_BS:\", PER_DEVICE_BS * GRAD_ACCUM)\n",
    "print(\"NUM_WORKERS:\", NUM_WORKERS, \"| PIN_MEMORY:\", PIN_MEMORY)\n",
    "print(\"USE_AMP:\", bool(USE_AMP and DEVICE.type == \"cuda\"))\n",
    "\n",
    "# -------------------------\n",
    "# Manifest load: keep VAL and TEST only, then assert D6\n",
    "# Inputs: manifest_all.csv\n",
    "# Output: val_df and test_df with stable columns + basic counts printed\n",
    "# -------------------------\n",
    "if not os.path.exists(MANIFEST_ALL):\n",
    "    raise FileNotFoundError(f\"Missing manifest_all.csv: {MANIFEST_ALL}\")\n",
    "\n",
    "m = pd.read_csv(MANIFEST_ALL)\n",
    "\n",
    "req_cols = {\"split\", \"clip_path\", \"label_num\", \"task\", \"sex\", \"age\"}\n",
    "missing = [c for c in sorted(req_cols) if c not in m.columns]\n",
    "if missing:\n",
    "    raise ValueError(f\"Manifest missing required columns: {missing}. Found: {list(m.columns)}\")\n",
    "\n",
    "m = m[m[\"split\"].isin([\"val\", \"test\"])].copy()\n",
    "if len(m) == 0:\n",
    "    raise RuntimeError(\"After filtering to split in {'val','test'}, manifest has 0 rows.\")\n",
    "\n",
    "if \"dataset\" in m.columns and m[\"dataset\"].notna().any():\n",
    "    dataset_id = str(m[\"dataset\"].astype(str).value_counts(dropna=True).idxmax())\n",
    "    m = m[m[\"dataset\"].astype(str) == dataset_id].copy()\n",
    "else:\n",
    "    dataset_id = \"DX\"\n",
    "\n",
    "# Guard A: prevent accidental evaluation on a different dataset\n",
    "if dataset_id != \"D6\":\n",
    "    raise RuntimeError(\n",
    "        f\"Expected dataset_id=='D6' but got {dataset_id!r}. \"\n",
    "        \"This usually means DX_OUT_ROOT was inherited from a previous cell or the manifest is not D6. \"\n",
    "        f\"DX_OUT_ROOT={DX_OUT_ROOT}\"\n",
    "    )\n",
    "\n",
    "keep_cols = [\"clip_path\", \"label_num\", \"task\", \"speaker_id\", \"sex\", \"age\", \"duration_sec\", \"split\"]\n",
    "for c in keep_cols:\n",
    "    if c not in m.columns:\n",
    "        m[c] = np.nan\n",
    "m = m[keep_cols].copy()\n",
    "\n",
    "val_df  = m[m[\"split\"].astype(str) == \"val\"].reset_index(drop=True)\n",
    "test_df = m[m[\"split\"].astype(str) == \"test\"].reset_index(drop=True)\n",
    "\n",
    "print(f\"\\nDataset inferred: {dataset_id}\")\n",
    "print(f\"Val rows:  {len(val_df)}\")\n",
    "print(f\"Test rows: {len(test_df)}\")\n",
    "print(\"Val label counts:\",  val_df[\"label_num\"].value_counts(dropna=False).to_dict() if len(val_df) else {})\n",
    "print(\"Test label counts:\", test_df[\"label_num\"].value_counts(dropna=False).to_dict())\n",
    "print(\"Val sex counts (raw):\",  val_df[\"sex\"].value_counts(dropna=False).to_dict() if len(val_df) else {})\n",
    "print(\"Test sex counts (raw):\", test_df[\"sex\"].value_counts(dropna=False).to_dict())\n",
    "\n",
    "if len(val_df) == 0:\n",
    "    raise RuntimeError(\"Val split has 0 rows. VAL is required to compute VAL-opt threshold (Youden J).\")\n",
    "if len(test_df) == 0:\n",
    "    raise RuntimeError(\"Test split has 0 rows.\")\n",
    "\n",
    "# -------------------------\n",
    "# Clip existence checks (fail early, show a few missing)\n",
    "# Inputs: val_df/test_df clip_path\n",
    "# Output: raises with example missing paths\n",
    "# -------------------------\n",
    "def _fail_fast_missing_paths(df: pd.DataFrame, name: str):\n",
    "    missing_paths = []\n",
    "    for p in tqdm(df[\"clip_path\"].astype(str).tolist(), desc=f\"Check {name} clip_path exists\", dynamic_ncols=True):\n",
    "        if not os.path.exists(p):\n",
    "            missing_paths.append(p)\n",
    "            if len(missing_paths) >= 10:\n",
    "                break\n",
    "    if missing_paths:\n",
    "        raise FileNotFoundError(f\"{name}: missing clip_path(s). Examples: {missing_paths}\")\n",
    "\n",
    "_fail_fast_missing_paths(val_df, \"VAL\")\n",
    "_fail_fast_missing_paths(test_df, \"TEST\")\n",
    "\n",
    "# -------------------------\n",
    "# Task grouping: choose vowel head vs other head\n",
    "# Inputs: task column values\n",
    "# Output: task_group used during inference\n",
    "# -------------------------\n",
    "def _task_group(task_val) -> str:\n",
    "    return \"vowel\" if str(task_val) == \"vowl\" else \"other\"\n",
    "\n",
    "val_df[\"task_group\"]  = val_df[\"task\"].apply(_task_group)\n",
    "test_df[\"task_group\"] = test_df[\"task\"].apply(_task_group)\n",
    "\n",
    "# -------------------------\n",
    "# Sex normalization for fairness and sex-specific plots\n",
    "# Inputs: sex values from the manifest\n",
    "# Output: sex_norm in {M, F, UNK}\n",
    "# -------------------------\n",
    "def normalize_sex(val) -> str:\n",
    "    \"\"\"\n",
    "    Returns 'M', 'F', or 'UNK'\n",
    "    Handles common strings (M/F, male/female, etc.)\n",
    "    Numeric encodings are not guessed to avoid silent mis-mapping.\n",
    "    \"\"\"\n",
    "    if pd.isna(val):\n",
    "        return \"UNK\"\n",
    "    s = str(val).strip().lower()\n",
    "\n",
    "    if s in {\"m\", \"male\", \"man\", \"masc\", \"masculine\"}:\n",
    "        return \"M\"\n",
    "    if s in {\"f\", \"female\", \"woman\", \"fem\", \"feminine\"}:\n",
    "        return \"F\"\n",
    "\n",
    "    return \"UNK\"\n",
    "\n",
    "val_df[\"sex_norm\"]  = val_df[\"sex\"].apply(normalize_sex)\n",
    "test_df[\"sex_norm\"] = test_df[\"sex\"].apply(normalize_sex)\n",
    "\n",
    "print(\"Val sex counts (normalized):\",  val_df[\"sex_norm\"].value_counts(dropna=False).to_dict())\n",
    "print(\"Test sex counts (normalized):\", test_df[\"sex_norm\"].value_counts(dropna=False).to_dict())\n",
    "if (val_df[\"sex_norm\"] == \"UNK\").any() or (test_df[\"sex_norm\"] == \"UNK\").any():\n",
    "    print(\"NOTE: Some 'sex' values could not be normalized to M/F and were counted as 'UNK' for fairness and sex charts.\")\n",
    "\n",
    "# -------------------------\n",
    "# Audio dataset + collator (padding + attention masks)\n",
    "# Inputs: val_df/test_df and audio files on disk\n",
    "# Output: batches with padded input_values and attention_mask\n",
    "# -------------------------\n",
    "class AudioManifestDataset(Dataset):\n",
    "    def __init__(self, df: pd.DataFrame):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        row = self.df.iloc[idx]\n",
    "        clip_path = str(row[\"clip_path\"])\n",
    "        label = int(row[\"label_num\"])\n",
    "        task_group = str(row[\"task_group\"])\n",
    "        sex_norm = str(row[\"sex_norm\"])\n",
    "\n",
    "        if not os.path.exists(clip_path):\n",
    "            raise FileNotFoundError(f\"Missing clip_path: {clip_path}\")\n",
    "\n",
    "        y, sr = sf.read(clip_path, always_2d=False)\n",
    "        if isinstance(y, np.ndarray) and y.ndim > 1:\n",
    "            y = np.mean(y, axis=1)\n",
    "        y = np.asarray(y, dtype=np.float32)\n",
    "\n",
    "        if int(sr) != SR_EXPECTED:\n",
    "            raise ValueError(f\"Sample rate mismatch (expected {SR_EXPECTED}): {clip_path} has sr={sr}\")\n",
    "\n",
    "        # For vowel clips, mask out trailing near-zero padding\n",
    "        attn = np.ones((len(y),), dtype=np.int64)\n",
    "        if task_group == \"vowel\":\n",
    "            k = -1\n",
    "            for j in range(len(y) - 1, -1, -1):\n",
    "                if abs(float(y[j])) > TINY_THRESH:\n",
    "                    k = j\n",
    "                    break\n",
    "            if k >= 0:\n",
    "                attn[:k+1] = 1\n",
    "                attn[k+1:] = 0\n",
    "            else:\n",
    "                attn[:] = 1\n",
    "        else:\n",
    "            attn[:] = 1\n",
    "\n",
    "        return {\n",
    "            \"input_values\": torch.from_numpy(y),\n",
    "            \"attention_mask\": torch.from_numpy(attn),\n",
    "            \"labels\": torch.tensor(label, dtype=torch.long),\n",
    "            \"task_group\": task_group,\n",
    "            \"sex_norm\": sex_norm,\n",
    "        }\n",
    "\n",
    "def collate_fn(batch):\n",
    "    max_len = int(max(b[\"input_values\"].numel() for b in batch))\n",
    "    input_vals, attn_masks, labels, task_groups, sex_norms = [], [], [], [], []\n",
    "    for b in batch:\n",
    "        x = b[\"input_values\"]\n",
    "        a = b[\"attention_mask\"]\n",
    "        pad = max_len - x.numel()\n",
    "        if pad > 0:\n",
    "            x = torch.cat([x, torch.zeros(pad, dtype=x.dtype)], dim=0)\n",
    "            a = torch.cat([a, torch.zeros(pad, dtype=a.dtype)], dim=0)\n",
    "        input_vals.append(x)\n",
    "        attn_masks.append(a)\n",
    "        labels.append(b[\"labels\"])\n",
    "        task_groups.append(b[\"task_group\"])\n",
    "        sex_norms.append(b[\"sex_norm\"])\n",
    "    return {\n",
    "        \"input_values\": torch.stack(input_vals, dim=0),\n",
    "        \"attention_mask\": torch.stack(attn_masks, dim=0),\n",
    "        \"labels\": torch.stack(labels, dim=0),\n",
    "        \"task_group\": task_groups,\n",
    "        \"sex_norm\": sex_norms,\n",
    "    }\n",
    "\n",
    "# -------------------------\n",
    "# Model: frozen wav2vec2 + two heads (vowel vs other)\n",
    "# Inputs: BACKBONE_CKPT and dropout\n",
    "# Output: logits for PD vs healthy using the correct head per task_group\n",
    "# -------------------------\n",
    "class Wav2Vec2TwoHeadClassifier(nn.Module):\n",
    "    def __init__(self, ckpt: str, dropout_p: float):\n",
    "        super().__init__()\n",
    "        self.backbone = Wav2Vec2Model.from_pretrained(ckpt, use_safetensors=True)\n",
    "        for p in self.backbone.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "        H = int(self.backbone.config.hidden_size)\n",
    "        self.pre_vowel = nn.Sequential(nn.LayerNorm(H), nn.Dropout(float(dropout_p)))\n",
    "        self.pre_other = nn.Sequential(nn.LayerNorm(H), nn.Dropout(float(dropout_p)))\n",
    "        self.head_vowel = nn.Linear(H, 2)\n",
    "        self.head_other = nn.Linear(H, 2)\n",
    "\n",
    "    def masked_mean_pool(self, last_hidden, attn_mask_samples):\n",
    "        feat_mask = self.backbone._get_feature_vector_attention_mask(last_hidden.shape[1], attn_mask_samples)\n",
    "        feat_mask = feat_mask.to(last_hidden.device).unsqueeze(-1).type_as(last_hidden)\n",
    "        masked = last_hidden * feat_mask\n",
    "        denom = feat_mask.sum(dim=1).clamp(min=1.0)\n",
    "        return masked.sum(dim=1) / denom\n",
    "\n",
    "    def _heads_fp32(self, x_fp_any: torch.Tensor, head: nn.Module) -> torch.Tensor:\n",
    "        # Keep head math in fp32 even under AMP\n",
    "        x = x_fp_any.float()\n",
    "        if x.is_cuda:\n",
    "            with torch.amp.autocast(device_type=\"cuda\", enabled=False):\n",
    "                return head(x)\n",
    "        return head(x)\n",
    "\n",
    "    def forward_logits(self, input_values, attention_mask, task_group):\n",
    "        with torch.no_grad():\n",
    "            out = self.backbone(input_values=input_values, attention_mask=attention_mask)\n",
    "            last_hidden = out.last_hidden_state\n",
    "\n",
    "        pooled = self.masked_mean_pool(last_hidden, attention_mask)  # [B,H]\n",
    "\n",
    "        z_v = self.pre_vowel(pooled.float())\n",
    "        z_o = self.pre_other(pooled.float())\n",
    "\n",
    "        logits_v = self._heads_fp32(z_v, self.head_vowel)\n",
    "        logits_o = self._heads_fp32(z_o, self.head_other)\n",
    "\n",
    "        is_vowel = torch.tensor([tg == \"vowel\" for tg in task_group], device=pooled.device, dtype=torch.bool)\n",
    "        logits = logits_o.clone()\n",
    "        if is_vowel.any():\n",
    "            logits[is_vowel] = logits_v[is_vowel]\n",
    "        return logits\n",
    "\n",
    "# -------------------------\n",
    "# Metrics + plots\n",
    "# Inputs: y_true and y_prob (PD probability), plus threshold when needed\n",
    "# Output: metric dicts and PNG plots saved to each seed folder\n",
    "# -------------------------\n",
    "def compute_auc(y_true, y_prob):\n",
    "    y_true = np.asarray(y_true, dtype=np.int64)\n",
    "    y_prob = np.asarray(y_prob, dtype=np.float64)\n",
    "    if len(np.unique(y_true)) < 2:\n",
    "        return float(\"nan\")\n",
    "    return float(roc_auc_score(y_true, y_prob))\n",
    "\n",
    "def compute_threshold_metrics(y_true, y_prob, thr=0.5):\n",
    "    y_true = np.asarray(y_true, dtype=np.int64)\n",
    "    y_prob = np.asarray(y_prob, dtype=np.float64)\n",
    "    y_pred = (y_prob >= float(thr)).astype(np.int64)\n",
    "\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=[0, 1])\n",
    "    TN, FP, FN, TP = int(cm[0, 0]), int(cm[0, 1]), int(cm[1, 0]), int(cm[1, 1])\n",
    "\n",
    "    eps = 1e-12\n",
    "    acc = (TP + TN) / max(1, (TP + TN + FP + FN))\n",
    "    prec = TP / (TP + FP + eps)\n",
    "    rec = TP / (TP + FN + eps)\n",
    "    f1 = 2 * prec * rec / (prec + rec + eps)\n",
    "    spec = TN / (TN + FP + eps)\n",
    "\n",
    "    try:\n",
    "        mcc = float(matthews_corrcoef(y_true, y_pred)) if len(np.unique(y_pred)) > 1 else float(\"nan\")\n",
    "    except Exception:\n",
    "        mcc = float(\"nan\")\n",
    "\n",
    "    try:\n",
    "        _, pval = fisher_exact([[TN, FP], [FN, TP]], alternative=\"two-sided\")\n",
    "        pval = float(pval)\n",
    "    except Exception:\n",
    "        pval = float(\"nan\")\n",
    "\n",
    "    return {\n",
    "        \"threshold\": float(thr),\n",
    "        \"confusion_matrix\": {\"TN\": TN, \"FP\": FP, \"FN\": FN, \"TP\": TP},\n",
    "        \"accuracy\": float(acc),\n",
    "        \"precision\": float(prec),\n",
    "        \"recall\": float(rec),\n",
    "        \"f1_score\": float(f1),\n",
    "        \"sensitivity\": float(rec),\n",
    "        \"specificity\": float(spec),\n",
    "        \"mcc\": float(mcc),\n",
    "        \"p_value_fisher_two_sided\": float(pval),\n",
    "    }\n",
    "\n",
    "def save_roc_curve_png(y_true, y_prob, out_png, title_suffix=\"Test\"):\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_prob)\n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr)\n",
    "    plt.plot([0, 1], [0, 1], linestyle=\"--\")\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(f\"ROC Curve ({title_suffix})\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_png, dpi=150)\n",
    "    plt.close()\n",
    "\n",
    "def save_confusion_png(y_true, y_prob, out_png, thr=0.5, title_suffix=\"Test\"):\n",
    "    y_true = np.asarray(y_true, dtype=np.int64)\n",
    "    y_prob = np.asarray(y_prob, dtype=np.float64)\n",
    "    y_pred = (y_prob >= float(thr)).astype(int)\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=[0, 1])\n",
    "\n",
    "    plt.figure()\n",
    "    plt.imshow(cm)\n",
    "    plt.xticks([0, 1], [\"Healthy(0)\", \"PD(1)\"])\n",
    "    plt.yticks([0, 1], [\"Healthy(0)\", \"PD(1)\"])\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"True\")\n",
    "    plt.title(f\"Confusion Matrix ({title_suffix}, thr={thr:.2f})\")\n",
    "    for (i, j), v in np.ndenumerate(cm):\n",
    "        plt.text(j, i, str(v), ha=\"center\", va=\"center\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_png, dpi=150)\n",
    "    plt.close()\n",
    "\n",
    "# -------------------------\n",
    "# VAL-opt threshold (Youden J): choose threshold with max (TPR - FPR)\n",
    "# Inputs: VAL labels and probabilities\n",
    "# Output: best threshold and its ROC point\n",
    "# -------------------------\n",
    "def compute_val_opt_threshold_youden_j(y_true, y_prob):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      thr_opt, youden_j_opt, fpr_opt, tpr_opt\n",
    "    If ROC cannot be computed (single-class), returns (nan, nan, nan, nan).\n",
    "    \"\"\"\n",
    "    y_true = np.asarray(y_true, dtype=np.int64)\n",
    "    y_prob = np.asarray(y_prob, dtype=np.float64)\n",
    "\n",
    "    if len(np.unique(y_true)) < 2:\n",
    "        return float(\"nan\"), float(\"nan\"), float(\"nan\"), float(\"nan\")\n",
    "\n",
    "    fpr, tpr, thr = roc_curve(y_true, y_prob)\n",
    "    j = tpr - fpr\n",
    "    if len(j) == 0:\n",
    "        return float(\"nan\"), float(\"nan\"), float(\"nan\"), float(\"nan\")\n",
    "    k = int(np.nanargmax(j))\n",
    "    return float(thr[k]), float(j[k]), float(fpr[k]), float(tpr[k])\n",
    "\n",
    "# -------------------------\n",
    "# Fairness (H3): FNR per sex and ΔFNR = FNR(F) - FNR(M)\n",
    "# Inputs: TEST labels/probabilities, sex_norm, threshold\n",
    "# Output: per-sex FNR details + signed and absolute gap\n",
    "# -------------------------\n",
    "def compute_fnr_by_group_signed(y_true, y_prob, groups, thr=0.5):\n",
    "    \"\"\"\n",
    "    FNR = FN/(FN+TP) computed only on PD ground-truth rows.\n",
    "    \"\"\"\n",
    "    y_true = np.asarray(y_true, dtype=np.int64)\n",
    "    y_prob = np.asarray(y_prob, dtype=np.float64)\n",
    "    groups = np.asarray(list(groups), dtype=object)\n",
    "    y_pred = (y_prob >= float(thr)).astype(np.int64)\n",
    "\n",
    "    out = {}\n",
    "    for g in sorted(set(groups.tolist())):\n",
    "        mask_g = (groups == g)\n",
    "        if mask_g.sum() == 0:\n",
    "            continue\n",
    "\n",
    "        pos_mask = mask_g & (y_true == 1)\n",
    "        n_pos = int(pos_mask.sum())\n",
    "        if n_pos == 0:\n",
    "            out[g] = {\"n_total\": int(mask_g.sum()), \"n_pos\": 0, \"tp\": 0, \"fn\": 0, \"fnr\": float(\"nan\")}\n",
    "            continue\n",
    "\n",
    "        tp = int(((y_pred == 1) & pos_mask).sum())\n",
    "        fn = int(((y_pred == 0) & pos_mask).sum())\n",
    "        fnr = float(fn / max(1, (fn + tp)))\n",
    "\n",
    "        out[g] = {\"n_total\": int(mask_g.sum()), \"n_pos\": int(n_pos), \"tp\": int(tp), \"fn\": int(fn), \"fnr\": float(fnr)}\n",
    "\n",
    "    fnr_m = out.get(\"M\", {}).get(\"fnr\", float(\"nan\"))\n",
    "    fnr_f = out.get(\"F\", {}).get(\"fnr\", float(\"nan\"))\n",
    "\n",
    "    if (not np.isnan(fnr_m)) and (not np.isnan(fnr_f)):\n",
    "        delta_signed = float(fnr_f - fnr_m)\n",
    "        delta_abs = float(abs(delta_signed))\n",
    "    else:\n",
    "        delta_signed = float(\"nan\")\n",
    "        delta_abs = float(\"nan\")\n",
    "\n",
    "    return out, delta_signed, delta_abs\n",
    "\n",
    "# -------------------------\n",
    "# Confusion counts by sex group (for reporting, not plotting)\n",
    "# Inputs: TEST labels/probabilities, sex_norm, threshold\n",
    "# Output: TN/FP/FN/TP per sex group\n",
    "# -------------------------\n",
    "def compute_confusion_counts(y_true, y_prob, thr=0.5):\n",
    "    y_true = np.asarray(y_true, dtype=np.int64)\n",
    "    y_prob = np.asarray(y_prob, dtype=np.float64)\n",
    "    y_pred = (y_prob >= float(thr)).astype(np.int64)\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=[0, 1])\n",
    "    TN, FP, FN, TP = int(cm[0, 0]), int(cm[0, 1]), int(cm[1, 0]), int(cm[1, 1])\n",
    "    return {\"TN\": TN, \"FP\": FP, \"FN\": FN, \"TP\": TP}\n",
    "\n",
    "def compute_confusion_by_group(y_true, y_prob, groups, thr=0.5):\n",
    "    y_true = np.asarray(y_true, dtype=np.int64)\n",
    "    y_prob = np.asarray(y_prob, dtype=np.float64)\n",
    "    groups = np.asarray(list(groups), dtype=object)\n",
    "    out = {}\n",
    "    for g in sorted(set(groups.tolist())):\n",
    "        mask = (groups == g)\n",
    "        if int(mask.sum()) == 0:\n",
    "            continue\n",
    "        out[g] = {\n",
    "            \"n\": int(mask.sum()),\n",
    "            \"confusion\": compute_confusion_counts(y_true[mask], y_prob[mask], thr=thr),\n",
    "        }\n",
    "    return out\n",
    "\n",
    "# -------------------------\n",
    "# Seeding utilities\n",
    "# Inputs: seed\n",
    "# Output: stable randomness across numpy/torch\n",
    "# -------------------------\n",
    "def set_all_seeds(seed: int):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# -------------------------\n",
    "# Select most recent trainval experiment with all seed checkpoints\n",
    "# Inputs: trainval_runs folder\n",
    "# Output: chosen_exp (source of best_heads.pt)\n",
    "# -------------------------\n",
    "TRAINVAL_ROOT = Path(DX_OUT_ROOT) / \"trainval_runs\"\n",
    "if not TRAINVAL_ROOT.exists():\n",
    "    raise FileNotFoundError(f\"Missing trainval_runs folder: {str(TRAINVAL_ROOT)}\")\n",
    "\n",
    "exp_dirs = sorted([p for p in TRAINVAL_ROOT.glob(\"exp_*\") if p.is_dir()], key=lambda p: p.stat().st_mtime, reverse=True)\n",
    "if not exp_dirs:\n",
    "    raise FileNotFoundError(f\"No exp_* folders found under: {str(TRAINVAL_ROOT)}\")\n",
    "\n",
    "def _has_all_seeds(exp_path: Path, dataset_id: str, seeds: list):\n",
    "    for s in seeds:\n",
    "        p = exp_path / f\"run_{dataset_id}_seed{s}\" / \"best_heads.pt\"\n",
    "        if not p.exists():\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "chosen_exp = None\n",
    "for ed in exp_dirs:\n",
    "    if _has_all_seeds(ed, dataset_id, SEEDS):\n",
    "        chosen_exp = ed\n",
    "        break\n",
    "\n",
    "if chosen_exp is None:\n",
    "    sample = exp_dirs[0]\n",
    "    raise FileNotFoundError(\n",
    "        \"Could not find a recent trainval experiment with all 3 best_heads.pt files.\\n\"\n",
    "        f\"Expected under: {str(TRAINVAL_ROOT)}/exp_*/run_{dataset_id}_seedXXXX/best_heads.pt\\n\"\n",
    "        f\"Most recent exp checked: {str(sample)}\"\n",
    "    )\n",
    "\n",
    "print(\"\\nUsing Train+Val experiment folder:\")\n",
    "print(\" \", str(chosen_exp))\n",
    "\n",
    "# Guard B: re-check the three checkpoint files immediately after selection\n",
    "for s in SEEDS:\n",
    "    p = chosen_exp / f\"run_{dataset_id}_seed{s}\" / \"best_heads.pt\"\n",
    "    if not p.exists():\n",
    "        raise RuntimeError(f\"Trainval artifact missing after choosing exp. Missing: {str(p)}\")\n",
    "\n",
    "# -------------------------\n",
    "# Output root for this evaluation\n",
    "# Inputs: DX_OUT_ROOT\n",
    "# Output: per-seed folders and summary files under monolingual_test_runs/\n",
    "# -------------------------\n",
    "TEST_ROOT = Path(DX_OUT_ROOT) / \"monolingual_test_runs\"\n",
    "TEST_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# -------------------------\n",
    "# DataLoaders (VAL for threshold, TEST for final metrics)\n",
    "# Inputs: val_df and test_df\n",
    "# Output: val_loader and test_loader\n",
    "# -------------------------\n",
    "val_loader = DataLoader(\n",
    "    AudioManifestDataset(val_df),\n",
    "    batch_size=PER_DEVICE_BS,\n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=PIN_MEMORY,\n",
    "    collate_fn=collate_fn,\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    AudioManifestDataset(test_df),\n",
    "    batch_size=PER_DEVICE_BS,\n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=PIN_MEMORY,\n",
    "    collate_fn=collate_fn,\n",
    ")\n",
    "\n",
    "# -------------------------\n",
    "# Warm-up reads (quick sanity check that loaders work)\n",
    "# Inputs: loaders\n",
    "# Output: loads a few batches to surface issues early\n",
    "# -------------------------\n",
    "def _warmup(loader, name):\n",
    "    print(f\"\\nWarm-up: loading up to 3 {name} batches...\")\n",
    "    t0 = time.time()\n",
    "    num_batches = len(loader)\n",
    "    warmup_batches = min(3, num_batches)\n",
    "    if warmup_batches == 0:\n",
    "        raise RuntimeError(f\"{name} DataLoader has 0 batches. Check df length and PER_DEVICE_BS.\")\n",
    "    it = iter(loader)\n",
    "    for i in range(warmup_batches):\n",
    "        _ = next(it)\n",
    "        print(f\"  loaded warmup batch {i+1}/{warmup_batches}\")\n",
    "    print(f\"Warm-up done in {time.time()-t0:.2f}s\")\n",
    "\n",
    "_warmup(val_loader, \"VAL\")\n",
    "_warmup(test_loader, \"TEST\")\n",
    "\n",
    "# -------------------------\n",
    "# One-seed evaluation: pick VAL threshold, then score TEST at that threshold\n",
    "# Inputs: seed, best_heads.pt, loaders\n",
    "# Output: per-seed metrics.json and PNG plots under run_<DATASET>_seedXXXX/\n",
    "# -------------------------\n",
    "def load_heads_into_model(model: Wav2Vec2TwoHeadClassifier, best_heads_path: Path):\n",
    "    if not best_heads_path.exists():\n",
    "        raise FileNotFoundError(f\"Missing best_heads.pt: {str(best_heads_path)}\")\n",
    "    state = torch.load(str(best_heads_path), map_location=\"cpu\")\n",
    "    model.pre_vowel.load_state_dict(state[\"pre_vowel\"], strict=True)\n",
    "    model.pre_other.load_state_dict(state[\"pre_other\"], strict=True)\n",
    "    model.head_vowel.load_state_dict(state[\"head_vowel\"], strict=True)\n",
    "    model.head_other.load_state_dict(state[\"head_other\"], strict=True)\n",
    "    return model\n",
    "\n",
    "def _infer_probs(loader, model, seed, stage_name):\n",
    "    use_amp = bool(USE_AMP and DEVICE.type == \"cuda\")\n",
    "    all_probs, all_true, all_sex = [], [], []\n",
    "    pbar = tqdm(loader, desc=f\"[seed={seed}] {stage_name}\", dynamic_ncols=True)\n",
    "    with torch.inference_mode():\n",
    "        for batch in pbar:\n",
    "            input_values = batch[\"input_values\"].to(DEVICE, non_blocking=False)\n",
    "            attention_mask = batch[\"attention_mask\"].to(DEVICE, non_blocking=False)\n",
    "            labels = batch[\"labels\"].to(DEVICE, non_blocking=False)\n",
    "            task_group = batch[\"task_group\"]\n",
    "            sex_norm = batch[\"sex_norm\"]\n",
    "\n",
    "            with torch.amp.autocast(device_type=\"cuda\", enabled=use_amp):\n",
    "                logits = model.forward_logits(input_values, attention_mask, task_group)\n",
    "\n",
    "            probs = torch.softmax(logits.float(), dim=-1)[:, 1].detach().cpu().numpy().astype(np.float64)\n",
    "            all_probs.extend(probs.tolist())\n",
    "            all_true.extend(labels.detach().cpu().numpy().astype(np.int64).tolist())\n",
    "            all_sex.extend(list(sex_norm))\n",
    "    return np.asarray(all_true, dtype=np.int64), np.asarray(all_probs, dtype=np.float64), np.asarray(all_sex, dtype=object)\n",
    "\n",
    "def run_test_once(seed: int):\n",
    "    set_all_seeds(seed)\n",
    "\n",
    "    run_dir = TEST_ROOT / f\"run_{dataset_id}_seed{seed}\"\n",
    "    run_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    best_heads_path = chosen_exp / f\"run_{dataset_id}_seed{seed}\" / \"best_heads.pt\"\n",
    "\n",
    "    print(f\"\\n[seed={seed}] Loading model + heads from:\")\n",
    "    print(\" \", str(best_heads_path))\n",
    "\n",
    "    model = Wav2Vec2TwoHeadClassifier(BACKBONE_CKPT, dropout_p=DROPOUT_P).to(DEVICE)\n",
    "    model = load_heads_into_model(model, best_heads_path)\n",
    "    model.eval()\n",
    "\n",
    "    # VAL: compute per-seed threshold (Youden J)\n",
    "    yv_true, yv_prob, _ = _infer_probs(val_loader, model, seed, \"Val (for threshold)\")\n",
    "    val_thr_opt, val_j_opt, val_fpr_opt, val_tpr_opt = compute_val_opt_threshold_youden_j(yv_true, yv_prob)\n",
    "\n",
    "    print(f\"[seed={seed}] VAL-OPT threshold (Youden J): {val_thr_opt:.6f}\" if not np.isnan(val_thr_opt) else f\"[seed={seed}] VAL-OPT threshold (Youden J): nan\")\n",
    "    if not np.isnan(val_thr_opt):\n",
    "        print(f\"[seed={seed}] Youden J stats on VAL: J={val_j_opt:.6f} | TPR={val_tpr_opt:.6f} | FPR={val_fpr_opt:.6f}\")\n",
    "\n",
    "    # TEST: metrics at the VAL-opt threshold for this seed\n",
    "    yt_true, yt_prob, yt_sex = _infer_probs(test_loader, model, seed, \"Test\")\n",
    "\n",
    "    test_auc = compute_auc(yt_true, yt_prob)\n",
    "    thr_use = float(val_thr_opt)\n",
    "    thr_metrics = compute_threshold_metrics(yt_true, yt_prob, thr=thr_use)\n",
    "\n",
    "    fnr_by_sex, delta_f_minus_m, delta_abs = compute_fnr_by_group_signed(\n",
    "        yt_true, yt_prob, yt_sex, thr=thr_use\n",
    "    )\n",
    "\n",
    "    confusion_by_sex = compute_confusion_by_group(yt_true, yt_prob, yt_sex, thr=thr_use)\n",
    "\n",
    "    # Plots: overall ROC + overall confusion\n",
    "    roc_png = run_dir / \"roc_curve.png\"\n",
    "    cm_png  = run_dir / \"confusion_matrix.png\"\n",
    "    save_roc_curve_png(yt_true, yt_prob, str(roc_png), title_suffix=f\"Test (seed={seed})\")\n",
    "    save_confusion_png(yt_true, yt_prob, str(cm_png), thr=thr_use, title_suffix=f\"Test (seed={seed})\")\n",
    "\n",
    "    # Plots: sex-specific confusion (if that group exists)\n",
    "    cm_m_png = None\n",
    "    cm_f_png = None\n",
    "    mask_m = (yt_sex == \"M\")\n",
    "    mask_f = (yt_sex == \"F\")\n",
    "\n",
    "    if int(mask_m.sum()) > 0:\n",
    "        cm_m_png = run_dir / \"confusion_matrix_M.png\"\n",
    "        save_confusion_png(\n",
    "            yt_true[mask_m], yt_prob[mask_m], str(cm_m_png),\n",
    "            thr=thr_use, title_suffix=f\"Test SEX=M (seed={seed})\"\n",
    "        )\n",
    "\n",
    "    if int(mask_f.sum()) > 0:\n",
    "        cm_f_png = run_dir / \"confusion_matrix_F.png\"\n",
    "        save_confusion_png(\n",
    "            yt_true[mask_f], yt_prob[mask_f], str(cm_f_png),\n",
    "            thr=thr_use, title_suffix=f\"Test SEX=F (seed={seed})\"\n",
    "        )\n",
    "\n",
    "    metrics = {\n",
    "        \"dataset\": dataset_id,\n",
    "        \"seed\": int(seed),\n",
    "\n",
    "        \"n_val\": int(len(val_df)),\n",
    "        \"label_counts_val\": val_df[\"label_num\"].value_counts(dropna=False).to_dict(),\n",
    "        \"sex_counts_val_norm\": val_df[\"sex_norm\"].value_counts(dropna=False).to_dict(),\n",
    "\n",
    "        \"n_test\": int(len(test_df)),\n",
    "        \"label_counts_test\": test_df[\"label_num\"].value_counts(dropna=False).to_dict(),\n",
    "        \"sex_counts_test_norm\": test_df[\"sex_norm\"].value_counts(dropna=False).to_dict(),\n",
    "\n",
    "        \"val_opt_threshold\": float(val_thr_opt),\n",
    "        \"val_opt_threshold_method\": \"Youden J (maximize TPR - FPR) on VAL ROC\",\n",
    "        \"val_youden_j\": float(val_j_opt),\n",
    "        \"val_youden_tpr\": float(val_tpr_opt),\n",
    "        \"val_youden_fpr\": float(val_fpr_opt),\n",
    "\n",
    "        \"test_auroc\": float(test_auc),\n",
    "        \"threshold_metrics_test_at_val_opt\": thr_metrics,\n",
    "\n",
    "        \"fairness_test_at_val_opt\": {\n",
    "            \"definition\": \"H3: ΔFNR = FNR(F) - FNR(M), where FNR(sex)=FN/(FN+TP) computed on PD-only true labels at val_opt_threshold.\",\n",
    "            \"fnr_by_sex_norm\": fnr_by_sex,\n",
    "            \"delta_fnr_F_minus_M\": float(delta_f_minus_m),\n",
    "            \"delta_fnr_abs\": float(delta_abs),\n",
    "            \"note\": \"If n_PD for a sex is 0, its FNR is NaN and ΔFNR is NaN.\",\n",
    "            \"sex_normalization_note\": \"sex_norm in {M,F,UNK}. Values not mapped to M/F counted as UNK.\",\n",
    "        },\n",
    "\n",
    "        \"confusion_by_sex_norm_at_val_opt\": confusion_by_sex,\n",
    "\n",
    "        \"artifacts\": {\n",
    "            \"roc_curve_png\": str(roc_png),\n",
    "            \"confusion_matrix_png\": str(cm_png),\n",
    "            \"confusion_matrix_M_png\": str(cm_m_png) if cm_m_png is not None else None,\n",
    "            \"confusion_matrix_F_png\": str(cm_f_png) if cm_f_png is not None else None,\n",
    "        },\n",
    "\n",
    "        \"dx_out_root\": DX_OUT_ROOT,\n",
    "        \"trainval_experiment_used\": str(chosen_exp),\n",
    "        \"best_heads_path\": str(best_heads_path),\n",
    "        \"backbone_ckpt\": BACKBONE_CKPT,\n",
    "        \"dropout_p\": float(DROPOUT_P),\n",
    "        \"timestamp\": time.strftime(\"%Y%m%d_%H%M%S\"),\n",
    "    }\n",
    "\n",
    "    metrics_path = run_dir / \"metrics.json\"\n",
    "    with open(metrics_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(metrics, f, indent=2)\n",
    "\n",
    "    print(f\"[seed={seed}] DONE | test_AUROC={test_auc:.6f}\")\n",
    "\n",
    "    def _fmt_fnr(g):\n",
    "        d = fnr_by_sex.get(g, None)\n",
    "        if d is None:\n",
    "            return \"n/a\"\n",
    "        return f\"fnr={d['fnr']:.6f} (n_PD={d['n_pos']}, fn={d['fn']}, tp={d['tp']})\"\n",
    "\n",
    "    print(f\"[seed={seed}] FAIRNESS (H3) on TEST @ VAL-OPT thr={thr_use:.6f}:\")\n",
    "    print(\"  M:\", _fmt_fnr(\"M\"))\n",
    "    print(\"  F:\", _fmt_fnr(\"F\"))\n",
    "    if \"UNK\" in fnr_by_sex:\n",
    "        print(\"  UNK:\", _fmt_fnr(\"UNK\"))\n",
    "    print(\"  ΔFNR (F-M):\", f\"{delta_f_minus_m:.6f}\" if not np.isnan(delta_f_minus_m) else \"nan\")\n",
    "    print(\"  |ΔFNR|:\", f\"{delta_abs:.6f}\" if not np.isnan(delta_abs) else \"nan\")\n",
    "\n",
    "    print(f\"[seed={seed}] WROTE:\")\n",
    "    print(\" \", str(metrics_path))\n",
    "    print(\" \", str(roc_png))\n",
    "    print(\" \", str(cm_png))\n",
    "    if cm_m_png is not None:\n",
    "        print(\" \", str(cm_m_png))\n",
    "    if cm_f_png is not None:\n",
    "        print(\" \", str(cm_f_png))\n",
    "\n",
    "    return {\n",
    "        \"seed\": int(seed),\n",
    "        \"val_opt_threshold\": float(val_thr_opt),\n",
    "        \"test_auroc\": float(test_auc),\n",
    "        \"threshold_metrics\": thr_metrics,\n",
    "        \"fnr_by_sex\": fnr_by_sex,\n",
    "        \"delta_signed\": float(delta_f_minus_m),\n",
    "        \"delta_abs\": float(delta_abs),\n",
    "        \"run_dir\": str(run_dir),\n",
    "    }\n",
    "\n",
    "# -------------------------\n",
    "# Cross-seed summary: AUROC CI, threshold stats, metrics mean±SD, fairness mean±SD\n",
    "# Inputs: 3 per-seed results\n",
    "# Output: summary_test.json and history_index.jsonl under monolingual_test_runs/\n",
    "# -------------------------\n",
    "results = []\n",
    "for seed in SEEDS:\n",
    "    results.append(run_test_once(seed))\n",
    "\n",
    "aurocs = [r[\"test_auroc\"] for r in results]\n",
    "val_thrs = [r[\"val_opt_threshold\"] for r in results]\n",
    "thr_list = [r[\"threshold_metrics\"] for r in results]\n",
    "run_dirs = [r[\"run_dir\"] for r in results]\n",
    "\n",
    "t_crit = 4.302652729911275  # df=2, 95% CI\n",
    "n = len(aurocs)\n",
    "mean_auc = float(np.mean(aurocs))\n",
    "std_auc = float(np.std(aurocs, ddof=1)) if n > 1 else 0.0\n",
    "half_width = float(t_crit * (std_auc / math.sqrt(n))) if n > 1 else 0.0\n",
    "ci95 = [float(mean_auc - half_width), float(mean_auc + half_width)]\n",
    "\n",
    "def _mean_sd(vals):\n",
    "    vals = np.asarray(vals, dtype=np.float64)\n",
    "    return float(np.nanmean(vals)), float(np.nanstd(vals, ddof=1)) if np.sum(~np.isnan(vals)) > 1 else 0.0\n",
    "\n",
    "keys = [\"accuracy\",\"precision\",\"recall\",\"f1_score\",\"sensitivity\",\"specificity\",\"mcc\",\"p_value_fisher_two_sided\"]\n",
    "agg = {}\n",
    "for k in keys:\n",
    "    v = [float(tm.get(k, float(\"nan\"))) for tm in thr_list]\n",
    "    mu, sd = _mean_sd(v)\n",
    "    agg[k] = {\n",
    "        \"mean\": mu,\n",
    "        \"sd\": sd,\n",
    "        \"values_by_seed\": {str(r[\"seed\"]): float(r[\"threshold_metrics\"].get(k, float(\"nan\"))) for r in results},\n",
    "    }\n",
    "cm_by_seed = {str(r[\"seed\"]): r[\"threshold_metrics\"][\"confusion_matrix\"] for r in results}\n",
    "\n",
    "fnr_by_seed = {str(r[\"seed\"]): r[\"fnr_by_sex\"] for r in results}\n",
    "delta_signed_by_seed = {str(r[\"seed\"]): float(r[\"delta_signed\"]) for r in results}\n",
    "delta_abs_by_seed = {str(r[\"seed\"]): float(r[\"delta_abs\"]) for r in results}\n",
    "\n",
    "fnr_m_vals, fnr_f_vals, n_pd_m_vals, n_pd_f_vals = [], [], [], []\n",
    "d_signed_vals, d_abs_vals = [], []\n",
    "for r in results:\n",
    "    d = r[\"fnr_by_sex\"]\n",
    "    fnr_m_vals.append(float(d.get(\"M\", {}).get(\"fnr\", float(\"nan\"))))\n",
    "    fnr_f_vals.append(float(d.get(\"F\", {}).get(\"fnr\", float(\"nan\"))))\n",
    "    n_pd_m_vals.append(float(d.get(\"M\", {}).get(\"n_pos\", float(\"nan\"))))\n",
    "    n_pd_f_vals.append(float(d.get(\"F\", {}).get(\"n_pos\", float(\"nan\"))))\n",
    "    d_signed_vals.append(float(r[\"delta_signed\"]))\n",
    "    d_abs_vals.append(float(r[\"delta_abs\"]))\n",
    "\n",
    "fnr_m_mean, fnr_m_sd = _mean_sd(fnr_m_vals)\n",
    "fnr_f_mean, fnr_f_sd = _mean_sd(fnr_f_vals)\n",
    "d_signed_mean, d_signed_sd = _mean_sd(d_signed_vals)\n",
    "d_abs_mean, d_abs_sd = _mean_sd(d_abs_vals)\n",
    "\n",
    "thr_mean, thr_sd = _mean_sd(val_thrs)\n",
    "\n",
    "print(\"\\nTest AUROC by seed:\")\n",
    "for r in results:\n",
    "    print(f\"  seed {r['seed']}: {r['test_auroc']:.6f}\")\n",
    "\n",
    "print(f\"\\nMean Test AUROC: {mean_auc:.6f}\")\n",
    "print(f\"95% CI (t, n=3): [{ci95[0]:.6f}, {ci95[1]:.6f}]\")\n",
    "\n",
    "print(\"\\nVAL-OPT thresholds (Youden J) by seed:\")\n",
    "for r in results:\n",
    "    v = r[\"val_opt_threshold\"]\n",
    "    print(f\"  seed {r['seed']}: {v:.6f}\" if not np.isnan(v) else f\"  seed {r['seed']}: nan\")\n",
    "print(f\"  mean ± SD: {thr_mean:.6f} ± {thr_sd:.6f}\" if not np.isnan(thr_mean) else \"  mean ± SD: nan\")\n",
    "\n",
    "print(\"\\nThreshold metrics on TEST @ VAL-OPT threshold (mean ± SD across seeds):\")\n",
    "for k in [\"accuracy\",\"precision\",\"sensitivity\",\"specificity\",\"f1_score\",\"mcc\"]:\n",
    "    mu = agg[k][\"mean\"]\n",
    "    sd = agg[k][\"sd\"]\n",
    "    print(f\"  {k}: {mu:.6f} ± {sd:.6f}\")\n",
    "print(\"  fisher_p_value_two_sided:\", f\"{agg['p_value_fisher_two_sided']['mean']:.6g} ± {agg['p_value_fisher_two_sided']['sd']:.6g}\")\n",
    "\n",
    "print(\"\\nFAIRNESS (H3) on TEST @ VAL-OPT threshold across seeds (mean ± SD):\")\n",
    "print(f\"  FNR_M: {fnr_m_mean:.6f} ± {fnr_m_sd:.6f}\")\n",
    "print(f\"  FNR_F: {fnr_f_mean:.6f} ± {fnr_f_sd:.6f}\")\n",
    "print(f\"  ΔFNR (F-M): {d_signed_mean:.6f} ± {d_signed_sd:.6f}\")\n",
    "print(f\"  |ΔFNR|: {d_abs_mean:.6f} ± {d_abs_sd:.6f}\")\n",
    "print(\"  Per-seed:\", {\n",
    "    str(r[\"seed\"]): {\n",
    "        \"val_opt_threshold\": r[\"val_opt_threshold\"],\n",
    "        \"FNR_M\": fnr_m_vals[i],\n",
    "        \"n_PD_M\": n_pd_m_vals[i],\n",
    "        \"FNR_F\": fnr_f_vals[i],\n",
    "        \"n_PD_F\": n_pd_f_vals[i],\n",
    "        \"delta_F_minus_M\": d_signed_vals[i],\n",
    "        \"abs_delta\": d_abs_vals[i],\n",
    "    } for i, r in enumerate(results)\n",
    "})\n",
    "\n",
    "summary = {\n",
    "    \"dataset\": dataset_id,\n",
    "    \"dx_out_root\": DX_OUT_ROOT,\n",
    "    \"manifest_all\": MANIFEST_ALL,\n",
    "    \"trainval_experiment_used\": str(chosen_exp),\n",
    "    \"seeds\": SEEDS,\n",
    "\n",
    "    \"n_val\": int(len(val_df)),\n",
    "    \"label_counts_val\": val_df[\"label_num\"].value_counts(dropna=False).to_dict(),\n",
    "    \"sex_counts_val_norm\": val_df[\"sex_norm\"].value_counts(dropna=False).to_dict(),\n",
    "\n",
    "    \"n_test\": int(len(test_df)),\n",
    "    \"label_counts_test\": test_df[\"label_num\"].value_counts(dropna=False).to_dict(),\n",
    "    \"sex_counts_test_norm\": test_df[\"sex_norm\"].value_counts(dropna=False).to_dict(),\n",
    "\n",
    "    \"val_opt_thresholds_by_seed\": {str(r[\"seed\"]): float(r[\"val_opt_threshold\"]) for r in results},\n",
    "    \"val_opt_threshold_mean_sd\": {\"mean\": float(thr_mean), \"sd\": float(thr_sd)},\n",
    "\n",
    "    \"test_aurocs_by_seed\": {str(r[\"seed\"]): float(r[\"test_auroc\"]) for r in results},\n",
    "    \"mean_test_auroc\": float(mean_auc),\n",
    "    \"t_crit_df2_95\": float(t_crit),\n",
    "    \"half_width_95_ci\": float(half_width),\n",
    "    \"ci95_test_auroc\": ci95,\n",
    "\n",
    "    \"threshold_metrics_test_at_val_opt_mean_sd\": agg,\n",
    "    \"confusion_matrix_by_seed_at_val_opt\": cm_by_seed,\n",
    "    \"run_dirs\": run_dirs,\n",
    "\n",
    "    \"fairness_test_at_val_opt\": {\n",
    "        \"definition\": \"H3: ΔFNR = FNR(F) - FNR(M), where FNR(sex)=FN/(FN+TP) computed on PD-only true labels at val_opt_threshold.\",\n",
    "        \"fnr_by_sex_norm_by_seed\": fnr_by_seed,\n",
    "        \"delta_fnr_F_minus_M_by_seed\": delta_signed_by_seed,\n",
    "        \"delta_fnr_abs_by_seed\": delta_abs_by_seed,\n",
    "        \"fnr_M_mean_sd\": {\"mean\": float(fnr_m_mean), \"sd\": float(fnr_m_sd), \"values_by_seed\": {str(r['seed']): float(fnr_m_vals[i]) for i, r in enumerate(results)}},\n",
    "        \"fnr_F_mean_sd\": {\"mean\": float(fnr_f_mean), \"sd\": float(fnr_f_sd), \"values_by_seed\": {str(r['seed']): float(fnr_f_vals[i]) for i, r in enumerate(results)}},\n",
    "        \"delta_fnr_F_minus_M_mean_sd\": {\"mean\": float(d_signed_mean), \"sd\": float(d_signed_sd), \"values_by_seed\": {str(r['seed']): float(d_signed_vals[i]) for i, r in enumerate(results)}},\n",
    "        \"delta_fnr_abs_mean_sd\": {\"mean\": float(d_abs_mean), \"sd\": float(d_abs_sd), \"values_by_seed\": {str(r['seed']): float(d_abs_vals[i]) for i, r in enumerate(results)}},\n",
    "        \"denominators_PD_by_seed\": {str(r[\"seed\"]): {\"n_PD_M\": float(n_pd_m_vals[i]), \"n_PD_F\": float(n_pd_f_vals[i])} for i, r in enumerate(results)},\n",
    "        \"sex_normalization_note\": \"sex_norm in {M,F,UNK}. Values not mapped to M/F counted as UNK. ΔFNR computed only when both M and F have defined FNR.\",\n",
    "    },\n",
    "\n",
    "    \"timestamp\": time.strftime(\"%Y%m%d_%H%M%S\"),\n",
    "}\n",
    "\n",
    "summary_path = TEST_ROOT / \"summary_test.json\"\n",
    "with open(summary_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "history_path = TEST_ROOT / \"history_index.jsonl\"\n",
    "with open(history_path, \"a\", encoding=\"utf-8\") as f:\n",
    "    f.write(json.dumps(summary) + \"\\n\")\n",
    "\n",
    "print(\"\\nWROTE summary:\", str(summary_path))\n",
    "print(\"APPENDED history index:\", str(history_path))\n",
    "print(\"Open this folder to access artifacts:\", str(TEST_ROOT))\n",
    "\n",
    "# -------------------------\n",
    "# Stop runtime (Colab)\n",
    "# Inputs: none\n",
    "# Output: attempts to release the GPU instance\n",
    "# -------------------------\n",
    "print(\"\\nAll done. Unassigning the runtime to stop the L4 instance...\")\n",
    "try:\n",
    "    from google.colab import runtime  # type: ignore\n",
    "    print(\"Calling runtime.unassign() now.\")\n",
    "    runtime.unassign()\n",
    "except Exception as e:\n",
    "    print(\"Could not unassign runtime automatically. You can stop the runtime manually in Colab.\")\n",
    "    print(\"Reason:\", repr(e))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 2510,
     "referenced_widgets": [
      "977f66843a5d4153b78eb6d1e218d94c",
      "52a516d2c4544cda9ab376d58e538e8b",
      "cdd77c49088846d98eed1515d674d8e3",
      "cc38821439fe4ed0bb0123d5b08dd76b",
      "a78bf287e0fc4cf487bf3f6a5e3d8b34",
      "ee135f55ec3147f2a5a6739aba4aa1d2",
      "b54e554c4e3f4e338c97160e6bf3e83a",
      "7e4da300b2cc4bf1b58a515a39b5c9df",
      "662ff55b7303461b8cc85f37242fb941",
      "56ee24b4112d4bc4a8c1a4393032ec1a",
      "c5d756096fd54fcd831100f1064186e4",
      "b755f043e7a44856ab3efd564b499f3d",
      "c129c9ccbb344e74ba490201635ce90b",
      "01277cdcec064e6f93238d2bb63cfddc",
      "f304377dbb4f4788b2560640b0c78ca2",
      "2325ca37d4b24214ba8a16712e2bcdd3",
      "fca562f0363f400287f39502b5b87852",
      "36459777fcaf4dc788057d272d42a877",
      "71bc9b5939084f3c9bc035f821052026",
      "467cdb2ebadd4e54ae184e170a3a5f7d",
      "e593de9931484510a31ba87503056362",
      "389034e3f5414f619dfafa1a20fb7917",
      "c59c66caa5b5400b96ea76ee6fec5117",
      "f1e73f44193e42bdbd22b362c30bb995",
      "13f685b671fe48ffb47224201b78a8ad",
      "2c3c7d07702a4c31825eeb0d03c7a7ef",
      "83b9213575cf462d9cf4e1a6a14e104f",
      "af5ee1da4ee8404f8f066104f9994c9d",
      "a01b54416c8a4aed92400515b70bfdf3",
      "f9003e0d8c3f486884ef70612cd732b4",
      "2d1f55c1e67743869b992937a9534f47",
      "ca0392a561af44678fe0ff47e35f9693",
      "121b7cf0c9774040b0b459dfdb04be5b",
      "6928776d776b4742b05603f7c20199d0",
      "d2f4a66a881049259866273a3d5297e8",
      "efa11284bc73420a959016f2a9558bdf",
      "d57d7ff0399747d29b2bbf174ef45ca0",
      "5009eadcd0cc4387816b910734a84cbf",
      "4809ca60189c49ca80a61b6cf6ba93a5",
      "54609cb4e2e349e4af100b8dc4d95a71",
      "5402b55336464bf3b1bb57776e3c24a7",
      "2bdd08ead6a145ba9a9b268a85c93787",
      "698019845ef64d579da965c2e672e830",
      "6037400f27c0493cac664ba6275679ea",
      "abf0b191166a4820b93b7b020434f301",
      "f637daa43fa447c997e4f4fff1273032",
      "b1fa9d604f6644a390aa724dd004e86e",
      "937f4f28ffbb4acc8b89817ca26e1f7c",
      "b9b6640b7751445ebe4adffc4686d58b",
      "d080df16bed3407b95809657f28ed2a9",
      "1bedf7e8f77249b0b5907a282d17106a",
      "f2cc77bf889645a1b870a062d9db7693",
      "3749cc1cc1a4403796f50b0f2b65af99",
      "a8ef540c31a04d14a6b68128c3d1cb53",
      "9660ecbc069c459f8e43a37ca1b326c6",
      "f8a186732f3247fd86ee33fa1ae3004c",
      "f848b4bd68094c74b4b48787231c25e9",
      "7fbee9c7c24c43e3a367387a2a98e59f",
      "8fae93b34dfa46d3b9bec5c329b94f4b",
      "19eeb6ecf28848efa90fd30337be1604",
      "4d26852efa634147a12f223f84238189",
      "9d942baf1177490eb35d99057d541b61",
      "37cba31cc78d4f4bb01b36d07baddfea",
      "c7c66178ff8443f794c3a7e66c8ab2d4",
      "96c4d32826ea47a087fe7b109513fe54",
      "ce71983c9d0341778b3ad7e020891280",
      "fdd4eec36a104e35a41514b3bdfd6615",
      "be7fb10ea8b247458b57e782d5f4b355",
      "4c0f78a13183459c9e96d412d7d8f7ab",
      "6afa2d44a4e846bb8365e52f9d5120b6",
      "bb562693888f40f0b8eccbbb48790801",
      "085d26b5f5a04a85aa5f97db045d274d",
      "656b67becede4cd2ac05be050e067d0a",
      "ea37cc8429d54ec4b4bdb2dcf64addcf",
      "d7b9d7b67614444ba7b84314ae082c3d",
      "7e71776ec6d0498bbc9302edd82c7ecb",
      "8c3ba82c9ce94c3faf790f3f188e9f7e",
      "2072f818459a479f9f142783a77e734f",
      "ab5f2c49f98540d58d0bed9b7a2d577a",
      "b8d9aac0b895463fbc0ea06afed2fc54",
      "d771bf04fb174b42ad2704e51ec0cfea",
      "c39842c1ada14ef1a0ad6915787fabc2",
      "afd79f5b6f0344dd8c8fac95f8648d5f",
      "e5ec5bf131104a338eef57686e4c7480",
      "cb929915624b4021974e5c269c6c14f7",
      "596b0261c5f247a3a439b345c97c48a3",
      "6a646205b15748158f87e34cc489493a",
      "cb0aa2a566874abc833d662060ceab8a",
      "7c96b2a2e23349b9bac5f7a6d5c63da0",
      "e2e6aef9082148e18931bd8df62dcc0a",
      "3f2c77cb1afc4d1daeb4267eda4cf8fa",
      "72d65f6349e949468fb59765e733ac4e",
      "4f7fc5628f3c44a5ac7f55566787d65f",
      "935f7fe9676743698073094475c4b65d",
      "f7d8405169bb4b789424e41f15581070",
      "44e85e933b5e41a3aaf567777d35e3b0",
      "49e8f201b1274178bf1ff4e794679ee5",
      "cd34e7390d154c6daa7fd682b2f1d403",
      "e81fe39b0d234cc2a2a86cf3b8387ca6",
      "9bdf37e943f246f8bdf30a09868cf0af",
      "815a1e0fc5794a41a48e7a4a233749e4",
      "497e3217706d4505a34526ee2f4a703d",
      "3a622fc4cc4b4ae99fe4edb3a31f572c",
      "aaf2423f3669483592745e2cca268504",
      "d576babc93a24118acef8ecd1e58038e",
      "a6c56d53d1bd47da9898c6a51cbd004a",
      "3f745a8806e348bab6eb5ce2cd64c970",
      "65be8f8dfd684e2cac0a3fc09ac9129b",
      "7daca9ec52bd44e2ac2b6b0f12765f87",
      "9a15eea4932e4073b27688a1eda41ffa"
     ]
    },
    "id": "ZaFAEWX3FoGE"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "#Cross-Language Zero-Shot Transfer Tests"
   ],
   "metadata": {
    "id": "FQkLHZOW9pOu"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The following cell runs a cross language zero shot test from Spanish (D1) to Slovak (D2). It checks how well the classifier heads trained on the Spanish dataset (D1, NeuroVoz) perform on the Slovak dataset (D2, EWA DB) without any training, fine tuning, or calibration on D2. The evaluation uses only the D2 test split from `D2/manifests/manifest_all.csv`. The cell automatically finds the most recent completed D1 train and validation experiment that contains saved head checkpoints for three random seeds (1337, 2024, 7777), and applies those D1 trained heads directly to D2 audio.\n",
    "\n",
    "A single fixed decision threshold is used for all three seeds. This threshold is taken from D1’s saved `monolingual_test_runs/summary_test.json` and represents the mean validation selected threshold from D1. Because the summary file structure can differ across versions, the cell includes a robust reader that searches multiple known locations for the threshold and, if needed, falls back to averaging per seed threshold values. If no valid threshold can be found, the cell stops with a clear error message.\n",
    "\n",
    "Before running inference, the cell performs several safety checks to avoid silent errors. It mounts Google Drive if needed, confirms that local files are not overriding PyTorch or Transformers, prints key paths and the active compute device, verifies that the D2 manifest contains all required columns, confirms the dataset label is truly D2, filters strictly to the test split, and checks that all referenced audio files exist on disk. Each clip is assigned a task group, where `task == \"vowl\"` is treated as vowel and everything else is treated as other. Sex values in D2, such as male or female and common variants, are normalized to `sex_norm` values of M, F, or UNK so reporting is consistent.\n",
    "\n",
    "For each seed, the cell loads the matching D1 head checkpoint (`best_heads.pt`) into a frozen Wav2Vec2 two head classifier, with one head for vowel clips and one for other speech. It then runs inference on all D2 test clips to produce Parkinson’s probabilities. Using these scores, it computes overall AUROC, which does not depend on a threshold, and fixed threshold metrics at the D1 derived threshold, including the confusion matrix, accuracy, precision, sensitivity or recall, specificity, F1 score, MCC, and Fisher exact test p value. Fairness is computed as ΔFNR, defined as FNR(F) minus FNR(M), measured on Parkinson’s positive clips only. Confusion counts by sex are also recorded. For each seed, the cell saves a ROC curve, an overall confusion matrix, additional sex specific confusion matrices when both M and F are present, and a detailed `metrics.json` file under\n",
    "`<D2_OUT_ROOT>/Cross_Language_Zero_Shot_Runs/run_ES_to_SK_seed<seed>/`.\n",
    "\n",
    "After all three seeds finish, the cell combines results across seeds. It reports the mean AUROC with a 95 percent confidence interval using a t distribution with n equal to 3, along with the mean and standard deviation of the fixed threshold metrics and fairness values. A combined summary is written to `summary_zero_shot.json` and the same record is appended to `history_index.jsonl`, both under `<D2_OUT_ROOT>/Cross_Language_Zero_Shot_Runs/`. Finally, the Colab runtime is unassigned to shut down the GPU instance."
   ],
   "metadata": {
    "id": "-eVqD-DBz63_"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# =========================\n",
    "# ES → SK Zero-shot (D1 → D2): Test-only transfer with fixed threshold\n",
    "# - Loads D1-trained heads (most recent train+val experiment)\n",
    "# - Runs inference on D2 TEST only (from manifest_all.csv)\n",
    "# - Uses one fixed threshold read from D1 monolingual summary_test.json (mean VAL-opt)\n",
    "# - Saves per-seed metrics + plots, plus an aggregated summary under D2\n",
    "# =========================\n",
    "\n",
    "import os, json, math, random, time, warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import soundfile as sf\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from transformers import Wav2Vec2Model\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, confusion_matrix, matthews_corrcoef\n",
    "from scipy.stats import fisher_exact\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# -------------------------\n",
    "# Import safety: prevent local files from overriding torch/transformers\n",
    "# Inputs: /content directory\n",
    "# Output: raises early if a conflicting file/folder exists\n",
    "# -------------------------\n",
    "if os.path.exists(\"/content/torch.py\") or os.path.exists(\"/content/torch/__init__.py\"):\n",
    "    raise RuntimeError(\"Found local /content/torch.py or /content/torch/ that shadows PyTorch. Rename/remove it and restart runtime.\")\n",
    "if os.path.exists(\"/content/transformers.py\") or os.path.exists(\"/content/transformers/__init__.py\"):\n",
    "    raise RuntimeError(\"Found local /content/transformers.py or /content/transformers/ that shadows Hugging Face Transformers. Rename/remove it and restart runtime.\")\n",
    "\n",
    "# -------------------------\n",
    "# Drive mount (Colab)\n",
    "# Inputs: Google Drive\n",
    "# Output: /content/drive mounted for reading inputs and writing results\n",
    "# -------------------------\n",
    "if not os.path.isdir(\"/content/drive/MyDrive\"):\n",
    "    from google.colab import drive  # type: ignore\n",
    "    drive.mount(\"/content/drive\")\n",
    "\n",
    "# -------------------------\n",
    "# Paths: target (D2) + source (D1)\n",
    "# Inputs: optional globals D1_OUT_ROOT, D2_OUT_ROOT (if set earlier), else fallbacks\n",
    "# Output: D2_MANIFEST_ALL and D1_MONO_SUMMARY paths\n",
    "# -------------------------\n",
    "D1_OUT_ROOT_FALLBACK = \"/content/drive/MyDrive/AI_PD_Project/Datasets/D1-NeuroVoz-Castillan Spanish/preprocessed_v1\"\n",
    "D2_OUT_ROOT_FALLBACK = \"/content/drive/MyDrive/AI_PD_Project/Datasets/D2-Slovak (EWA-DB)/EWA-DB/preprocessed_v1\"\n",
    "\n",
    "D1_OUT_ROOT = globals().get(\"D1_OUT_ROOT\", D1_OUT_ROOT_FALLBACK)\n",
    "D2_OUT_ROOT = globals().get(\"D2_OUT_ROOT\", D2_OUT_ROOT_FALLBACK)\n",
    "\n",
    "D2_MANIFEST_ALL = f\"{D2_OUT_ROOT}/manifests/manifest_all.csv\"\n",
    "\n",
    "# D1 monolingual summary (used to read the fixed transfer threshold)\n",
    "D1_MONO_SUMMARY = Path(D1_OUT_ROOT) / \"monolingual_test_runs\" / \"summary_test.json\"\n",
    "\n",
    "# -------------------------\n",
    "# Run settings (kept consistent with other test cells)\n",
    "# Inputs: constants below\n",
    "# Output: printed configuration + runtime behavior (batching, AMP, device)\n",
    "# -------------------------\n",
    "TRANSFER_TAG    = \"ES_to_SK\"          # D1 -> D2\n",
    "SEEDS           = [1337, 2024, 7777]\n",
    "BACKBONE_CKPT   = \"facebook/wav2vec2-base\"\n",
    "SR_EXPECTED     = 16000\n",
    "TINY_THRESH     = 1e-4\n",
    "\n",
    "EFFECTIVE_BS    = 64\n",
    "PER_DEVICE_BS   = 16\n",
    "GRAD_ACCUM      = max(1, EFFECTIVE_BS // PER_DEVICE_BS)  # printed only\n",
    "\n",
    "DROPOUT_P       = 0.2\n",
    "\n",
    "NUM_WORKERS     = 0\n",
    "PIN_MEMORY      = False\n",
    "\n",
    "USE_AMP         = True\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "try:\n",
    "    torch.set_float32_matmul_precision(\"high\")\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", message=\"None of the inputs have requires_grad=True. Gradients will be None\")\n",
    "warnings.filterwarnings(\"ignore\", message=\"Passing `gradient_checkpointing` to a config initialization is deprecated\")\n",
    "\n",
    "print(\"D2_OUT_ROOT (target):\", D2_OUT_ROOT)\n",
    "print(\"D2_MANIFEST_ALL:\", D2_MANIFEST_ALL)\n",
    "print(\"D1_OUT_ROOT (source):\", D1_OUT_ROOT)\n",
    "print(\"D1_MONO_SUMMARY:\", str(D1_MONO_SUMMARY))\n",
    "print(\"DEVICE:\", DEVICE)\n",
    "if DEVICE.type == \"cuda\":\n",
    "    print(\"GPU:\", torch.cuda.get_device_name(0))\n",
    "print(\"PER_DEVICE_BS:\", PER_DEVICE_BS, \"| EFFECTIVE_BS:\", PER_DEVICE_BS * GRAD_ACCUM)\n",
    "print(\"NUM_WORKERS:\", NUM_WORKERS, \"| PIN_MEMORY:\", PIN_MEMORY)\n",
    "print(\"USE_AMP:\", bool(USE_AMP and DEVICE.type == \"cuda\"))\n",
    "\n",
    "# -------------------------\n",
    "# Fixed threshold: read from D1 monolingual summary_test.json (schema-robust)\n",
    "# Inputs: D1_MONO_SUMMARY (JSON)\n",
    "# Output: FIXED_THR (float) used for all three seeds on D2\n",
    "# -------------------------\n",
    "def _dig(d, path):\n",
    "    cur = d\n",
    "    for k in path:\n",
    "        if not isinstance(cur, dict) or k not in cur:\n",
    "            return None\n",
    "        cur = cur[k]\n",
    "    return cur\n",
    "\n",
    "def load_fixed_threshold_from_summary(summary_dict: dict) -> float:\n",
    "    \"\"\"\n",
    "    Returns a finite float threshold.\n",
    "    Tries multiple key paths to stay compatible with older/newer summary schemas.\n",
    "\n",
    "    Preferred:\n",
    "      threshold_selection.val_selected_threshold_mean_sd.mean\n",
    "\n",
    "    Alternatives:\n",
    "      val_opt_threshold_mean_sd.mean\n",
    "      val_selected_threshold_mean_sd.mean\n",
    "      threshold_selection.val_opt_threshold_mean_sd.mean\n",
    "      mean of val_opt_threshold_by_seed / val_selected_threshold_by_seed\n",
    "    \"\"\"\n",
    "    candidate_paths = [\n",
    "        [\"threshold_selection\", \"val_selected_threshold_mean_sd\", \"mean\"],\n",
    "        [\"val_selected_threshold_mean_sd\", \"mean\"],\n",
    "        [\"threshold_selection\", \"val_opt_threshold_mean_sd\", \"mean\"],\n",
    "        [\"val_opt_threshold_mean_sd\", \"mean\"],\n",
    "        [\"val_optimal_threshold\", \"mean_sd\", \"mean\"],\n",
    "        [\"val_optimal_threshold_mean_sd\", \"mean\"],\n",
    "        [\"val_optimal_threshold\", \"mean\"],\n",
    "    ]\n",
    "\n",
    "    for p in candidate_paths:\n",
    "        v = _dig(summary_dict, p)\n",
    "        if v is None:\n",
    "            continue\n",
    "        try:\n",
    "            fv = float(v)\n",
    "            if np.isfinite(fv):\n",
    "                return fv\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    candidate_by_seed_paths = [\n",
    "        [\"threshold_selection\", \"val_selected_threshold_by_seed\"],\n",
    "        [\"val_selected_threshold_by_seed\"],\n",
    "        [\"threshold_selection\", \"val_opt_threshold_by_seed\"],\n",
    "        [\"val_opt_threshold_by_seed\"],\n",
    "        [\"val_optimal_threshold_by_seed\"],\n",
    "        [\"val_opt_threshold\", \"by_seed\"],\n",
    "    ]\n",
    "    for p in candidate_by_seed_paths:\n",
    "        d = _dig(summary_dict, p)\n",
    "        if isinstance(d, dict) and len(d) > 0:\n",
    "            vals = []\n",
    "            for _, vv in d.items():\n",
    "                try:\n",
    "                    vals.append(float(vv))\n",
    "                except Exception:\n",
    "                    continue\n",
    "            if len(vals) > 0:\n",
    "                fv = float(np.nanmean(np.asarray(vals, dtype=np.float64)))\n",
    "                if np.isfinite(fv):\n",
    "                    return fv\n",
    "\n",
    "    return float(\"nan\")\n",
    "\n",
    "if not D1_MONO_SUMMARY.exists():\n",
    "    raise FileNotFoundError(\n",
    "        \"Missing D1 monolingual summary_test.json.\\n\"\n",
    "        f\"Expected: {str(D1_MONO_SUMMARY)}\\n\"\n",
    "        \"Run the D1 monolingual test cell first (the one that writes summary_test.json).\"\n",
    "    )\n",
    "\n",
    "with open(D1_MONO_SUMMARY, \"r\", encoding=\"utf-8\") as f:\n",
    "    d1_sum = json.load(f)\n",
    "\n",
    "FIXED_THR = load_fixed_threshold_from_summary(d1_sum)\n",
    "\n",
    "if not np.isfinite(FIXED_THR):\n",
    "    raise RuntimeError(\n",
    "        \"Could not read a finite fixed threshold from D1 monolingual summary_test.json.\\n\"\n",
    "        \"Tried known key paths including:\\n\"\n",
    "        \"  threshold_selection.val_selected_threshold_mean_sd.mean (preferred)\\n\"\n",
    "        \"  val_opt_threshold_mean_sd.mean\\n\"\n",
    "        \"  and by-seed fallbacks.\\n\"\n",
    "        f\"File: {str(D1_MONO_SUMMARY)}\\n\"\n",
    "        \"Open that JSON and confirm where the mean VAL-opt threshold is stored.\"\n",
    "    )\n",
    "\n",
    "print(f\"\\nFixed transfer threshold (from D1 monolingual MEAN VAL-opt): {FIXED_THR:.6f}\")\n",
    "\n",
    "# -------------------------\n",
    "# Target manifest: load D2 and keep TEST only\n",
    "# Inputs: D2 manifest_all.csv\n",
    "# Output: test_df with required columns + basic counts printed\n",
    "# -------------------------\n",
    "if not os.path.exists(D2_MANIFEST_ALL):\n",
    "    raise FileNotFoundError(f\"Missing D2 manifest_all.csv: {D2_MANIFEST_ALL}\")\n",
    "\n",
    "m_all = pd.read_csv(D2_MANIFEST_ALL)\n",
    "\n",
    "# Require these fields for inference + fairness plots\n",
    "req_cols = {\"split\", \"clip_path\", \"label_num\", \"task\", \"sex\"}\n",
    "missing = [c for c in sorted(req_cols) if c not in m_all.columns]\n",
    "if missing:\n",
    "    raise ValueError(f\"D2 manifest missing required columns: {missing}. Found: {list(m_all.columns)}\")\n",
    "\n",
    "# Ensure the manifest content is actually D2\n",
    "if \"dataset\" in m_all.columns and m_all[\"dataset\"].notna().any():\n",
    "    dataset_id = str(m_all[\"dataset\"].astype(str).value_counts(dropna=True).idxmax())\n",
    "    m_all = m_all[m_all[\"dataset\"].astype(str) == dataset_id].copy()\n",
    "else:\n",
    "    dataset_id = \"DX\"\n",
    "\n",
    "if dataset_id != \"D2\":\n",
    "    raise RuntimeError(\n",
    "        f\"Expected dataset_id=='D2' but got {dataset_id!r}. \"\n",
    "        \"This usually means D2_OUT_ROOT is wrong or inherited from a previous cell.\\n\"\n",
    "        f\"D2_OUT_ROOT={D2_OUT_ROOT}\"\n",
    "    )\n",
    "\n",
    "# Keep a stable column set (missing ones are filled with NaN)\n",
    "keep_cols = [\"clip_path\", \"label_num\", \"task\", \"speaker_id\", \"sex\", \"age\", \"duration_sec\", \"split\"]\n",
    "for c in keep_cols:\n",
    "    if c not in m_all.columns:\n",
    "        m_all[c] = np.nan\n",
    "m_all = m_all[keep_cols].copy()\n",
    "\n",
    "test_df = m_all[m_all[\"split\"].isin([\"test\"])].copy().reset_index(drop=True)\n",
    "\n",
    "print(f\"\\nTarget dataset inferred: {dataset_id}\")\n",
    "print(f\"TEST rows: {len(test_df)}\")\n",
    "if len(test_df) == 0:\n",
    "    raise RuntimeError(\"After filtering to split=='test', D2 manifest has 0 rows.\")\n",
    "\n",
    "print(\"TEST label counts:\", test_df[\"label_num\"].value_counts(dropna=False).to_dict())\n",
    "print(\"TEST sex counts (raw):\", test_df[\"sex\"].value_counts(dropna=False).to_dict())\n",
    "\n",
    "# -------------------------\n",
    "# Path check: ensure target audio files exist\n",
    "# Inputs: test_df.clip_path\n",
    "# Output: raises early if any clip files are missing (shows a few examples)\n",
    "# -------------------------\n",
    "def _fail_fast_missing_paths(df: pd.DataFrame, name: str):\n",
    "    missing_paths = []\n",
    "    for p in tqdm(df[\"clip_path\"].astype(str).tolist(), desc=f\"Check {name} clip_path exists\", dynamic_ncols=True):\n",
    "        if not os.path.exists(p):\n",
    "            missing_paths.append(p)\n",
    "            if len(missing_paths) >= 10:\n",
    "                break\n",
    "    if missing_paths:\n",
    "        raise FileNotFoundError(f\"{name}: missing clip_path(s). Examples: {missing_paths}\")\n",
    "\n",
    "_fail_fast_missing_paths(test_df, \"TEST\")\n",
    "\n",
    "# -------------------------\n",
    "# Task grouping: map each row to vowel vs other\n",
    "# Inputs: test_df.task\n",
    "# Output: test_df.task_group used to pick the correct head during inference\n",
    "# -------------------------\n",
    "def _task_group(task_val) -> str:\n",
    "    return \"vowel\" if str(task_val) == \"vowl\" else \"other\"\n",
    "\n",
    "test_df[\"task_group\"] = test_df[\"task\"].apply(_task_group)\n",
    "\n",
    "# -------------------------\n",
    "# Sex normalization for fairness/charts (D2 uses male/female)\n",
    "# Inputs: test_df.sex\n",
    "# Output: test_df.sex_norm in {M,F,UNK}\n",
    "# -------------------------\n",
    "def normalize_sex_d2(val) -> str:\n",
    "    \"\"\"\n",
    "    Returns 'M', 'F', or 'UNK'\n",
    "    D2 expected encoding: 'male'/'female' (case-insensitive).\n",
    "    Also handles M/F and 0/1 defensively.\n",
    "    \"\"\"\n",
    "    if pd.isna(val):\n",
    "        return \"UNK\"\n",
    "\n",
    "    # Defensive numeric mapping (only used if sex is stored as 0/1)\n",
    "    try:\n",
    "        fv = float(val)\n",
    "        if np.isfinite(fv) and abs(fv - round(fv)) < 1e-9:\n",
    "            iv = int(round(fv))\n",
    "            if iv == 0:\n",
    "                return \"F\"\n",
    "            if iv == 1:\n",
    "                return \"M\"\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    s = str(val).strip().lower()\n",
    "    if s in {\"m\", \"male\", \"man\", \"masc\", \"masculine\"}:\n",
    "        return \"M\"\n",
    "    if s in {\"f\", \"female\", \"woman\", \"fem\", \"feminine\"}:\n",
    "        return \"F\"\n",
    "    return \"UNK\"\n",
    "\n",
    "test_df[\"sex_norm\"] = test_df[\"sex\"].apply(normalize_sex_d2)\n",
    "print(\"TEST sex counts (normalized):\", test_df[\"sex_norm\"].value_counts(dropna=False).to_dict())\n",
    "if (test_df[\"sex_norm\"] == \"UNK\").any():\n",
    "    print(\"NOTE: Some D2 'sex' values could not be normalized to M/F and were counted as 'UNK' for fairness and sex charts.\")\n",
    "\n",
    "# -------------------------\n",
    "# Dataset + collator: read audio and build attention masks\n",
    "# Inputs: test_df rows + audio files\n",
    "# Output: DataLoader batches with padded input_values, attention_mask, labels, task_group, sex_norm\n",
    "# -------------------------\n",
    "class AudioManifestDataset(Dataset):\n",
    "    def __init__(self, df: pd.DataFrame):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        row = self.df.iloc[idx]\n",
    "        clip_path = str(row[\"clip_path\"])\n",
    "        label = int(row[\"label_num\"])\n",
    "        task_group = str(row[\"task_group\"])\n",
    "        sex_norm = str(row[\"sex_norm\"])\n",
    "\n",
    "        if not os.path.exists(clip_path):\n",
    "            raise FileNotFoundError(f\"Missing clip_path: {clip_path}\")\n",
    "\n",
    "        y, sr = sf.read(clip_path, always_2d=False)\n",
    "        if isinstance(y, np.ndarray) and y.ndim > 1:\n",
    "            y = np.mean(y, axis=1)\n",
    "        y = np.asarray(y, dtype=np.float32)\n",
    "\n",
    "        if int(sr) != SR_EXPECTED:\n",
    "            raise ValueError(f\"Sample rate mismatch (expected {SR_EXPECTED}): {clip_path} has sr={sr}\")\n",
    "\n",
    "        # attention_mask mainly matters for vowel clips (ignore padded tail)\n",
    "        attn = np.ones((len(y),), dtype=np.int64)\n",
    "        if task_group == \"vowel\":\n",
    "            k = -1\n",
    "            for j in range(len(y) - 1, -1, -1):\n",
    "                if abs(float(y[j])) > TINY_THRESH:\n",
    "                    k = j\n",
    "                    break\n",
    "            if k >= 0:\n",
    "                attn[:k+1] = 1\n",
    "                attn[k+1:] = 0\n",
    "            else:\n",
    "                attn[:] = 1\n",
    "        else:\n",
    "            attn[:] = 1\n",
    "\n",
    "        return {\n",
    "            \"input_values\": torch.from_numpy(y),\n",
    "            \"attention_mask\": torch.from_numpy(attn),\n",
    "            \"labels\": torch.tensor(label, dtype=torch.long),\n",
    "            \"task_group\": task_group,\n",
    "            \"sex_norm\": sex_norm,\n",
    "        }\n",
    "\n",
    "def collate_fn(batch):\n",
    "    max_len = int(max(b[\"input_values\"].numel() for b in batch))\n",
    "    input_vals, attn_masks, labels, task_groups, sex_norms = [], [], [], [], []\n",
    "    for b in batch:\n",
    "        x = b[\"input_values\"]\n",
    "        a = b[\"attention_mask\"]\n",
    "        pad = max_len - x.numel()\n",
    "        if pad > 0:\n",
    "            x = torch.cat([x, torch.zeros(pad, dtype=x.dtype)], dim=0)\n",
    "            a = torch.cat([a, torch.zeros(pad, dtype=a.dtype)], dim=0)\n",
    "        input_vals.append(x)\n",
    "        attn_masks.append(a)\n",
    "        labels.append(b[\"labels\"])\n",
    "        task_groups.append(b[\"task_group\"])\n",
    "        sex_norms.append(b[\"sex_norm\"])\n",
    "    return {\n",
    "        \"input_values\": torch.stack(input_vals, dim=0),\n",
    "        \"attention_mask\": torch.stack(attn_masks, dim=0),\n",
    "        \"labels\": torch.stack(labels, dim=0),\n",
    "        \"task_group\": task_groups,\n",
    "        \"sex_norm\": sex_norms,\n",
    "    }\n",
    "\n",
    "# -------------------------\n",
    "# Model: frozen wav2vec2 backbone + two heads (vowel vs other)\n",
    "# Inputs: backbone checkpoint name + dropout_p\n",
    "# Output: logits for PD vs healthy for each clip, using the correct head per task_group\n",
    "# -------------------------\n",
    "class Wav2Vec2TwoHeadClassifier(nn.Module):\n",
    "    def __init__(self, ckpt: str, dropout_p: float):\n",
    "        super().__init__()\n",
    "        self.backbone = Wav2Vec2Model.from_pretrained(ckpt, use_safetensors=True)\n",
    "        for p in self.backbone.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "        H = int(self.backbone.config.hidden_size)\n",
    "        self.pre_vowel = nn.Sequential(nn.LayerNorm(H), nn.Dropout(float(dropout_p)))\n",
    "        self.pre_other = nn.Sequential(nn.LayerNorm(H), nn.Dropout(float(dropout_p)))\n",
    "        self.head_vowel = nn.Linear(H, 2)\n",
    "        self.head_other = nn.Linear(H, 2)\n",
    "\n",
    "    def masked_mean_pool(self, last_hidden, attn_mask_samples):\n",
    "        feat_mask = self.backbone._get_feature_vector_attention_mask(last_hidden.shape[1], attn_mask_samples)\n",
    "        feat_mask = feat_mask.to(last_hidden.device).unsqueeze(-1).type_as(last_hidden)\n",
    "        masked = last_hidden * feat_mask\n",
    "        denom = feat_mask.sum(dim=1).clamp(min=1.0)\n",
    "        return masked.sum(dim=1) / denom\n",
    "\n",
    "    def _heads_fp32(self, x_fp_any: torch.Tensor, head: nn.Module) -> torch.Tensor:\n",
    "        # Keep head math in fp32 even when AMP is enabled\n",
    "        x = x_fp_any.float()\n",
    "        if x.is_cuda:\n",
    "            with torch.amp.autocast(device_type=\"cuda\", enabled=False):\n",
    "                return head(x)\n",
    "        return head(x)\n",
    "\n",
    "    def forward_logits(self, input_values, attention_mask, task_group):\n",
    "        with torch.no_grad():\n",
    "            out = self.backbone(input_values=input_values, attention_mask=attention_mask)\n",
    "            last_hidden = out.last_hidden_state\n",
    "\n",
    "        pooled = self.masked_mean_pool(last_hidden, attention_mask)  # [B,H]\n",
    "\n",
    "        z_v = self.pre_vowel(pooled.float())\n",
    "        z_o = self.pre_other(pooled.float())\n",
    "\n",
    "        logits_v = self._heads_fp32(z_v, self.head_vowel)\n",
    "        logits_o = self._heads_fp32(z_o, self.head_other)\n",
    "\n",
    "        # Select the correct head per row based on task_group\n",
    "        is_vowel = torch.tensor([tg == \"vowel\" for tg in task_group], device=pooled.device, dtype=torch.bool)\n",
    "        logits = logits_o.clone()\n",
    "        if is_vowel.any():\n",
    "            logits[is_vowel] = logits_v[is_vowel]\n",
    "        return logits\n",
    "\n",
    "# -------------------------\n",
    "# Metrics + plot helpers\n",
    "# Inputs: y_true and y_prob (PD probability), plus a threshold for thresholded metrics\n",
    "# Output: numeric metrics dict + PNG charts saved to disk\n",
    "# -------------------------\n",
    "def compute_auc(y_true, y_prob):\n",
    "    y_true = np.asarray(y_true, dtype=np.int64)\n",
    "    y_prob = np.asarray(y_prob, dtype=np.float64)\n",
    "    if len(np.unique(y_true)) < 2:\n",
    "        return float(\"nan\")\n",
    "    return float(roc_auc_score(y_true, y_prob))\n",
    "\n",
    "def compute_threshold_metrics(y_true, y_prob, thr=0.5):\n",
    "    y_true = np.asarray(y_true, dtype=np.int64)\n",
    "    y_prob = np.asarray(y_prob, dtype=np.float64)\n",
    "    y_pred = (y_prob >= float(thr)).astype(np.int64)\n",
    "\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=[0, 1])\n",
    "    TN, FP, FN, TP = int(cm[0, 0]), int(cm[0, 1]), int(cm[1, 0]), int(cm[1, 1])\n",
    "\n",
    "    eps = 1e-12\n",
    "    acc = (TP + TN) / max(1, (TP + TN + FP + FN))\n",
    "    prec = TP / (TP + FP + eps)\n",
    "    rec = TP / (TP + FN + eps)     # sensitivity\n",
    "    f1 = 2 * prec * rec / (prec + rec + eps)\n",
    "    spec = TN / (TN + FP + eps)\n",
    "\n",
    "    try:\n",
    "        mcc = float(matthews_corrcoef(y_true, y_pred)) if len(np.unique(y_pred)) > 1 else float(\"nan\")\n",
    "    except Exception:\n",
    "        mcc = float(\"nan\")\n",
    "\n",
    "    try:\n",
    "        _, pval = fisher_exact([[TN, FP], [FN, TP]], alternative=\"two-sided\")\n",
    "        pval = float(pval)\n",
    "    except Exception:\n",
    "        pval = float(\"nan\")\n",
    "\n",
    "    return {\n",
    "        \"threshold\": float(thr),\n",
    "        \"confusion_matrix\": {\"TN\": TN, \"FP\": FP, \"FN\": FN, \"TP\": TP},\n",
    "        \"accuracy\": float(acc),\n",
    "        \"precision\": float(prec),\n",
    "        \"recall\": float(rec),\n",
    "        \"f1_score\": float(f1),\n",
    "        \"sensitivity\": float(rec),\n",
    "        \"specificity\": float(spec),\n",
    "        \"mcc\": float(mcc),\n",
    "        \"p_value_fisher_two_sided\": float(pval),\n",
    "    }\n",
    "\n",
    "def save_roc_curve_png(y_true, y_prob, out_png, title_suffix=\"Test\"):\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_prob)\n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr)\n",
    "    plt.plot([0, 1], [0, 1], linestyle=\"--\")\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(f\"ROC Curve ({title_suffix})\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_png, dpi=150)\n",
    "    plt.close()\n",
    "\n",
    "def save_confusion_png(y_true, y_prob, out_png, thr=0.5, title_suffix=\"Test\"):\n",
    "    y_true = np.asarray(y_true, dtype=np.int64)\n",
    "    y_prob = np.asarray(y_prob, dtype=np.float64)\n",
    "    y_pred = (y_prob >= float(thr)).astype(int)\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=[0, 1])\n",
    "\n",
    "    plt.figure()\n",
    "    plt.imshow(cm)\n",
    "    plt.xticks([0, 1], [\"Healthy(0)\", \"PD(1)\"])\n",
    "    plt.yticks([0, 1], [\"Healthy(0)\", \"PD(1)\"])\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"True\")\n",
    "    plt.title(f\"Confusion Matrix ({title_suffix}, thr={thr:.2f})\")\n",
    "    for (i, j), v in np.ndenumerate(cm):\n",
    "        plt.text(j, i, str(v), ha=\"center\", va=\"center\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_png, dpi=150)\n",
    "    plt.close()\n",
    "\n",
    "# -------------------------\n",
    "# Fairness: FNR per sex and ΔFNR = FNR(F) - FNR(M)\n",
    "# Inputs: y_true, y_prob, sex_norm array, threshold\n",
    "# Output: per-group FNR details + signed and absolute gap\n",
    "# -------------------------\n",
    "def compute_fnr_by_group_signed(y_true, y_prob, groups, thr=0.5):\n",
    "    y_true = np.asarray(y_true, dtype=np.int64)\n",
    "    y_prob = np.asarray(y_prob, dtype=np.float64)\n",
    "    groups = np.asarray(list(groups), dtype=object)\n",
    "    y_pred = (y_prob >= float(thr)).astype(np.int64)\n",
    "\n",
    "    out = {}\n",
    "    for g in sorted(set(groups.tolist())):\n",
    "        mask_g = (groups == g)\n",
    "        if int(mask_g.sum()) == 0:\n",
    "            continue\n",
    "\n",
    "        pos_mask = mask_g & (y_true == 1)\n",
    "        n_pos = int(pos_mask.sum())\n",
    "        if n_pos == 0:\n",
    "            out[g] = {\"n_total\": int(mask_g.sum()), \"n_pos\": 0, \"tp\": 0, \"fn\": 0, \"fnr\": float(\"nan\")}\n",
    "            continue\n",
    "\n",
    "        tp = int(((y_pred == 1) & pos_mask).sum())\n",
    "        fn = int(((y_pred == 0) & pos_mask).sum())\n",
    "        fnr = float(fn / max(1, (fn + tp)))\n",
    "\n",
    "        out[g] = {\"n_total\": int(mask_g.sum()), \"n_pos\": int(n_pos), \"tp\": int(tp), \"fn\": int(fn), \"fnr\": float(fnr)}\n",
    "\n",
    "    fnr_m = out.get(\"M\", {}).get(\"fnr\", float(\"nan\"))\n",
    "    fnr_f = out.get(\"F\", {}).get(\"fnr\", float(\"nan\"))\n",
    "\n",
    "    if (not np.isnan(fnr_m)) and (not np.isnan(fnr_f)):\n",
    "        delta_signed = float(fnr_f - fnr_m)\n",
    "        delta_abs = float(abs(delta_signed))\n",
    "    else:\n",
    "        delta_signed = float(\"nan\")\n",
    "        delta_abs = float(\"nan\")\n",
    "\n",
    "    return out, delta_signed, delta_abs\n",
    "\n",
    "def compute_confusion_counts(y_true, y_prob, thr=0.5):\n",
    "    y_true = np.asarray(y_true, dtype=np.int64)\n",
    "    y_prob = np.asarray(y_prob, dtype=np.float64)\n",
    "    y_pred = (y_prob >= float(thr)).astype(np.int64)\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=[0, 1])\n",
    "    TN, FP, FN, TP = int(cm[0, 0]), int(cm[0, 1]), int(cm[1, 0]), int(cm[1, 1])\n",
    "    return {\"TN\": TN, \"FP\": FP, \"FN\": FN, \"TP\": TP}\n",
    "\n",
    "def compute_confusion_by_group(y_true, y_prob, groups, thr=0.5):\n",
    "    y_true = np.asarray(y_true, dtype=np.int64)\n",
    "    y_prob = np.asarray(y_prob, dtype=np.float64)\n",
    "    groups = np.asarray(list(groups), dtype=object)\n",
    "    out = {}\n",
    "    for g in sorted(set(groups.tolist())):\n",
    "        mask = (groups == g)\n",
    "        if int(mask.sum()) == 0:\n",
    "            continue\n",
    "        out[g] = {\"n\": int(mask.sum()), \"confusion\": compute_confusion_counts(y_true[mask], y_prob[mask], thr=thr)}\n",
    "    return out\n",
    "\n",
    "# -------------------------\n",
    "# Reproducibility: set all RNG seeds\n",
    "# Inputs: seed int\n",
    "# Output: deterministic settings for random, numpy, torch\n",
    "# -------------------------\n",
    "def set_all_seeds(seed: int):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# -------------------------\n",
    "# Source checkpoint selection: pick most recent D1 trainval exp with all seeds\n",
    "# Inputs: D1 trainval_runs folder\n",
    "# Output: chosen_exp folder used to load best_heads.pt per seed\n",
    "# -------------------------\n",
    "D1_TRAINVAL_ROOT = Path(D1_OUT_ROOT) / \"trainval_runs\"\n",
    "if not D1_TRAINVAL_ROOT.exists():\n",
    "    raise FileNotFoundError(f\"Missing D1 trainval_runs folder: {str(D1_TRAINVAL_ROOT)}\")\n",
    "\n",
    "exp_dirs = sorted([p for p in D1_TRAINVAL_ROOT.glob(\"exp_*\") if p.is_dir()], key=lambda p: p.stat().st_mtime, reverse=True)\n",
    "if not exp_dirs:\n",
    "    raise FileNotFoundError(f\"No exp_* folders found under: {str(D1_TRAINVAL_ROOT)}\")\n",
    "\n",
    "def _has_all_seeds(exp_path: Path, seeds: list):\n",
    "    for s in seeds:\n",
    "        p = exp_path / f\"run_D1_seed{s}\" / \"best_heads.pt\"\n",
    "        if not p.exists():\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "chosen_exp = None\n",
    "for ed in exp_dirs:\n",
    "    if _has_all_seeds(ed, SEEDS):\n",
    "        chosen_exp = ed\n",
    "        break\n",
    "\n",
    "if chosen_exp is None:\n",
    "    sample = exp_dirs[0]\n",
    "    raise FileNotFoundError(\n",
    "        \"Could not find a D1 trainval experiment with all 3 best_heads.pt files.\\n\"\n",
    "        f\"Expected: {str(D1_TRAINVAL_ROOT)}/exp_*/run_D1_seedXXXX/best_heads.pt\\n\"\n",
    "        f\"Most recent exp checked: {str(sample)}\"\n",
    "    )\n",
    "\n",
    "print(\"\\nUsing SOURCE (D1) Train+Val experiment folder:\")\n",
    "print(\" \", str(chosen_exp))\n",
    "\n",
    "# -------------------------\n",
    "# Output folder: write only under D2\n",
    "# Inputs: D2_OUT_ROOT\n",
    "# Output: ZS_ROOT folder for per-seed runs + summary files\n",
    "# -------------------------\n",
    "ZS_ROOT = Path(D2_OUT_ROOT) / \"Cross_Language_Zero_Shot_Runs\"\n",
    "ZS_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# -------------------------\n",
    "# DataLoader: D2 TEST only (plus a small warm-up read)\n",
    "# Inputs: test_df and audio files\n",
    "# Output: test_loader ready for inference\n",
    "# -------------------------\n",
    "test_ds = AudioManifestDataset(test_df)\n",
    "test_loader = DataLoader(\n",
    "    test_ds,\n",
    "    batch_size=PER_DEVICE_BS,\n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=PIN_MEMORY,\n",
    "    collate_fn=collate_fn,\n",
    ")\n",
    "\n",
    "print(\"\\nWarm-up: loading up to 2 TEST batches...\")\n",
    "t0 = time.time()\n",
    "nb = len(test_loader)\n",
    "wb = min(2, nb)\n",
    "if wb == 0:\n",
    "    raise RuntimeError(\"TEST DataLoader has 0 batches. Check test_df length and PER_DEVICE_BS.\")\n",
    "it = iter(test_loader)\n",
    "for i in range(wb):\n",
    "    _ = next(it)\n",
    "    print(f\"  loaded TEST warmup batch {i+1}/{wb}\")\n",
    "print(f\"Warm-up done in {time.time()-t0:.2f}s\")\n",
    "\n",
    "# -------------------------\n",
    "# Checkpoint loader: load only the trained heads from best_heads.pt\n",
    "# Inputs: model instance + best_heads.pt path\n",
    "# Output: model with head weights restored for this seed\n",
    "# -------------------------\n",
    "def load_heads_into_model(model, best_heads_path: Path):\n",
    "    if not best_heads_path.exists():\n",
    "        raise FileNotFoundError(f\"Missing best_heads.pt: {str(best_heads_path)}\")\n",
    "    state = torch.load(str(best_heads_path), map_location=\"cpu\")\n",
    "    needed = [\"pre_vowel\", \"pre_other\", \"head_vowel\", \"head_other\"]\n",
    "    missing = [k for k in needed if k not in state]\n",
    "    if missing:\n",
    "        raise KeyError(\n",
    "            f\"best_heads.pt missing keys {missing}. Found keys: {list(state.keys())}. \"\n",
    "            \"This zero-shot code expects the trainval save format.\"\n",
    "        )\n",
    "    model.pre_vowel.load_state_dict(state[\"pre_vowel\"], strict=True)\n",
    "    model.pre_other.load_state_dict(state[\"pre_other\"], strict=True)\n",
    "    model.head_vowel.load_state_dict(state[\"head_vowel\"], strict=True)\n",
    "    model.head_other.load_state_dict(state[\"head_other\"], strict=True)\n",
    "    return model\n",
    "\n",
    "# -------------------------\n",
    "# Inference: run model over D2 test_loader and collect probabilities\n",
    "# Inputs: loader + model\n",
    "# Output: arrays (y_true, y_prob, sex_norm) for metrics and plots\n",
    "# -------------------------\n",
    "def run_inference(loader, model, desc):\n",
    "    use_amp = bool(USE_AMP and DEVICE.type == \"cuda\")\n",
    "    all_probs, all_true, all_sex = [], [], []\n",
    "\n",
    "    pbar = tqdm(loader, desc=desc, dynamic_ncols=True)\n",
    "    with torch.inference_mode():\n",
    "        for batch in pbar:\n",
    "            input_values = batch[\"input_values\"].to(DEVICE, non_blocking=False)\n",
    "            attention_mask = batch[\"attention_mask\"].to(DEVICE, non_blocking=False)\n",
    "            labels = batch[\"labels\"].to(DEVICE, non_blocking=False)\n",
    "            task_group = batch[\"task_group\"]\n",
    "            sex_norm = batch[\"sex_norm\"]\n",
    "\n",
    "            with torch.amp.autocast(device_type=\"cuda\", enabled=use_amp):\n",
    "                logits = model.forward_logits(input_values, attention_mask, task_group)\n",
    "\n",
    "            probs = torch.softmax(logits.float(), dim=-1)[:, 1].detach().cpu().numpy().astype(np.float64)\n",
    "            all_probs.extend(probs.tolist())\n",
    "            all_true.extend(labels.detach().cpu().numpy().astype(np.int64).tolist())\n",
    "            all_sex.extend(list(sex_norm))\n",
    "\n",
    "    return np.asarray(all_true, dtype=np.int64), np.asarray(all_probs, dtype=np.float64), np.asarray(all_sex, dtype=object)\n",
    "\n",
    "# -------------------------\n",
    "# Per-seed run: load D1 heads, evaluate D2 at FIXED_THR, save metrics + PNGs\n",
    "# Inputs: seed, chosen_exp, FIXED_THR, D2 test_loader\n",
    "# Output: run_<TRANSFER_TAG>_seedXXXX folder with metrics.json + plots\n",
    "# -------------------------\n",
    "def run_seed(seed: int):\n",
    "    set_all_seeds(seed)\n",
    "\n",
    "    run_dir = ZS_ROOT / f\"run_{TRANSFER_TAG}_seed{seed}\"\n",
    "    run_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    best_heads_path = chosen_exp / f\"run_D1_seed{seed}\" / \"best_heads.pt\"\n",
    "\n",
    "    print(f\"\\n[seed={seed}] Loading SOURCE heads from:\")\n",
    "    print(\" \", str(best_heads_path))\n",
    "    print(f\"[seed={seed}] Evaluating TARGET D2 TEST @ fixed_thr={FIXED_THR:.6f}\")\n",
    "\n",
    "    model = Wav2Vec2TwoHeadClassifier(BACKBONE_CKPT, dropout_p=DROPOUT_P).to(DEVICE)\n",
    "    model = load_heads_into_model(model, best_heads_path)\n",
    "    model.eval()\n",
    "\n",
    "    yt, pt, st = run_inference(test_loader, model, desc=f\"[seed={seed}] D2 TEST (zero-shot)\")\n",
    "\n",
    "    test_auc = compute_auc(yt, pt)  # threshold-free\n",
    "    thr_metrics = compute_threshold_metrics(yt, pt, thr=FIXED_THR)\n",
    "\n",
    "    fnr_by_sex, delta_f_minus_m, delta_abs = compute_fnr_by_group_signed(yt, pt, st, thr=FIXED_THR)\n",
    "    confusion_by_sex = compute_confusion_by_group(yt, pt, st, thr=FIXED_THR)\n",
    "\n",
    "    # Plots: always save overall ROC + confusion matrix\n",
    "    roc_png = run_dir / \"roc_curve.png\"\n",
    "    cm_png  = run_dir / \"confusion_matrix.png\"\n",
    "    save_roc_curve_png(yt, pt, str(roc_png), title_suffix=f\"D2 Test (seed={seed})\")\n",
    "    save_confusion_png(yt, pt, str(cm_png), thr=FIXED_THR, title_suffix=f\"D2 Test (seed={seed})\")\n",
    "\n",
    "    # Sex-specific confusion matrices (only if that sex exists in D2 TEST)\n",
    "    cm_m_png = None\n",
    "    cm_f_png = None\n",
    "    mask_m = (st == \"M\")\n",
    "    mask_f = (st == \"F\")\n",
    "\n",
    "    if int(mask_m.sum()) > 0:\n",
    "        cm_m_png = run_dir / \"confusion_matrix_M.png\"\n",
    "        save_confusion_png(yt[mask_m], pt[mask_m], str(cm_m_png), thr=FIXED_THR, title_suffix=f\"D2 Test SEX=M (seed={seed})\")\n",
    "\n",
    "    if int(mask_f.sum()) > 0:\n",
    "        cm_f_png = run_dir / \"confusion_matrix_F.png\"\n",
    "        save_confusion_png(yt[mask_f], pt[mask_f], str(cm_f_png), thr=FIXED_THR, title_suffix=f\"D2 Test SEX=F (seed={seed})\")\n",
    "\n",
    "    metrics = {\n",
    "        \"transfer\": {\n",
    "            \"tag\": TRANSFER_TAG,\n",
    "            \"source_dataset\": \"D1\",\n",
    "            \"target_dataset\": \"D2\",\n",
    "            \"threshold_policy\": \"Fixed threshold = mean VAL-opt threshold from D1 monolingual test summary (applied to ALL seeds)\",\n",
    "            \"fixed_threshold_value\": float(FIXED_THR),\n",
    "            \"d1_mono_summary_path\": str(D1_MONO_SUMMARY),\n",
    "        },\n",
    "\n",
    "        \"target\": {\n",
    "            \"dataset\": \"D2\",\n",
    "            \"seed\": int(seed),\n",
    "            \"n_test\": int(len(test_df)),\n",
    "            \"label_counts_test\": test_df[\"label_num\"].value_counts(dropna=False).to_dict(),\n",
    "            \"sex_counts_test_norm\": test_df[\"sex_norm\"].value_counts(dropna=False).to_dict(),\n",
    "        },\n",
    "\n",
    "        \"test_auroc\": float(test_auc),\n",
    "\n",
    "        \"threshold_metrics_test\": thr_metrics,\n",
    "        \"test_threshold_used\": float(FIXED_THR),\n",
    "\n",
    "        \"fairness_test\": {\n",
    "            \"definition\": \"H3: ΔFNR = FNR(F) - FNR(M), where FNR(sex)=FN/(FN+TP) computed on PD-only true labels at fixed transfer threshold.\",\n",
    "            \"fnr_by_sex_norm\": fnr_by_sex,\n",
    "            \"delta_fnr_F_minus_M\": float(delta_f_minus_m),\n",
    "            \"delta_fnr_abs\": float(delta_abs),\n",
    "            \"note\": \"If n_PD for a sex is 0, its FNR is NaN and ΔFNR is NaN.\",\n",
    "            \"sex_normalization_note\": \"D2 mapping: male->M, female->F; otherwise UNK (case-insensitive).\",\n",
    "        },\n",
    "\n",
    "        \"confusion_by_sex_norm\": confusion_by_sex,\n",
    "\n",
    "        \"artifacts\": {\n",
    "            \"roc_curve_png\": str(roc_png),\n",
    "            \"confusion_matrix_png\": str(cm_png),\n",
    "            \"confusion_matrix_M_png\": str(cm_m_png) if cm_m_png is not None else None,\n",
    "            \"confusion_matrix_F_png\": str(cm_f_png) if cm_f_png is not None else None,\n",
    "        },\n",
    "\n",
    "        \"paths\": {\n",
    "            \"d2_out_root\": str(D2_OUT_ROOT),\n",
    "            \"zero_shot_root\": str(ZS_ROOT),\n",
    "            \"run_dir\": str(run_dir),\n",
    "            \"source_trainval_experiment_used\": str(chosen_exp),\n",
    "            \"source_best_heads_path\": str(best_heads_path),\n",
    "        },\n",
    "\n",
    "        \"backbone_ckpt\": BACKBONE_CKPT,\n",
    "        \"dropout_p\": float(DROPOUT_P),\n",
    "        \"timestamp\": time.strftime(\"%Y%m%d_%H%M%S\"),\n",
    "    }\n",
    "\n",
    "    metrics_path = run_dir / \"metrics.json\"\n",
    "    with open(metrics_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(metrics, f, indent=2)\n",
    "\n",
    "    print(f\"[seed={seed}] DONE | test_AUROC={test_auc:.6f}\")\n",
    "\n",
    "    def _fmt_fnr(g):\n",
    "        d = fnr_by_sex.get(g, None)\n",
    "        if d is None:\n",
    "            return \"n/a\"\n",
    "        return f\"fnr={d['fnr']:.6f} (n_PD={d['n_pos']}, fn={d['fn']}, tp={d['tp']})\"\n",
    "\n",
    "    print(f\"[seed={seed}] TEST metrics @ fixed_thr={FIXED_THR:.6f}\")\n",
    "    print(f\"[seed={seed}] FAIRNESS (H3) @ fixed_thr={FIXED_THR:.6f}:\")\n",
    "    print(\"  M:\", _fmt_fnr(\"M\"))\n",
    "    print(\"  F:\", _fmt_fnr(\"F\"))\n",
    "    if \"UNK\" in fnr_by_sex:\n",
    "        print(\"  UNK:\", _fmt_fnr(\"UNK\"))\n",
    "    print(\"  ΔFNR (F-M):\", f\"{delta_f_minus_m:.6f}\" if not np.isnan(delta_f_minus_m) else \"nan\")\n",
    "    print(\"  |ΔFNR|:\", f\"{delta_abs:.6f}\" if not np.isnan(delta_abs) else \"nan\")\n",
    "\n",
    "    print(f\"[seed={seed}] WROTE:\")\n",
    "    print(\" \", str(metrics_path))\n",
    "    print(\" \", str(roc_png))\n",
    "    print(\" \", str(cm_png))\n",
    "    if cm_m_png is not None:\n",
    "        print(\" \", str(cm_m_png))\n",
    "    if cm_f_png is not None:\n",
    "        print(\" \", str(cm_f_png))\n",
    "\n",
    "    return {\n",
    "        \"seed\": int(seed),\n",
    "        \"test_auroc\": float(test_auc),\n",
    "        \"thr_metrics_test\": thr_metrics,\n",
    "        \"fnr_by_sex_test\": fnr_by_sex,\n",
    "        \"delta_f_minus_m_test\": float(delta_f_minus_m),\n",
    "        \"delta_abs_test\": float(delta_abs),\n",
    "        \"run_dir\": str(run_dir),\n",
    "    }\n",
    "\n",
    "# -------------------------\n",
    "# Aggregate across 3 seeds: AUROC mean ± 95% CI, others mean ± SD\n",
    "# Inputs: per-seed outputs from run_seed()\n",
    "# Output: summary_zero_shot.json + history_index.jsonl under ZS_ROOT\n",
    "# -------------------------\n",
    "seed_results = []\n",
    "for seed in SEEDS:\n",
    "    seed_results.append(run_seed(seed))\n",
    "\n",
    "aurocs = [r[\"test_auroc\"] for r in seed_results]\n",
    "t_crit = 4.302652729911275  # df=2, 95% CI\n",
    "n = len(aurocs)\n",
    "mean_auc = float(np.mean(aurocs))\n",
    "std_auc = float(np.std(aurocs, ddof=1)) if n > 1 else 0.0\n",
    "half_width = float(t_crit * (std_auc / math.sqrt(n))) if n > 1 else 0.0\n",
    "ci95 = [float(mean_auc - half_width), float(mean_auc + half_width)]\n",
    "\n",
    "def _mean_sd(vals):\n",
    "    vals = np.asarray(vals, dtype=np.float64)\n",
    "    return float(np.nanmean(vals)), float(np.nanstd(vals, ddof=1)) if np.sum(~np.isnan(vals)) > 1 else 0.0\n",
    "\n",
    "thr_list = [r[\"thr_metrics_test\"] for r in seed_results]\n",
    "keys = [\"accuracy\",\"precision\",\"recall\",\"f1_score\",\"sensitivity\",\"specificity\",\"mcc\",\"p_value_fisher_two_sided\"]\n",
    "\n",
    "agg = {}\n",
    "for k in keys:\n",
    "    v = [float(tm.get(k, float(\"nan\"))) for tm in thr_list]\n",
    "    mu, sd = _mean_sd(v)\n",
    "    agg[k] = {\n",
    "        \"mean\": mu,\n",
    "        \"sd\": sd,\n",
    "        \"values_by_seed\": {str(r[\"seed\"]): float(tm.get(k, float(\"nan\"))) for r, tm in zip(seed_results, thr_list)}\n",
    "    }\n",
    "\n",
    "cm_by_seed = {str(r[\"seed\"]): r[\"thr_metrics_test\"][\"confusion_matrix\"] for r in seed_results}\n",
    "\n",
    "# Fairness aggregation (H3): mean ± SD across seeds\n",
    "fnr_by_seed = {str(r[\"seed\"]): r[\"fnr_by_sex_test\"] for r in seed_results}\n",
    "delta_signed_by_seed = {str(r[\"seed\"]): float(r[\"delta_f_minus_m_test\"]) for r in seed_results}\n",
    "delta_abs_by_seed = {str(r[\"seed\"]): float(r[\"delta_abs_test\"]) for r in seed_results}\n",
    "\n",
    "fnr_m_vals, fnr_f_vals, n_pd_m_vals, n_pd_f_vals = [], [], [], []\n",
    "d_signed_vals, d_abs_vals = [], []\n",
    "\n",
    "for r in seed_results:\n",
    "    s = str(r[\"seed\"])\n",
    "    d = fnr_by_seed.get(s, {})\n",
    "    fnr_m_vals.append(float(d.get(\"M\", {}).get(\"fnr\", float(\"nan\"))))\n",
    "    fnr_f_vals.append(float(d.get(\"F\", {}).get(\"fnr\", float(\"nan\"))))\n",
    "    n_pd_m_vals.append(float(d.get(\"M\", {}).get(\"n_pos\", float(\"nan\"))))\n",
    "    n_pd_f_vals.append(float(d.get(\"F\", {}).get(\"n_pos\", float(\"nan\"))))\n",
    "    d_signed_vals.append(float(delta_signed_by_seed.get(s, float(\"nan\"))))\n",
    "    d_abs_vals.append(float(delta_abs_by_seed.get(s, float(\"nan\"))))\n",
    "\n",
    "fnr_m_mean, fnr_m_sd = _mean_sd(fnr_m_vals)\n",
    "fnr_f_mean, fnr_f_sd = _mean_sd(fnr_f_vals)\n",
    "d_signed_mean, d_signed_sd = _mean_sd(d_signed_vals)\n",
    "d_abs_mean, d_abs_sd = _mean_sd(d_abs_vals)\n",
    "\n",
    "print(\"\\nZero-shot TEST AUROC by seed:\")\n",
    "for r in seed_results:\n",
    "    print(f\"  seed {r['seed']}: {r['test_auroc']:.6f}\")\n",
    "print(f\"\\nMean Zero-shot TEST AUROC: {mean_auc:.6f}\")\n",
    "print(f\"95% CI (t, n=3): [{ci95[0]:.6f}, {ci95[1]:.6f}]\")\n",
    "\n",
    "print(f\"\\nFixed threshold used for ALL seeds (MEAN VAL-opt from D1 mono): {FIXED_THR:.6f}\")\n",
    "\n",
    "print(\"\\nThreshold metrics on D2 TEST @ fixed threshold (mean ± SD across seeds):\")\n",
    "for k in [\"accuracy\",\"precision\",\"sensitivity\",\"specificity\",\"f1_score\",\"mcc\"]:\n",
    "    mu = agg[k][\"mean\"]\n",
    "    sd = agg[k][\"sd\"]\n",
    "    print(f\"  {k}: {mu:.6f} ± {sd:.6f}\")\n",
    "print(\"  fisher_p_value_two_sided:\", f\"{agg['p_value_fisher_two_sided']['mean']:.6g} ± {agg['p_value_fisher_two_sided']['sd']:.6g}\")\n",
    "\n",
    "print(\"\\nFAIRNESS (H3) on D2 TEST @ fixed threshold (mean ± SD across seeds):\")\n",
    "print(f\"  FNR_M: {fnr_m_mean:.6f} ± {fnr_m_sd:.6f}\")\n",
    "print(f\"  FNR_F: {fnr_f_mean:.6f} ± {fnr_f_sd:.6f}\")\n",
    "print(f\"  ΔFNR (F-M): {d_signed_mean:.6f} ± {d_signed_sd:.6f}\")\n",
    "print(f\"  |ΔFNR|: {d_abs_mean:.6f} ± {d_abs_sd:.6f}\")\n",
    "\n",
    "summary = {\n",
    "    \"transfer\": {\n",
    "        \"tag\": TRANSFER_TAG,\n",
    "        \"source_dataset\": \"D1\",\n",
    "        \"target_dataset\": \"D2\",\n",
    "        \"fixed_threshold_value\": float(FIXED_THR),\n",
    "        \"fixed_threshold_source\": {\n",
    "            \"path\": str(D1_MONO_SUMMARY),\n",
    "            \"note\": \"Mean across seeds of D1 VAL-opt thresholds as recorded by the D1 monolingual test run.\",\n",
    "        },\n",
    "        \"source_trainval_experiment_used\": str(chosen_exp),\n",
    "    },\n",
    "\n",
    "    \"target\": {\n",
    "        \"dx_out_root\": str(D2_OUT_ROOT),\n",
    "        \"manifest_all\": str(D2_MANIFEST_ALL),\n",
    "        \"n_test\": int(len(test_df)),\n",
    "        \"label_counts_test\": test_df[\"label_num\"].value_counts(dropna=False).to_dict(),\n",
    "        \"sex_counts_test_norm\": test_df[\"sex_norm\"].value_counts(dropna=False).to_dict(),\n",
    "    },\n",
    "\n",
    "    \"seeds\": SEEDS,\n",
    "\n",
    "    \"test_aurocs_by_seed\": {str(r[\"seed\"]): float(r[\"test_auroc\"]) for r in seed_results},\n",
    "    \"mean_test_auroc\": float(mean_auc),\n",
    "    \"t_crit_df2_95\": float(t_crit),\n",
    "    \"half_width_95_ci\": float(half_width),\n",
    "    \"ci95_test_auroc\": ci95,\n",
    "\n",
    "    \"threshold_metrics_test_mean_sd\": agg,\n",
    "    \"confusion_matrix_by_seed\": cm_by_seed,\n",
    "\n",
    "    \"fairness_test\": {\n",
    "        \"definition\": \"H3: ΔFNR = FNR(F) - FNR(M), where FNR(sex)=FN/(FN+TP) computed on PD-only true labels at the fixed transfer threshold.\",\n",
    "        \"fnr_by_sex_norm_by_seed\": fnr_by_seed,\n",
    "        \"delta_fnr_F_minus_M_by_seed\": delta_signed_by_seed,\n",
    "        \"delta_fnr_abs_by_seed\": delta_abs_by_seed,\n",
    "        \"fnr_M_mean_sd\": {\"mean\": float(fnr_m_mean), \"sd\": float(fnr_m_sd), \"values_by_seed\": {str(s): float(v) for s, v in zip(SEEDS, fnr_m_vals)}},\n",
    "        \"fnr_F_mean_sd\": {\"mean\": float(fnr_f_mean), \"sd\": float(fnr_f_sd), \"values_by_seed\": {str(s): float(v) for s, v in zip(SEEDS, fnr_f_vals)}},\n",
    "        \"delta_fnr_F_minus_M_mean_sd\": {\"mean\": float(d_signed_mean), \"sd\": float(d_signed_sd), \"values_by_seed\": {str(s): float(v) for s, v in zip(SEEDS, d_signed_vals)}},\n",
    "        \"delta_fnr_abs_mean_sd\": {\"mean\": float(d_abs_mean), \"sd\": float(d_abs_sd), \"values_by_seed\": {str(s): float(v) for s, v in zip(SEEDS, d_abs_vals)}},\n",
    "        \"denominators_PD_by_seed\": {str(s): {\"n_PD_M\": float(n_pd_m_vals[i]), \"n_PD_F\": float(n_pd_f_vals[i])} for i, s in enumerate(SEEDS)},\n",
    "        \"sex_normalization_note\": \"D2 mapping: male->M, female->F; otherwise UNK. ΔFNR computed only when both M and F have defined FNR.\",\n",
    "    },\n",
    "\n",
    "    \"run_dirs\": [r[\"run_dir\"] for r in seed_results],\n",
    "    \"timestamp\": time.strftime(\"%Y%m%d_%H%M%S\"),\n",
    "}\n",
    "\n",
    "summary_path = ZS_ROOT / \"summary_zero_shot.json\"\n",
    "with open(summary_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "history_path = ZS_ROOT / \"history_index.jsonl\"\n",
    "with open(history_path, \"a\", encoding=\"utf-8\") as f:\n",
    "    f.write(json.dumps(summary) + \"\\n\")\n",
    "\n",
    "print(\"\\nWROTE summary:\", str(summary_path))\n",
    "print(\"APPENDED history index:\", str(history_path))\n",
    "print(\"Open this folder to access artifacts:\", str(ZS_ROOT))\n",
    "\n",
    "# -------------------------\n",
    "# Stop runtime (Colab)\n",
    "# Inputs: none\n",
    "# Output: attempts to release the GPU instance\n",
    "# -------------------------\n",
    "print(\"\\nAll done. Unassigning the runtime to stop the L4 instance...\")\n",
    "try:\n",
    "    from google.colab import runtime  # type: ignore\n",
    "    print(\"Calling runtime.unassign() now.\")\n",
    "    runtime.unassign()\n",
    "except Exception as e:\n",
    "    print(\"Could not unassign runtime automatically. You can stop the runtime manually in Colab.\")\n",
    "    print(\"Reason:\", repr(e))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "5b0826ac6bc24cf79c71a89d4f5d7bc1",
      "bec307a3118e486aa5d2f68e4edb803b",
      "f4238be4a8fb4e41afddd9846df4e037",
      "fdf5f91bf3c143aaa9d831416c7e87b7",
      "ec4332eb1d294264921dd1e6fc5de66c",
      "a44bcc219e524eec9b376af5a102d8d8",
      "e2384764ebc04807bba377c54050c479",
      "aba3efd7d80c4174a4e87c09086a1a15",
      "fa02d1d512124a5ca8d149811de67187",
      "36911210bd874bf88344b2cbb1077abc",
      "03ea4fa6034545d6a15a618115639e46",
      "be94b89e522e447eb67feff7106a6c7e",
      "fbd50ef613274c1fb8eb29cc0ad2169e",
      "211bb3ed29fc45e8a2e1049e64d5ea69",
      "fff0798c1faf42f499b29090ec0de86f",
      "71aae2a7b9f44f2b851e4fd6fb335d45",
      "2c8e597cd6af410eaf14e2948d117c3d",
      "12383f3a2a5f4b589a2315d0fd565113",
      "0a95f378cc9b456688568141c0e2d1f6",
      "17e3854dfe854347a325834847b1cfce",
      "14cd9d67fe884d96a2806abf23a0f08a",
      "c73637fc7c044aa3904d754cba6d83de",
      "9952de7f42ad4fd480b9b7e7f0461c5c",
      "a0082b7d07a3486fa62e48bf4a9fdd4d",
      "84a25b2091fc402d81c5c718e96654f8",
      "a86b9df1e6004eef87020793f66e50cc",
      "bce25260017245c5b41c97686cfe97cb",
      "169170464a91495690149661c855948d",
      "1828dff2b6844dbfbfb0f7f2bcf0d347",
      "fecf462003f4428690e0e02a7dbcbade",
      "c3cd019743964f52b57a373b79501f09",
      "b35db04dd3c74cfa9c64df4deb2da4f3",
      "9636b749476243a68720bb6d4add4311",
      "fb654faeabfe4dedba356aa9cb1428ff",
      "c47cb642693143d2b4c7d0957247accd",
      "983fa2bd8c034fb584df92e8a0579f0f",
      "29f6e716b0284b51b0841c1fbb78b650",
      "97b299e11d274d8199120c600978f08c",
      "d2a88050534841d1a3757674f142f849",
      "ed9aa6099a82448cb697d6cd55d7ce54",
      "d7f913b0975a4c4295d41dcc3a14cd57",
      "1e6531ce99fe43c7aeb70f131614f31b",
      "4ead3c2418e4491689b8df62a9341c75",
      "6f1dc582e7954078a77e4cc596f8c484",
      "04ab517ce4bb4615a485fd7c1a74c5c6",
      "d9a7730bec7f4b48a85f9459822ad3ca",
      "bf4e24de4ce64a8591232fe06edc2277",
      "9443e64bcf8a4e69a6d29ec7222eda50",
      "11c3daf39ea0458383dc371f43a482c7",
      "27ae06c814e64160ac7b97aea26474e4",
      "e22422a658df40489144c896dd322a17",
      "be65ee46b2b5412db6eed2a7a582de26",
      "c83be848c7294b30bace9d4e8c1f511d",
      "fc263ce74b944fc9b92a12438b346ec4",
      "c3180519bf4d4077b120016f6d49f5ad",
      "e3751108d1ab4a6aa905916bd62adb55",
      "6ad2dfab26654bc29c42d004e8149968",
      "4e5e925d92ae42018d496a14bb71a340",
      "b433e36bce124246949713cc3686e878",
      "f8de681d77784eac8969726e425c79ed",
      "4fadd4bff04d4c838bf2a71e3b9c9909",
      "1a21eb8f26c14d25b028441872e2be42",
      "03337fc271f04d91b09b081b311f0c66",
      "4add61e7d6374df997f9ef0de31c81e2",
      "818fc6f792964166808a806a63df02e9",
      "b482c3fcedb24351a9667b341202bf78"
     ]
    },
    "id": "2yzA4dquB8Wz"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "The following cell runs a cross language zero shot test from Italian (D4) to Slovak (D2). It evaluates whether classifier heads trained on the Italian dataset (D4, IPVS) can detect Parkinson’s disease in the Slovak dataset (D2, EWA-DB) without any training or fine tuning on D2. The evaluation uses only the D2 test split from `D2/manifests/manifest_all.csv`. The cell automatically finds the most recent completed D4 train and validation experiment that contains saved head checkpoints for the three seeds 1337, 2024, and 7777, and applies those D4 trained heads directly to D2 audio.\n",
    "\n",
    "A single fixed decision threshold is used for all three seeds. This threshold is read from D4’s saved `monolingual_test_runs/summary_test.json` and represents the mean validation selected threshold from D4. The code first looks for the preferred key `val_opt_threshold_mean_sd.mean`, and if that is not found, it checks known fallback locations used in other summary formats. This approach keeps the D2 evaluation independent by avoiding any threshold tuning on D2.\n",
    "\n",
    "The cell includes several setup and validation steps to prevent silent errors. It ensures Google Drive is mounted, checks that no local files are shadowing PyTorch or Transformers, prints key paths and the active compute device, confirms that the D2 manifest contains all required columns, verifies that the manifest belongs to dataset D2, filters strictly to the test split, and stops early if any listed audio files are missing. Each clip is assigned a simple task group, where `task == \"vowl\"` is treated as vowel and everything else is treated as other. Sex values in D2, such as male or female and common variants, are normalized to `sex_norm` values of M, F, or UNK so sex based reporting stays consistent even if unexpected values appear.\n",
    "\n",
    "For each seed, the cell loads the matching D4 head checkpoint (`best_heads.pt`) into a frozen Wav2Vec2 two head classifier, with one head for vowel clips and one for other speech. It then runs inference on all D2 test clips to produce a Parkinson’s probability for each clip. Using these scores, it computes overall AUROC, which does not depend on a threshold, and fixed threshold metrics at the D4 derived threshold. These metrics include the confusion matrix, accuracy, precision, sensitivity or recall, specificity, F1 score, MCC, and the Fisher exact test p value. Fairness is computed as ΔFNR, defined as FNR(F) minus FNR(M), measured on Parkinson’s positive clips only, along with confusion counts split by sex. For each seed, the cell saves a ROC curve, an overall confusion matrix, additional sex specific confusion matrices when both M and F are present, and a detailed `metrics.json` file under\n",
    "`<D2_OUT_ROOT>/Cross_Language_Zero_Shot_Runs/run_IT_to_SK_seed<seed>/`.\n",
    "\n",
    "After all three seeds finish, results are combined across seeds. The cell reports the mean AUROC with a 95 percent confidence interval using a t distribution with n equal to 3, along with the mean and standard deviation of the fixed threshold metrics and fairness values. A combined summary is saved as `summary_zero_shot2.json` and the same record is appended to `history_index.jsonl`, both under `<D2_OUT_ROOT>/Cross_Language_Zero_Shot_Runs/`. The cell ends by unassigning the Colab runtime to shut down the GPU."
   ],
   "metadata": {
    "id": "QWu-2Y-6zomD"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# =========================\n",
    "# IT → SK Zero-shot (D4 → D2): Test-only transfer with fixed threshold\n",
    "# - Loads D4-trained heads (most recent train+val experiment)\n",
    "# - Runs inference on D2 TEST only (from manifest_all.csv)\n",
    "# - Uses one fixed threshold read from D4 monolingual summary_test.json\n",
    "# - Saves per-seed metrics + plots, plus an aggregated summary under D2\n",
    "# =========================\n",
    "\n",
    "import os, json, math, random, time, warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import soundfile as sf\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from transformers import Wav2Vec2Model\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, confusion_matrix, matthews_corrcoef\n",
    "from scipy.stats import fisher_exact\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# -------------------------\n",
    "# Import safety: prevent local files from overriding torch/transformers\n",
    "# Inputs: /content directory\n",
    "# Output: raises early if a conflicting file/folder exists\n",
    "# -------------------------\n",
    "if os.path.exists(\"/content/torch.py\") or os.path.exists(\"/content/torch/__init__.py\"):\n",
    "    raise RuntimeError(\"Found local /content/torch.py or /content/torch/ that shadows PyTorch. Rename/remove it and restart runtime.\")\n",
    "if os.path.exists(\"/content/transformers.py\") or os.path.exists(\"/content/transformers/__init__.py\"):\n",
    "    raise RuntimeError(\"Found local /content/transformers.py or /content/transformers/ that shadows Hugging Face Transformers. Rename/remove it and restart runtime.\")\n",
    "\n",
    "# -------------------------\n",
    "# Drive mount (Colab)\n",
    "# Inputs: Google Drive\n",
    "# Output: /content/drive mounted for reading inputs and writing results\n",
    "# -------------------------\n",
    "if not os.path.isdir(\"/content/drive/MyDrive\"):\n",
    "    from google.colab import drive  # type: ignore\n",
    "    drive.mount(\"/content/drive\")\n",
    "\n",
    "# -------------------------\n",
    "# Paths: target (D2) + source (D4)\n",
    "# Inputs: optional globals D2_OUT_ROOT, D4_OUT_ROOT (if set earlier), else fallbacks\n",
    "# Output: D2_MANIFEST_ALL and D4_MONO_SUMMARY paths\n",
    "# -------------------------\n",
    "D2_OUT_ROOT_FALLBACK = \"/content/drive/MyDrive/AI_PD_Project/Datasets/D2-Slovak (EWA-DB)/EWA-DB/preprocessed_v1\"\n",
    "D4_OUT_ROOT_FALLBACK = \"/content/drive/MyDrive/AI_PD_Project/Datasets/D4-Italian (IPVS)/preprocessed_v1\"\n",
    "\n",
    "D2_OUT_ROOT = globals().get(\"D2_OUT_ROOT\", D2_OUT_ROOT_FALLBACK)\n",
    "D4_OUT_ROOT = globals().get(\"D4_OUT_ROOT\", D4_OUT_ROOT_FALLBACK)\n",
    "\n",
    "D2_MANIFEST_ALL = f\"{D2_OUT_ROOT}/manifests/manifest_all.csv\"\n",
    "\n",
    "# D4 monolingual summary (provides the fixed threshold source)\n",
    "D4_MONO_SUMMARY = Path(D4_OUT_ROOT) / \"monolingual_test_runs\" / \"summary_test.json\"\n",
    "\n",
    "# -------------------------\n",
    "# Run settings (kept consistent with other test cells)\n",
    "# Inputs: constants below\n",
    "# Output: printed configuration + runtime behavior (batching, AMP, device)\n",
    "# -------------------------\n",
    "TRANSFER_TAG    = \"IT_to_SK\"          # confirmed\n",
    "SEEDS           = [1337, 2024, 7777]\n",
    "BACKBONE_CKPT   = \"facebook/wav2vec2-base\"\n",
    "SR_EXPECTED     = 16000\n",
    "TINY_THRESH     = 1e-4\n",
    "\n",
    "EFFECTIVE_BS    = 64\n",
    "PER_DEVICE_BS   = 16\n",
    "GRAD_ACCUM      = max(1, EFFECTIVE_BS // PER_DEVICE_BS)  # printed only\n",
    "\n",
    "DROPOUT_P       = 0.2\n",
    "\n",
    "NUM_WORKERS     = 0\n",
    "PIN_MEMORY      = False\n",
    "\n",
    "USE_AMP         = True\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "try:\n",
    "    torch.set_float32_matmul_precision(\"high\")\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", message=\"None of the inputs have requires_grad=True. Gradients will be None\")\n",
    "warnings.filterwarnings(\"ignore\", message=\"Passing `gradient_checkpointing` to a config initialization is deprecated\")\n",
    "\n",
    "print(\"D2_OUT_ROOT (target):\", D2_OUT_ROOT)\n",
    "print(\"D2_MANIFEST_ALL:\", D2_MANIFEST_ALL)\n",
    "print(\"D4_OUT_ROOT (source):\", D4_OUT_ROOT)\n",
    "print(\"D4_MONO_SUMMARY:\", str(D4_MONO_SUMMARY))\n",
    "print(\"DEVICE:\", DEVICE)\n",
    "if DEVICE.type == \"cuda\":\n",
    "    print(\"GPU:\", torch.cuda.get_device_name(0))\n",
    "print(\"PER_DEVICE_BS:\", PER_DEVICE_BS, \"| EFFECTIVE_BS:\", PER_DEVICE_BS * GRAD_ACCUM)\n",
    "print(\"NUM_WORKERS:\", NUM_WORKERS, \"| PIN_MEMORY:\", PIN_MEMORY)\n",
    "print(\"USE_AMP:\", bool(USE_AMP and DEVICE.type == \"cuda\"))\n",
    "\n",
    "# -------------------------\n",
    "# Fixed threshold: read from D4 monolingual summary_test.json\n",
    "# Inputs: D4_MONO_SUMMARY (JSON)\n",
    "# Output: FIXED_THR (float) used for all three seeds on D2\n",
    "# -------------------------\n",
    "def _safe_float(x):\n",
    "    try:\n",
    "        v = float(x)\n",
    "        return v if np.isfinite(v) else float(\"nan\")\n",
    "    except Exception:\n",
    "        return float(\"nan\")\n",
    "\n",
    "if not D4_MONO_SUMMARY.exists():\n",
    "    raise FileNotFoundError(\n",
    "        \"Missing D4 monolingual summary_test.json.\\n\"\n",
    "        f\"Expected: {str(D4_MONO_SUMMARY)}\\n\"\n",
    "        \"Run the D4 monolingual test cell first (the one that writes summary_test.json).\"\n",
    "    )\n",
    "\n",
    "with open(D4_MONO_SUMMARY, \"r\", encoding=\"utf-8\") as f:\n",
    "    d4_sum = json.load(f)\n",
    "\n",
    "FIXED_THR = float(\"nan\")\n",
    "\n",
    "# Preferred: mean threshold already aggregated across seeds\n",
    "if isinstance(d4_sum.get(\"val_opt_threshold_mean_sd\"), dict):\n",
    "    FIXED_THR = _safe_float(d4_sum[\"val_opt_threshold_mean_sd\"].get(\"mean\", float(\"nan\")))\n",
    "\n",
    "# Fallback: compute mean from per-seed thresholds\n",
    "if not np.isfinite(FIXED_THR):\n",
    "    by_seed = d4_sum.get(\"val_opt_threshold_by_seed\", None)\n",
    "    if isinstance(by_seed, dict) and len(by_seed) > 0:\n",
    "        FIXED_THR = _safe_float(np.nanmean([_safe_float(v) for v in by_seed.values()]))\n",
    "\n",
    "# Optional fallback for alternate schema (if present)\n",
    "if not np.isfinite(FIXED_THR):\n",
    "    ts = d4_sum.get(\"threshold_selection\", None)\n",
    "    if isinstance(ts, dict) and isinstance(ts.get(\"val_selected_threshold_mean_sd\"), dict):\n",
    "        FIXED_THR = _safe_float(ts[\"val_selected_threshold_mean_sd\"].get(\"mean\", float(\"nan\")))\n",
    "\n",
    "if not np.isfinite(FIXED_THR):\n",
    "    raise RuntimeError(\n",
    "        \"Could not read a finite fixed threshold from D4 monolingual summary_test.json.\\n\"\n",
    "        \"Expected keys (preferred): val_opt_threshold_mean_sd.mean, or fallback val_opt_threshold_by_seed.\"\n",
    "    )\n",
    "\n",
    "print(f\"\\nFixed transfer threshold (mean D4 VAL-opt, applied to ALL seeds): {FIXED_THR:.6f}\")\n",
    "\n",
    "# -------------------------\n",
    "# Target manifest: load D2 and keep TEST only\n",
    "# Inputs: D2 manifest_all.csv\n",
    "# Output: test_df with required columns + basic counts printed\n",
    "# -------------------------\n",
    "if not os.path.exists(D2_MANIFEST_ALL):\n",
    "    raise FileNotFoundError(f\"Missing D2 manifest_all.csv: {D2_MANIFEST_ALL}\")\n",
    "\n",
    "m_all = pd.read_csv(D2_MANIFEST_ALL)\n",
    "\n",
    "# Require these fields for inference + fairness plots\n",
    "req_cols = {\"split\", \"clip_path\", \"label_num\", \"task\", \"sex\"}\n",
    "missing = [c for c in sorted(req_cols) if c not in m_all.columns]\n",
    "if missing:\n",
    "    raise ValueError(f\"D2 manifest missing required columns: {missing}. Found: {list(m_all.columns)}\")\n",
    "\n",
    "# Ensure the manifest content is actually D2\n",
    "if \"dataset\" in m_all.columns and m_all[\"dataset\"].notna().any():\n",
    "    dataset_id = str(m_all[\"dataset\"].astype(str).value_counts(dropna=True).idxmax())\n",
    "    m_all = m_all[m_all[\"dataset\"].astype(str) == dataset_id].copy()\n",
    "else:\n",
    "    dataset_id = \"DX\"\n",
    "\n",
    "if dataset_id != \"D2\":\n",
    "    raise RuntimeError(\n",
    "        f\"Expected dataset_id=='D2' but got {dataset_id!r}. \"\n",
    "        \"This usually means D2_OUT_ROOT is wrong or inherited from a previous cell.\\n\"\n",
    "        f\"D2_OUT_ROOT={D2_OUT_ROOT}\"\n",
    "    )\n",
    "\n",
    "# Keep a stable column set (missing ones are filled with NaN)\n",
    "keep_cols = [\"clip_path\", \"label_num\", \"task\", \"speaker_id\", \"sex\", \"age\", \"duration_sec\", \"split\"]\n",
    "for c in keep_cols:\n",
    "    if c not in m_all.columns:\n",
    "        m_all[c] = np.nan\n",
    "m_all = m_all[keep_cols].copy()\n",
    "\n",
    "test_df = m_all[m_all[\"split\"].isin([\"test\"])].copy().reset_index(drop=True)\n",
    "\n",
    "print(f\"\\nTarget dataset inferred: {dataset_id}\")\n",
    "print(f\"TEST rows: {len(test_df)}\")\n",
    "if len(test_df) == 0:\n",
    "    raise RuntimeError(\"After filtering to split=='test', D2 manifest has 0 rows.\")\n",
    "\n",
    "print(\"TEST label counts:\", test_df[\"label_num\"].value_counts(dropna=False).to_dict())\n",
    "print(\"TEST sex counts (raw):\", test_df[\"sex\"].value_counts(dropna=False).to_dict())\n",
    "\n",
    "# -------------------------\n",
    "# Path check: ensure target audio files exist\n",
    "# Inputs: test_df.clip_path\n",
    "# Output: raises early if any clip files are missing (shows a few examples)\n",
    "# -------------------------\n",
    "def _fail_fast_missing_paths(df: pd.DataFrame, name: str):\n",
    "    missing_paths = []\n",
    "    for p in tqdm(df[\"clip_path\"].astype(str).tolist(), desc=f\"Check {name} clip_path exists\", dynamic_ncols=True):\n",
    "        if not os.path.exists(p):\n",
    "            missing_paths.append(p)\n",
    "            if len(missing_paths) >= 10:\n",
    "                break\n",
    "    if missing_paths:\n",
    "        raise FileNotFoundError(f\"{name}: missing clip_path(s). Examples: {missing_paths}\")\n",
    "\n",
    "_fail_fast_missing_paths(test_df, \"TEST\")\n",
    "\n",
    "# -------------------------\n",
    "# Task grouping: map each row to vowel vs other\n",
    "# Inputs: test_df.task\n",
    "# Output: test_df.task_group used to pick the correct head during inference\n",
    "# -------------------------\n",
    "def _task_group(task_val) -> str:\n",
    "    return \"vowel\" if str(task_val) == \"vowl\" else \"other\"\n",
    "\n",
    "test_df[\"task_group\"] = test_df[\"task\"].apply(_task_group)\n",
    "\n",
    "# -------------------------\n",
    "# Sex normalization for fairness/charts (D2 uses male/female)\n",
    "# Inputs: test_df.sex\n",
    "# Output: test_df.sex_norm in {M,F,UNK}\n",
    "# -------------------------\n",
    "def normalize_sex_d2(val) -> str:\n",
    "    \"\"\"\n",
    "    Returns 'M', 'F', or 'UNK'\n",
    "    D2 encoding: male/female (strings)\n",
    "    Also handles common variants.\n",
    "    \"\"\"\n",
    "    if pd.isna(val):\n",
    "        return \"UNK\"\n",
    "    s = str(val).strip().lower()\n",
    "    if s in {\"m\", \"male\", \"man\", \"masc\", \"masculine\"}:\n",
    "        return \"M\"\n",
    "    if s in {\"f\", \"female\", \"woman\", \"fem\", \"feminine\"}:\n",
    "        return \"F\"\n",
    "    return \"UNK\"\n",
    "\n",
    "test_df[\"sex_norm\"] = test_df[\"sex\"].apply(normalize_sex_d2)\n",
    "print(\"TEST sex counts (normalized):\", test_df[\"sex_norm\"].value_counts(dropna=False).to_dict())\n",
    "if (test_df[\"sex_norm\"] == \"UNK\").any():\n",
    "    print(\"NOTE: Some D2 'sex' values could not be normalized to M/F and were counted as 'UNK' for fairness and sex charts.\")\n",
    "\n",
    "# -------------------------\n",
    "# Dataset + collator: read audio and build attention masks\n",
    "# Inputs: test_df rows + audio files\n",
    "# Output: DataLoader batches with padded input_values, attention_mask, labels, task_group, sex_norm\n",
    "# -------------------------\n",
    "class AudioManifestDataset(Dataset):\n",
    "    def __init__(self, df: pd.DataFrame):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        row = self.df.iloc[idx]\n",
    "        clip_path = str(row[\"clip_path\"])\n",
    "        label = int(row[\"label_num\"])\n",
    "        task_group = str(row[\"task_group\"])\n",
    "        sex_norm = str(row[\"sex_norm\"])\n",
    "\n",
    "        if not os.path.exists(clip_path):\n",
    "            raise FileNotFoundError(f\"Missing clip_path: {clip_path}\")\n",
    "\n",
    "        y, sr = sf.read(clip_path, always_2d=False)\n",
    "        if isinstance(y, np.ndarray) and y.ndim > 1:\n",
    "            y = np.mean(y, axis=1)\n",
    "        y = np.asarray(y, dtype=np.float32)\n",
    "\n",
    "        if int(sr) != SR_EXPECTED:\n",
    "            raise ValueError(f\"Sample rate mismatch (expected {SR_EXPECTED}): {clip_path} has sr={sr}\")\n",
    "\n",
    "        # attention_mask mainly matters for vowel clips (ignore padded tail)\n",
    "        attn = np.ones((len(y),), dtype=np.int64)\n",
    "        if task_group == \"vowel\":\n",
    "            k = -1\n",
    "            for j in range(len(y) - 1, -1, -1):\n",
    "                if abs(float(y[j])) > TINY_THRESH:\n",
    "                    k = j\n",
    "                    break\n",
    "            if k >= 0:\n",
    "                attn[:k+1] = 1\n",
    "                attn[k+1:] = 0\n",
    "            else:\n",
    "                attn[:] = 1\n",
    "        else:\n",
    "            attn[:] = 1\n",
    "\n",
    "        return {\n",
    "            \"input_values\": torch.from_numpy(y),\n",
    "            \"attention_mask\": torch.from_numpy(attn),\n",
    "            \"labels\": torch.tensor(label, dtype=torch.long),\n",
    "            \"task_group\": task_group,\n",
    "            \"sex_norm\": sex_norm,\n",
    "        }\n",
    "\n",
    "def collate_fn(batch):\n",
    "    max_len = int(max(b[\"input_values\"].numel() for b in batch))\n",
    "    input_vals, attn_masks, labels, task_groups, sex_norms = [], [], [], [], []\n",
    "    for b in batch:\n",
    "        x = b[\"input_values\"]\n",
    "        a = b[\"attention_mask\"]\n",
    "        pad = max_len - x.numel()\n",
    "        if pad > 0:\n",
    "            x = torch.cat([x, torch.zeros(pad, dtype=x.dtype)], dim=0)\n",
    "            a = torch.cat([a, torch.zeros(pad, dtype=a.dtype)], dim=0)\n",
    "        input_vals.append(x)\n",
    "        attn_masks.append(a)\n",
    "        labels.append(b[\"labels\"])\n",
    "        task_groups.append(b[\"task_group\"])\n",
    "        sex_norms.append(b[\"sex_norm\"])\n",
    "    return {\n",
    "        \"input_values\": torch.stack(input_vals, dim=0),\n",
    "        \"attention_mask\": torch.stack(attn_masks, dim=0),\n",
    "        \"labels\": torch.stack(labels, dim=0),\n",
    "        \"task_group\": task_groups,\n",
    "        \"sex_norm\": sex_norms,\n",
    "    }\n",
    "\n",
    "# -------------------------\n",
    "# Model: frozen wav2vec2 backbone + two heads (vowel vs other)\n",
    "# Inputs: backbone checkpoint name + dropout_p\n",
    "# Output: logits for PD vs healthy for each clip, using the correct head per task_group\n",
    "# -------------------------\n",
    "class Wav2Vec2TwoHeadClassifier(nn.Module):\n",
    "    def __init__(self, ckpt: str, dropout_p: float):\n",
    "        super().__init__()\n",
    "        self.backbone = Wav2Vec2Model.from_pretrained(ckpt, use_safetensors=True)\n",
    "        for p in self.backbone.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "        H = int(self.backbone.config.hidden_size)\n",
    "        self.pre_vowel = nn.Sequential(nn.LayerNorm(H), nn.Dropout(float(dropout_p)))\n",
    "        self.pre_other = nn.Sequential(nn.LayerNorm(H), nn.Dropout(float(dropout_p)))\n",
    "        self.head_vowel = nn.Linear(H, 2)\n",
    "        self.head_other = nn.Linear(H, 2)\n",
    "\n",
    "    def masked_mean_pool(self, last_hidden, attn_mask_samples):\n",
    "        feat_mask = self.backbone._get_feature_vector_attention_mask(last_hidden.shape[1], attn_mask_samples)\n",
    "        feat_mask = feat_mask.to(last_hidden.device).unsqueeze(-1).type_as(last_hidden)\n",
    "        masked = last_hidden * feat_mask\n",
    "        denom = feat_mask.sum(dim=1).clamp(min=1.0)\n",
    "        return masked.sum(dim=1) / denom\n",
    "\n",
    "    def _heads_fp32(self, x_fp_any: torch.Tensor, head: nn.Module) -> torch.Tensor:\n",
    "        # Keep head math in fp32 even when AMP is enabled\n",
    "        x = x_fp_any.float()\n",
    "        if x.is_cuda:\n",
    "            with torch.amp.autocast(device_type=\"cuda\", enabled=False):\n",
    "                return head(x)\n",
    "        return head(x)\n",
    "\n",
    "    def forward_logits(self, input_values, attention_mask, task_group):\n",
    "        with torch.no_grad():\n",
    "            out = self.backbone(input_values=input_values, attention_mask=attention_mask)\n",
    "            last_hidden = out.last_hidden_state\n",
    "\n",
    "        pooled = self.masked_mean_pool(last_hidden, attention_mask)  # [B,H]\n",
    "\n",
    "        z_v = self.pre_vowel(pooled.float())\n",
    "        z_o = self.pre_other(pooled.float())\n",
    "\n",
    "        logits_v = self._heads_fp32(z_v, self.head_vowel)\n",
    "        logits_o = self._heads_fp32(z_o, self.head_other)\n",
    "\n",
    "        # Select the correct head per row based on task_group\n",
    "        is_vowel = torch.tensor([tg == \"vowel\" for tg in task_group], device=pooled.device, dtype=torch.bool)\n",
    "        logits = logits_o.clone()\n",
    "        if is_vowel.any():\n",
    "            logits[is_vowel] = logits_v[is_vowel]\n",
    "        return logits\n",
    "\n",
    "# -------------------------\n",
    "# Metrics + plot helpers\n",
    "# Inputs: y_true and y_prob (PD probability), plus a threshold for thresholded metrics\n",
    "# Output: numeric metrics dict + PNG charts saved to disk\n",
    "# -------------------------\n",
    "def compute_auc(y_true, y_prob):\n",
    "    y_true = np.asarray(y_true, dtype=np.int64)\n",
    "    y_prob = np.asarray(y_prob, dtype=np.float64)\n",
    "    if len(np.unique(y_true)) < 2:\n",
    "        return float(\"nan\")\n",
    "    return float(roc_auc_score(y_true, y_prob))\n",
    "\n",
    "def compute_threshold_metrics(y_true, y_prob, thr=0.5):\n",
    "    y_true = np.asarray(y_true, dtype=np.int64)\n",
    "    y_prob = np.asarray(y_prob, dtype=np.float64)\n",
    "    y_pred = (y_prob >= float(thr)).astype(np.int64)\n",
    "\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=[0, 1])\n",
    "    TN, FP, FN, TP = int(cm[0, 0]), int(cm[0, 1]), int(cm[1, 0]), int(cm[1, 1])\n",
    "\n",
    "    eps = 1e-12\n",
    "    acc = (TP + TN) / max(1, (TP + TN + FP + FN))\n",
    "    prec = TP / (TP + FP + eps)\n",
    "    rec = TP / (TP + FN + eps)     # sensitivity\n",
    "    f1 = 2 * prec * rec / (prec + rec + eps)\n",
    "    spec = TN / (TN + FP + eps)\n",
    "\n",
    "    try:\n",
    "        mcc = float(matthews_corrcoef(y_true, y_pred)) if len(np.unique(y_pred)) > 1 else float(\"nan\")\n",
    "    except Exception:\n",
    "        mcc = float(\"nan\")\n",
    "\n",
    "    try:\n",
    "        _, pval = fisher_exact([[TN, FP], [FN, TP]], alternative=\"two-sided\")\n",
    "        pval = float(pval)\n",
    "    except Exception:\n",
    "        pval = float(\"nan\")\n",
    "\n",
    "    return {\n",
    "        \"threshold\": float(thr),\n",
    "        \"confusion_matrix\": {\"TN\": TN, \"FP\": FP, \"FN\": FN, \"TP\": TP},\n",
    "        \"accuracy\": float(acc),\n",
    "        \"precision\": float(prec),\n",
    "        \"recall\": float(rec),\n",
    "        \"f1_score\": float(f1),\n",
    "        \"sensitivity\": float(rec),\n",
    "        \"specificity\": float(spec),\n",
    "        \"mcc\": float(mcc),\n",
    "        \"p_value_fisher_two_sided\": float(pval),\n",
    "    }\n",
    "\n",
    "def save_roc_curve_png(y_true, y_prob, out_png, title_suffix=\"Test\"):\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_prob)\n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr)\n",
    "    plt.plot([0, 1], [0, 1], linestyle=\"--\")\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(f\"ROC Curve ({title_suffix})\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_png, dpi=150)\n",
    "    plt.close()\n",
    "\n",
    "def save_confusion_png(y_true, y_prob, out_png, thr=0.5, title_suffix=\"Test\"):\n",
    "    y_true = np.asarray(y_true, dtype=np.int64)\n",
    "    y_prob = np.asarray(y_prob, dtype=np.float64)\n",
    "    y_pred = (y_prob >= float(thr)).astype(int)\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=[0, 1])\n",
    "\n",
    "    plt.figure()\n",
    "    plt.imshow(cm)\n",
    "    plt.xticks([0, 1], [\"Healthy(0)\", \"PD(1)\"])\n",
    "    plt.yticks([0, 1], [\"Healthy(0)\", \"PD(1)\"])\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"True\")\n",
    "    plt.title(f\"Confusion Matrix ({title_suffix}, thr={thr:.2f})\")\n",
    "    for (i, j), v in np.ndenumerate(cm):\n",
    "        plt.text(j, i, str(v), ha=\"center\", va=\"center\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_png, dpi=150)\n",
    "    plt.close()\n",
    "\n",
    "# -------------------------\n",
    "# Fairness: FNR per sex and ΔFNR = FNR(F) - FNR(M)\n",
    "# Inputs: y_true, y_prob, sex_norm array, threshold\n",
    "# Output: per-group FNR details + signed and absolute gap\n",
    "# -------------------------\n",
    "def compute_fnr_by_group_signed(y_true, y_prob, groups, thr=0.5):\n",
    "    y_true = np.asarray(y_true, dtype=np.int64)\n",
    "    y_prob = np.asarray(y_prob, dtype=np.float64)\n",
    "    groups = np.asarray(list(groups), dtype=object)\n",
    "    y_pred = (y_prob >= float(thr)).astype(np.int64)\n",
    "\n",
    "    out = {}\n",
    "    for g in sorted(set(groups.tolist())):\n",
    "        mask_g = (groups == g)\n",
    "        if int(mask_g.sum()) == 0:\n",
    "            continue\n",
    "\n",
    "        pos_mask = mask_g & (y_true == 1)\n",
    "        n_pos = int(pos_mask.sum())\n",
    "        if n_pos == 0:\n",
    "            out[g] = {\"n_total\": int(mask_g.sum()), \"n_pos\": 0, \"tp\": 0, \"fn\": 0, \"fnr\": float(\"nan\")}\n",
    "            continue\n",
    "\n",
    "        tp = int(((y_pred == 1) & pos_mask).sum())\n",
    "        fn = int(((y_pred == 0) & pos_mask).sum())\n",
    "        fnr = float(fn / max(1, (fn + tp)))\n",
    "\n",
    "        out[g] = {\"n_total\": int(mask_g.sum()), \"n_pos\": int(n_pos), \"tp\": int(tp), \"fn\": int(fn), \"fnr\": float(fnr)}\n",
    "\n",
    "    fnr_m = out.get(\"M\", {}).get(\"fnr\", float(\"nan\"))\n",
    "    fnr_f = out.get(\"F\", {}).get(\"fnr\", float(\"nan\"))\n",
    "\n",
    "    if (not np.isnan(fnr_m)) and (not np.isnan(fnr_f)):\n",
    "        delta_signed = float(fnr_f - fnr_m)\n",
    "        delta_abs = float(abs(delta_signed))\n",
    "    else:\n",
    "        delta_signed = float(\"nan\")\n",
    "        delta_abs = float(\"nan\")\n",
    "\n",
    "    return out, delta_signed, delta_abs\n",
    "\n",
    "def compute_confusion_counts(y_true, y_prob, thr=0.5):\n",
    "    y_true = np.asarray(y_true, dtype=np.int64)\n",
    "    y_prob = np.asarray(y_prob, dtype=np.float64)\n",
    "    y_pred = (y_prob >= float(thr)).astype(np.int64)\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=[0, 1])\n",
    "    TN, FP, FN, TP = int(cm[0, 0]), int(cm[0, 1]), int(cm[1, 0]), int(cm[1, 1])\n",
    "    return {\"TN\": TN, \"FP\": FP, \"FN\": FN, \"TP\": TP}\n",
    "\n",
    "def compute_confusion_by_group(y_true, y_prob, groups, thr=0.5):\n",
    "    y_true = np.asarray(y_true, dtype=np.int64)\n",
    "    y_prob = np.asarray(y_prob, dtype=np.float64)\n",
    "    groups = np.asarray(list(groups), dtype=object)\n",
    "    out = {}\n",
    "    for g in sorted(set(groups.tolist())):\n",
    "        mask = (groups == g)\n",
    "        if int(mask.sum()) == 0:\n",
    "            continue\n",
    "        out[g] = {\"n\": int(mask.sum()), \"confusion\": compute_confusion_counts(y_true[mask], y_prob[mask], thr=thr)}\n",
    "    return out\n",
    "\n",
    "# -------------------------\n",
    "# Reproducibility: set all RNG seeds\n",
    "# Inputs: seed int\n",
    "# Output: deterministic settings for random, numpy, torch\n",
    "# -------------------------\n",
    "def set_all_seeds(seed: int):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# -------------------------\n",
    "# Source checkpoint selection: pick most recent D4 trainval exp with all seeds\n",
    "# Inputs: D4 trainval_runs folder\n",
    "# Output: chosen_exp folder used to load best_heads.pt per seed\n",
    "# -------------------------\n",
    "D4_TRAINVAL_ROOT = Path(D4_OUT_ROOT) / \"trainval_runs\"\n",
    "if not D4_TRAINVAL_ROOT.exists():\n",
    "    raise FileNotFoundError(f\"Missing D4 trainval_runs folder: {str(D4_TRAINVAL_ROOT)}\")\n",
    "\n",
    "exp_dirs = sorted([p for p in D4_TRAINVAL_ROOT.glob(\"exp_*\") if p.is_dir()], key=lambda p: p.stat().st_mtime, reverse=True)\n",
    "if not exp_dirs:\n",
    "    raise FileNotFoundError(f\"No exp_* folders found under: {str(D4_TRAINVAL_ROOT)}\")\n",
    "\n",
    "def _has_all_seeds(exp_path: Path, seeds: list):\n",
    "    for s in seeds:\n",
    "        p = exp_path / f\"run_D4_seed{s}\" / \"best_heads.pt\"\n",
    "        if not p.exists():\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "chosen_exp = None\n",
    "for ed in exp_dirs:\n",
    "    if _has_all_seeds(ed, SEEDS):\n",
    "        chosen_exp = ed\n",
    "        break\n",
    "\n",
    "if chosen_exp is None:\n",
    "    sample = exp_dirs[0]\n",
    "    raise FileNotFoundError(\n",
    "        \"Could not find a D4 trainval experiment with all 3 best_heads.pt files.\\n\"\n",
    "        f\"Expected: {str(D4_TRAINVAL_ROOT)}/exp_*/run_D4_seedXXXX/best_heads.pt\\n\"\n",
    "        f\"Most recent exp checked: {str(sample)}\"\n",
    "    )\n",
    "\n",
    "print(\"\\nUsing SOURCE (D4) Train+Val experiment folder:\")\n",
    "print(\" \", str(chosen_exp))\n",
    "\n",
    "# -------------------------\n",
    "# Output folder: write only under D2\n",
    "# Inputs: D2_OUT_ROOT\n",
    "# Output: ZS_ROOT folder for per-seed runs + summary files\n",
    "# -------------------------\n",
    "ZS_ROOT = Path(D2_OUT_ROOT) / \"Cross_Language_Zero_Shot_Runs\"\n",
    "ZS_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# -------------------------\n",
    "# DataLoader: D2 TEST only (plus a small warm-up read)\n",
    "# Inputs: test_df and audio files\n",
    "# Output: test_loader ready for inference\n",
    "# -------------------------\n",
    "test_ds = AudioManifestDataset(test_df)\n",
    "test_loader = DataLoader(\n",
    "    test_ds,\n",
    "    batch_size=PER_DEVICE_BS,\n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=PIN_MEMORY,\n",
    "    collate_fn=collate_fn,\n",
    ")\n",
    "\n",
    "print(\"\\nWarm-up: loading up to 2 TEST batches...\")\n",
    "t0 = time.time()\n",
    "nb = len(test_loader)\n",
    "wb = min(2, nb)\n",
    "if wb == 0:\n",
    "    raise RuntimeError(\"TEST DataLoader has 0 batches. Check test_df length and PER_DEVICE_BS.\")\n",
    "it = iter(test_loader)\n",
    "for i in range(wb):\n",
    "    _ = next(it)\n",
    "    print(f\"  loaded TEST warmup batch {i+1}/{wb}\")\n",
    "print(f\"Warm-up done in {time.time()-t0:.2f}s\")\n",
    "\n",
    "# -------------------------\n",
    "# Checkpoint loader: load only the trained heads from best_heads.pt\n",
    "# Inputs: model instance + best_heads.pt path\n",
    "# Output: model with head weights restored for this seed\n",
    "# -------------------------\n",
    "def load_heads_into_model(model, best_heads_path: Path):\n",
    "    if not best_heads_path.exists():\n",
    "        raise FileNotFoundError(f\"Missing best_heads.pt: {str(best_heads_path)}\")\n",
    "    state = torch.load(str(best_heads_path), map_location=\"cpu\")\n",
    "    needed = [\"pre_vowel\", \"pre_other\", \"head_vowel\", \"head_other\"]\n",
    "    missing = [k for k in needed if k not in state]\n",
    "    if missing:\n",
    "        raise KeyError(\n",
    "            f\"best_heads.pt missing keys {missing}. Found keys: {list(state.keys())}. \"\n",
    "            \"This zero-shot code expects the trainval save format.\"\n",
    "        )\n",
    "    model.pre_vowel.load_state_dict(state[\"pre_vowel\"], strict=True)\n",
    "    model.pre_other.load_state_dict(state[\"pre_other\"], strict=True)\n",
    "    model.head_vowel.load_state_dict(state[\"head_vowel\"], strict=True)\n",
    "    model.head_other.load_state_dict(state[\"head_other\"], strict=True)\n",
    "    return model\n",
    "\n",
    "# -------------------------\n",
    "# Inference: run model over D2 test_loader and collect probabilities\n",
    "# Inputs: loader + model\n",
    "# Output: arrays (y_true, y_prob, sex_norm) for metrics and plots\n",
    "# -------------------------\n",
    "def run_inference(loader, model, desc):\n",
    "    use_amp = bool(USE_AMP and DEVICE.type == \"cuda\")\n",
    "    all_probs, all_true, all_sex = [], [], []\n",
    "\n",
    "    pbar = tqdm(loader, desc=desc, dynamic_ncols=True)\n",
    "    with torch.inference_mode():\n",
    "        for batch in pbar:\n",
    "            input_values = batch[\"input_values\"].to(DEVICE, non_blocking=False)\n",
    "            attention_mask = batch[\"attention_mask\"].to(DEVICE, non_blocking=False)\n",
    "            labels = batch[\"labels\"].to(DEVICE, non_blocking=False)\n",
    "            task_group = batch[\"task_group\"]\n",
    "            sex_norm = batch[\"sex_norm\"]\n",
    "\n",
    "            with torch.amp.autocast(device_type=\"cuda\", enabled=use_amp):\n",
    "                logits = model.forward_logits(input_values, attention_mask, task_group)\n",
    "\n",
    "            probs = torch.softmax(logits.float(), dim=-1)[:, 1].detach().cpu().numpy().astype(np.float64)\n",
    "            all_probs.extend(probs.tolist())\n",
    "            all_true.extend(labels.detach().cpu().numpy().astype(np.int64).tolist())\n",
    "            all_sex.extend(list(sex_norm))\n",
    "\n",
    "    return np.asarray(all_true, dtype=np.int64), np.asarray(all_probs, dtype=np.float64), np.asarray(all_sex, dtype=object)\n",
    "\n",
    "# -------------------------\n",
    "# Per-seed run: load D4 heads, evaluate D2 at FIXED_THR, save metrics + PNGs\n",
    "# Inputs: seed, chosen_exp, FIXED_THR, D2 test_loader\n",
    "# Output: run_<TRANSFER_TAG>_seedXXXX folder with metrics.json + plots\n",
    "# -------------------------\n",
    "def run_seed(seed: int):\n",
    "    set_all_seeds(seed)\n",
    "\n",
    "    run_dir = ZS_ROOT / f\"run_{TRANSFER_TAG}_seed{seed}\"\n",
    "    run_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    best_heads_path = chosen_exp / f\"run_D4_seed{seed}\" / \"best_heads.pt\"\n",
    "\n",
    "    print(f\"\\n[seed={seed}] Loading SOURCE heads from:\")\n",
    "    print(\" \", str(best_heads_path))\n",
    "    print(f\"[seed={seed}] Evaluating TARGET D2 TEST @ fixed_thr={FIXED_THR:.6f}\")\n",
    "\n",
    "    model = Wav2Vec2TwoHeadClassifier(BACKBONE_CKPT, dropout_p=DROPOUT_P).to(DEVICE)\n",
    "    model = load_heads_into_model(model, best_heads_path)\n",
    "    model.eval()\n",
    "\n",
    "    yt, pt, st = run_inference(test_loader, model, desc=f\"[seed={seed}] D2 TEST (zero-shot)\")\n",
    "\n",
    "    test_auc = compute_auc(yt, pt)  # threshold-free\n",
    "    thr_metrics = compute_threshold_metrics(yt, pt, thr=FIXED_THR)\n",
    "\n",
    "    fnr_by_sex, delta_f_minus_m, delta_abs = compute_fnr_by_group_signed(yt, pt, st, thr=FIXED_THR)\n",
    "    confusion_by_sex = compute_confusion_by_group(yt, pt, st, thr=FIXED_THR)\n",
    "\n",
    "    # Plots: always save overall ROC + confusion matrix\n",
    "    roc_png = run_dir / \"roc_curve.png\"\n",
    "    cm_png  = run_dir / \"confusion_matrix.png\"\n",
    "    save_roc_curve_png(yt, pt, str(roc_png), title_suffix=f\"D2 Test (seed={seed})\")\n",
    "    save_confusion_png(yt, pt, str(cm_png), thr=FIXED_THR, title_suffix=f\"D2 Test (seed={seed})\")\n",
    "\n",
    "    # Sex-specific confusion matrices (only if that sex exists in D2 TEST)\n",
    "    cm_m_png = None\n",
    "    cm_f_png = None\n",
    "    mask_m = (st == \"M\")\n",
    "    mask_f = (st == \"F\")\n",
    "\n",
    "    if int(mask_m.sum()) > 0:\n",
    "        cm_m_png = run_dir / \"confusion_matrix_M.png\"\n",
    "        save_confusion_png(yt[mask_m], pt[mask_m], str(cm_m_png), thr=FIXED_THR, title_suffix=f\"D2 Test SEX=M (seed={seed})\")\n",
    "\n",
    "    if int(mask_f.sum()) > 0:\n",
    "        cm_f_png = run_dir / \"confusion_matrix_F.png\"\n",
    "        save_confusion_png(yt[mask_f], pt[mask_f], str(cm_f_png), thr=FIXED_THR, title_suffix=f\"D2 Test SEX=F (seed={seed})\")\n",
    "\n",
    "    metrics = {\n",
    "        \"transfer\": {\n",
    "            \"tag\": TRANSFER_TAG,\n",
    "            \"source_dataset\": \"D4\",\n",
    "            \"target_dataset\": \"D2\",\n",
    "            \"threshold_policy\": \"Fixed threshold = mean D4 VAL-opt threshold from D4 monolingual test summary (applied to ALL seeds)\",\n",
    "            \"fixed_threshold_value\": float(FIXED_THR),\n",
    "            \"d4_mono_summary_path\": str(D4_MONO_SUMMARY),\n",
    "        },\n",
    "\n",
    "        \"target\": {\n",
    "            \"dataset\": \"D2\",\n",
    "            \"seed\": int(seed),\n",
    "            \"n_test\": int(len(test_df)),\n",
    "            \"label_counts_test\": test_df[\"label_num\"].value_counts(dropna=False).to_dict(),\n",
    "            \"sex_counts_test_norm\": test_df[\"sex_norm\"].value_counts(dropna=False).to_dict(),\n",
    "        },\n",
    "\n",
    "        \"test_auroc\": float(test_auc),\n",
    "\n",
    "        \"threshold_metrics_test\": thr_metrics,\n",
    "        \"test_threshold_used\": float(FIXED_THR),\n",
    "\n",
    "        \"fairness_test\": {\n",
    "            \"definition\": \"H3: ΔFNR = FNR(F) - FNR(M), where FNR(sex)=FN/(FN+TP) computed on PD-only true labels at fixed transfer threshold.\",\n",
    "            \"fnr_by_sex_norm\": fnr_by_sex,\n",
    "            \"delta_fnr_F_minus_M\": float(delta_f_minus_m),\n",
    "            \"delta_fnr_abs\": float(delta_abs),\n",
    "            \"note\": \"If n_PD for a sex is 0, its FNR is NaN and ΔFNR is NaN.\",\n",
    "            \"sex_normalization_note\": \"D2 mapping: male->M, female->F; otherwise UNK.\",\n",
    "        },\n",
    "\n",
    "        \"confusion_by_sex_norm\": confusion_by_sex,\n",
    "\n",
    "        \"artifacts\": {\n",
    "            \"roc_curve_png\": str(roc_png),\n",
    "            \"confusion_matrix_png\": str(cm_png),\n",
    "            \"confusion_matrix_M_png\": str(cm_m_png) if cm_m_png is not None else None,\n",
    "            \"confusion_matrix_F_png\": str(cm_f_png) if cm_f_png is not None else None,\n",
    "        },\n",
    "\n",
    "        \"paths\": {\n",
    "            \"d2_out_root\": str(D2_OUT_ROOT),\n",
    "            \"zero_shot_root\": str(ZS_ROOT),\n",
    "            \"run_dir\": str(run_dir),\n",
    "            \"source_trainval_experiment_used\": str(chosen_exp),\n",
    "            \"source_best_heads_path\": str(best_heads_path),\n",
    "        },\n",
    "\n",
    "        \"backbone_ckpt\": BACKBONE_CKPT,\n",
    "        \"dropout_p\": float(DROPOUT_P),\n",
    "        \"timestamp\": time.strftime(\"%Y%m%d_%H%M%S\"),\n",
    "    }\n",
    "\n",
    "    metrics_path = run_dir / \"metrics.json\"\n",
    "    with open(metrics_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(metrics, f, indent=2)\n",
    "\n",
    "    print(f\"[seed={seed}] DONE | test_AUROC={test_auc:.6f}\")\n",
    "\n",
    "    def _fmt_fnr(g):\n",
    "        d = fnr_by_sex.get(g, None)\n",
    "        if d is None:\n",
    "            return \"n/a\"\n",
    "        return f\"fnr={d['fnr']:.6f} (n_PD={d['n_pos']}, fn={d['fn']}, tp={d['tp']})\"\n",
    "\n",
    "    print(f\"[seed={seed}] TEST metrics @ fixed_thr={FIXED_THR:.6f}\")\n",
    "    print(f\"[seed={seed}] FAIRNESS (H3) @ fixed_thr={FIXED_THR:.6f}:\")\n",
    "    print(\"  M:\", _fmt_fnr(\"M\"))\n",
    "    print(\"  F:\", _fmt_fnr(\"F\"))\n",
    "    if \"UNK\" in fnr_by_sex:\n",
    "        print(\"  UNK:\", _fmt_fnr(\"UNK\"))\n",
    "    print(\"  ΔFNR (F-M):\", f\"{delta_f_minus_m:.6f}\" if not np.isnan(delta_f_minus_m) else \"nan\")\n",
    "    print(\"  |ΔFNR|:\", f\"{delta_abs:.6f}\" if not np.isnan(delta_abs) else \"nan\")\n",
    "\n",
    "    print(f\"[seed={seed}] WROTE:\")\n",
    "    print(\" \", str(metrics_path))\n",
    "    print(\" \", str(roc_png))\n",
    "    print(\" \", str(cm_png))\n",
    "    if cm_m_png is not None:\n",
    "        print(\" \", str(cm_m_png))\n",
    "    if cm_f_png is not None:\n",
    "        print(\" \", str(cm_f_png))\n",
    "\n",
    "    return {\n",
    "        \"seed\": int(seed),\n",
    "        \"test_auroc\": float(test_auc),\n",
    "        \"thr_metrics_test\": thr_metrics,\n",
    "        \"fnr_by_sex_test\": fnr_by_sex,\n",
    "        \"delta_f_minus_m_test\": float(delta_f_minus_m),\n",
    "        \"delta_abs_test\": float(delta_abs),\n",
    "        \"run_dir\": str(run_dir),\n",
    "    }\n",
    "\n",
    "# -------------------------\n",
    "# Aggregate across 3 seeds: AUROC mean ± 95% CI, others mean ± SD\n",
    "# Inputs: per-seed outputs from run_seed()\n",
    "# Output: summary_zero_shot2.json + history_index.jsonl under ZS_ROOT\n",
    "# -------------------------\n",
    "seed_results = []\n",
    "for seed in SEEDS:\n",
    "    seed_results.append(run_seed(seed))\n",
    "\n",
    "aurocs = [r[\"test_auroc\"] for r in seed_results]\n",
    "t_crit = 4.302652729911275  # df=2, 95% CI\n",
    "n = len(aurocs)\n",
    "mean_auc = float(np.mean(aurocs))\n",
    "std_auc = float(np.std(aurocs, ddof=1)) if n > 1 else 0.0\n",
    "half_width = float(t_crit * (std_auc / math.sqrt(n))) if n > 1 else 0.0\n",
    "ci95 = [float(mean_auc - half_width), float(mean_auc + half_width)]\n",
    "\n",
    "def _mean_sd(vals):\n",
    "    vals = np.asarray(vals, dtype=np.float64)\n",
    "    return float(np.nanmean(vals)), float(np.nanstd(vals, ddof=1)) if np.sum(~np.isnan(vals)) > 1 else 0.0\n",
    "\n",
    "thr_list = [r[\"thr_metrics_test\"] for r in seed_results]\n",
    "keys = [\"accuracy\",\"precision\",\"recall\",\"f1_score\",\"sensitivity\",\"specificity\",\"mcc\",\"p_value_fisher_two_sided\"]\n",
    "\n",
    "agg = {}\n",
    "for k in keys:\n",
    "    v = [float(tm.get(k, float(\"nan\"))) for tm in thr_list]\n",
    "    mu, sd = _mean_sd(v)\n",
    "    agg[k] = {\n",
    "        \"mean\": mu,\n",
    "        \"sd\": sd,\n",
    "        \"values_by_seed\": {str(r[\"seed\"]): float(tm.get(k, float(\"nan\"))) for r, tm in zip(seed_results, thr_list)}\n",
    "    }\n",
    "\n",
    "cm_by_seed = {str(r[\"seed\"]): r[\"thr_metrics_test\"][\"confusion_matrix\"] for r in seed_results}\n",
    "\n",
    "# Fairness aggregation (H3): mean ± SD across seeds\n",
    "fnr_by_seed = {str(r[\"seed\"]): r[\"fnr_by_sex_test\"] for r in seed_results}\n",
    "delta_signed_by_seed = {str(r[\"seed\"]): float(r[\"delta_f_minus_m_test\"]) for r in seed_results}\n",
    "delta_abs_by_seed = {str(r[\"seed\"]): float(r[\"delta_abs_test\"]) for r in seed_results}\n",
    "\n",
    "fnr_m_vals, fnr_f_vals, n_pd_m_vals, n_pd_f_vals = [], [], [], []\n",
    "d_signed_vals, d_abs_vals = [], []\n",
    "\n",
    "for r in seed_results:\n",
    "    s = str(r[\"seed\"])\n",
    "    d = fnr_by_seed.get(s, {})\n",
    "    fnr_m_vals.append(float(d.get(\"M\", {}).get(\"fnr\", float(\"nan\"))))\n",
    "    fnr_f_vals.append(float(d.get(\"F\", {}).get(\"fnr\", float(\"nan\"))))\n",
    "    n_pd_m_vals.append(float(d.get(\"M\", {}).get(\"n_pos\", float(\"nan\"))))\n",
    "    n_pd_f_vals.append(float(d.get(\"F\", {}).get(\"n_pos\", float(\"nan\"))))\n",
    "    d_signed_vals.append(float(delta_signed_by_seed.get(s, float(\"nan\"))))\n",
    "    d_abs_vals.append(float(delta_abs_by_seed.get(s, float(\"nan\"))))\n",
    "\n",
    "fnr_m_mean, fnr_m_sd = _mean_sd(fnr_m_vals)\n",
    "fnr_f_mean, fnr_f_sd = _mean_sd(fnr_f_vals)\n",
    "d_signed_mean, d_signed_sd = _mean_sd(d_signed_vals)\n",
    "d_abs_mean, d_abs_sd = _mean_sd(d_abs_vals)\n",
    "\n",
    "print(\"\\nZero-shot TEST AUROC by seed:\")\n",
    "for r in seed_results:\n",
    "    print(f\"  seed {r['seed']}: {r['test_auroc']:.6f}\")\n",
    "print(f\"\\nMean Zero-shot TEST AUROC: {mean_auc:.6f}\")\n",
    "print(f\"95% CI (t, n=3): [{ci95[0]:.6f}, {ci95[1]:.6f}]\")\n",
    "\n",
    "print(f\"\\nFixed threshold used for ALL seeds (mean D4 VAL-opt): {FIXED_THR:.6f}\")\n",
    "\n",
    "print(\"\\nThreshold metrics on D2 TEST @ fixed threshold (mean ± SD across seeds):\")\n",
    "for k in [\"accuracy\",\"precision\",\"sensitivity\",\"specificity\",\"f1_score\",\"mcc\"]:\n",
    "    mu = agg[k][\"mean\"]\n",
    "    sd = agg[k][\"sd\"]\n",
    "    print(f\"  {k}: {mu:.6f} ± {sd:.6f}\")\n",
    "print(\"  fisher_p_value_two_sided:\", f\"{agg['p_value_fisher_two_sided']['mean']:.6g} ± {agg['p_value_fisher_two_sided']['sd']:.6g}\")\n",
    "\n",
    "print(\"\\nFAIRNESS (H3) on D2 TEST @ fixed threshold (mean ± SD across seeds):\")\n",
    "print(f\"  FNR_M: {fnr_m_mean:.6f} ± {fnr_m_sd:.6f}\")\n",
    "print(f\"  FNR_F: {fnr_f_mean:.6f} ± {fnr_f_sd:.6f}\")\n",
    "print(f\"  ΔFNR (F-M): {d_signed_mean:.6f} ± {d_signed_sd:.6f}\")\n",
    "print(f\"  |ΔFNR|: {d_abs_mean:.6f} ± {d_abs_sd:.6f}\")\n",
    "\n",
    "summary = {\n",
    "    \"transfer\": {\n",
    "        \"tag\": TRANSFER_TAG,\n",
    "        \"source_dataset\": \"D4\",\n",
    "        \"target_dataset\": \"D2\",\n",
    "        \"fixed_threshold_value\": float(FIXED_THR),\n",
    "        \"fixed_threshold_source\": {\n",
    "            \"path\": str(D4_MONO_SUMMARY),\n",
    "            \"key\": \"val_opt_threshold_mean_sd.mean\",\n",
    "            \"note\": \"Mean D4 VAL-opt threshold recorded by D4 monolingual test run, applied to ALL seeds.\",\n",
    "        },\n",
    "        \"source_trainval_experiment_used\": str(chosen_exp),\n",
    "    },\n",
    "\n",
    "    \"target\": {\n",
    "        \"dx_out_root\": str(D2_OUT_ROOT),\n",
    "        \"manifest_all\": str(D2_MANIFEST_ALL),\n",
    "        \"n_test\": int(len(test_df)),\n",
    "        \"label_counts_test\": test_df[\"label_num\"].value_counts(dropna=False).to_dict(),\n",
    "        \"sex_counts_test_norm\": test_df[\"sex_norm\"].value_counts(dropna=False).to_dict(),\n",
    "    },\n",
    "\n",
    "    \"seeds\": SEEDS,\n",
    "\n",
    "    \"test_aurocs_by_seed\": {str(r[\"seed\"]): float(r[\"test_auroc\"]) for r in seed_results},\n",
    "    \"mean_test_auroc\": float(mean_auc),\n",
    "    \"t_crit_df2_95\": float(t_crit),\n",
    "    \"half_width_95_ci\": float(half_width),\n",
    "    \"ci95_test_auroc\": ci95,\n",
    "\n",
    "    \"threshold_metrics_test_mean_sd\": agg,\n",
    "    \"confusion_matrix_by_seed\": cm_by_seed,\n",
    "\n",
    "    \"fairness_test\": {\n",
    "        \"definition\": \"H3: ΔFNR = FNR(F) - FNR(M), where FNR(sex)=FN/(FN+TP) computed on PD-only true labels at the fixed transfer threshold.\",\n",
    "        \"fnr_by_sex_norm_by_seed\": fnr_by_seed,\n",
    "        \"delta_fnr_F_minus_M_by_seed\": delta_signed_by_seed,\n",
    "        \"delta_fnr_abs_by_seed\": delta_abs_by_seed,\n",
    "        \"fnr_M_mean_sd\": {\"mean\": float(fnr_m_mean), \"sd\": float(fnr_m_sd), \"values_by_seed\": {str(s): float(v) for s, v in zip(SEEDS, fnr_m_vals)}},\n",
    "        \"fnr_F_mean_sd\": {\"mean\": float(fnr_f_mean), \"sd\": float(fnr_f_sd), \"values_by_seed\": {str(s): float(v) for s, v in zip(SEEDS, fnr_f_vals)}},\n",
    "        \"delta_fnr_F_minus_M_mean_sd\": {\"mean\": float(d_signed_mean), \"sd\": float(d_signed_sd), \"values_by_seed\": {str(s): float(v) for s, v in zip(SEEDS, d_signed_vals)}},\n",
    "        \"delta_fnr_abs_mean_sd\": {\"mean\": float(d_abs_mean), \"sd\": float(d_abs_sd), \"values_by_seed\": {str(s): float(v) for s, v in zip(SEEDS, d_abs_vals)}},\n",
    "        \"denominators_PD_by_seed\": {str(s): {\"n_PD_M\": float(n_pd_m_vals[i]), \"n_PD_F\": float(n_pd_f_vals[i])} for i, s in enumerate(SEEDS)},\n",
    "        \"sex_normalization_note\": \"D2 mapping: male->M, female->F; otherwise UNK. ΔFNR computed only when both M and F have defined FNR.\",\n",
    "    },\n",
    "\n",
    "    \"run_dirs\": [r[\"run_dir\"] for r in seed_results],\n",
    "    \"timestamp\": time.strftime(\"%Y%m%d_%H%M%S\"),\n",
    "}\n",
    "\n",
    "# Save summary as requested\n",
    "summary_path = ZS_ROOT / \"summary_zero_shot2.json\"\n",
    "with open(summary_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "history_path = ZS_ROOT / \"history_index.jsonl\"\n",
    "with open(history_path, \"a\", encoding=\"utf-8\") as f:\n",
    "    f.write(json.dumps(summary) + \"\\n\")\n",
    "\n",
    "print(\"\\nWROTE summary:\", str(summary_path))\n",
    "print(\"APPENDED history index:\", str(history_path))\n",
    "print(\"Open this folder to access artifacts:\", str(ZS_ROOT))\n",
    "\n",
    "# -------------------------\n",
    "# Stop runtime (Colab)\n",
    "# Inputs: none\n",
    "# Output: attempts to release the GPU instance\n",
    "# -------------------------\n",
    "print(\"\\nAll done. Unassigning the runtime to stop the L4 instance...\")\n",
    "try:\n",
    "    from google.colab import runtime  # type: ignore\n",
    "    print(\"Calling runtime.unassign() now.\")\n",
    "    runtime.unassign()\n",
    "except Exception as e:\n",
    "    print(\"Could not unassign runtime automatically. You can stop the runtime manually in Colab.\")\n",
    "    print(\"Reason:\", repr(e))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "a0ddfc7fcf78486a94ae02904735be40",
      "7341946f4b79441d8590a8553874efb2",
      "51f418177db14d94a0f04f1938e33486",
      "6627870205584e5984fe4f561e242a02",
      "bb7e28974f1a4249b0d26e566e95d7a0",
      "7855a0bfac194e3eaabfaf49c52dfd66",
      "743e3bc85c314bdc98d86400f0191612",
      "c620e339f63b4e3984d7609c2be49d30",
      "c0984242463a401da6dd34495c848d0d",
      "7b4985a91bc34c90b881304aba892835",
      "c31289d2e7094737b01d0822bdc3de40",
      "fd06782b8d7e4b6bb98b7c34f5bed895",
      "d195306137da469ab3b525323d7ff3ff",
      "9873ee752a364377b2dd548fd02de583",
      "e22e05da99234df984c85288dc8be7f5",
      "16719e6edc84462db3bb43f23ac432ad",
      "3ed7a5596b254a76842bbda2e84c4ba6",
      "2f37bb16af7541d0a0587e6cb3657ece",
      "9c2a272e1657401bb2ee5732fdf31057",
      "d641a6ecb9044e1ca61fd12eb598bcab",
      "2115447e4b3342ebb7a10b9957d49e8f",
      "1f1ca269a67746d389a2429f12650114",
      "843235e487d843d2b4d63e8d5a03b68a",
      "3d3497aad724473d8d5401c6e01b4582",
      "1af305ffdc754b5f94b24505ed885522",
      "8b0aeddba24641feb43e60ad03fafc8a",
      "fdcb14ae2a624551a90ee33cc3564443",
      "418cb4b2e64b4f08885bfa8957c55565",
      "55cba840a0b14ebc8bfc6244ebeecd59",
      "7c85c5722879430882c06e12c1c5b468",
      "8a328d28820b4b60a45f2237f57d1ed5",
      "d000ff731d53464bb02e326e46ae60d9",
      "2ae1191fbb0144fab56d5f5656dadd5e",
      "4ae464a9bf6840a0a47680d018035c07",
      "df2454b70ef04ce598917fcb24fb3059",
      "03005435695244f29d531a0efdc6a668",
      "b70e1d33dafc48ceb20bf80a05a52ce2",
      "d605de50c3c445bc869e9bbd47b46202",
      "ff1ffc235138472a83cb1555d9cee83a",
      "184c02de47534fb7a7a289363267fbf6",
      "e64a4611df2a48d79eb68ae82a80f14f",
      "df7b63e546244593800d94f28ce96e85",
      "f4fc680d9fa3468ab91043bb5bd3d346",
      "17649dd60aed49d2a8c3cbf3191e95cb",
      "2391b23ae7a34b3cb139059e932b1432",
      "6e9efd18ec714a62b7afca3f0f0ad1ef",
      "71be9356928f44caa5b3fc75c9c09001",
      "74a750ce0fad4f3090e9259919a5671c",
      "e93ca91ce3a04c3397d3b245aae5c28f",
      "db7d77b6341a43578d0765c423189219",
      "dfb66ccb25cf48dcab2e39c8cb5af8f8",
      "72bb567f69014c1d8f438e02dad31f72",
      "1d3d7712c8534ecfbe840e96a49b4a6e",
      "2e61daa2c631440791fca60af7da3525",
      "74aef1b22e244dafba4aa1372c93d659",
      "cb5d3edf15134419baa424d61542a2a1",
      "b29f5c39d43540ddaceb3f63d66018df",
      "0a5d27c6a9e04b9bba27ed97846583a9",
      "5373e4a28d484851b7681133b82d2d7a",
      "70b1362becc646d285cae9b7e0101b96",
      "07ec65d9f62f4da7acf3127dc6896a99",
      "e2a6ffb62e1c48aca2fdc27d4581dc98",
      "08d84cdcacdc461dac73981952fb5607",
      "ba98c42d8a7d4fb591286f5090f932b5",
      "63643f2fd3f044be903ce81b9f2a47a6",
      "38169b6e1bb342cc83563ddc89630f4e"
     ]
    },
    "id": "Dn55MoB2GWZ1"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "The following cell runs a cross language zero shot test from English (UK, D5) to Slovak (D2). It checks whether classifier heads trained on the source dataset (D5, MDVR-KCL English) can detect Parkinson’s disease in the target dataset (D2, Slovak) without any retraining on D2. The evaluation uses only the D2 test split from `D2/manifests/manifest_all.csv`. The cell finds the most recent completed D5 train and validation experiment that contains saved head checkpoints for three seeds (1337, 2024, 7777) and applies those trained heads directly to D2 audio.\n",
    "\n",
    "A single fixed decision threshold is used for all seeds and all D2 test clips. This threshold is taken from D5’s previously saved `monolingual_test_runs/summary_test.json`. The preferred value is `val_opt_threshold_mean_sd.mean`, with a fallback to the mean of `val_opt_threshold_by_seed` if needed. This avoids tuning any threshold on D2 and keeps the transfer evaluation consistent and fair.\n",
    "\n",
    "Before running inference, the cell performs setup and safety checks. It makes sure Google Drive is mounted, confirms that no local files are shadowing PyTorch or Transformers, prints key paths and the active compute device, loads the D2 manifest, checks that all required columns exist, filters strictly to the test split, and stops early if any referenced audio files are missing. Each clip is assigned to a simple task group, where `task == \"vowl\"` is treated as vowel and everything else is treated as other. Sex values in D2, such as male or female and common variants, are normalized to `sex_norm` values of M, F, or UNK so sex based reporting remains consistent even if unexpected values appear.\n",
    "\n",
    "For each seed, the cell loads the matching D5 head checkpoint (`best_heads.pt`) into a frozen Wav2Vec2 two head classifier, with one head for vowel clips and one for other clips. It then runs inference on the full D2 test set to produce a Parkinson’s probability for each clip. Using these scores, it computes overall AUROC, which does not depend on a threshold, and fixed threshold metrics at the D5 derived threshold. These include the confusion matrix, accuracy, precision, sensitivity or recall, specificity, F1 score, MCC, and the Fisher exact test p value. Fairness is computed as ΔFNR, defined as FNR(F) minus FNR(M), using only true Parkinson’s clips, and confusion counts are also reported by sex. For each seed, the cell saves a ROC curve, an overall confusion matrix, additional sex specific confusion matrices when both M and F are present, and a detailed `metrics.json` file under\n",
    "`<D2_OUT_ROOT>/Cross_Language_Zero_Shot_Runs/run_ENUK_to_SK_seed<seed>/`.\n",
    "\n",
    "After all three seeds finish, results are combined across seeds. The cell reports the mean AUROC with a 95 percent confidence interval using a t distribution with n equal to 3, along with the mean and standard deviation of the fixed threshold metrics and fairness values. A combined summary is saved as `summary_zero_shot3.json` and the same record is appended to `history_index.jsonl`, both under `<D2_OUT_ROOT>/Cross_Language_Zero_Shot_Runs/`. The cell ends by unassigning the Colab runtime to shut down the GPU."
   ],
   "metadata": {
    "id": "PNibJw4zzcVL"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# =========================\n",
    "# ENUK → SK Zero-shot (D5 → D2): Test-only transfer with fixed threshold\n",
    "# - Loads D5-trained heads (most recent train+val experiment)\n",
    "# - Runs inference on D2 TEST only (from manifest_all.csv)\n",
    "# - Uses one fixed threshold read from D5 monolingual summary_test.json\n",
    "# - Saves per-seed metrics + plots, plus an aggregated summary under D2\n",
    "# =========================\n",
    "\n",
    "import os, json, math, random, time, warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import soundfile as sf\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from transformers import Wav2Vec2Model\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, confusion_matrix, matthews_corrcoef\n",
    "from scipy.stats import fisher_exact\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# -------------------------\n",
    "# Import safety: ensure local files do not override core libraries\n",
    "# Inputs: /content directory\n",
    "# Output: raises early if torch/transformers are shadowed by local names\n",
    "# -------------------------\n",
    "if os.path.exists(\"/content/torch.py\") or os.path.exists(\"/content/torch/__init__.py\"):\n",
    "    raise RuntimeError(\"Found local /content/torch.py or /content/torch/ that shadows PyTorch. Rename/remove it and restart runtime.\")\n",
    "if os.path.exists(\"/content/transformers.py\") or os.path.exists(\"/content/transformers/__init__.py\"):\n",
    "    raise RuntimeError(\"Found local /content/transformers.py or /content/transformers/ that shadows Hugging Face Transformers. Rename/remove it and restart runtime.\")\n",
    "\n",
    "# -------------------------\n",
    "# Drive mount (Colab)\n",
    "# Inputs: Google Drive\n",
    "# Output: /content/drive mounted for reading manifests/checkpoints and writing results\n",
    "# -------------------------\n",
    "if not os.path.isdir(\"/content/drive/MyDrive\"):\n",
    "    from google.colab import drive  # type: ignore\n",
    "    drive.mount(\"/content/drive\")\n",
    "\n",
    "# -------------------------\n",
    "# Paths: target (D2) + source (D5)\n",
    "# Inputs: optional globals D2_OUT_ROOT, D5_OUT_ROOT (if set earlier), else fallbacks\n",
    "# Output: D2_MANIFEST_ALL and D5_MONO_SUMMARY paths\n",
    "# -------------------------\n",
    "D2_OUT_ROOT_FALLBACK = \"/content/drive/MyDrive/AI_PD_Project/Datasets/D2-Slovak (EWA-DB)/EWA-DB/preprocessed_v1\"\n",
    "D5_OUT_ROOT_FALLBACK = \"/content/drive/MyDrive/AI_PD_Project/Datasets/D5-English (MDVR-KCL)/preprocessed_v2\"\n",
    "\n",
    "D2_OUT_ROOT = globals().get(\"D2_OUT_ROOT\", D2_OUT_ROOT_FALLBACK)\n",
    "D5_OUT_ROOT = globals().get(\"D5_OUT_ROOT\", D5_OUT_ROOT_FALLBACK)\n",
    "\n",
    "D2_MANIFEST_ALL = f\"{D2_OUT_ROOT}/manifests/manifest_all.csv\"\n",
    "\n",
    "# D5 monolingual summary (provides the fixed threshold source)\n",
    "D5_MONO_SUMMARY = Path(D5_OUT_ROOT) / \"monolingual_test_runs\" / \"summary_test.json\"\n",
    "\n",
    "# -------------------------\n",
    "# Run settings (kept consistent with other test cells)\n",
    "# Inputs: constants below\n",
    "# Output: printed configuration + runtime behavior (batching, AMP, device)\n",
    "# -------------------------\n",
    "TRANSFER_TAG    = \"ENUK_to_SK\"        # D5 -> D2\n",
    "SEEDS           = [1337, 2024, 7777]\n",
    "BACKBONE_CKPT   = \"facebook/wav2vec2-base\"\n",
    "SR_EXPECTED     = 16000\n",
    "TINY_THRESH     = 1e-4\n",
    "\n",
    "EFFECTIVE_BS    = 64\n",
    "PER_DEVICE_BS   = 16\n",
    "GRAD_ACCUM      = max(1, EFFECTIVE_BS // PER_DEVICE_BS)  # printed only\n",
    "\n",
    "DROPOUT_P       = 0.2\n",
    "\n",
    "NUM_WORKERS     = 0\n",
    "PIN_MEMORY      = False\n",
    "\n",
    "USE_AMP         = True\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "try:\n",
    "    torch.set_float32_matmul_precision(\"high\")\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", message=\"None of the inputs have requires_grad=True. Gradients will be None\")\n",
    "warnings.filterwarnings(\"ignore\", message=\"Passing `gradient_checkpointing` to a config initialization is deprecated\")\n",
    "\n",
    "print(\"D2_OUT_ROOT (target):\", D2_OUT_ROOT)\n",
    "print(\"D2_MANIFEST_ALL:\", D2_MANIFEST_ALL)\n",
    "print(\"D5_OUT_ROOT (source):\", D5_OUT_ROOT)\n",
    "print(\"D5_MONO_SUMMARY:\", str(D5_MONO_SUMMARY))\n",
    "print(\"DEVICE:\", DEVICE)\n",
    "if DEVICE.type == \"cuda\":\n",
    "    print(\"GPU:\", torch.cuda.get_device_name(0))\n",
    "print(\"PER_DEVICE_BS:\", PER_DEVICE_BS, \"| EFFECTIVE_BS:\", PER_DEVICE_BS * GRAD_ACCUM)\n",
    "print(\"NUM_WORKERS:\", NUM_WORKERS, \"| PIN_MEMORY:\", PIN_MEMORY)\n",
    "print(\"USE_AMP:\", bool(USE_AMP and DEVICE.type == \"cuda\"))\n",
    "\n",
    "# -------------------------\n",
    "# Fixed threshold: read from D5 monolingual summary_test.json\n",
    "# Inputs: D5_MONO_SUMMARY (JSON)\n",
    "# Output: FIXED_THR (float) used for all three seeds on D2\n",
    "# -------------------------\n",
    "if not D5_MONO_SUMMARY.exists():\n",
    "    raise FileNotFoundError(\n",
    "        \"Missing D5 monolingual summary_test.json.\\n\"\n",
    "        f\"Expected: {str(D5_MONO_SUMMARY)}\\n\"\n",
    "        \"Run the D5 monolingual test cell first (the one that writes summary_test.json).\"\n",
    "    )\n",
    "\n",
    "with open(D5_MONO_SUMMARY, \"r\", encoding=\"utf-8\") as f:\n",
    "    d5_sum = json.load(f)\n",
    "\n",
    "# Preferred: summary[\"val_opt_threshold_mean_sd\"][\"mean\"]\n",
    "if \"val_opt_threshold_mean_sd\" in d5_sum and isinstance(d5_sum[\"val_opt_threshold_mean_sd\"], dict):\n",
    "    FIXED_THR = float(d5_sum[\"val_opt_threshold_mean_sd\"].get(\"mean\", float(\"nan\")))\n",
    "else:\n",
    "    # Fallback: mean of per-seed values if present\n",
    "    by_seed = d5_sum.get(\"val_opt_threshold_by_seed\", None)\n",
    "    if isinstance(by_seed, dict) and len(by_seed) > 0:\n",
    "        FIXED_THR = float(np.nanmean([float(v) for v in by_seed.values()]))\n",
    "    else:\n",
    "        FIXED_THR = float(\"nan\")\n",
    "\n",
    "if not np.isfinite(FIXED_THR):\n",
    "    raise RuntimeError(\n",
    "        \"Could not read a finite fixed threshold from D5 monolingual summary_test.json.\\n\"\n",
    "        \"Expected keys: val_opt_threshold_mean_sd.mean (preferred) or val_opt_threshold_by_seed.\"\n",
    "    )\n",
    "\n",
    "print(f\"\\nFixed transfer threshold (from D5 monolingual mean VAL-opt): {FIXED_THR:.6f}\")\n",
    "\n",
    "# -------------------------\n",
    "# Target manifest: load D2 and keep TEST only\n",
    "# Inputs: D2 manifest_all.csv\n",
    "# Output: test_df with required columns + basic counts printed\n",
    "# -------------------------\n",
    "if not os.path.exists(D2_MANIFEST_ALL):\n",
    "    raise FileNotFoundError(f\"Missing D2 manifest_all.csv: {D2_MANIFEST_ALL}\")\n",
    "\n",
    "m_all = pd.read_csv(D2_MANIFEST_ALL)\n",
    "\n",
    "# Require these fields for inference + fairness plots\n",
    "req_cols = {\"split\", \"clip_path\", \"label_num\", \"task\", \"sex\"}\n",
    "missing = [c for c in sorted(req_cols) if c not in m_all.columns]\n",
    "if missing:\n",
    "    raise ValueError(f\"D2 manifest missing required columns: {missing}. Found: {list(m_all.columns)}\")\n",
    "\n",
    "# Ensure the manifest content is actually D2\n",
    "if \"dataset\" in m_all.columns and m_all[\"dataset\"].notna().any():\n",
    "    dataset_id = str(m_all[\"dataset\"].astype(str).value_counts(dropna=True).idxmax())\n",
    "    m_all = m_all[m_all[\"dataset\"].astype(str) == dataset_id].copy()\n",
    "else:\n",
    "    dataset_id = \"DX\"\n",
    "\n",
    "if dataset_id != \"D2\":\n",
    "    raise RuntimeError(\n",
    "        f\"Expected dataset_id=='D2' but got {dataset_id!r}. \"\n",
    "        \"This usually means D2_OUT_ROOT is wrong or inherited from a previous cell.\\n\"\n",
    "        f\"D2_OUT_ROOT={D2_OUT_ROOT}\"\n",
    "    )\n",
    "\n",
    "# Keep a stable column set (missing ones are filled with NaN)\n",
    "keep_cols = [\"clip_path\", \"label_num\", \"task\", \"speaker_id\", \"sex\", \"age\", \"duration_sec\", \"split\"]\n",
    "for c in keep_cols:\n",
    "    if c not in m_all.columns:\n",
    "        m_all[c] = np.nan\n",
    "m_all = m_all[keep_cols].copy()\n",
    "\n",
    "test_df = m_all[m_all[\"split\"].isin([\"test\"])].copy().reset_index(drop=True)\n",
    "\n",
    "print(f\"\\nTarget dataset inferred: {dataset_id}\")\n",
    "print(f\"TEST rows: {len(test_df)}\")\n",
    "if len(test_df) == 0:\n",
    "    raise RuntimeError(\"After filtering to split=='test', D2 manifest has 0 rows.\")\n",
    "\n",
    "print(\"TEST label counts:\", test_df[\"label_num\"].value_counts(dropna=False).to_dict())\n",
    "print(\"TEST sex counts (raw):\", test_df[\"sex\"].value_counts(dropna=False).to_dict())\n",
    "\n",
    "# -------------------------\n",
    "# Path check: ensure target audio files exist\n",
    "# Inputs: test_df.clip_path\n",
    "# Output: raises early if any clip files are missing (shows a few examples)\n",
    "# -------------------------\n",
    "def _fail_fast_missing_paths(df: pd.DataFrame, name: str):\n",
    "    missing_paths = []\n",
    "    for p in tqdm(df[\"clip_path\"].astype(str).tolist(), desc=f\"Check {name} clip_path exists\", dynamic_ncols=True):\n",
    "        if not os.path.exists(p):\n",
    "            missing_paths.append(p)\n",
    "            if len(missing_paths) >= 10:\n",
    "                break\n",
    "    if missing_paths:\n",
    "        raise FileNotFoundError(f\"{name}: missing clip_path(s). Examples: {missing_paths}\")\n",
    "\n",
    "_fail_fast_missing_paths(test_df, \"TEST\")\n",
    "\n",
    "# -------------------------\n",
    "# Task grouping: map each row to vowel vs other\n",
    "# Inputs: test_df.task\n",
    "# Output: test_df.task_group used to pick the correct head during inference\n",
    "# -------------------------\n",
    "def _task_group(task_val) -> str:\n",
    "    return \"vowel\" if str(task_val) == \"vowl\" else \"other\"\n",
    "\n",
    "test_df[\"task_group\"] = test_df[\"task\"].apply(_task_group)\n",
    "\n",
    "# -------------------------\n",
    "# Sex normalization for fairness/charts (D2 uses male/female)\n",
    "# Inputs: test_df.sex\n",
    "# Output: test_df.sex_norm in {M,F,UNK}\n",
    "# -------------------------\n",
    "def normalize_sex_d2(val) -> str:\n",
    "    \"\"\"\n",
    "    Returns 'M', 'F', or 'UNK'\n",
    "    D2 encoding: male/female (strings)\n",
    "    Also handles common variants.\n",
    "    \"\"\"\n",
    "    if pd.isna(val):\n",
    "        return \"UNK\"\n",
    "    s = str(val).strip().lower()\n",
    "    if s in {\"m\", \"male\", \"man\", \"masc\", \"masculine\"}:\n",
    "        return \"M\"\n",
    "    if s in {\"f\", \"female\", \"woman\", \"fem\", \"feminine\"}:\n",
    "        return \"F\"\n",
    "    return \"UNK\"\n",
    "\n",
    "test_df[\"sex_norm\"] = test_df[\"sex\"].apply(normalize_sex_d2)\n",
    "print(\"TEST sex counts (normalized):\", test_df[\"sex_norm\"].value_counts(dropna=False).to_dict())\n",
    "if (test_df[\"sex_norm\"] == \"UNK\").any():\n",
    "    print(\"NOTE: Some D2 'sex' values could not be normalized to M/F and were counted as 'UNK' for fairness and sex charts.\")\n",
    "\n",
    "# -------------------------\n",
    "# Dataset + collator: read audio and build attention masks\n",
    "# Inputs: test_df rows + audio files\n",
    "# Output: DataLoader batches with padded input_values, attention_mask, labels, task_group, sex_norm\n",
    "# -------------------------\n",
    "class AudioManifestDataset(Dataset):\n",
    "    def __init__(self, df: pd.DataFrame):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        row = self.df.iloc[idx]\n",
    "        clip_path = str(row[\"clip_path\"])\n",
    "        label = int(row[\"label_num\"])\n",
    "        task_group = str(row[\"task_group\"])\n",
    "        sex_norm = str(row[\"sex_norm\"])\n",
    "\n",
    "        if not os.path.exists(clip_path):\n",
    "            raise FileNotFoundError(f\"Missing clip_path: {clip_path}\")\n",
    "\n",
    "        y, sr = sf.read(clip_path, always_2d=False)\n",
    "        if isinstance(y, np.ndarray) and y.ndim > 1:\n",
    "            y = np.mean(y, axis=1)\n",
    "        y = np.asarray(y, dtype=np.float32)\n",
    "\n",
    "        if int(sr) != SR_EXPECTED:\n",
    "            raise ValueError(f\"Sample rate mismatch (expected {SR_EXPECTED}): {clip_path} has sr={sr}\")\n",
    "\n",
    "        # attention_mask mainly matters for vowel clips (ignore padded tail)\n",
    "        attn = np.ones((len(y),), dtype=np.int64)\n",
    "        if task_group == \"vowel\":\n",
    "            k = -1\n",
    "            for j in range(len(y) - 1, -1, -1):\n",
    "                if abs(float(y[j])) > TINY_THRESH:\n",
    "                    k = j\n",
    "                    break\n",
    "            if k >= 0:\n",
    "                attn[:k+1] = 1\n",
    "                attn[k+1:] = 0\n",
    "            else:\n",
    "                attn[:] = 1\n",
    "        else:\n",
    "            attn[:] = 1\n",
    "\n",
    "        return {\n",
    "            \"input_values\": torch.from_numpy(y),\n",
    "            \"attention_mask\": torch.from_numpy(attn),\n",
    "            \"labels\": torch.tensor(label, dtype=torch.long),\n",
    "            \"task_group\": task_group,\n",
    "            \"sex_norm\": sex_norm,\n",
    "        }\n",
    "\n",
    "def collate_fn(batch):\n",
    "    max_len = int(max(b[\"input_values\"].numel() for b in batch))\n",
    "    input_vals, attn_masks, labels, task_groups, sex_norms = [], [], [], [], []\n",
    "    for b in batch:\n",
    "        x = b[\"input_values\"]\n",
    "        a = b[\"attention_mask\"]\n",
    "        pad = max_len - x.numel()\n",
    "        if pad > 0:\n",
    "            x = torch.cat([x, torch.zeros(pad, dtype=x.dtype)], dim=0)\n",
    "            a = torch.cat([a, torch.zeros(pad, dtype=a.dtype)], dim=0)\n",
    "        input_vals.append(x)\n",
    "        attn_masks.append(a)\n",
    "        labels.append(b[\"labels\"])\n",
    "        task_groups.append(b[\"task_group\"])\n",
    "        sex_norms.append(b[\"sex_norm\"])\n",
    "    return {\n",
    "        \"input_values\": torch.stack(input_vals, dim=0),\n",
    "        \"attention_mask\": torch.stack(attn_masks, dim=0),\n",
    "        \"labels\": torch.stack(labels, dim=0),\n",
    "        \"task_group\": task_groups,\n",
    "        \"sex_norm\": sex_norms,\n",
    "    }\n",
    "\n",
    "# -------------------------\n",
    "# Model: frozen wav2vec2 backbone + two heads (vowel vs other)\n",
    "# Inputs: backbone checkpoint name + dropout_p\n",
    "# Output: logits for PD vs healthy for each clip, using the correct head per task_group\n",
    "# -------------------------\n",
    "class Wav2Vec2TwoHeadClassifier(nn.Module):\n",
    "    def __init__(self, ckpt: str, dropout_p: float):\n",
    "        super().__init__()\n",
    "        self.backbone = Wav2Vec2Model.from_pretrained(ckpt, use_safetensors=True)\n",
    "        for p in self.backbone.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "        H = int(self.backbone.config.hidden_size)\n",
    "        self.pre_vowel = nn.Sequential(nn.LayerNorm(H), nn.Dropout(float(dropout_p)))\n",
    "        self.pre_other = nn.Sequential(nn.LayerNorm(H), nn.Dropout(float(dropout_p)))\n",
    "        self.head_vowel = nn.Linear(H, 2)\n",
    "        self.head_other = nn.Linear(H, 2)\n",
    "\n",
    "    def masked_mean_pool(self, last_hidden, attn_mask_samples):\n",
    "        feat_mask = self.backbone._get_feature_vector_attention_mask(last_hidden.shape[1], attn_mask_samples)\n",
    "        feat_mask = feat_mask.to(last_hidden.device).unsqueeze(-1).type_as(last_hidden)\n",
    "        masked = last_hidden * feat_mask\n",
    "        denom = feat_mask.sum(dim=1).clamp(min=1.0)\n",
    "        return masked.sum(dim=1) / denom\n",
    "\n",
    "    def _heads_fp32(self, x_fp_any: torch.Tensor, head: nn.Module) -> torch.Tensor:\n",
    "        # Keep head math in fp32 even when AMP is enabled\n",
    "        x = x_fp_any.float()\n",
    "        if x.is_cuda:\n",
    "            with torch.amp.autocast(device_type=\"cuda\", enabled=False):\n",
    "                return head(x)\n",
    "        return head(x)\n",
    "\n",
    "    def forward_logits(self, input_values, attention_mask, task_group):\n",
    "        with torch.no_grad():\n",
    "            out = self.backbone(input_values=input_values, attention_mask=attention_mask)\n",
    "            last_hidden = out.last_hidden_state\n",
    "\n",
    "        pooled = self.masked_mean_pool(last_hidden, attention_mask)  # [B,H]\n",
    "\n",
    "        z_v = self.pre_vowel(pooled.float())\n",
    "        z_o = self.pre_other(pooled.float())\n",
    "\n",
    "        logits_v = self._heads_fp32(z_v, self.head_vowel)\n",
    "        logits_o = self._heads_fp32(z_o, self.head_other)\n",
    "\n",
    "        # Select the correct head per row based on task_group\n",
    "        is_vowel = torch.tensor([tg == \"vowel\" for tg in task_group], device=pooled.device, dtype=torch.bool)\n",
    "        logits = logits_o.clone()\n",
    "        if is_vowel.any():\n",
    "            logits[is_vowel] = logits_v[is_vowel]\n",
    "        return logits\n",
    "\n",
    "# -------------------------\n",
    "# Metrics + plot helpers\n",
    "# Inputs: y_true and y_prob (PD probability), plus a threshold for thresholded metrics\n",
    "# Output: numeric metrics dict + PNG charts saved to disk\n",
    "# -------------------------\n",
    "def compute_auc(y_true, y_prob):\n",
    "    y_true = np.asarray(y_true, dtype=np.int64)\n",
    "    y_prob = np.asarray(y_prob, dtype=np.float64)\n",
    "    if len(np.unique(y_true)) < 2:\n",
    "        return float(\"nan\")\n",
    "    return float(roc_auc_score(y_true, y_prob))\n",
    "\n",
    "def compute_threshold_metrics(y_true, y_prob, thr=0.5):\n",
    "    y_true = np.asarray(y_true, dtype=np.int64)\n",
    "    y_prob = np.asarray(y_prob, dtype=np.float64)\n",
    "    y_pred = (y_prob >= float(thr)).astype(np.int64)\n",
    "\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=[0, 1])\n",
    "    TN, FP, FN, TP = int(cm[0, 0]), int(cm[0, 1]), int(cm[1, 0]), int(cm[1, 1])\n",
    "\n",
    "    eps = 1e-12\n",
    "    acc = (TP + TN) / max(1, (TP + TN + FP + FN))\n",
    "    prec = TP / (TP + FP + eps)\n",
    "    rec = TP / (TP + FN + eps)     # sensitivity\n",
    "    f1 = 2 * prec * rec / (prec + rec + eps)\n",
    "    spec = TN / (TN + FP + eps)\n",
    "\n",
    "    try:\n",
    "        mcc = float(matthews_corrcoef(y_true, y_pred)) if len(np.unique(y_pred)) > 1 else float(\"nan\")\n",
    "    except Exception:\n",
    "        mcc = float(\"nan\")\n",
    "\n",
    "    try:\n",
    "        _, pval = fisher_exact([[TN, FP], [FN, TP]], alternative=\"two-sided\")\n",
    "        pval = float(pval)\n",
    "    except Exception:\n",
    "        pval = float(\"nan\")\n",
    "\n",
    "    return {\n",
    "        \"threshold\": float(thr),\n",
    "        \"confusion_matrix\": {\"TN\": TN, \"FP\": FP, \"FN\": FN, \"TP\": TP},\n",
    "        \"accuracy\": float(acc),\n",
    "        \"precision\": float(prec),\n",
    "        \"recall\": float(rec),\n",
    "        \"f1_score\": float(f1),\n",
    "        \"sensitivity\": float(rec),\n",
    "        \"specificity\": float(spec),\n",
    "        \"mcc\": float(mcc),\n",
    "        \"p_value_fisher_two_sided\": float(pval),\n",
    "    }\n",
    "\n",
    "def save_roc_curve_png(y_true, y_prob, out_png, title_suffix=\"Test\"):\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_prob)\n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr)\n",
    "    plt.plot([0, 1], [0, 1], linestyle=\"--\")\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(f\"ROC Curve ({title_suffix})\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_png, dpi=150)\n",
    "    plt.close()\n",
    "\n",
    "def save_confusion_png(y_true, y_prob, out_png, thr=0.5, title_suffix=\"Test\"):\n",
    "    y_true = np.asarray(y_true, dtype=np.int64)\n",
    "    y_prob = np.asarray(y_prob, dtype=np.float64)\n",
    "    y_pred = (y_prob >= float(thr)).astype(int)\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=[0, 1])\n",
    "\n",
    "    plt.figure()\n",
    "    plt.imshow(cm)\n",
    "    plt.xticks([0, 1], [\"Healthy(0)\", \"PD(1)\"])\n",
    "    plt.yticks([0, 1], [\"Healthy(0)\", \"PD(1)\"])\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"True\")\n",
    "    plt.title(f\"Confusion Matrix ({title_suffix}, thr={thr:.2f})\")\n",
    "    for (i, j), v in np.ndenumerate(cm):\n",
    "        plt.text(j, i, str(v), ha=\"center\", va=\"center\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_png, dpi=150)\n",
    "    plt.close()\n",
    "\n",
    "# -------------------------\n",
    "# Fairness: FNR per sex and ΔFNR = FNR(F) - FNR(M)\n",
    "# Inputs: y_true, y_prob, sex_norm array, threshold\n",
    "# Output: per-group FNR details + signed and absolute gap\n",
    "# -------------------------\n",
    "def compute_fnr_by_group_signed(y_true, y_prob, groups, thr=0.5):\n",
    "    y_true = np.asarray(y_true, dtype=np.int64)\n",
    "    y_prob = np.asarray(y_prob, dtype=np.float64)\n",
    "    groups = np.asarray(list(groups), dtype=object)\n",
    "    y_pred = (y_prob >= float(thr)).astype(np.int64)\n",
    "\n",
    "    out = {}\n",
    "    for g in sorted(set(groups.tolist())):\n",
    "        mask_g = (groups == g)\n",
    "        if int(mask_g.sum()) == 0:\n",
    "            continue\n",
    "\n",
    "        pos_mask = mask_g & (y_true == 1)\n",
    "        n_pos = int(pos_mask.sum())\n",
    "        if n_pos == 0:\n",
    "            out[g] = {\"n_total\": int(mask_g.sum()), \"n_pos\": 0, \"tp\": 0, \"fn\": 0, \"fnr\": float(\"nan\")}\n",
    "            continue\n",
    "\n",
    "        tp = int(((y_pred == 1) & pos_mask).sum())\n",
    "        fn = int(((y_pred == 0) & pos_mask).sum())\n",
    "        fnr = float(fn / max(1, (fn + tp)))\n",
    "\n",
    "        out[g] = {\"n_total\": int(mask_g.sum()), \"n_pos\": int(n_pos), \"tp\": int(tp), \"fn\": int(fn), \"fnr\": float(fnr)}\n",
    "\n",
    "    fnr_m = out.get(\"M\", {}).get(\"fnr\", float(\"nan\"))\n",
    "    fnr_f = out.get(\"F\", {}).get(\"fnr\", float(\"nan\"))\n",
    "\n",
    "    if (not np.isnan(fnr_m)) and (not np.isnan(fnr_f)):\n",
    "        delta_signed = float(fnr_f - fnr_m)\n",
    "        delta_abs = float(abs(delta_signed))\n",
    "    else:\n",
    "        delta_signed = float(\"nan\")\n",
    "        delta_abs = float(\"nan\")\n",
    "\n",
    "    return out, delta_signed, delta_abs\n",
    "\n",
    "def compute_confusion_counts(y_true, y_prob, thr=0.5):\n",
    "    y_true = np.asarray(y_true, dtype=np.int64)\n",
    "    y_prob = np.asarray(y_prob, dtype=np.float64)\n",
    "    y_pred = (y_prob >= float(thr)).astype(np.int64)\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=[0, 1])\n",
    "    TN, FP, FN, TP = int(cm[0, 0]), int(cm[0, 1]), int(cm[1, 0]), int(cm[1, 1])\n",
    "    return {\"TN\": TN, \"FP\": FP, \"FN\": FN, \"TP\": TP}\n",
    "\n",
    "def compute_confusion_by_group(y_true, y_prob, groups, thr=0.5):\n",
    "    y_true = np.asarray(y_true, dtype=np.int64)\n",
    "    y_prob = np.asarray(y_prob, dtype=np.float64)\n",
    "    groups = np.asarray(list(groups), dtype=object)\n",
    "    out = {}\n",
    "    for g in sorted(set(groups.tolist())):\n",
    "        mask = (groups == g)\n",
    "        if int(mask.sum()) == 0:\n",
    "            continue\n",
    "        out[g] = {\"n\": int(mask.sum()), \"confusion\": compute_confusion_counts(y_true[mask], y_prob[mask], thr=thr)}\n",
    "    return out\n",
    "\n",
    "# -------------------------\n",
    "# Reproducibility: set all RNG seeds\n",
    "# Inputs: seed int\n",
    "# Output: deterministic settings for random, numpy, torch\n",
    "# -------------------------\n",
    "def set_all_seeds(seed: int):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# -------------------------\n",
    "# Source checkpoint selection: pick most recent D5 trainval exp with all seeds\n",
    "# Inputs: D5 trainval_runs folder\n",
    "# Output: chosen_exp folder used to load best_heads.pt per seed\n",
    "# -------------------------\n",
    "D5_TRAINVAL_ROOT = Path(D5_OUT_ROOT) / \"trainval_runs\"\n",
    "if not D5_TRAINVAL_ROOT.exists():\n",
    "    raise FileNotFoundError(f\"Missing D5 trainval_runs folder: {str(D5_TRAINVAL_ROOT)}\")\n",
    "\n",
    "exp_dirs = sorted([p for p in D5_TRAINVAL_ROOT.glob(\"exp_*\") if p.is_dir()], key=lambda p: p.stat().st_mtime, reverse=True)\n",
    "if not exp_dirs:\n",
    "    raise FileNotFoundError(f\"No exp_* folders found under: {str(D5_TRAINVAL_ROOT)}\")\n",
    "\n",
    "def _has_all_seeds(exp_path: Path, seeds: list):\n",
    "    for s in seeds:\n",
    "        p = exp_path / f\"run_D5_seed{s}\" / \"best_heads.pt\"\n",
    "        if not p.exists():\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "chosen_exp = None\n",
    "for ed in exp_dirs:\n",
    "    if _has_all_seeds(ed, SEEDS):\n",
    "        chosen_exp = ed\n",
    "        break\n",
    "\n",
    "if chosen_exp is None:\n",
    "    sample = exp_dirs[0]\n",
    "    raise FileNotFoundError(\n",
    "        \"Could not find a D5 trainval experiment with all 3 best_heads.pt files.\\n\"\n",
    "        f\"Expected: {str(D5_TRAINVAL_ROOT)}/exp_*/run_D5_seedXXXX/best_heads.pt\\n\"\n",
    "        f\"Most recent exp checked: {str(sample)}\"\n",
    "    )\n",
    "\n",
    "print(\"\\nUsing SOURCE (D5) Train+Val experiment folder:\")\n",
    "print(\" \", str(chosen_exp))\n",
    "\n",
    "# -------------------------\n",
    "# Output folder: write only under D2\n",
    "# Inputs: D2_OUT_ROOT\n",
    "# Output: ZS_ROOT folder for per-seed runs + summary files\n",
    "# -------------------------\n",
    "ZS_ROOT = Path(D2_OUT_ROOT) / \"Cross_Language_Zero_Shot_Runs\"\n",
    "ZS_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# -------------------------\n",
    "# DataLoader: D2 TEST only (plus a small warm-up read)\n",
    "# Inputs: test_df and audio files\n",
    "# Output: test_loader ready for inference\n",
    "# -------------------------\n",
    "test_ds = AudioManifestDataset(test_df)\n",
    "test_loader = DataLoader(\n",
    "    test_ds,\n",
    "    batch_size=PER_DEVICE_BS,\n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=PIN_MEMORY,\n",
    "    collate_fn=collate_fn,\n",
    ")\n",
    "\n",
    "print(\"\\nWarm-up: loading up to 2 TEST batches...\")\n",
    "t0 = time.time()\n",
    "nb = len(test_loader)\n",
    "wb = min(2, nb)\n",
    "if wb == 0:\n",
    "    raise RuntimeError(\"TEST DataLoader has 0 batches. Check test_df length and PER_DEVICE_BS.\")\n",
    "it = iter(test_loader)\n",
    "for i in range(wb):\n",
    "    _ = next(it)\n",
    "    print(f\"  loaded TEST warmup batch {i+1}/{wb}\")\n",
    "print(f\"Warm-up done in {time.time()-t0:.2f}s\")\n",
    "\n",
    "# -------------------------\n",
    "# Checkpoint loader: load only the trained heads from best_heads.pt\n",
    "# Inputs: model instance + best_heads.pt path\n",
    "# Output: model with head weights restored for this seed\n",
    "# -------------------------\n",
    "def load_heads_into_model(model: Wav2Vec2TwoHeadClassifier, best_heads_path: Path):\n",
    "    if not best_heads_path.exists():\n",
    "        raise FileNotFoundError(f\"Missing best_heads.pt: {str(best_heads_path)}\")\n",
    "    state = torch.load(str(best_heads_path), map_location=\"cpu\")\n",
    "    needed = [\"pre_vowel\", \"pre_other\", \"head_vowel\", \"head_other\"]\n",
    "    missing = [k for k in needed if k not in state]\n",
    "    if missing:\n",
    "        raise KeyError(\n",
    "            f\"best_heads.pt missing keys {missing}. Found keys: {list(state.keys())}. \"\n",
    "            \"This zero-shot code expects the trainval save format.\"\n",
    "        )\n",
    "    model.pre_vowel.load_state_dict(state[\"pre_vowel\"], strict=True)\n",
    "    model.pre_other.load_state_dict(state[\"pre_other\"], strict=True)\n",
    "    model.head_vowel.load_state_dict(state[\"head_vowel\"], strict=True)\n",
    "    model.head_other.load_state_dict(state[\"head_other\"], strict=True)\n",
    "    return model\n",
    "\n",
    "# -------------------------\n",
    "# Inference: run model over D2 test_loader and collect probabilities\n",
    "# Inputs: loader + model\n",
    "# Output: arrays (y_true, y_prob, sex_norm) for metrics and plots\n",
    "# -------------------------\n",
    "def run_inference(loader, model, desc):\n",
    "    use_amp = bool(USE_AMP and DEVICE.type == \"cuda\")\n",
    "    all_probs, all_true, all_sex = [], [], []\n",
    "\n",
    "    pbar = tqdm(loader, desc=desc, dynamic_ncols=True)\n",
    "    with torch.inference_mode():\n",
    "        for batch in pbar:\n",
    "            input_values = batch[\"input_values\"].to(DEVICE, non_blocking=False)\n",
    "            attention_mask = batch[\"attention_mask\"].to(DEVICE, non_blocking=False)\n",
    "            labels = batch[\"labels\"].to(DEVICE, non_blocking=False)\n",
    "            task_group = batch[\"task_group\"]\n",
    "            sex_norm = batch[\"sex_norm\"]\n",
    "\n",
    "            with torch.amp.autocast(device_type=\"cuda\", enabled=use_amp):\n",
    "                logits = model.forward_logits(input_values, attention_mask, task_group)\n",
    "\n",
    "            probs = torch.softmax(logits.float(), dim=-1)[:, 1].detach().cpu().numpy().astype(np.float64)\n",
    "            all_probs.extend(probs.tolist())\n",
    "            all_true.extend(labels.detach().cpu().numpy().astype(np.int64).tolist())\n",
    "            all_sex.extend(list(sex_norm))\n",
    "\n",
    "    return np.asarray(all_true, dtype=np.int64), np.asarray(all_probs, dtype=np.float64), np.asarray(all_sex, dtype=object)\n",
    "\n",
    "# -------------------------\n",
    "# Per-seed run: load D5 heads, evaluate D2 at FIXED_THR, save metrics + PNGs\n",
    "# Inputs: seed, chosen_exp, FIXED_THR, D2 test_loader\n",
    "# Output: run_<TRANSFER_TAG>_seedXXXX folder with metrics.json + plots\n",
    "# -------------------------\n",
    "def run_seed(seed: int):\n",
    "    set_all_seeds(seed)\n",
    "\n",
    "    run_dir = ZS_ROOT / f\"run_{TRANSFER_TAG}_seed{seed}\"\n",
    "    run_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    best_heads_path = chosen_exp / f\"run_D5_seed{seed}\" / \"best_heads.pt\"\n",
    "\n",
    "    print(f\"\\n[seed={seed}] Loading SOURCE heads from:\")\n",
    "    print(\" \", str(best_heads_path))\n",
    "    print(f\"[seed={seed}] Evaluating TARGET D2 TEST @ fixed_thr={FIXED_THR:.6f}\")\n",
    "\n",
    "    model = Wav2Vec2TwoHeadClassifier(BACKBONE_CKPT, dropout_p=DROPOUT_P).to(DEVICE)\n",
    "    model = load_heads_into_model(model, best_heads_path)\n",
    "    model.eval()\n",
    "\n",
    "    yt, pt, st = run_inference(test_loader, model, desc=f\"[seed={seed}] D2 TEST (zero-shot)\")\n",
    "\n",
    "    test_auc = compute_auc(yt, pt)  # threshold-free\n",
    "    thr_metrics = compute_threshold_metrics(yt, pt, thr=FIXED_THR)\n",
    "\n",
    "    fnr_by_sex, delta_f_minus_m, delta_abs = compute_fnr_by_group_signed(yt, pt, st, thr=FIXED_THR)\n",
    "    confusion_by_sex = compute_confusion_by_group(yt, pt, st, thr=FIXED_THR)\n",
    "\n",
    "    # Plots: always save overall ROC + confusion matrix\n",
    "    roc_png = run_dir / \"roc_curve.png\"\n",
    "    cm_png  = run_dir / \"confusion_matrix.png\"\n",
    "    save_roc_curve_png(yt, pt, str(roc_png), title_suffix=f\"D2 Test (seed={seed})\")\n",
    "    save_confusion_png(yt, pt, str(cm_png), thr=FIXED_THR, title_suffix=f\"D2 Test (seed={seed})\")\n",
    "\n",
    "    # Sex-specific confusion matrices (only if that sex exists in D2 TEST)\n",
    "    cm_m_png = None\n",
    "    cm_f_png = None\n",
    "    mask_m = (st == \"M\")\n",
    "    mask_f = (st == \"F\")\n",
    "\n",
    "    if int(mask_m.sum()) > 0:\n",
    "        cm_m_png = run_dir / \"confusion_matrix_M.png\"\n",
    "        save_confusion_png(yt[mask_m], pt[mask_m], str(cm_m_png), thr=FIXED_THR, title_suffix=f\"D2 Test SEX=M (seed={seed})\")\n",
    "\n",
    "    if int(mask_f.sum()) > 0:\n",
    "        cm_f_png = run_dir / \"confusion_matrix_F.png\"\n",
    "        save_confusion_png(yt[mask_f], pt[mask_f], str(cm_f_png), thr=FIXED_THR, title_suffix=f\"D2 Test SEX=F (seed={seed})\")\n",
    "\n",
    "    # Structured metrics saved for later analysis/paper tables\n",
    "    metrics = {\n",
    "        \"transfer\": {\n",
    "            \"tag\": TRANSFER_TAG,\n",
    "            \"source_dataset\": \"D5\",\n",
    "            \"target_dataset\": \"D2\",\n",
    "            \"threshold_policy\": \"Fixed threshold = mean VAL-opt threshold from D5 monolingual test summary (Youden J on D5 VAL)\",\n",
    "            \"fixed_threshold_value\": float(FIXED_THR),\n",
    "            \"d5_mono_summary_path\": str(D5_MONO_SUMMARY),\n",
    "        },\n",
    "\n",
    "        \"target\": {\n",
    "            \"dataset\": \"D2\",\n",
    "            \"seed\": int(seed),\n",
    "            \"n_test\": int(len(test_df)),\n",
    "            \"label_counts_test\": test_df[\"label_num\"].value_counts(dropna=False).to_dict(),\n",
    "            \"sex_counts_test_norm\": test_df[\"sex_norm\"].value_counts(dropna=False).to_dict(),\n",
    "        },\n",
    "\n",
    "        \"test_auroc\": float(test_auc),\n",
    "\n",
    "        \"threshold_metrics_test\": thr_metrics,\n",
    "        \"test_threshold_used\": float(FIXED_THR),\n",
    "\n",
    "        \"fairness_test\": {\n",
    "            \"definition\": \"H3: ΔFNR = FNR(F) - FNR(M), where FNR(sex)=FN/(FN+TP) computed on PD-only true labels at fixed transfer threshold.\",\n",
    "            \"fnr_by_sex_norm\": fnr_by_sex,\n",
    "            \"delta_fnr_F_minus_M\": float(delta_f_minus_m),\n",
    "            \"delta_fnr_abs\": float(delta_abs),\n",
    "            \"note\": \"If n_PD for a sex is 0, its FNR is NaN and ΔFNR is NaN.\",\n",
    "            \"sex_normalization_note\": \"D2 mapping: male->M, female->F; otherwise UNK.\",\n",
    "        },\n",
    "\n",
    "        \"confusion_by_sex_norm\": confusion_by_sex,\n",
    "\n",
    "        \"artifacts\": {\n",
    "            \"roc_curve_png\": str(roc_png),\n",
    "            \"confusion_matrix_png\": str(cm_png),\n",
    "            \"confusion_matrix_M_png\": str(cm_m_png) if cm_m_png is not None else None,\n",
    "            \"confusion_matrix_F_png\": str(cm_f_png) if cm_f_png is not None else None,\n",
    "        },\n",
    "\n",
    "        \"paths\": {\n",
    "            \"d2_out_root\": str(D2_OUT_ROOT),\n",
    "            \"zero_shot_root\": str(ZS_ROOT),\n",
    "            \"run_dir\": str(run_dir),\n",
    "            \"source_trainval_experiment_used\": str(chosen_exp),\n",
    "            \"source_best_heads_path\": str(best_heads_path),\n",
    "        },\n",
    "\n",
    "        \"backbone_ckpt\": BACKBONE_CKPT,\n",
    "        \"dropout_p\": float(DROPOUT_P),\n",
    "        \"timestamp\": time.strftime(\"%Y%m%d_%H%M%S\"),\n",
    "    }\n",
    "\n",
    "    metrics_path = run_dir / \"metrics.json\"\n",
    "    with open(metrics_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(metrics, f, indent=2)\n",
    "\n",
    "    print(f\"[seed={seed}] DONE | test_AUROC={test_auc:.6f}\")\n",
    "\n",
    "    def _fmt_fnr(g):\n",
    "        d = fnr_by_sex.get(g, None)\n",
    "        if d is None:\n",
    "            return \"n/a\"\n",
    "        return f\"fnr={d['fnr']:.6f} (n_PD={d['n_pos']}, fn={d['fn']}, tp={d['tp']})\"\n",
    "\n",
    "    print(f\"[seed={seed}] TEST metrics @ fixed_thr={FIXED_THR:.6f}\")\n",
    "    print(f\"[seed={seed}] FAIRNESS (H3) @ fixed_thr={FIXED_THR:.6f}:\")\n",
    "    print(\"  M:\", _fmt_fnr(\"M\"))\n",
    "    print(\"  F:\", _fmt_fnr(\"F\"))\n",
    "    if \"UNK\" in fnr_by_sex:\n",
    "        print(\"  UNK:\", _fmt_fnr(\"UNK\"))\n",
    "    print(\"  ΔFNR (F-M):\", f\"{delta_f_minus_m:.6f}\" if not np.isnan(delta_f_minus_m) else \"nan\")\n",
    "    print(\"  |ΔFNR|:\", f\"{delta_abs:.6f}\" if not np.isnan(delta_abs) else \"nan\")\n",
    "\n",
    "    print(f\"[seed={seed}] WROTE:\")\n",
    "    print(\" \", str(metrics_path))\n",
    "    print(\" \", str(roc_png))\n",
    "    print(\" \", str(cm_png))\n",
    "    if cm_m_png is not None:\n",
    "        print(\" \", str(cm_m_png))\n",
    "    if cm_f_png is not None:\n",
    "        print(\" \", str(cm_f_png))\n",
    "\n",
    "    return {\n",
    "        \"seed\": int(seed),\n",
    "        \"test_auroc\": float(test_auc),\n",
    "        \"thr_metrics_test\": thr_metrics,\n",
    "        \"fnr_by_sex_test\": fnr_by_sex,\n",
    "        \"delta_f_minus_m_test\": float(delta_f_minus_m),\n",
    "        \"delta_abs_test\": float(delta_abs),\n",
    "        \"run_dir\": str(run_dir),\n",
    "    }\n",
    "\n",
    "# -------------------------\n",
    "# Aggregate across 3 seeds: AUROC mean ± 95% CI, others mean ± SD\n",
    "# Inputs: per-seed outputs from run_seed()\n",
    "# Output: summary_zero_shot3.json + history_index.jsonl under ZS_ROOT\n",
    "# -------------------------\n",
    "seed_results = []\n",
    "for seed in SEEDS:\n",
    "    seed_results.append(run_seed(seed))\n",
    "\n",
    "aurocs = [r[\"test_auroc\"] for r in seed_results]\n",
    "t_crit = 4.302652729911275  # df=2, 95% CI\n",
    "n = len(aurocs)\n",
    "mean_auc = float(np.mean(aurocs))\n",
    "std_auc = float(np.std(aurocs, ddof=1)) if n > 1 else 0.0\n",
    "half_width = float(t_crit * (std_auc / math.sqrt(n))) if n > 1 else 0.0\n",
    "ci95 = [float(mean_auc - half_width), float(mean_auc + half_width)]\n",
    "\n",
    "def _mean_sd(vals):\n",
    "    vals = np.asarray(vals, dtype=np.float64)\n",
    "    return float(np.nanmean(vals)), float(np.nanstd(vals, ddof=1)) if np.sum(~np.isnan(vals)) > 1 else 0.0\n",
    "\n",
    "thr_list = [r[\"thr_metrics_test\"] for r in seed_results]\n",
    "keys = [\"accuracy\",\"precision\",\"recall\",\"f1_score\",\"sensitivity\",\"specificity\",\"mcc\",\"p_value_fisher_two_sided\"]\n",
    "\n",
    "agg = {}\n",
    "for k in keys:\n",
    "    v = [float(tm.get(k, float(\"nan\"))) for tm in thr_list]\n",
    "    mu, sd = _mean_sd(v)\n",
    "    agg[k] = {\n",
    "        \"mean\": mu,\n",
    "        \"sd\": sd,\n",
    "        \"values_by_seed\": {str(r[\"seed\"]): float(tm.get(k, float(\"nan\"))) for r, tm in zip(seed_results, thr_list)}\n",
    "    }\n",
    "\n",
    "cm_by_seed = {str(r[\"seed\"]): r[\"thr_metrics_test\"][\"confusion_matrix\"] for r in seed_results}\n",
    "\n",
    "# Fairness aggregation (H3): mean ± SD across seeds\n",
    "fnr_by_seed = {str(r[\"seed\"]): r[\"fnr_by_sex_test\"] for r in seed_results}\n",
    "delta_signed_by_seed = {str(r[\"seed\"]): float(r[\"delta_f_minus_m_test\"]) for r in seed_results}\n",
    "delta_abs_by_seed = {str(r[\"seed\"]): float(r[\"delta_abs_test\"]) for r in seed_results}\n",
    "\n",
    "fnr_m_vals, fnr_f_vals, n_pd_m_vals, n_pd_f_vals = [], [], [], []\n",
    "d_signed_vals, d_abs_vals = [], []\n",
    "\n",
    "for r in seed_results:\n",
    "    s = str(r[\"seed\"])\n",
    "    d = fnr_by_seed.get(s, {})\n",
    "    fnr_m_vals.append(float(d.get(\"M\", {}).get(\"fnr\", float(\"nan\"))))\n",
    "    fnr_f_vals.append(float(d.get(\"F\", {}).get(\"fnr\", float(\"nan\"))))\n",
    "    n_pd_m_vals.append(float(d.get(\"M\", {}).get(\"n_pos\", float(\"nan\"))))\n",
    "    n_pd_f_vals.append(float(d.get(\"F\", {}).get(\"n_pos\", float(\"nan\"))))\n",
    "    d_signed_vals.append(float(delta_signed_by_seed.get(s, float(\"nan\"))))\n",
    "    d_abs_vals.append(float(delta_abs_by_seed.get(s, float(\"nan\"))))\n",
    "\n",
    "fnr_m_mean, fnr_m_sd = _mean_sd(fnr_m_vals)\n",
    "fnr_f_mean, fnr_f_sd = _mean_sd(fnr_f_vals)\n",
    "d_signed_mean, d_signed_sd = _mean_sd(d_signed_vals)\n",
    "d_abs_mean, d_abs_sd = _mean_sd(d_abs_vals)\n",
    "\n",
    "print(\"\\nZero-shot TEST AUROC by seed:\")\n",
    "for r in seed_results:\n",
    "    print(f\"  seed {r['seed']}: {r['test_auroc']:.6f}\")\n",
    "print(f\"\\nMean Zero-shot TEST AUROC: {mean_auc:.6f}\")\n",
    "print(f\"95% CI (t, n=3): [{ci95[0]:.6f}, {ci95[1]:.6f}]\")\n",
    "\n",
    "print(f\"\\nFixed threshold used for ALL seeds (from D5 mono mean VAL-opt): {FIXED_THR:.6f}\")\n",
    "\n",
    "print(\"\\nThreshold metrics on D2 TEST @ fixed threshold (mean ± SD across seeds):\")\n",
    "for k in [\"accuracy\",\"precision\",\"sensitivity\",\"specificity\",\"f1_score\",\"mcc\"]:\n",
    "    mu = agg[k][\"mean\"]\n",
    "    sd = agg[k][\"sd\"]\n",
    "    print(f\"  {k}: {mu:.6f} ± {sd:.6f}\")\n",
    "print(\"  fisher_p_value_two_sided:\", f\"{agg['p_value_fisher_two_sided']['mean']:.6g} ± {agg['p_value_fisher_two_sided']['sd']:.6g}\")\n",
    "\n",
    "print(\"\\nFAIRNESS (H3) on D2 TEST @ fixed threshold (mean ± SD across seeds):\")\n",
    "print(f\"  FNR_M: {fnr_m_mean:.6f} ± {fnr_m_sd:.6f}\")\n",
    "print(f\"  FNR_F: {fnr_f_mean:.6f} ± {fnr_f_sd:.6f}\")\n",
    "print(f\"  ΔFNR (F-M): {d_signed_mean:.6f} ± {d_signed_sd:.6f}\")\n",
    "print(f\"  |ΔFNR|: {d_abs_mean:.6f} ± {d_abs_sd:.6f}\")\n",
    "\n",
    "summary = {\n",
    "    \"transfer\": {\n",
    "        \"tag\": TRANSFER_TAG,\n",
    "        \"source_dataset\": \"D5\",\n",
    "        \"target_dataset\": \"D2\",\n",
    "        \"fixed_threshold_value\": float(FIXED_THR),\n",
    "        \"fixed_threshold_source\": {\n",
    "            \"path\": str(D5_MONO_SUMMARY),\n",
    "            \"key\": \"val_opt_threshold_mean_sd.mean\",\n",
    "            \"note\": \"Mean across seeds of D5 VAL-opt thresholds (Youden J) as recorded by D5 monolingual test run.\",\n",
    "        },\n",
    "        \"source_trainval_experiment_used\": str(chosen_exp),\n",
    "    },\n",
    "\n",
    "    \"target\": {\n",
    "        \"dx_out_root\": str(D2_OUT_ROOT),\n",
    "        \"manifest_all\": str(D2_MANIFEST_ALL),\n",
    "        \"n_test\": int(len(test_df)),\n",
    "        \"label_counts_test\": test_df[\"label_num\"].value_counts(dropna=False).to_dict(),\n",
    "        \"sex_counts_test_norm\": test_df[\"sex_norm\"].value_counts(dropna=False).to_dict(),\n",
    "    },\n",
    "\n",
    "    \"seeds\": SEEDS,\n",
    "\n",
    "    \"test_aurocs_by_seed\": {str(r[\"seed\"]): float(r[\"test_auroc\"]) for r in seed_results},\n",
    "    \"mean_test_auroc\": float(mean_auc),\n",
    "    \"t_crit_df2_95\": float(t_crit),\n",
    "    \"half_width_95_ci\": float(half_width),\n",
    "    \"ci95_test_auroc\": ci95,\n",
    "\n",
    "    \"threshold_metrics_test_mean_sd\": agg,\n",
    "    \"confusion_matrix_by_seed\": cm_by_seed,\n",
    "\n",
    "    \"fairness_test\": {\n",
    "        \"definition\": \"H3: ΔFNR = FNR(F) - FNR(M), where FNR(sex)=FN/(FN+TP) computed on PD-only true labels at the fixed transfer threshold.\",\n",
    "        \"fnr_by_sex_norm_by_seed\": fnr_by_seed,\n",
    "        \"delta_fnr_F_minus_M_by_seed\": delta_signed_by_seed,\n",
    "        \"delta_fnr_abs_by_seed\": delta_abs_by_seed,\n",
    "        \"fnr_M_mean_sd\": {\"mean\": float(fnr_m_mean), \"sd\": float(fnr_m_sd), \"values_by_seed\": {str(s): float(v) for s, v in zip(SEEDS, fnr_m_vals)}},\n",
    "        \"fnr_F_mean_sd\": {\"mean\": float(fnr_f_mean), \"sd\": float(fnr_f_sd), \"values_by_seed\": {str(s): float(v) for s, v in zip(SEEDS, fnr_f_vals)}},\n",
    "        \"delta_fnr_F_minus_M_mean_sd\": {\"mean\": float(d_signed_mean), \"sd\": float(d_signed_sd), \"values_by_seed\": {str(s): float(v) for s, v in zip(SEEDS, d_signed_vals)}},\n",
    "        \"delta_fnr_abs_mean_sd\": {\"mean\": float(d_abs_mean), \"sd\": float(d_abs_sd), \"values_by_seed\": {str(s): float(v) for s, v in zip(SEEDS, d_abs_vals)}},\n",
    "        \"denominators_PD_by_seed\": {str(s): {\"n_PD_M\": float(n_pd_m_vals[i]), \"n_PD_F\": float(n_pd_f_vals[i])} for i, s in enumerate(SEEDS)},\n",
    "        \"sex_normalization_note\": \"D2 mapping: male->M, female->F; otherwise UNK. ΔFNR computed only when both M and F have defined FNR.\",\n",
    "    },\n",
    "\n",
    "    \"run_dirs\": [r[\"run_dir\"] for r in seed_results],\n",
    "    \"timestamp\": time.strftime(\"%Y%m%d_%H%M%S\"),\n",
    "}\n",
    "\n",
    "# Save summary as requested\n",
    "summary_path = ZS_ROOT / \"summary_zero_shot3.json\"\n",
    "with open(summary_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "history_path = ZS_ROOT / \"history_index.jsonl\"\n",
    "with open(history_path, \"a\", encoding=\"utf-8\") as f:\n",
    "    f.write(json.dumps(summary) + \"\\n\")\n",
    "\n",
    "print(\"\\nWROTE summary:\", str(summary_path))\n",
    "print(\"APPENDED history index:\", str(history_path))\n",
    "print(\"Open this folder to access artifacts:\", str(ZS_ROOT))\n",
    "\n",
    "# -------------------------\n",
    "# Stop runtime (Colab)\n",
    "# Inputs: none\n",
    "# Output: attempts to release the GPU instance\n",
    "# -------------------------\n",
    "print(\"\\nAll done. Unassigning the runtime to stop the L4 instance...\")\n",
    "try:\n",
    "    from google.colab import runtime  # type: ignore\n",
    "    print(\"Calling runtime.unassign() now.\")\n",
    "    runtime.unassign()\n",
    "except Exception as e:\n",
    "    print(\"Could not unassign runtime automatically. You can stop the runtime manually in Colab.\")\n",
    "    print(\"Reason:\", repr(e))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "17a2095c538e4b5283d15fac30409329",
      "85933ec8f0d641139902d3e5653ab1d4",
      "672c01d999124733a48008f9cc1719c9",
      "e2bcb24b053d4da6ae36fdefb803a426",
      "b6e7afa59a3d48e097253ab4e3bc9993",
      "b8885c1454c64bf69fe21ee8d03154a9",
      "4e3b04b6f460450b981e8cdac99ecbb8",
      "e23e9fd03538416eb151edf1ccc2dde3",
      "9612ec2c1f504fa5921529c5ca179f85",
      "bd7fd7db82554f0390c9cec0d473ab2b",
      "3aeba02aa1684102b26807f59845bff2",
      "980ed0cf8fdc444baba4490fff02129e",
      "ad8a155f24394c55b426f848aea3d7f9",
      "fdf9e5096f944f5a900d4a998586c578",
      "c99054c3f8a94a6db6514ac87bc29552",
      "06f308d7921e43a6a9836c2629bdddb2",
      "ed30ae92d3b84837a81540947de0ed86",
      "0a90ce3ffb16469aa502439e4249c763",
      "563d55bcbc1f4857b460eff7e03e29ee",
      "2f683ae0c95c4ca1ba91061538771018",
      "dbb5b79905bd42d182e6907e124b8159",
      "734b6507335646c5a9425867e364022f",
      "10c3c8a2cce7496583efc38777764a07",
      "9806e1da9ac843368e6e2d83d4a94bd1",
      "7df8fe47ee574ab9b66c23ede5f83234",
      "7681be0e688147a985508d625ba95362",
      "9bde19a38d7348fb926a7d8d7d95ee34",
      "a12a560405f34dee844a2a2f8fd91324",
      "e82e6269e5e8452cb6b84cdf2926f587",
      "3e1acc33b1584db69ec39e0ee835272b",
      "441e6d9dddda47bdbdee475d8af63aff",
      "1e1060fb8e984526b54daeac893ca53a",
      "0d5882ccd1db4dff9d92cf7e584e9949",
      "1bd896afaf124f8bac600cd2ca905ff9",
      "b9a61754558141d6989b304e5b3870de",
      "f4780f841cf84109a434789469e94984",
      "e5704c4d73c947c8a8df8a98bfdd845e",
      "53f1007ec51a49a59b5cdfe4909f7d15",
      "5a1d8d8db4bf4d589f58db78be836f14",
      "209c22d041584faf824ff42c58c21ab2",
      "d4ac007bebec40f3a61d4f87054c7868",
      "b953d9c6c86b44f887c851bb3051d46e",
      "999944e39dbb46feb2ec3c259ed2af28",
      "cf7f9fb71091473a8bd7fc5b7e9d351b",
      "6d83ffc7a13c4da482d1ff5bfb712604",
      "4fb57d0c4bbc4f158b4cfbc9dad477a1",
      "f4e79f35c9504b23b64689897e694ea1",
      "88189e2433cd4ebb98211c2b3e727379",
      "0ce0efaf29d941a784106c57dc05a8e9",
      "7fbaf2a6b2c8480e8e9c543192aceef5",
      "bb204a7f5b7940bd92a454bffe4fab82",
      "7a502799803643e9a9fe67f55972d9d0",
      "53af7fc261434c608f118e013be97bd7",
      "05e8891c558e483aa82e261b393dd59c",
      "ed519967df88423bb257ab456e7b9030",
      "9ca8036b07304fa8a851f6233ebe7288",
      "636cbb38af834da8a49c88db301e92a5",
      "715803826f444c50bb6e026b14bcad17",
      "6421712fa3824a6eb2825d649f068fab",
      "ae9a11a3ab3a4859a471e2ab0fc506af",
      "c60dcf9d784f498c863e1782393f299a",
      "9a169d72f58a4384a8659f7ef5d7d8f5",
      "ec3e89beec7f4bcc85529ee9593971ce",
      "4782fe2ef1d04c5595f74525cb9c0a42",
      "7af1479f6dfd4f2aa09cbca2905bc64c",
      "f2451de8ea7f44babd49d1ad517f6183"
     ]
    },
    "id": "wMs0jVG7J5Nu"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "The following cell runs a cross language zero shot test from English Ah Sound (D6) to Slovak (D2). It evaluates whether classifier heads trained on the source dataset D6 can detect Parkinson’s disease in the target dataset D2 without any retraining on D2. The cell reads only the D2 test split from `D2/manifests/manifest_all.csv` and evaluates it using the most recent completed D6 train and validation experiment that contains saved head checkpoints for three seeds, 1337, 2024, and 7777.\n",
    "\n",
    "A single fixed decision threshold is used for all seeds and all D2 test clips. This threshold is taken from D6’s saved `monolingual_test_runs/summary_test.json`. The preferred value is `val_opt_threshold_mean_sd.mean`, with a fallback to the mean of `val_opt_threshold_by_seed` if needed. This avoids tuning any threshold on D2 and keeps the transfer evaluation consistent.\n",
    "\n",
    "Before inference begins, the cell runs several safety and data preparation checks. It confirms that Google Drive is mounted, checks that no local files are shadowing PyTorch or Transformers, prints the active paths and compute device, loads the D2 manifest, verifies that all required columns are present, filters strictly to the test split, and stops early if any referenced audio files are missing. Each clip is assigned to a simple task group, where `task == \"vowl\"` is treated as vowel and everything else is treated as other. Sex values in D2, such as male or female and common variants, are normalized to `sex_norm` values of M, F, or UNK so sex based reporting works even if unexpected values appear.\n",
    "\n",
    "For each seed, the cell finds the correct D6 head checkpoint (`best_heads.pt`) from the selected D6 experiment folder. It rebuilds the same frozen Wav2Vec2 two head classifier used elsewhere, with one head for vowel clips and one for other clips, and loads the saved head weights. Inference is then run on the full D2 test set to produce a Parkinson’s probability for each clip. Using these scores, the cell computes overall AUROC, which is threshold free, and fixed threshold metrics at the D6 derived threshold. These include the confusion matrix, accuracy, precision, sensitivity or recall, specificity, F1 score, MCC, and the Fisher exact test p value. Fairness is computed as ΔFNR, defined as FNR(F) minus FNR(M), using only true Parkinson’s clips. Confusion counts by sex are also recorded. For each seed, the cell saves a ROC curve, an overall confusion matrix, additional sex specific confusion matrices when both M and F are present, and a detailed `metrics.json` file under\n",
    "`<D2_OUT_ROOT>/Cross_Language_Zero_Shot_Runs/run_ENUS_AH_to_SK_seed<seed>/`.\n",
    "\n",
    "After all three seeds complete, results are combined across seeds. The cell reports the mean AUROC with a 95 percent confidence interval using a t distribution with n equal to 3, along with the mean and standard deviation of the fixed threshold metrics and fairness values. A combined summary is written to `summary_zero_shot4.json` and the same record is appended to `history_index.jsonl`, both under `<D2_OUT_ROOT>/Cross_Language_Zero_Shot_Runs/`. Finally, the Colab runtime is unassigned to shut down the GPU."
   ],
   "metadata": {
    "id": "nt-gNLG2zM1D"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# =========================\n",
    "# ENUS_AH → SK Zero-shot (D6 → D2): Test-only transfer with fixed threshold\n",
    "# - Loads D6-trained heads (most recent train+val experiment)\n",
    "# - Runs inference on D2 TEST only (from manifest_all.csv)\n",
    "# - Uses one fixed threshold read from D6 monolingual summary_test.json\n",
    "# - Saves per-seed metrics + plots, plus an aggregated summary under D2\n",
    "# =========================\n",
    "\n",
    "import os, json, math, random, time, warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import soundfile as sf\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from transformers import Wav2Vec2Model\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, confusion_matrix, matthews_corrcoef\n",
    "from scipy.stats import fisher_exact\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# -------------------------\n",
    "# Safety checks: avoid importing local files named like core libraries\n",
    "# Inputs: /content directory\n",
    "# Output: raises an error early if a naming conflict exists\n",
    "# -------------------------\n",
    "if os.path.exists(\"/content/torch.py\") or os.path.exists(\"/content/torch/__init__.py\"):\n",
    "    raise RuntimeError(\"Found local /content/torch.py or /content/torch/ that shadows PyTorch. Rename/remove it and restart runtime.\")\n",
    "if os.path.exists(\"/content/transformers.py\") or os.path.exists(\"/content/transformers/__init__.py\"):\n",
    "    raise RuntimeError(\"Found local /content/transformers.py or /content/transformers/ that shadows Hugging Face Transformers. Rename/remove it and restart runtime.\")\n",
    "\n",
    "# -------------------------\n",
    "# Drive mount (Colab)\n",
    "# Inputs: Google Drive\n",
    "# Output: /content/drive mounted so manifests, checkpoints, and outputs can be read/written\n",
    "# -------------------------\n",
    "if not os.path.isdir(\"/content/drive/MyDrive\"):\n",
    "    from google.colab import drive  # type: ignore\n",
    "    drive.mount(\"/content/drive\")\n",
    "\n",
    "# -------------------------\n",
    "# Paths: target (D2) + source (D6)\n",
    "# Inputs: optional globals D2_OUT_ROOT, D6_OUT_ROOT (if set earlier), else fallbacks\n",
    "# Output: D2_MANIFEST_ALL and D6_MONO_SUMMARY paths\n",
    "# -------------------------\n",
    "D2_OUT_ROOT_FALLBACK = \"/content/drive/MyDrive/AI_PD_Project/Datasets/D2-Slovak (EWA-DB)/EWA-DB/preprocessed_v1\"\n",
    "D6_OUT_ROOT_FALLBACK = \"/content/drive/MyDrive/AI_PD_Project/Datasets/D6-Ah Sound (Figshare)/preprocessed_v1\"\n",
    "\n",
    "D2_OUT_ROOT = globals().get(\"D2_OUT_ROOT\", D2_OUT_ROOT_FALLBACK)\n",
    "D6_OUT_ROOT = globals().get(\"D6_OUT_ROOT\", D6_OUT_ROOT_FALLBACK)\n",
    "\n",
    "D2_MANIFEST_ALL = f\"{D2_OUT_ROOT}/manifests/manifest_all.csv\"\n",
    "\n",
    "# D6 monolingual summary path (contains the fixed threshold source)\n",
    "D6_MONO_SUMMARY = Path(D6_OUT_ROOT) / \"monolingual_test_runs\" / \"summary_test.json\"\n",
    "\n",
    "# -------------------------\n",
    "# Run settings (kept consistent with other test cells)\n",
    "# Inputs: constants below\n",
    "# Output: printed configuration + runtime behavior (batching, AMP, device)\n",
    "# -------------------------\n",
    "TRANSFER_TAG    = \"ENUS_AH_to_SK\"     # D6 -> D2\n",
    "SEEDS           = [1337, 2024, 7777]\n",
    "BACKBONE_CKPT   = \"facebook/wav2vec2-base\"\n",
    "SR_EXPECTED     = 16000\n",
    "TINY_THRESH     = 1e-4\n",
    "\n",
    "EFFECTIVE_BS    = 64\n",
    "PER_DEVICE_BS   = 16\n",
    "GRAD_ACCUM      = max(1, EFFECTIVE_BS // PER_DEVICE_BS)  # printed only\n",
    "\n",
    "DROPOUT_P       = 0.2\n",
    "\n",
    "NUM_WORKERS     = 0\n",
    "PIN_MEMORY      = False\n",
    "\n",
    "USE_AMP         = True\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "try:\n",
    "    torch.set_float32_matmul_precision(\"high\")\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", message=\"None of the inputs have requires_grad=True. Gradients will be None\")\n",
    "warnings.filterwarnings(\"ignore\", message=\"Passing `gradient_checkpointing` to a config initialization is deprecated\")\n",
    "\n",
    "print(\"D2_OUT_ROOT (target):\", D2_OUT_ROOT)\n",
    "print(\"D2_MANIFEST_ALL:\", D2_MANIFEST_ALL)\n",
    "print(\"D6_OUT_ROOT (source):\", D6_OUT_ROOT)\n",
    "print(\"D6_MONO_SUMMARY:\", str(D6_MONO_SUMMARY))\n",
    "print(\"DEVICE:\", DEVICE)\n",
    "if DEVICE.type == \"cuda\":\n",
    "    print(\"GPU:\", torch.cuda.get_device_name(0))\n",
    "print(\"PER_DEVICE_BS:\", PER_DEVICE_BS, \"| EFFECTIVE_BS:\", PER_DEVICE_BS * GRAD_ACCUM)\n",
    "print(\"NUM_WORKERS:\", NUM_WORKERS, \"| PIN_MEMORY:\", PIN_MEMORY)\n",
    "print(\"USE_AMP:\", bool(USE_AMP and DEVICE.type == \"cuda\"))\n",
    "\n",
    "# -------------------------\n",
    "# Fixed threshold: read from D6 monolingual summary_test.json\n",
    "# Inputs: D6_MONO_SUMMARY (JSON)\n",
    "# Output: FIXED_THR (float) used for all three seeds on D2\n",
    "# -------------------------\n",
    "if not D6_MONO_SUMMARY.exists():\n",
    "    raise FileNotFoundError(\n",
    "        \"Missing D6 monolingual summary_test.json.\\n\"\n",
    "        f\"Expected: {str(D6_MONO_SUMMARY)}\\n\"\n",
    "        \"Run the D6 monolingual test cell first (the one that writes summary_test.json).\"\n",
    "    )\n",
    "\n",
    "with open(D6_MONO_SUMMARY, \"r\", encoding=\"utf-8\") as f:\n",
    "    d6_sum = json.load(f)\n",
    "\n",
    "# D6 schema: summary['val_opt_threshold_mean_sd']['mean'] (preferred)\n",
    "if \"val_opt_threshold_mean_sd\" in d6_sum and isinstance(d6_sum[\"val_opt_threshold_mean_sd\"], dict):\n",
    "    FIXED_THR = float(d6_sum[\"val_opt_threshold_mean_sd\"].get(\"mean\", float(\"nan\")))\n",
    "else:\n",
    "    # Fallback: mean of per-seed values if present\n",
    "    by_seed = d6_sum.get(\"val_opt_threshold_by_seed\", None)\n",
    "    if isinstance(by_seed, dict) and len(by_seed) > 0:\n",
    "        FIXED_THR = float(np.nanmean([float(v) for v in by_seed.values()]))\n",
    "    else:\n",
    "        FIXED_THR = float(\"nan\")\n",
    "\n",
    "if not np.isfinite(FIXED_THR):\n",
    "    raise RuntimeError(\n",
    "        \"Could not read a finite fixed threshold from D6 monolingual summary_test.json.\\n\"\n",
    "        \"Expected keys: val_opt_threshold_mean_sd.mean (preferred) or val_opt_threshold_by_seed.\"\n",
    "    )\n",
    "\n",
    "print(f\"\\nFixed transfer threshold (from D6 monolingual mean VAL-opt): {FIXED_THR:.6f}\")\n",
    "\n",
    "# -------------------------\n",
    "# Target manifest: load D2 and keep TEST only\n",
    "# Inputs: D2 manifest_all.csv\n",
    "# Output: test_df with required columns + basic counts printed\n",
    "# -------------------------\n",
    "if not os.path.exists(D2_MANIFEST_ALL):\n",
    "    raise FileNotFoundError(f\"Missing D2 manifest_all.csv: {D2_MANIFEST_ALL}\")\n",
    "\n",
    "m_all = pd.read_csv(D2_MANIFEST_ALL)\n",
    "\n",
    "# Keep sex for fairness and sex-specific confusion matrices\n",
    "req_cols = {\"split\", \"clip_path\", \"label_num\", \"task\", \"sex\"}\n",
    "missing = [c for c in sorted(req_cols) if c not in m_all.columns]\n",
    "if missing:\n",
    "    raise ValueError(f\"D2 manifest missing required columns: {missing}. Found: {list(m_all.columns)}\")\n",
    "\n",
    "# Infer dataset_id and ensure this is really D2\n",
    "if \"dataset\" in m_all.columns and m_all[\"dataset\"].notna().any():\n",
    "    dataset_id = str(m_all[\"dataset\"].astype(str).value_counts(dropna=True).idxmax())\n",
    "    m_all = m_all[m_all[\"dataset\"].astype(str) == dataset_id].copy()\n",
    "else:\n",
    "    dataset_id = \"DX\"\n",
    "\n",
    "if dataset_id != \"D2\":\n",
    "    raise RuntimeError(\n",
    "        f\"Expected dataset_id=='D2' but got {dataset_id!r}. \"\n",
    "        \"This usually means D2_OUT_ROOT is wrong or inherited from a previous cell.\\n\"\n",
    "        f\"D2_OUT_ROOT={D2_OUT_ROOT}\"\n",
    "    )\n",
    "\n",
    "# Keep a stable column set (missing ones are filled with NaN)\n",
    "keep_cols = [\"clip_path\", \"label_num\", \"task\", \"speaker_id\", \"sex\", \"age\", \"duration_sec\", \"split\"]\n",
    "for c in keep_cols:\n",
    "    if c not in m_all.columns:\n",
    "        m_all[c] = np.nan\n",
    "m_all = m_all[keep_cols].copy()\n",
    "\n",
    "test_df = m_all[m_all[\"split\"].isin([\"test\"])].copy().reset_index(drop=True)\n",
    "\n",
    "print(f\"\\nTarget dataset inferred: {dataset_id}\")\n",
    "print(f\"TEST rows: {len(test_df)}\")\n",
    "if len(test_df) == 0:\n",
    "    raise RuntimeError(\"After filtering to split=='test', D2 manifest has 0 rows.\")\n",
    "\n",
    "print(\"TEST label counts:\", test_df[\"label_num\"].value_counts(dropna=False).to_dict())\n",
    "print(\"TEST sex counts (raw):\", test_df[\"sex\"].value_counts(dropna=False).to_dict())\n",
    "\n",
    "# -------------------------\n",
    "# Path check: ensure target audio files exist\n",
    "# Inputs: test_df.clip_path\n",
    "# Output: raises early if any clip files are missing (shows a few examples)\n",
    "# -------------------------\n",
    "def _fail_fast_missing_paths(df: pd.DataFrame, name: str):\n",
    "    missing_paths = []\n",
    "    for p in tqdm(df[\"clip_path\"].astype(str).tolist(), desc=f\"Check {name} clip_path exists\", dynamic_ncols=True):\n",
    "        if not os.path.exists(p):\n",
    "            missing_paths.append(p)\n",
    "            if len(missing_paths) >= 10:\n",
    "                break\n",
    "    if missing_paths:\n",
    "        raise FileNotFoundError(f\"{name}: missing clip_path(s). Examples: {missing_paths}\")\n",
    "\n",
    "_fail_fast_missing_paths(test_df, \"TEST\")\n",
    "\n",
    "# -------------------------\n",
    "# Task grouping: map each row to vowel vs other\n",
    "# Inputs: test_df.task\n",
    "# Output: test_df.task_group used to pick the correct head during inference\n",
    "# -------------------------\n",
    "def _task_group(task_val) -> str:\n",
    "    return \"vowel\" if str(task_val) == \"vowl\" else \"other\"\n",
    "\n",
    "test_df[\"task_group\"] = test_df[\"task\"].apply(_task_group)\n",
    "\n",
    "# -------------------------\n",
    "# Sex normalization for fairness/charts (D2 uses male/female)\n",
    "# Inputs: test_df.sex\n",
    "# Output: test_df.sex_norm in {M,F,UNK}\n",
    "# -------------------------\n",
    "def normalize_sex_d2(val) -> str:\n",
    "    \"\"\"\n",
    "    Returns 'M', 'F', or 'UNK'\n",
    "    D2 encoding (per manifest):\n",
    "      male / female\n",
    "    Also handles common variants.\n",
    "    \"\"\"\n",
    "    if pd.isna(val):\n",
    "        return \"UNK\"\n",
    "\n",
    "    s = str(val).strip().lower()\n",
    "    if s in {\"m\", \"male\", \"man\", \"masc\", \"masculine\"}:\n",
    "        return \"M\"\n",
    "    if s in {\"f\", \"female\", \"woman\", \"fem\", \"feminine\"}:\n",
    "        return \"F\"\n",
    "    return \"UNK\"\n",
    "\n",
    "test_df[\"sex_norm\"] = test_df[\"sex\"].apply(normalize_sex_d2)\n",
    "print(\"TEST sex counts (normalized):\", test_df[\"sex_norm\"].value_counts(dropna=False).to_dict())\n",
    "if (test_df[\"sex_norm\"] == \"UNK\").any():\n",
    "    print(\"NOTE: Some D2 'sex' values could not be normalized to M/F and were counted as 'UNK' for fairness and sex charts.\")\n",
    "\n",
    "# -------------------------\n",
    "# Dataset + collator: read audio and build attention masks\n",
    "# Inputs: test_df rows + audio files\n",
    "# Output: DataLoader batches with padded input_values, attention_mask, labels, task_group, sex_norm\n",
    "# -------------------------\n",
    "class AudioManifestDataset(Dataset):\n",
    "    def __init__(self, df: pd.DataFrame):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        row = self.df.iloc[idx]\n",
    "        clip_path = str(row[\"clip_path\"])\n",
    "        label = int(row[\"label_num\"])\n",
    "        task_group = str(row[\"task_group\"])\n",
    "        sex_norm = str(row[\"sex_norm\"])\n",
    "\n",
    "        if not os.path.exists(clip_path):\n",
    "            raise FileNotFoundError(f\"Missing clip_path: {clip_path}\")\n",
    "\n",
    "        y, sr = sf.read(clip_path, always_2d=False)\n",
    "        if isinstance(y, np.ndarray) and y.ndim > 1:\n",
    "            y = np.mean(y, axis=1)\n",
    "        y = np.asarray(y, dtype=np.float32)\n",
    "\n",
    "        if int(sr) != SR_EXPECTED:\n",
    "            raise ValueError(f\"Sample rate mismatch (expected {SR_EXPECTED}): {clip_path} has sr={sr}\")\n",
    "\n",
    "        # attention_mask mainly matters for vowel clips (ignore padded tail)\n",
    "        attn = np.ones((len(y),), dtype=np.int64)\n",
    "        if task_group == \"vowel\":\n",
    "            k = -1\n",
    "            for j in range(len(y) - 1, -1, -1):\n",
    "                if abs(float(y[j])) > TINY_THRESH:\n",
    "                    k = j\n",
    "                    break\n",
    "            if k >= 0:\n",
    "                attn[:k+1] = 1\n",
    "                attn[k+1:] = 0\n",
    "            else:\n",
    "                attn[:] = 1\n",
    "        else:\n",
    "            attn[:] = 1\n",
    "\n",
    "        return {\n",
    "            \"input_values\": torch.from_numpy(y),\n",
    "            \"attention_mask\": torch.from_numpy(attn),\n",
    "            \"labels\": torch.tensor(label, dtype=torch.long),\n",
    "            \"task_group\": task_group,\n",
    "            \"sex_norm\": sex_norm,\n",
    "        }\n",
    "\n",
    "def collate_fn(batch):\n",
    "    max_len = int(max(b[\"input_values\"].numel() for b in batch))\n",
    "    input_vals, attn_masks, labels, task_groups, sex_norms = [], [], [], [], []\n",
    "    for b in batch:\n",
    "        x = b[\"input_values\"]\n",
    "        a = b[\"attention_mask\"]\n",
    "        pad = max_len - x.numel()\n",
    "        if pad > 0:\n",
    "            x = torch.cat([x, torch.zeros(pad, dtype=x.dtype)], dim=0)\n",
    "            a = torch.cat([a, torch.zeros(pad, dtype=a.dtype)], dim=0)\n",
    "        input_vals.append(x)\n",
    "        attn_masks.append(a)\n",
    "        labels.append(b[\"labels\"])\n",
    "        task_groups.append(b[\"task_group\"])\n",
    "        sex_norms.append(b[\"sex_norm\"])\n",
    "    return {\n",
    "        \"input_values\": torch.stack(input_vals, dim=0),\n",
    "        \"attention_mask\": torch.stack(attn_masks, dim=0),\n",
    "        \"labels\": torch.stack(labels, dim=0),\n",
    "        \"task_group\": task_groups,\n",
    "        \"sex_norm\": sex_norms,\n",
    "    }\n",
    "\n",
    "# -------------------------\n",
    "# Model: frozen wav2vec2 backbone + two heads (vowel vs other)\n",
    "# Inputs: backbone checkpoint name + dropout_p\n",
    "# Output: logits for PD vs healthy for each clip, using the correct head per task_group\n",
    "# -------------------------\n",
    "class Wav2Vec2TwoHeadClassifier(nn.Module):\n",
    "    def __init__(self, ckpt: str, dropout_p: float):\n",
    "        super().__init__()\n",
    "        self.backbone = Wav2Vec2Model.from_pretrained(ckpt, use_safetensors=True)\n",
    "        for p in self.backbone.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "        H = int(self.backbone.config.hidden_size)\n",
    "        self.pre_vowel = nn.Sequential(nn.LayerNorm(H), nn.Dropout(float(dropout_p)))\n",
    "        self.pre_other = nn.Sequential(nn.LayerNorm(H), nn.Dropout(float(dropout_p)))\n",
    "        self.head_vowel = nn.Linear(H, 2)\n",
    "        self.head_other = nn.Linear(H, 2)\n",
    "\n",
    "    def masked_mean_pool(self, last_hidden, attn_mask_samples):\n",
    "        feat_mask = self.backbone._get_feature_vector_attention_mask(last_hidden.shape[1], attn_mask_samples)\n",
    "        feat_mask = feat_mask.to(last_hidden.device).unsqueeze(-1).type_as(last_hidden)\n",
    "        masked = last_hidden * feat_mask\n",
    "        denom = feat_mask.sum(dim=1).clamp(min=1.0)\n",
    "        return masked.sum(dim=1) / denom\n",
    "\n",
    "    def _heads_fp32(self, x_fp_any: torch.Tensor, head: nn.Module) -> torch.Tensor:\n",
    "        x = x_fp_any.float()\n",
    "        if x.is_cuda:\n",
    "            with torch.amp.autocast(device_type=\"cuda\", enabled=False):\n",
    "                return head(x)\n",
    "        return head(x)\n",
    "\n",
    "    def forward_logits(self, input_values, attention_mask, task_group):\n",
    "        with torch.no_grad():\n",
    "            out = self.backbone(input_values=input_values, attention_mask=attention_mask)\n",
    "            last_hidden = out.last_hidden_state\n",
    "\n",
    "        pooled = self.masked_mean_pool(last_hidden, attention_mask)  # [B,H]\n",
    "\n",
    "        z_v = self.pre_vowel(pooled.float())\n",
    "        z_o = self.pre_other(pooled.float())\n",
    "\n",
    "        logits_v = self._heads_fp32(z_v, self.head_vowel)\n",
    "        logits_o = self._heads_fp32(z_o, self.head_other)\n",
    "\n",
    "        # Select the correct head per row based on task_group\n",
    "        is_vowel = torch.tensor([tg == \"vowel\" for tg in task_group], device=pooled.device, dtype=torch.bool)\n",
    "        logits = logits_o.clone()\n",
    "        if is_vowel.any():\n",
    "            logits[is_vowel] = logits_v[is_vowel]\n",
    "        return logits\n",
    "\n",
    "# -------------------------\n",
    "# Metrics + plot helpers\n",
    "# Inputs: y_true and y_prob (PD probability), plus a threshold for thresholded metrics\n",
    "# Output: numeric metrics dict + PNG charts saved to disk\n",
    "# -------------------------\n",
    "def compute_auc(y_true, y_prob):\n",
    "    y_true = np.asarray(y_true, dtype=np.int64)\n",
    "    y_prob = np.asarray(y_prob, dtype=np.float64)\n",
    "    if len(np.unique(y_true)) < 2:\n",
    "        return float(\"nan\")\n",
    "    return float(roc_auc_score(y_true, y_prob))\n",
    "\n",
    "def compute_threshold_metrics(y_true, y_prob, thr=0.5):\n",
    "    y_true = np.asarray(y_true, dtype=np.int64)\n",
    "    y_prob = np.asarray(y_prob, dtype=np.float64)\n",
    "    y_pred = (y_prob >= float(thr)).astype(np.int64)\n",
    "\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=[0, 1])\n",
    "    TN, FP, FN, TP = int(cm[0, 0]), int(cm[0, 1]), int(cm[1, 0]), int(cm[1, 1])\n",
    "\n",
    "    eps = 1e-12\n",
    "    acc = (TP + TN) / max(1, (TP + TN + FP + FN))\n",
    "    prec = TP / (TP + FP + eps)\n",
    "    rec = TP / (TP + FN + eps)     # sensitivity\n",
    "    f1 = 2 * prec * rec / (prec + rec + eps)\n",
    "    spec = TN / (TN + FP + eps)\n",
    "\n",
    "    try:\n",
    "        mcc = float(matthews_corrcoef(y_true, y_pred)) if len(np.unique(y_pred)) > 1 else float(\"nan\")\n",
    "    except Exception:\n",
    "        mcc = float(\"nan\")\n",
    "\n",
    "    try:\n",
    "        _, pval = fisher_exact([[TN, FP], [FN, TP]], alternative=\"two-sided\")\n",
    "        pval = float(pval)\n",
    "    except Exception:\n",
    "        pval = float(\"nan\")\n",
    "\n",
    "    return {\n",
    "        \"threshold\": float(thr),\n",
    "        \"confusion_matrix\": {\"TN\": TN, \"FP\": FP, \"FN\": FN, \"TP\": TP},\n",
    "        \"accuracy\": float(acc),\n",
    "        \"precision\": float(prec),\n",
    "        \"recall\": float(rec),\n",
    "        \"f1_score\": float(f1),\n",
    "        \"sensitivity\": float(rec),\n",
    "        \"specificity\": float(spec),\n",
    "        \"mcc\": float(mcc),\n",
    "        \"p_value_fisher_two_sided\": float(pval),\n",
    "    }\n",
    "\n",
    "def save_roc_curve_png(y_true, y_prob, out_png, title_suffix=\"Test\"):\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_prob)\n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr)\n",
    "    plt.plot([0, 1], [0, 1], linestyle=\"--\")\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(f\"ROC Curve ({title_suffix})\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_png, dpi=150)\n",
    "    plt.close()\n",
    "\n",
    "def save_confusion_png(y_true, y_prob, out_png, thr=0.5, title_suffix=\"Test\"):\n",
    "    y_true = np.asarray(y_true, dtype=np.int64)\n",
    "    y_prob = np.asarray(y_prob, dtype=np.float64)\n",
    "    y_pred = (y_prob >= float(thr)).astype(int)\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=[0, 1])\n",
    "\n",
    "    plt.figure()\n",
    "    plt.imshow(cm)\n",
    "    plt.xticks([0, 1], [\"Healthy(0)\", \"PD(1)\"])\n",
    "    plt.yticks([0, 1], [\"Healthy(0)\", \"PD(1)\"])\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"True\")\n",
    "    plt.title(f\"Confusion Matrix ({title_suffix}, thr={thr:.2f})\")\n",
    "    for (i, j), v in np.ndenumerate(cm):\n",
    "        plt.text(j, i, str(v), ha=\"center\", va=\"center\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_png, dpi=150)\n",
    "    plt.close()\n",
    "\n",
    "# -------------------------\n",
    "# Fairness: FNR per sex and ΔFNR = FNR(F) - FNR(M)\n",
    "# Inputs: y_true, y_prob, sex_norm array, threshold\n",
    "# Output: per-group FNR details + signed and absolute gap\n",
    "# -------------------------\n",
    "def compute_fnr_by_group_signed(y_true, y_prob, groups, thr=0.5):\n",
    "    y_true = np.asarray(y_true, dtype=np.int64)\n",
    "    y_prob = np.asarray(y_prob, dtype=np.float64)\n",
    "    groups = np.asarray(list(groups), dtype=object)\n",
    "    y_pred = (y_prob >= float(thr)).astype(np.int64)\n",
    "\n",
    "    out = {}\n",
    "    for g in sorted(set(groups.tolist())):\n",
    "        mask_g = (groups == g)\n",
    "        if int(mask_g.sum()) == 0:\n",
    "            continue\n",
    "\n",
    "        pos_mask = mask_g & (y_true == 1)\n",
    "        n_pos = int(pos_mask.sum())\n",
    "        if n_pos == 0:\n",
    "            out[g] = {\"n_total\": int(mask_g.sum()), \"n_pos\": 0, \"tp\": 0, \"fn\": 0, \"fnr\": float(\"nan\")}\n",
    "            continue\n",
    "\n",
    "        tp = int(((y_pred == 1) & pos_mask).sum())\n",
    "        fn = int(((y_pred == 0) & pos_mask).sum())\n",
    "        fnr = float(fn / max(1, (fn + tp)))\n",
    "\n",
    "        out[g] = {\"n_total\": int(mask_g.sum()), \"n_pos\": int(n_pos), \"tp\": int(tp), \"fn\": int(fn), \"fnr\": float(fnr)}\n",
    "\n",
    "    fnr_m = out.get(\"M\", {}).get(\"fnr\", float(\"nan\"))\n",
    "    fnr_f = out.get(\"F\", {}).get(\"fnr\", float(\"nan\"))\n",
    "\n",
    "    if (not np.isnan(fnr_m)) and (not np.isnan(fnr_f)):\n",
    "        delta_signed = float(fnr_f - fnr_m)\n",
    "        delta_abs = float(abs(delta_signed))\n",
    "    else:\n",
    "        delta_signed = float(\"nan\")\n",
    "        delta_abs = float(\"nan\")\n",
    "\n",
    "    return out, delta_signed, delta_abs\n",
    "\n",
    "def compute_confusion_counts(y_true, y_prob, thr=0.5):\n",
    "    y_true = np.asarray(y_true, dtype=np.int64)\n",
    "    y_prob = np.asarray(y_prob, dtype=np.float64)\n",
    "    y_pred = (y_prob >= float(thr)).astype(np.int64)\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=[0, 1])\n",
    "    TN, FP, FN, TP = int(cm[0, 0]), int(cm[0, 1]), int(cm[1, 0]), int(cm[1, 1])\n",
    "    return {\"TN\": TN, \"FP\": FP, \"FN\": FN, \"TP\": TP}\n",
    "\n",
    "def compute_confusion_by_group(y_true, y_prob, groups, thr=0.5):\n",
    "    y_true = np.asarray(y_true, dtype=np.int64)\n",
    "    y_prob = np.asarray(y_prob, dtype=np.float64)\n",
    "    groups = np.asarray(list(groups), dtype=object)\n",
    "    out = {}\n",
    "    for g in sorted(set(groups.tolist())):\n",
    "        mask = (groups == g)\n",
    "        if int(mask.sum()) == 0:\n",
    "            continue\n",
    "        out[g] = {\"n\": int(mask.sum()), \"confusion\": compute_confusion_counts(y_true[mask], y_prob[mask], thr=thr)}\n",
    "    return out\n",
    "\n",
    "# -------------------------\n",
    "# Reproducibility: set all RNG seeds\n",
    "# Inputs: seed int\n",
    "# Output: deterministic settings for random, numpy, torch\n",
    "# -------------------------\n",
    "def set_all_seeds(seed: int):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# -------------------------\n",
    "# Source checkpoint selection: pick most recent D6 trainval exp with all seeds\n",
    "# Inputs: D6 trainval_runs folder\n",
    "# Output: chosen_exp folder used to load best_heads.pt per seed\n",
    "# -------------------------\n",
    "D6_TRAINVAL_ROOT = Path(D6_OUT_ROOT) / \"trainval_runs\"\n",
    "if not D6_TRAINVAL_ROOT.exists():\n",
    "    raise FileNotFoundError(f\"Missing D6 trainval_runs folder: {str(D6_TRAINVAL_ROOT)}\")\n",
    "\n",
    "exp_dirs = sorted([p for p in D6_TRAINVAL_ROOT.glob(\"exp_*\") if p.is_dir()], key=lambda p: p.stat().st_mtime, reverse=True)\n",
    "if not exp_dirs:\n",
    "    raise FileNotFoundError(f\"No exp_* folders found under: {str(D6_TRAINVAL_ROOT)}\")\n",
    "\n",
    "def _has_all_seeds(exp_path: Path, seeds: list):\n",
    "    for s in seeds:\n",
    "        p = exp_path / f\"run_D6_seed{s}\" / \"best_heads.pt\"\n",
    "        if not p.exists():\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "chosen_exp = None\n",
    "for ed in exp_dirs:\n",
    "    if _has_all_seeds(ed, SEEDS):\n",
    "        chosen_exp = ed\n",
    "        break\n",
    "\n",
    "if chosen_exp is None:\n",
    "    sample = exp_dirs[0]\n",
    "    raise FileNotFoundError(\n",
    "        \"Could not find a D6 trainval experiment with all 3 best_heads.pt files.\\n\"\n",
    "        f\"Expected: {str(D6_TRAINVAL_ROOT)}/exp_*/run_D6_seedXXXX/best_heads.pt\\n\"\n",
    "        f\"Most recent exp checked: {str(sample)}\"\n",
    "    )\n",
    "\n",
    "print(\"\\nUsing SOURCE (D6) Train+Val experiment folder:\")\n",
    "print(\" \", str(chosen_exp))\n",
    "\n",
    "# -------------------------\n",
    "# Output folder: write only under D2\n",
    "# Inputs: D2_OUT_ROOT\n",
    "# Output: ZS_ROOT folder for per-seed runs + summary files\n",
    "# -------------------------\n",
    "ZS_ROOT = Path(D2_OUT_ROOT) / \"Cross_Language_Zero_Shot_Runs\"\n",
    "ZS_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# -------------------------\n",
    "# DataLoader: D2 TEST only (plus a small warm-up read)\n",
    "# Inputs: test_df and audio files\n",
    "# Output: test_loader ready for inference\n",
    "# -------------------------\n",
    "test_ds = AudioManifestDataset(test_df)\n",
    "test_loader = DataLoader(\n",
    "    test_ds,\n",
    "    batch_size=PER_DEVICE_BS,\n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=PIN_MEMORY,\n",
    "    collate_fn=collate_fn,\n",
    ")\n",
    "\n",
    "print(\"\\nWarm-up: loading up to 2 TEST batches...\")\n",
    "t0 = time.time()\n",
    "nb = len(test_loader)\n",
    "wb = min(2, nb)\n",
    "if wb == 0:\n",
    "    raise RuntimeError(\"TEST DataLoader has 0 batches. Check test_df length and PER_DEVICE_BS.\")\n",
    "it = iter(test_loader)\n",
    "for i in range(wb):\n",
    "    _ = next(it)\n",
    "    print(f\"  loaded TEST warmup batch {i+1}/{wb}\")\n",
    "print(f\"Warm-up done in {time.time()-t0:.2f}s\")\n",
    "\n",
    "# -------------------------\n",
    "# Checkpoint loader: load only the trained heads from best_heads.pt\n",
    "# Inputs: model instance + best_heads.pt path\n",
    "# Output: model with head weights restored for this seed\n",
    "# -------------------------\n",
    "def load_heads_into_model(model: Wav2Vec2TwoHeadClassifier, best_heads_path: Path):\n",
    "    if not best_heads_path.exists():\n",
    "        raise FileNotFoundError(f\"Missing best_heads.pt: {str(best_heads_path)}\")\n",
    "    state = torch.load(str(best_heads_path), map_location=\"cpu\")\n",
    "    needed = [\"pre_vowel\", \"pre_other\", \"head_vowel\", \"head_other\"]\n",
    "    missing = [k for k in needed if k not in state]\n",
    "    if missing:\n",
    "        raise KeyError(\n",
    "            f\"best_heads.pt missing keys {missing}. Found keys: {list(state.keys())}. \"\n",
    "            \"This zero-shot code expects the trainval save format.\"\n",
    "        )\n",
    "    model.pre_vowel.load_state_dict(state[\"pre_vowel\"], strict=True)\n",
    "    model.pre_other.load_state_dict(state[\"pre_other\"], strict=True)\n",
    "    model.head_vowel.load_state_dict(state[\"head_vowel\"], strict=True)\n",
    "    model.head_other.load_state_dict(state[\"head_other\"], strict=True)\n",
    "    return model\n",
    "\n",
    "# -------------------------\n",
    "# Inference: run model over D2 test_loader and collect probabilities\n",
    "# Inputs: loader + model\n",
    "# Output: arrays (y_true, y_prob, sex_norm) for metrics and plots\n",
    "# -------------------------\n",
    "def run_inference(loader, model, desc):\n",
    "    use_amp = bool(USE_AMP and DEVICE.type == \"cuda\")\n",
    "    all_probs, all_true, all_sex = [], [], []\n",
    "\n",
    "    pbar = tqdm(loader, desc=desc, dynamic_ncols=True)\n",
    "    with torch.inference_mode():\n",
    "        for batch in pbar:\n",
    "            input_values = batch[\"input_values\"].to(DEVICE, non_blocking=False)\n",
    "            attention_mask = batch[\"attention_mask\"].to(DEVICE, non_blocking=False)\n",
    "            labels = batch[\"labels\"].to(DEVICE, non_blocking=False)\n",
    "            task_group = batch[\"task_group\"]\n",
    "            sex_norm = batch[\"sex_norm\"]\n",
    "\n",
    "            with torch.amp.autocast(device_type=\"cuda\", enabled=use_amp):\n",
    "                logits = model.forward_logits(input_values, attention_mask, task_group)\n",
    "\n",
    "            probs = torch.softmax(logits.float(), dim=-1)[:, 1].detach().cpu().numpy().astype(np.float64)\n",
    "            all_probs.extend(probs.tolist())\n",
    "            all_true.extend(labels.detach().cpu().numpy().astype(np.int64).tolist())\n",
    "            all_sex.extend(list(sex_norm))\n",
    "\n",
    "    return np.asarray(all_true, dtype=np.int64), np.asarray(all_probs, dtype=np.float64), np.asarray(all_sex, dtype=object)\n",
    "\n",
    "# -------------------------\n",
    "# Per-seed run: load D6 heads, evaluate D2 at FIXED_THR, save metrics + PNGs\n",
    "# Inputs: seed, chosen_exp, FIXED_THR, D2 test_loader\n",
    "# Output: run_<TRANSFER_TAG>_seedXXXX folder with metrics.json + plots\n",
    "# -------------------------\n",
    "def run_seed(seed: int):\n",
    "    set_all_seeds(seed)\n",
    "\n",
    "    run_dir = ZS_ROOT / f\"run_{TRANSFER_TAG}_seed{seed}\"\n",
    "    run_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    best_heads_path = chosen_exp / f\"run_D6_seed{seed}\" / \"best_heads.pt\"\n",
    "\n",
    "    print(f\"\\n[seed={seed}] Loading SOURCE heads from:\")\n",
    "    print(\" \", str(best_heads_path))\n",
    "    print(f\"[seed={seed}] Evaluating TARGET D2 TEST @ fixed_thr={FIXED_THR:.6f}\")\n",
    "\n",
    "    model = Wav2Vec2TwoHeadClassifier(BACKBONE_CKPT, dropout_p=DROPOUT_P).to(DEVICE)\n",
    "    model = load_heads_into_model(model, best_heads_path)\n",
    "    model.eval()\n",
    "\n",
    "    yt, pt, st = run_inference(test_loader, model, desc=f\"[seed={seed}] D2 TEST (zero-shot)\")\n",
    "\n",
    "    test_auc = compute_auc(yt, pt)  # threshold-free\n",
    "    thr_metrics = compute_threshold_metrics(yt, pt, thr=FIXED_THR)\n",
    "\n",
    "    fnr_by_sex, delta_f_minus_m, delta_abs = compute_fnr_by_group_signed(yt, pt, st, thr=FIXED_THR)\n",
    "    confusion_by_sex = compute_confusion_by_group(yt, pt, st, thr=FIXED_THR)\n",
    "\n",
    "    # Plots: always save overall ROC + confusion matrix\n",
    "    roc_png = run_dir / \"roc_curve.png\"\n",
    "    cm_png  = run_dir / \"confusion_matrix.png\"\n",
    "    save_roc_curve_png(yt, pt, str(roc_png), title_suffix=f\"D2 Test (seed={seed})\")\n",
    "    save_confusion_png(yt, pt, str(cm_png), thr=FIXED_THR, title_suffix=f\"D2 Test (seed={seed})\")\n",
    "\n",
    "    # Sex-specific confusion matrices (only if that sex exists in D2 TEST)\n",
    "    cm_m_png = None\n",
    "    cm_f_png = None\n",
    "    mask_m = (st == \"M\")\n",
    "    mask_f = (st == \"F\")\n",
    "\n",
    "    if int(mask_m.sum()) > 0:\n",
    "        cm_m_png = run_dir / \"confusion_matrix_M.png\"\n",
    "        save_confusion_png(yt[mask_m], pt[mask_m], str(cm_m_png), thr=FIXED_THR, title_suffix=f\"D2 Test SEX=M (seed={seed})\")\n",
    "\n",
    "    if int(mask_f.sum()) > 0:\n",
    "        cm_f_png = run_dir / \"confusion_matrix_F.png\"\n",
    "        save_confusion_png(yt[mask_f], pt[mask_f], str(cm_f_png), thr=FIXED_THR, title_suffix=f\"D2 Test SEX=F (seed={seed})\")\n",
    "\n",
    "    # Structured metrics saved for later analysis/paper tables\n",
    "    metrics = {\n",
    "        \"transfer\": {\n",
    "            \"tag\": TRANSFER_TAG,\n",
    "            \"source_dataset\": \"D6\",\n",
    "            \"target_dataset\": \"D2\",\n",
    "            \"threshold_policy\": \"Fixed threshold = mean VAL-opt threshold from D6 monolingual test summary (Youden J on D6 VAL)\",\n",
    "            \"fixed_threshold_value\": float(FIXED_THR),\n",
    "            \"d6_mono_summary_path\": str(D6_MONO_SUMMARY),\n",
    "        },\n",
    "\n",
    "        \"target\": {\n",
    "            \"dataset\": \"D2\",\n",
    "            \"seed\": int(seed),\n",
    "            \"n_test\": int(len(test_df)),\n",
    "            \"label_counts_test\": test_df[\"label_num\"].value_counts(dropna=False).to_dict(),\n",
    "            \"sex_counts_test_norm\": test_df[\"sex_norm\"].value_counts(dropna=False).to_dict(),\n",
    "        },\n",
    "\n",
    "        \"test_auroc\": float(test_auc),\n",
    "\n",
    "        \"threshold_metrics_test\": thr_metrics,\n",
    "        \"test_threshold_used\": float(FIXED_THR),\n",
    "\n",
    "        \"fairness_test\": {\n",
    "            \"definition\": \"H3: ΔFNR = FNR(F) - FNR(M), where FNR(sex)=FN/(FN+TP) computed on PD-only true labels at fixed transfer threshold.\",\n",
    "            \"fnr_by_sex_norm\": fnr_by_sex,\n",
    "            \"delta_fnr_F_minus_M\": float(delta_f_minus_m),\n",
    "            \"delta_fnr_abs\": float(delta_abs),\n",
    "            \"note\": \"If n_PD for a sex is 0, its FNR is NaN and ΔFNR is NaN.\",\n",
    "            \"sex_normalization_note\": \"D2 mapping: male->M, female->F; otherwise UNK.\",\n",
    "        },\n",
    "\n",
    "        \"confusion_by_sex_norm\": confusion_by_sex,\n",
    "\n",
    "        \"artifacts\": {\n",
    "            \"roc_curve_png\": str(roc_png),\n",
    "            \"confusion_matrix_png\": str(cm_png),\n",
    "            \"confusion_matrix_M_png\": str(cm_m_png) if cm_m_png is not None else None,\n",
    "            \"confusion_matrix_F_png\": str(cm_f_png) if cm_f_png is not None else None,\n",
    "        },\n",
    "\n",
    "        \"paths\": {\n",
    "            \"d2_out_root\": str(D2_OUT_ROOT),\n",
    "            \"zero_shot_root\": str(ZS_ROOT),\n",
    "            \"run_dir\": str(run_dir),\n",
    "            \"source_trainval_experiment_used\": str(chosen_exp),\n",
    "            \"source_best_heads_path\": str(best_heads_path),\n",
    "        },\n",
    "\n",
    "        \"backbone_ckpt\": BACKBONE_CKPT,\n",
    "        \"dropout_p\": float(DROPOUT_P),\n",
    "        \"timestamp\": time.strftime(\"%Y%m%d_%H%M%S\"),\n",
    "    }\n",
    "\n",
    "    metrics_path = run_dir / \"metrics.json\"\n",
    "    with open(metrics_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(metrics, f, indent=2)\n",
    "\n",
    "    print(f\"[seed={seed}] DONE | test_AUROC={test_auc:.6f}\")\n",
    "\n",
    "    def _fmt_fnr(g):\n",
    "        d = fnr_by_sex.get(g, None)\n",
    "        if d is None:\n",
    "            return \"n/a\"\n",
    "        return f\"fnr={d['fnr']:.6f} (n_PD={d['n_pos']}, fn={d['fn']}, tp={d['tp']})\"\n",
    "\n",
    "    print(f\"[seed={seed}] TEST metrics @ fixed_thr={FIXED_THR:.6f}\")\n",
    "    print(f\"[seed={seed}] FAIRNESS (H3) @ fixed_thr={FIXED_THR:.6f}:\")\n",
    "    print(\"  M:\", _fmt_fnr(\"M\"))\n",
    "    print(\"  F:\", _fmt_fnr(\"F\"))\n",
    "    if \"UNK\" in fnr_by_sex:\n",
    "        print(\"  UNK:\", _fmt_fnr(\"UNK\"))\n",
    "    print(\"  ΔFNR (F-M):\", f\"{delta_f_minus_m:.6f}\" if not np.isnan(delta_f_minus_m) else \"nan\")\n",
    "    print(\"  |ΔFNR|:\", f\"{delta_abs:.6f}\" if not np.isnan(delta_abs) else \"nan\")\n",
    "\n",
    "    print(f\"[seed={seed}] WROTE:\")\n",
    "    print(\" \", str(metrics_path))\n",
    "    print(\" \", str(roc_png))\n",
    "    print(\" \", str(cm_png))\n",
    "    if cm_m_png is not None:\n",
    "        print(\" \", str(cm_m_png))\n",
    "    if cm_f_png is not None:\n",
    "        print(\" \", str(cm_f_png))\n",
    "\n",
    "    return {\n",
    "        \"seed\": int(seed),\n",
    "        \"test_auroc\": float(test_auc),\n",
    "        \"thr_metrics_test\": thr_metrics,\n",
    "        \"fnr_by_sex_test\": fnr_by_sex,\n",
    "        \"delta_f_minus_m_test\": float(delta_f_minus_m),\n",
    "        \"delta_abs_test\": float(delta_abs),\n",
    "        \"run_dir\": str(run_dir),\n",
    "    }\n",
    "\n",
    "# -------------------------\n",
    "# Aggregate across 3 seeds: AUROC mean ± 95% CI, others mean ± SD\n",
    "# Inputs: per-seed outputs from run_seed()\n",
    "# Output: summary_zero_shot4.json + history_index.jsonl under ZS_ROOT\n",
    "# -------------------------\n",
    "seed_results = []\n",
    "for seed in SEEDS:\n",
    "    seed_results.append(run_seed(seed))\n",
    "\n",
    "aurocs = [r[\"test_auroc\"] for r in seed_results]\n",
    "t_crit = 4.302652729911275  # df=2, 95% CI\n",
    "n = len(aurocs)\n",
    "mean_auc = float(np.mean(aurocs))\n",
    "std_auc = float(np.std(aurocs, ddof=1)) if n > 1 else 0.0\n",
    "half_width = float(t_crit * (std_auc / math.sqrt(n))) if n > 1 else 0.0\n",
    "ci95 = [float(mean_auc - half_width), float(mean_auc + half_width)]\n",
    "\n",
    "def _mean_sd(vals):\n",
    "    vals = np.asarray(vals, dtype=np.float64)\n",
    "    return float(np.nanmean(vals)), float(np.nanstd(vals, ddof=1)) if np.sum(~np.isnan(vals)) > 1 else 0.0\n",
    "\n",
    "thr_list = [r[\"thr_metrics_test\"] for r in seed_results]\n",
    "keys = [\"accuracy\",\"precision\",\"recall\",\"f1_score\",\"sensitivity\",\"specificity\",\"mcc\",\"p_value_fisher_two_sided\"]\n",
    "\n",
    "agg = {}\n",
    "for k in keys:\n",
    "    v = [float(tm.get(k, float(\"nan\"))) for tm in thr_list]\n",
    "    mu, sd = _mean_sd(v)\n",
    "    agg[k] = {\n",
    "        \"mean\": mu,\n",
    "        \"sd\": sd,\n",
    "        \"values_by_seed\": {str(r[\"seed\"]): float(tm.get(k, float(\"nan\"))) for r, tm in zip(seed_results, thr_list)}\n",
    "    }\n",
    "\n",
    "cm_by_seed = {str(r[\"seed\"]): r[\"thr_metrics_test\"][\"confusion_matrix\"] for r in seed_results}\n",
    "\n",
    "# FAIRNESS aggregation (H3)\n",
    "fnr_by_seed = {str(r[\"seed\"]): r[\"fnr_by_sex_test\"] for r in seed_results}\n",
    "delta_signed_by_seed = {str(r[\"seed\"]): float(r[\"delta_f_minus_m_test\"]) for r in seed_results}\n",
    "delta_abs_by_seed = {str(r[\"seed\"]): float(r[\"delta_abs_test\"]) for r in seed_results}\n",
    "\n",
    "fnr_m_vals, fnr_f_vals, n_pd_m_vals, n_pd_f_vals = [], [], [], []\n",
    "d_signed_vals, d_abs_vals = [], []\n",
    "\n",
    "for r in seed_results:\n",
    "    s = str(r[\"seed\"])\n",
    "    d = fnr_by_seed.get(s, {})\n",
    "    fnr_m_vals.append(float(d.get(\"M\", {}).get(\"fnr\", float(\"nan\"))))\n",
    "    fnr_f_vals.append(float(d.get(\"F\", {}).get(\"fnr\", float(\"nan\"))))\n",
    "    n_pd_m_vals.append(float(d.get(\"M\", {}).get(\"n_pos\", float(\"nan\"))))\n",
    "    n_pd_f_vals.append(float(d.get(\"F\", {}).get(\"n_pos\", float(\"nan\"))))\n",
    "    d_signed_vals.append(float(delta_signed_by_seed.get(s, float(\"nan\"))))\n",
    "    d_abs_vals.append(float(delta_abs_by_seed.get(s, float(\"nan\"))))\n",
    "\n",
    "fnr_m_mean, fnr_m_sd = _mean_sd(fnr_m_vals)\n",
    "fnr_f_mean, fnr_f_sd = _mean_sd(fnr_f_vals)\n",
    "d_signed_mean, d_signed_sd = _mean_sd(d_signed_vals)\n",
    "d_abs_mean, d_abs_sd = _mean_sd(d_abs_vals)\n",
    "\n",
    "print(\"\\nZero-shot TEST AUROC by seed:\")\n",
    "for r in seed_results:\n",
    "    print(f\"  seed {r['seed']}: {r['test_auroc']:.6f}\")\n",
    "print(f\"\\nMean Zero-shot TEST AUROC: {mean_auc:.6f}\")\n",
    "print(f\"95% CI (t, n=3): [{ci95[0]:.6f}, {ci95[1]:.6f}]\")\n",
    "\n",
    "print(f\"\\nFixed threshold used for ALL seeds (from D6 mono mean VAL-opt): {FIXED_THR:.6f}\")\n",
    "\n",
    "print(\"\\nThreshold metrics on D2 TEST @ fixed threshold (mean ± SD across seeds):\")\n",
    "for k in [\"accuracy\",\"precision\",\"sensitivity\",\"specificity\",\"f1_score\",\"mcc\"]:\n",
    "    mu = agg[k][\"mean\"]\n",
    "    sd = agg[k][\"sd\"]\n",
    "    print(f\"  {k}: {mu:.6f} ± {sd:.6f}\")\n",
    "print(\"  fisher_p_value_two_sided:\", f\"{agg['p_value_fisher_two_sided']['mean']:.6g} ± {agg['p_value_fisher_two_sided']['sd']:.6g}\")\n",
    "\n",
    "print(\"\\nFAIRNESS (H3) on D2 TEST @ fixed threshold (mean ± SD across seeds):\")\n",
    "print(f\"  FNR_M: {fnr_m_mean:.6f} ± {fnr_m_sd:.6f}\")\n",
    "print(f\"  FNR_F: {fnr_f_mean:.6f} ± {fnr_f_sd:.6f}\")\n",
    "print(f\"  ΔFNR (F-M): {d_signed_mean:.6f} ± {d_signed_sd:.6f}\")\n",
    "print(f\"  |ΔFNR|: {d_abs_mean:.6f} ± {d_abs_sd:.6f}\")\n",
    "\n",
    "summary = {\n",
    "    \"transfer\": {\n",
    "        \"tag\": TRANSFER_TAG,\n",
    "        \"source_dataset\": \"D6\",\n",
    "        \"target_dataset\": \"D2\",\n",
    "        \"fixed_threshold_value\": float(FIXED_THR),\n",
    "        \"fixed_threshold_source\": {\n",
    "            \"path\": str(D6_MONO_SUMMARY),\n",
    "            \"key\": \"val_opt_threshold_mean_sd.mean\",\n",
    "            \"note\": \"Mean across seeds of D6 VAL-opt thresholds (Youden J) as recorded by D6 monolingual test run.\",\n",
    "        },\n",
    "        \"source_trainval_experiment_used\": str(chosen_exp),\n",
    "    },\n",
    "\n",
    "    \"target\": {\n",
    "        \"dx_out_root\": str(D2_OUT_ROOT),\n",
    "        \"manifest_all\": str(D2_MANIFEST_ALL),\n",
    "        \"n_test\": int(len(test_df)),\n",
    "        \"label_counts_test\": test_df[\"label_num\"].value_counts(dropna=False).to_dict(),\n",
    "        \"sex_counts_test_norm\": test_df[\"sex_norm\"].value_counts(dropna=False).to_dict(),\n",
    "    },\n",
    "\n",
    "    \"seeds\": SEEDS,\n",
    "\n",
    "    \"test_aurocs_by_seed\": {str(r[\"seed\"]): float(r[\"test_auroc\"]) for r in seed_results},\n",
    "    \"mean_test_auroc\": float(mean_auc),\n",
    "    \"t_crit_df2_95\": float(t_crit),\n",
    "    \"half_width_95_ci\": float(half_width),\n",
    "    \"ci95_test_auroc\": ci95,\n",
    "\n",
    "    \"threshold_metrics_test_mean_sd\": agg,\n",
    "    \"confusion_matrix_by_seed\": cm_by_seed,\n",
    "\n",
    "    \"fairness_test\": {\n",
    "        \"definition\": \"H3: ΔFNR = FNR(F) - FNR(M), where FNR(sex)=FN/(FN+TP) computed on PD-only true labels at the fixed transfer threshold.\",\n",
    "        \"fnr_by_sex_norm_by_seed\": fnr_by_seed,\n",
    "        \"delta_fnr_F_minus_M_by_seed\": delta_signed_by_seed,\n",
    "        \"delta_fnr_abs_by_seed\": delta_abs_by_seed,\n",
    "        \"fnr_M_mean_sd\": {\"mean\": float(fnr_m_mean), \"sd\": float(fnr_m_sd), \"values_by_seed\": {str(s): float(v) for s, v in zip(SEEDS, fnr_m_vals)}},\n",
    "        \"fnr_F_mean_sd\": {\"mean\": float(fnr_f_mean), \"sd\": float(fnr_f_sd), \"values_by_seed\": {str(s): float(v) for s, v in zip(SEEDS, fnr_f_vals)}},\n",
    "        \"delta_fnr_F_minus_M_mean_sd\": {\"mean\": float(d_signed_mean), \"sd\": float(d_signed_sd), \"values_by_seed\": {str(s): float(v) for s, v in zip(SEEDS, d_signed_vals)}},\n",
    "        \"delta_fnr_abs_mean_sd\": {\"mean\": float(d_abs_mean), \"sd\": float(d_abs_sd), \"values_by_seed\": {str(s): float(v) for s, v in zip(SEEDS, d_abs_vals)}},\n",
    "        \"denominators_PD_by_seed\": {str(s): {\"n_PD_M\": float(n_pd_m_vals[i]), \"n_PD_F\": float(n_pd_f_vals[i])} for i, s in enumerate(SEEDS)},\n",
    "        \"sex_normalization_note\": \"D2 mapping: male->M, female->F; otherwise UNK. ΔFNR computed only when both M and F have defined FNR.\",\n",
    "    },\n",
    "\n",
    "    \"run_dirs\": [r[\"run_dir\"] for r in seed_results],\n",
    "    \"timestamp\": time.strftime(\"%Y%m%d_%H%M%S\"),\n",
    "}\n",
    "\n",
    "summary_path = ZS_ROOT / \"summary_zero_shot4.json\"\n",
    "with open(summary_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "history_path = ZS_ROOT / \"history_index.jsonl\"\n",
    "with open(history_path, \"a\", encoding=\"utf-8\") as f:\n",
    "    f.write(json.dumps(summary) + \"\\n\")\n",
    "\n",
    "print(\"\\nWROTE summary:\", str(summary_path))\n",
    "print(\"APPENDED history index:\", str(history_path))\n",
    "print(\"Open this folder to access artifacts:\", str(ZS_ROOT))\n",
    "\n",
    "# -------------------------\n",
    "# Stop runtime (Colab)\n",
    "# Inputs: none\n",
    "# Output: attempts to release the GPU instance\n",
    "# -------------------------\n",
    "print(\"\\nAll done. Unassigning the runtime to stop the L4 instance...\")\n",
    "try:\n",
    "    from google.colab import runtime  # type: ignore\n",
    "    print(\"Calling runtime.unassign() now.\")\n",
    "    runtime.unassign()\n",
    "except Exception as e:\n",
    "    print(\"Could not unassign runtime automatically. You can stop the runtime manually in Colab.\")\n",
    "    print(\"Reason:\", repr(e))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "60fd1d4aa18441419352c2b083610b80",
      "24530e98a331408fb453e81c37780f01",
      "58ad10fbb11544e3ba2ec936dcdf6b39",
      "1429b4c445d74fcf9ea0666b6899887a",
      "d8fb9a782738417bbc02dc76f06969c5",
      "96302d42cb7d41bb89ad3bdca24227e7",
      "a7609b53ddae4f95840a7f79a282f8cc",
      "fb4681afbd8f41a8a5b00296e19d4693",
      "ca895b1288e54252973d1e9d10efe91b",
      "c0e100cb05dd4d3f9532a42acbb9c185",
      "71689ebf41ed4814a954025bcd1e9745",
      "33b04f03cc804afaaee9882558756ca0",
      "628bcf58b79b425c90aa4d2418765943",
      "f6266b3e3b4d4ae9bb9b7937501339cc",
      "cf43e9ca04c2432c94f9fc783fc335b7",
      "c57c76a110b646e7a4ef73be38a36c62",
      "cac5495214bf490ab69c5b260aa2b515",
      "262449aca44f417ebd69d5a87128d5a4",
      "f7a1a04004b44d66b4da4c04d59e8fcf",
      "876714199fe34cf7b8e6189f1991485f",
      "80443a6ae7344f0d9a0e66d0cf511a7d",
      "791aefefa40144cf8faf9db7becff668",
      "3ab1d9d6702c4a8d9b494135ef12ae11",
      "22bb4ca0f9d14eb290955c9f281f2204",
      "8a82f576cf9e4bde827db5b233beda43",
      "80c960d1bb3d405aae6bac6a3cd6641e",
      "4976a98bb6ad4c8383f08b6e650b1072",
      "f9bbc30ad849497e9f58ef2750e5ba5f",
      "f61960ce129b400a9c5b023d3d1512b5",
      "afc0a90bd12f4c53bbfdcc929c3b69be",
      "d15629f429044f05a8ac92c94804708f",
      "881a159c2f4f4e22b826a13fade20b66",
      "89f1490b66734092bd50afb412c303bb",
      "a11af5c29a684231aad4a64d09bfb173",
      "c5694753485c43e8b449c1eec55218c7",
      "447ba2ce73974f16b178798cc56690c3",
      "7957859715184076917bca50aa37e167",
      "20ff32916cd0408aabb7cb1ddda05015",
      "638777b2cf5946e2bc9e80e86cc77bcc",
      "053ecff20f1f4a6086e8dde1c8399dad",
      "a64ae627198041378fc45946973eb71a",
      "8190c127fc8b402c9bf07945dc441fd0",
      "64b0704915904c5da7f70219893a09c3",
      "f620d7558e8a4012b52d726d8975c6fb",
      "1d6465d56c0d46ec8a58e99528818bfb",
      "9ef3a8d7b8ac4f5a85d55c17b93c5563",
      "2a3bec3ed0d04cb497c3d03c73518791",
      "7837a53389224037904bfabbca4edd3c",
      "8eb0a639f3d14b70ac9cc753e76aead1",
      "35bd53b8f2d44b41a9f4a592ec60cc59",
      "cd3e16ed6b9d4947bb8d8185397f52ce",
      "0dcec26d9b8c4d01bd3d04674ad3734a",
      "ac5d677c6bd742f180ce2b37da14ae89",
      "496d0d98599245259883c2937b8b0688",
      "22c6da4182f2451d827d91e81edb9dbf",
      "808da9a6b74442399290e05bf64bf3f4",
      "ba3711373b784029a3cb75864140927b",
      "090acbafb6034c38a41f4aad0b4d3c77",
      "3978cdd3e44848c390076375d48fcc86",
      "45590294ca3d4f38943c8da68f3b3900",
      "3a5cdb5f3b1a4cc991ab434b93973be1",
      "d4611bc73ee14cc6b4d4b1a0366f1e6e",
      "ffce30eddaa94e99916a0ad1766e9122",
      "2cb0d8845cd549c3ab196f8f5f0a8cf4",
      "568836fd189842598cb41bedf85980c9",
      "993a18288b134a30a1a376324fa85e2d"
     ]
    },
    "id": "C26_D-BWRYnP"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "The following cell runs a cross-language zero-shot test from Italian to Spanish (D4 → D1). It checks whether a model trained on the Italian dataset (D4) can detect Parkinson’s disease on the Spanish dataset (D1) without any retraining on D1. The cell loads the most recent D4 train and validation experiment that contains completed head checkpoints for three seeds (1337, 2024, 7777), and applies those heads directly to the D1 test split only, using audio paths from `D1/manifests/manifest_all.csv`.\n",
    "\n",
    "A single fixed decision threshold is used for all seeds. This threshold is taken from D4’s saved `monolingual_test_runs/summary_test.json`, using `val_opt_threshold_mean_sd.mean` when available, or falling back to the mean of `val_opt_threshold_by_seed`. This avoids any threshold tuning on D1 and keeps the transfer test fair.\n",
    "\n",
    "Before evaluation, the cell prepares the D1 test data. It checks that all required manifest columns exist, filters the data to `split == \"test\"`, and stops early if any audio clip paths are missing. Each clip is assigned to a task group, with “vowel” used when `task == \"vowl\"` and “other” for all remaining cases. D1 sex values are normalized into `sex_norm` (M, F, UNK) using D1’s numeric encoding, where 0 maps to F and 1 maps to M. This allows consistent sex-based reporting and plots.\n",
    "\n",
    "For each seed, the cell loads `best_heads.pt` from the selected D4 experiment folder, rebuilds the same frozen Wav2Vec2 two-head classifier used during training, and runs inference on all D1 test clips to produce Parkinson’s disease probabilities. It then computes AUROC as a threshold-free measure of performance, followed by fixed-threshold metrics at the D4-derived threshold. These include the confusion matrix, accuracy, precision, sensitivity or recall, specificity, F1 score, MCC, and the Fisher exact test p-value. Fairness is measured as ΔFNR, defined as FNR(F) minus FNR(M), using only true Parkinson’s cases. The cell saves ROC curves and confusion matrix images for the full test set, as well as sex-specific confusion matrices when both M and F samples are present. A per-seed `metrics.json` file and all plots are written under\n",
    "`<D1_OUT_ROOT>/Cross_Language_Zero_Shot_Runs/run_IT_to_ES_seed<seed>/`.\n",
    "\n",
    "After all three seeds finish, the cell combines the results across seeds. It reports the mean AUROC with a 95 percent confidence interval using a t distribution with n equal to 3, along with the mean and standard deviation of the fixed-threshold metrics and fairness values. A combined `summary_zero_shot.json` file is written, the same summary is appended to `history_index.jsonl` under `<D1_OUT_ROOT>/Cross_Language_Zero_Shot_Runs/`, and the Colab runtime is unassigned to shut down the GPU."
   ],
   "metadata": {
    "id": "eLCp9Q02yxJo"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# =========================\n",
    "# IT → ES Zero-shot (D4 → D1): Test-only transfer with fixed threshold\n",
    "# - Loads D4-trained heads (most recent train+val experiment)\n",
    "# - Runs inference on D1 TEST only (from manifest_all.csv)\n",
    "# - Uses one fixed threshold read from D4 monolingual summary_test.json\n",
    "# - Saves per-seed metrics + plots, plus an aggregated summary under D1\n",
    "# =========================\n",
    "\n",
    "import os, json, math, random, time, warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import soundfile as sf\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from transformers import Wav2Vec2Model\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, confusion_matrix, matthews_corrcoef\n",
    "from scipy.stats import fisher_exact\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# -------------------------\n",
    "# Safety checks: avoid importing local files named like core libraries\n",
    "# Inputs: /content directory\n",
    "# Output: raises an error early if a naming conflict exists\n",
    "# -------------------------\n",
    "if os.path.exists(\"/content/torch.py\") or os.path.exists(\"/content/torch/__init__.py\"):\n",
    "    raise RuntimeError(\"Found local /content/torch.py or /content/torch/ that shadows PyTorch. Rename/remove it and restart runtime.\")\n",
    "if os.path.exists(\"/content/transformers.py\") or os.path.exists(\"/content/transformers/__init__.py\"):\n",
    "    raise RuntimeError(\"Found local /content/transformers.py or /content/transformers/ that shadows Hugging Face Transformers. Rename/remove it and restart runtime.\")\n",
    "\n",
    "# -------------------------\n",
    "# Drive mount (Colab)\n",
    "# Inputs: Google Drive\n",
    "# Output: /content/drive mounted so manifests, checkpoints, and outputs can be read/written\n",
    "# -------------------------\n",
    "if not os.path.isdir(\"/content/drive/MyDrive\"):\n",
    "    from google.colab import drive  # type: ignore\n",
    "    drive.mount(\"/content/drive\")\n",
    "\n",
    "# -------------------------\n",
    "# Paths: target (D1) + source (D4)\n",
    "# Inputs: optional globals D1_OUT_ROOT, D4_OUT_ROOT (if set earlier), else fallbacks\n",
    "# Output: D1_MANIFEST_ALL and D4_MONO_SUMMARY paths\n",
    "# -------------------------\n",
    "D1_OUT_ROOT_FALLBACK = \"/content/drive/MyDrive/AI_PD_Project/Datasets/D1-NeuroVoz-Castillan Spanish/preprocessed_v1\"\n",
    "D4_OUT_ROOT_FALLBACK = \"/content/drive/MyDrive/AI_PD_Project/Datasets/D4-Italian (IPVS)/preprocessed_v1\"\n",
    "\n",
    "D1_OUT_ROOT = globals().get(\"D1_OUT_ROOT\", D1_OUT_ROOT_FALLBACK)\n",
    "D4_OUT_ROOT = globals().get(\"D4_OUT_ROOT\", D4_OUT_ROOT_FALLBACK)\n",
    "\n",
    "D1_MANIFEST_ALL = f\"{D1_OUT_ROOT}/manifests/manifest_all.csv\"\n",
    "\n",
    "# D4 monolingual summary path (contains the fixed threshold source)\n",
    "D4_MONO_SUMMARY = Path(D4_OUT_ROOT) / \"monolingual_test_runs\" / \"summary_test.json\"\n",
    "\n",
    "# -------------------------\n",
    "# Run settings (kept consistent with other test cells)\n",
    "# Inputs: constants below\n",
    "# Output: printed configuration + runtime behavior (batching, AMP, device)\n",
    "# -------------------------\n",
    "TRANSFER_TAG    = \"IT_to_ES\"          # D4 -> D1\n",
    "SEEDS           = [1337, 2024, 7777]\n",
    "BACKBONE_CKPT   = \"facebook/wav2vec2-base\"\n",
    "SR_EXPECTED     = 16000\n",
    "TINY_THRESH     = 1e-4\n",
    "\n",
    "EFFECTIVE_BS    = 64\n",
    "PER_DEVICE_BS   = 16\n",
    "GRAD_ACCUM      = max(1, EFFECTIVE_BS // PER_DEVICE_BS)  # printed only\n",
    "\n",
    "DROPOUT_P       = 0.2\n",
    "\n",
    "NUM_WORKERS     = 0\n",
    "PIN_MEMORY      = False\n",
    "\n",
    "USE_AMP         = True\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "try:\n",
    "    torch.set_float32_matmul_precision(\"high\")\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", message=\"None of the inputs have requires_grad=True. Gradients will be None\")\n",
    "warnings.filterwarnings(\"ignore\", message=\"Passing `gradient_checkpointing` to a config initialization is deprecated\")\n",
    "\n",
    "print(\"D1_OUT_ROOT (target):\", D1_OUT_ROOT)\n",
    "print(\"D1_MANIFEST_ALL:\", D1_MANIFEST_ALL)\n",
    "print(\"D4_OUT_ROOT (source):\", D4_OUT_ROOT)\n",
    "print(\"D4_MONO_SUMMARY:\", str(D4_MONO_SUMMARY))\n",
    "print(\"DEVICE:\", DEVICE)\n",
    "if DEVICE.type == \"cuda\":\n",
    "    print(\"GPU:\", torch.cuda.get_device_name(0))\n",
    "print(\"PER_DEVICE_BS:\", PER_DEVICE_BS, \"| EFFECTIVE_BS:\", PER_DEVICE_BS * GRAD_ACCUM)\n",
    "print(\"NUM_WORKERS:\", NUM_WORKERS, \"| PIN_MEMORY:\", PIN_MEMORY)\n",
    "print(\"USE_AMP:\", bool(USE_AMP and DEVICE.type == \"cuda\"))\n",
    "\n",
    "# -------------------------\n",
    "# Fixed threshold: read from D4 monolingual summary_test.json\n",
    "# Inputs: D4_MONO_SUMMARY (JSON)\n",
    "# Output: FIXED_THR (float) used for all three seeds on D1\n",
    "# -------------------------\n",
    "if not D4_MONO_SUMMARY.exists():\n",
    "    raise FileNotFoundError(\n",
    "        \"Missing D4 monolingual summary_test.json.\\n\"\n",
    "        f\"Expected: {str(D4_MONO_SUMMARY)}\\n\"\n",
    "        \"Run the D4 monolingual test cell first (the one that writes summary_test.json).\"\n",
    "    )\n",
    "\n",
    "with open(D4_MONO_SUMMARY, \"r\", encoding=\"utf-8\") as f:\n",
    "    d4_sum = json.load(f)\n",
    "\n",
    "# D4 schema: summary[\"val_opt_threshold_mean_sd\"][\"mean\"] (preferred)\n",
    "if \"val_opt_threshold_mean_sd\" in d4_sum and isinstance(d4_sum[\"val_opt_threshold_mean_sd\"], dict):\n",
    "    FIXED_THR = float(d4_sum[\"val_opt_threshold_mean_sd\"].get(\"mean\", float(\"nan\")))\n",
    "else:\n",
    "    # Fallback: mean of per-seed values if present\n",
    "    by_seed = d4_sum.get(\"val_opt_threshold_by_seed\", None)\n",
    "    if isinstance(by_seed, dict) and len(by_seed) > 0:\n",
    "        FIXED_THR = float(np.nanmean([float(v) for v in by_seed.values()]))\n",
    "    else:\n",
    "        FIXED_THR = float(\"nan\")\n",
    "\n",
    "if not np.isfinite(FIXED_THR):\n",
    "    raise RuntimeError(\n",
    "        \"Could not read a finite fixed threshold from D4 monolingual summary_test.json.\\n\"\n",
    "        \"Expected keys: val_opt_threshold_mean_sd.mean (preferred) or val_opt_threshold_by_seed.\"\n",
    "    )\n",
    "\n",
    "print(f\"\\nFixed transfer threshold (from D4 monolingual mean VAL-opt): {FIXED_THR:.6f}\")\n",
    "\n",
    "# -------------------------\n",
    "# Target manifest: load D1 and keep TEST only\n",
    "# Inputs: D1 manifest_all.csv\n",
    "# Output: test_df with required columns + basic counts printed\n",
    "# -------------------------\n",
    "if not os.path.exists(D1_MANIFEST_ALL):\n",
    "    raise FileNotFoundError(f\"Missing D1 manifest_all.csv: {D1_MANIFEST_ALL}\")\n",
    "\n",
    "m_all = pd.read_csv(D1_MANIFEST_ALL)\n",
    "\n",
    "# Keep sex for fairness and sex-specific confusion matrices\n",
    "req_cols = {\"split\", \"clip_path\", \"label_num\", \"task\", \"sex\"}\n",
    "missing = [c for c in sorted(req_cols) if c not in m_all.columns]\n",
    "if missing:\n",
    "    raise ValueError(f\"D1 manifest missing required columns: {missing}. Found: {list(m_all.columns)}\")\n",
    "\n",
    "# Infer dataset_id and ensure this is really D1\n",
    "if \"dataset\" in m_all.columns and m_all[\"dataset\"].notna().any():\n",
    "    dataset_id = str(m_all[\"dataset\"].astype(str).value_counts(dropna=True).idxmax())\n",
    "    m_all = m_all[m_all[\"dataset\"].astype(str) == dataset_id].copy()\n",
    "else:\n",
    "    dataset_id = \"DX\"\n",
    "\n",
    "if dataset_id != \"D1\":\n",
    "    raise RuntimeError(\n",
    "        f\"Expected dataset_id=='D1' but got {dataset_id!r}. \"\n",
    "        \"This usually means D1_OUT_ROOT is wrong or inherited from a previous cell.\\n\"\n",
    "        f\"D1_OUT_ROOT={D1_OUT_ROOT}\"\n",
    "    )\n",
    "\n",
    "# Keep a stable column set (missing ones are filled with NaN)\n",
    "keep_cols = [\"clip_path\", \"label_num\", \"task\", \"speaker_id\", \"sex\", \"age\", \"duration_sec\", \"split\"]\n",
    "for c in keep_cols:\n",
    "    if c not in m_all.columns:\n",
    "        m_all[c] = np.nan\n",
    "m_all = m_all[keep_cols].copy()\n",
    "\n",
    "test_df = m_all[m_all[\"split\"].isin([\"test\"])].copy().reset_index(drop=True)\n",
    "\n",
    "print(f\"\\nTarget dataset inferred: {dataset_id}\")\n",
    "print(f\"TEST rows: {len(test_df)}\")\n",
    "if len(test_df) == 0:\n",
    "    raise RuntimeError(\"After filtering to split=='test', D1 manifest has 0 rows.\")\n",
    "\n",
    "print(\"TEST label counts:\", test_df[\"label_num\"].value_counts(dropna=False).to_dict())\n",
    "print(\"TEST sex counts (raw):\", test_df[\"sex\"].value_counts(dropna=False).to_dict())\n",
    "\n",
    "# -------------------------\n",
    "# Path check: ensure target audio files exist\n",
    "# Inputs: test_df.clip_path\n",
    "# Output: raises early if any clip files are missing (shows a few examples)\n",
    "# -------------------------\n",
    "def _fail_fast_missing_paths(df: pd.DataFrame, name: str):\n",
    "    missing_paths = []\n",
    "    for p in tqdm(df[\"clip_path\"].astype(str).tolist(), desc=f\"Check {name} clip_path exists\", dynamic_ncols=True):\n",
    "        if not os.path.exists(p):\n",
    "            missing_paths.append(p)\n",
    "            if len(missing_paths) >= 10:\n",
    "                break\n",
    "    if missing_paths:\n",
    "        raise FileNotFoundError(f\"{name}: missing clip_path(s). Examples: {missing_paths}\")\n",
    "\n",
    "_fail_fast_missing_paths(test_df, \"TEST\")\n",
    "\n",
    "# -------------------------\n",
    "# Task grouping: map each row to vowel vs other\n",
    "# Inputs: test_df.task\n",
    "# Output: test_df.task_group used to pick the correct head during inference\n",
    "# -------------------------\n",
    "def _task_group(task_val) -> str:\n",
    "    return \"vowel\" if str(task_val) == \"vowl\" else \"other\"\n",
    "\n",
    "test_df[\"task_group\"] = test_df[\"task\"].apply(_task_group)\n",
    "\n",
    "# -------------------------\n",
    "# Sex normalization for fairness/charts (D1 uses 0/1 encoding)\n",
    "# Inputs: test_df.sex\n",
    "# Output: test_df.sex_norm in {M,F,UNK}\n",
    "# -------------------------\n",
    "def normalize_sex_d1(val) -> str:\n",
    "    \"\"\"\n",
    "    Returns 'M', 'F', or 'UNK'\n",
    "    D1 encoding:\n",
    "      0 -> F\n",
    "      1 -> M\n",
    "    Also handles common string encodings.\n",
    "    \"\"\"\n",
    "    if pd.isna(val):\n",
    "        return \"UNK\"\n",
    "\n",
    "    # numeric\n",
    "    try:\n",
    "        fv = float(val)\n",
    "        if np.isfinite(fv) and abs(fv - round(fv)) < 1e-9:\n",
    "            iv = int(round(fv))\n",
    "            if iv == 0:\n",
    "                return \"F\"\n",
    "            if iv == 1:\n",
    "                return \"M\"\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # string\n",
    "    s = str(val).strip().lower()\n",
    "    if s in {\"m\", \"male\", \"man\", \"masc\", \"masculine\"}:\n",
    "        return \"M\"\n",
    "    if s in {\"f\", \"female\", \"woman\", \"fem\", \"feminine\"}:\n",
    "        return \"F\"\n",
    "    return \"UNK\"\n",
    "\n",
    "test_df[\"sex_norm\"] = test_df[\"sex\"].apply(normalize_sex_d1)\n",
    "print(\"TEST sex counts (normalized):\", test_df[\"sex_norm\"].value_counts(dropna=False).to_dict())\n",
    "if (test_df[\"sex_norm\"] == \"UNK\").any():\n",
    "    print(\"NOTE: Some D1 'sex' values could not be normalized to M/F and were counted as 'UNK' for fairness and sex charts.\")\n",
    "\n",
    "# -------------------------\n",
    "# Dataset + collator: read audio and build attention masks\n",
    "# Inputs: test_df rows + audio files\n",
    "# Output: DataLoader batches with padded input_values, attention_mask, labels, task_group, sex_norm\n",
    "# -------------------------\n",
    "class AudioManifestDataset(Dataset):\n",
    "    def __init__(self, df: pd.DataFrame):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        row = self.df.iloc[idx]\n",
    "        clip_path = str(row[\"clip_path\"])\n",
    "        label = int(row[\"label_num\"])\n",
    "        task_group = str(row[\"task_group\"])\n",
    "        sex_norm = str(row[\"sex_norm\"])\n",
    "\n",
    "        if not os.path.exists(clip_path):\n",
    "            raise FileNotFoundError(f\"Missing clip_path: {clip_path}\")\n",
    "\n",
    "        y, sr = sf.read(clip_path, always_2d=False)\n",
    "        if isinstance(y, np.ndarray) and y.ndim > 1:\n",
    "            y = np.mean(y, axis=1)\n",
    "        y = np.asarray(y, dtype=np.float32)\n",
    "\n",
    "        if int(sr) != SR_EXPECTED:\n",
    "            raise ValueError(f\"Sample rate mismatch (expected {SR_EXPECTED}): {clip_path} has sr={sr}\")\n",
    "\n",
    "        # attention_mask mainly matters for vowel clips (ignore padded tail)\n",
    "        attn = np.ones((len(y),), dtype=np.int64)\n",
    "        if task_group == \"vowel\":\n",
    "            k = -1\n",
    "            for j in range(len(y) - 1, -1, -1):\n",
    "                if abs(float(y[j])) > TINY_THRESH:\n",
    "                    k = j\n",
    "                    break\n",
    "            if k >= 0:\n",
    "                attn[:k+1] = 1\n",
    "                attn[k+1:] = 0\n",
    "            else:\n",
    "                attn[:] = 1\n",
    "        else:\n",
    "            attn[:] = 1\n",
    "\n",
    "        return {\n",
    "            \"input_values\": torch.from_numpy(y),\n",
    "            \"attention_mask\": torch.from_numpy(attn),\n",
    "            \"labels\": torch.tensor(label, dtype=torch.long),\n",
    "            \"task_group\": task_group,\n",
    "            \"sex_norm\": sex_norm,\n",
    "        }\n",
    "\n",
    "def collate_fn(batch):\n",
    "    max_len = int(max(b[\"input_values\"].numel() for b in batch))\n",
    "    input_vals, attn_masks, labels, task_groups, sex_norms = [], [], [], [], []\n",
    "    for b in batch:\n",
    "        x = b[\"input_values\"]\n",
    "        a = b[\"attention_mask\"]\n",
    "        pad = max_len - x.numel()\n",
    "        if pad > 0:\n",
    "            x = torch.cat([x, torch.zeros(pad, dtype=x.dtype)], dim=0)\n",
    "            a = torch.cat([a, torch.zeros(pad, dtype=a.dtype)], dim=0)\n",
    "        input_vals.append(x)\n",
    "        attn_masks.append(a)\n",
    "        labels.append(b[\"labels\"])\n",
    "        task_groups.append(b[\"task_group\"])\n",
    "        sex_norms.append(b[\"sex_norm\"])\n",
    "    return {\n",
    "        \"input_values\": torch.stack(input_vals, dim=0),\n",
    "        \"attention_mask\": torch.stack(attn_masks, dim=0),\n",
    "        \"labels\": torch.stack(labels, dim=0),\n",
    "        \"task_group\": task_groups,\n",
    "        \"sex_norm\": sex_norms,\n",
    "    }\n",
    "\n",
    "# -------------------------\n",
    "# Model: frozen wav2vec2 backbone + two heads (vowel vs other)\n",
    "# Inputs: backbone checkpoint name + dropout_p\n",
    "# Output: logits for PD vs healthy for each clip, using the correct head per task_group\n",
    "# -------------------------\n",
    "class Wav2Vec2TwoHeadClassifier(nn.Module):\n",
    "    def __init__(self, ckpt: str, dropout_p: float):\n",
    "        super().__init__()\n",
    "        self.backbone = Wav2Vec2Model.from_pretrained(ckpt, use_safetensors=True)\n",
    "        for p in self.backbone.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "        H = int(self.backbone.config.hidden_size)\n",
    "        self.pre_vowel = nn.Sequential(nn.LayerNorm(H), nn.Dropout(float(dropout_p)))\n",
    "        self.pre_other = nn.Sequential(nn.LayerNorm(H), nn.Dropout(float(dropout_p)))\n",
    "        self.head_vowel = nn.Linear(H, 2)\n",
    "        self.head_other = nn.Linear(H, 2)\n",
    "\n",
    "    def masked_mean_pool(self, last_hidden, attn_mask_samples):\n",
    "        feat_mask = self.backbone._get_feature_vector_attention_mask(last_hidden.shape[1], attn_mask_samples)\n",
    "        feat_mask = feat_mask.to(last_hidden.device).unsqueeze(-1).type_as(last_hidden)\n",
    "        masked = last_hidden * feat_mask\n",
    "        denom = feat_mask.sum(dim=1).clamp(min=1.0)\n",
    "        return masked.sum(dim=1) / denom\n",
    "\n",
    "    def _heads_fp32(self, x_fp_any: torch.Tensor, head: nn.Module) -> torch.Tensor:\n",
    "        x = x_fp_any.float()\n",
    "        if x.is_cuda:\n",
    "            with torch.amp.autocast(device_type=\"cuda\", enabled=False):\n",
    "                return head(x)\n",
    "        return head(x)\n",
    "\n",
    "    def forward_logits(self, input_values, attention_mask, task_group):\n",
    "        with torch.no_grad():\n",
    "            out = self.backbone(input_values=input_values, attention_mask=attention_mask)\n",
    "            last_hidden = out.last_hidden_state\n",
    "\n",
    "        pooled = self.masked_mean_pool(last_hidden, attention_mask)  # [B,H]\n",
    "\n",
    "        z_v = self.pre_vowel(pooled.float())\n",
    "        z_o = self.pre_other(pooled.float())\n",
    "\n",
    "        logits_v = self._heads_fp32(z_v, self.head_vowel)\n",
    "        logits_o = self._heads_fp32(z_o, self.head_other)\n",
    "\n",
    "        # Select the correct head per row based on task_group\n",
    "        is_vowel = torch.tensor([tg == \"vowel\" for tg in task_group], device=pooled.device, dtype=torch.bool)\n",
    "        logits = logits_o.clone()\n",
    "        if is_vowel.any():\n",
    "            logits[is_vowel] = logits_v[is_vowel]\n",
    "        return logits\n",
    "\n",
    "# -------------------------\n",
    "# Metrics + plot helpers\n",
    "# Inputs: y_true and y_prob (PD probability), plus a threshold for thresholded metrics\n",
    "# Output: numeric metrics dict + PNG charts saved to disk\n",
    "# -------------------------\n",
    "def compute_auc(y_true, y_prob):\n",
    "    y_true = np.asarray(y_true, dtype=np.int64)\n",
    "    y_prob = np.asarray(y_prob, dtype=np.float64)\n",
    "    if len(np.unique(y_true)) < 2:\n",
    "        return float(\"nan\")\n",
    "    return float(roc_auc_score(y_true, y_prob))\n",
    "\n",
    "def compute_threshold_metrics(y_true, y_prob, thr=0.5):\n",
    "    y_true = np.asarray(y_true, dtype=np.int64)\n",
    "    y_prob = np.asarray(y_prob, dtype=np.float64)\n",
    "    y_pred = (y_prob >= float(thr)).astype(np.int64)\n",
    "\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=[0, 1])\n",
    "    TN, FP, FN, TP = int(cm[0, 0]), int(cm[0, 1]), int(cm[1, 0]), int(cm[1, 1])\n",
    "\n",
    "    eps = 1e-12\n",
    "    acc = (TP + TN) / max(1, (TP + TN + FP + FN))\n",
    "    prec = TP / (TP + FP + eps)\n",
    "    rec = TP / (TP + FN + eps)     # sensitivity\n",
    "    f1 = 2 * prec * rec / (prec + rec + eps)\n",
    "    spec = TN / (TN + FP + eps)\n",
    "\n",
    "    try:\n",
    "        mcc = float(matthews_corrcoef(y_true, y_pred)) if len(np.unique(y_pred)) > 1 else float(\"nan\")\n",
    "    except Exception:\n",
    "        mcc = float(\"nan\")\n",
    "\n",
    "    try:\n",
    "        _, pval = fisher_exact([[TN, FP], [FN, TP]], alternative=\"two-sided\")\n",
    "        pval = float(pval)\n",
    "    except Exception:\n",
    "        pval = float(\"nan\")\n",
    "\n",
    "    return {\n",
    "        \"threshold\": float(thr),\n",
    "        \"confusion_matrix\": {\"TN\": TN, \"FP\": FP, \"FN\": FN, \"TP\": TP},\n",
    "        \"accuracy\": float(acc),\n",
    "        \"precision\": float(prec),\n",
    "        \"recall\": float(rec),\n",
    "        \"f1_score\": float(f1),\n",
    "        \"sensitivity\": float(rec),\n",
    "        \"specificity\": float(spec),\n",
    "        \"mcc\": float(mcc),\n",
    "        \"p_value_fisher_two_sided\": float(pval),\n",
    "    }\n",
    "\n",
    "def save_roc_curve_png(y_true, y_prob, out_png, title_suffix=\"Test\"):\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_prob)\n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr)\n",
    "    plt.plot([0, 1], [0, 1], linestyle=\"--\")\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(f\"ROC Curve ({title_suffix})\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_png, dpi=150)\n",
    "    plt.close()\n",
    "\n",
    "def save_confusion_png(y_true, y_prob, out_png, thr=0.5, title_suffix=\"Test\"):\n",
    "    y_true = np.asarray(y_true, dtype=np.int64)\n",
    "    y_prob = np.asarray(y_prob, dtype=np.float64)\n",
    "    y_pred = (y_prob >= float(thr)).astype(int)\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=[0, 1])\n",
    "\n",
    "    plt.figure()\n",
    "    plt.imshow(cm)\n",
    "    plt.xticks([0, 1], [\"Healthy(0)\", \"PD(1)\"])\n",
    "    plt.yticks([0, 1], [\"Healthy(0)\", \"PD(1)\"])\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"True\")\n",
    "    plt.title(f\"Confusion Matrix ({title_suffix}, thr={thr:.2f})\")\n",
    "    for (i, j), v in np.ndenumerate(cm):\n",
    "        plt.text(j, i, str(v), ha=\"center\", va=\"center\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_png, dpi=150)\n",
    "    plt.close()\n",
    "\n",
    "# -------------------------\n",
    "# Fairness: FNR per sex and ΔFNR = FNR(F) - FNR(M)\n",
    "# Inputs: y_true, y_prob, sex_norm array, threshold\n",
    "# Output: per-group FNR details + signed and absolute gap\n",
    "# -------------------------\n",
    "def compute_fnr_by_group_signed(y_true, y_prob, groups, thr=0.5):\n",
    "    y_true = np.asarray(y_true, dtype=np.int64)\n",
    "    y_prob = np.asarray(y_prob, dtype=np.float64)\n",
    "    groups = np.asarray(list(groups), dtype=object)\n",
    "    y_pred = (y_prob >= float(thr)).astype(np.int64)\n",
    "\n",
    "    out = {}\n",
    "    for g in sorted(set(groups.tolist())):\n",
    "        mask_g = (groups == g)\n",
    "        if int(mask_g.sum()) == 0:\n",
    "            continue\n",
    "\n",
    "        pos_mask = mask_g & (y_true == 1)\n",
    "        n_pos = int(pos_mask.sum())\n",
    "        if n_pos == 0:\n",
    "            out[g] = {\"n_total\": int(mask_g.sum()), \"n_pos\": 0, \"tp\": 0, \"fn\": 0, \"fnr\": float(\"nan\")}\n",
    "            continue\n",
    "\n",
    "        tp = int(((y_pred == 1) & pos_mask).sum())\n",
    "        fn = int(((y_pred == 0) & pos_mask).sum())\n",
    "        fnr = float(fn / max(1, (fn + tp)))\n",
    "\n",
    "        out[g] = {\"n_total\": int(mask_g.sum()), \"n_pos\": int(n_pos), \"tp\": int(tp), \"fn\": int(fn), \"fnr\": float(fnr)}\n",
    "\n",
    "    fnr_m = out.get(\"M\", {}).get(\"fnr\", float(\"nan\"))\n",
    "    fnr_f = out.get(\"F\", {}).get(\"fnr\", float(\"nan\"))\n",
    "\n",
    "    if (not np.isnan(fnr_m)) and (not np.isnan(fnr_f)):\n",
    "        delta_signed = float(fnr_f - fnr_m)\n",
    "        delta_abs = float(abs(delta_signed))\n",
    "    else:\n",
    "        delta_signed = float(\"nan\")\n",
    "        delta_abs = float(\"nan\")\n",
    "\n",
    "    return out, delta_signed, delta_abs\n",
    "\n",
    "def compute_confusion_counts(y_true, y_prob, thr=0.5):\n",
    "    y_true = np.asarray(y_true, dtype=np.int64)\n",
    "    y_prob = np.asarray(y_prob, dtype=np.float64)\n",
    "    y_pred = (y_prob >= float(thr)).astype(np.int64)\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=[0, 1])\n",
    "    TN, FP, FN, TP = int(cm[0, 0]), int(cm[0, 1]), int(cm[1, 0]), int(cm[1, 1])\n",
    "    return {\"TN\": TN, \"FP\": FP, \"FN\": FN, \"TP\": TP}\n",
    "\n",
    "def compute_confusion_by_group(y_true, y_prob, groups, thr=0.5):\n",
    "    y_true = np.asarray(y_true, dtype=np.int64)\n",
    "    y_prob = np.asarray(y_prob, dtype=np.float64)\n",
    "    groups = np.asarray(list(groups), dtype=object)\n",
    "    out = {}\n",
    "    for g in sorted(set(groups.tolist())):\n",
    "        mask = (groups == g)\n",
    "        if int(mask.sum()) == 0:\n",
    "            continue\n",
    "        out[g] = {\"n\": int(mask.sum()), \"confusion\": compute_confusion_counts(y_true[mask], y_prob[mask], thr=thr)}\n",
    "    return out\n",
    "\n",
    "# -------------------------\n",
    "# Reproducibility: set all RNG seeds\n",
    "# Inputs: seed int\n",
    "# Output: deterministic settings for random, numpy, torch\n",
    "# -------------------------\n",
    "def set_all_seeds(seed: int):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# -------------------------\n",
    "# Source checkpoint selection: pick most recent D4 trainval exp with all seeds\n",
    "# Inputs: D4 trainval_runs folder\n",
    "# Output: chosen_exp folder used to load best_heads.pt per seed\n",
    "# -------------------------\n",
    "D4_TRAINVAL_ROOT = Path(D4_OUT_ROOT) / \"trainval_runs\"\n",
    "if not D4_TRAINVAL_ROOT.exists():\n",
    "    raise FileNotFoundError(f\"Missing D4 trainval_runs folder: {str(D4_TRAINVAL_ROOT)}\")\n",
    "\n",
    "exp_dirs = sorted([p for p in D4_TRAINVAL_ROOT.glob(\"exp_*\") if p.is_dir()], key=lambda p: p.stat().st_mtime, reverse=True)\n",
    "if not exp_dirs:\n",
    "    raise FileNotFoundError(f\"No exp_* folders found under: {str(D4_TRAINVAL_ROOT)}\")\n",
    "\n",
    "def _has_all_seeds(exp_path: Path, seeds: list):\n",
    "    for s in seeds:\n",
    "        p = exp_path / f\"run_D4_seed{s}\" / \"best_heads.pt\"\n",
    "        if not p.exists():\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "chosen_exp = None\n",
    "for ed in exp_dirs:\n",
    "    if _has_all_seeds(ed, SEEDS):\n",
    "        chosen_exp = ed\n",
    "        break\n",
    "\n",
    "if chosen_exp is None:\n",
    "    sample = exp_dirs[0]\n",
    "    raise FileNotFoundError(\n",
    "        \"Could not find a D4 trainval experiment with all 3 best_heads.pt files.\\n\"\n",
    "        f\"Expected: {str(D4_TRAINVAL_ROOT)}/exp_*/run_D4_seedXXXX/best_heads.pt\\n\"\n",
    "        f\"Most recent exp checked: {str(sample)}\"\n",
    "    )\n",
    "\n",
    "print(\"\\nUsing SOURCE (D4) Train+Val experiment folder:\")\n",
    "print(\" \", str(chosen_exp))\n",
    "\n",
    "# -------------------------\n",
    "# Output folder: write only under D1\n",
    "# Inputs: D1_OUT_ROOT\n",
    "# Output: ZS_ROOT folder for per-seed runs + summary files\n",
    "# -------------------------\n",
    "ZS_ROOT = Path(D1_OUT_ROOT) / \"Cross_Language_Zero_Shot_Runs\"\n",
    "ZS_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# -------------------------\n",
    "# DataLoader: D1 TEST only (plus a small warm-up read)\n",
    "# Inputs: test_df and audio files\n",
    "# Output: test_loader ready for inference\n",
    "# -------------------------\n",
    "test_ds = AudioManifestDataset(test_df)\n",
    "test_loader = DataLoader(\n",
    "    test_ds,\n",
    "    batch_size=PER_DEVICE_BS,\n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=PIN_MEMORY,\n",
    "    collate_fn=collate_fn,\n",
    ")\n",
    "\n",
    "print(\"\\nWarm-up: loading up to 2 TEST batches...\")\n",
    "t0 = time.time()\n",
    "nb = len(test_loader)\n",
    "wb = min(2, nb)\n",
    "if wb == 0:\n",
    "    raise RuntimeError(\"TEST DataLoader has 0 batches. Check test_df length and PER_DEVICE_BS.\")\n",
    "it = iter(test_loader)\n",
    "for i in range(wb):\n",
    "    _ = next(it)\n",
    "    print(f\"  loaded TEST warmup batch {i+1}/{wb}\")\n",
    "print(f\"Warm-up done in {time.time()-t0:.2f}s\")\n",
    "\n",
    "# -------------------------\n",
    "# Checkpoint loader: load only the trained heads from best_heads.pt\n",
    "# Inputs: model instance + best_heads.pt path\n",
    "# Output: model with head weights restored for this seed\n",
    "# -------------------------\n",
    "def load_heads_into_model(model: Wav2Vec2TwoHeadClassifier, best_heads_path: Path):\n",
    "    if not best_heads_path.exists():\n",
    "        raise FileNotFoundError(f\"Missing best_heads.pt: {str(best_heads_path)}\")\n",
    "    state = torch.load(str(best_heads_path), map_location=\"cpu\")\n",
    "    needed = [\"pre_vowel\", \"pre_other\", \"head_vowel\", \"head_other\"]\n",
    "    missing = [k for k in needed if k not in state]\n",
    "    if missing:\n",
    "        raise KeyError(\n",
    "            f\"best_heads.pt missing keys {missing}. Found keys: {list(state.keys())}. \"\n",
    "            \"This zero-shot code expects the trainval save format.\"\n",
    "        )\n",
    "    model.pre_vowel.load_state_dict(state[\"pre_vowel\"], strict=True)\n",
    "    model.pre_other.load_state_dict(state[\"pre_other\"], strict=True)\n",
    "    model.head_vowel.load_state_dict(state[\"head_vowel\"], strict=True)\n",
    "    model.head_other.load_state_dict(state[\"head_other\"], strict=True)\n",
    "    return model\n",
    "\n",
    "# -------------------------\n",
    "# Inference: run model over D1 test_loader and collect probabilities\n",
    "# Inputs: loader + model\n",
    "# Output: arrays (y_true, y_prob, sex_norm) for metrics and plots\n",
    "# -------------------------\n",
    "def run_inference(loader, model, desc):\n",
    "    use_amp = bool(USE_AMP and DEVICE.type == \"cuda\")\n",
    "    all_probs, all_true, all_sex = [], [], []\n",
    "\n",
    "    pbar = tqdm(loader, desc=desc, dynamic_ncols=True)\n",
    "    with torch.inference_mode():\n",
    "        for batch in pbar:\n",
    "            input_values = batch[\"input_values\"].to(DEVICE, non_blocking=False)\n",
    "            attention_mask = batch[\"attention_mask\"].to(DEVICE, non_blocking=False)\n",
    "            labels = batch[\"labels\"].to(DEVICE, non_blocking=False)\n",
    "            task_group = batch[\"task_group\"]\n",
    "            sex_norm = batch[\"sex_norm\"]\n",
    "\n",
    "            with torch.amp.autocast(device_type=\"cuda\", enabled=use_amp):\n",
    "                logits = model.forward_logits(input_values, attention_mask, task_group)\n",
    "\n",
    "            probs = torch.softmax(logits.float(), dim=-1)[:, 1].detach().cpu().numpy().astype(np.float64)\n",
    "            all_probs.extend(probs.tolist())\n",
    "            all_true.extend(labels.detach().cpu().numpy().astype(np.int64).tolist())\n",
    "            all_sex.extend(list(sex_norm))\n",
    "\n",
    "    return np.asarray(all_true, dtype=np.int64), np.asarray(all_probs, dtype=np.float64), np.asarray(all_sex, dtype=object)\n",
    "\n",
    "# -------------------------\n",
    "# Per-seed run: load D4 heads, evaluate D1 at FIXED_THR, save metrics + PNGs\n",
    "# Inputs: seed, chosen_exp, FIXED_THR, D1 test_loader\n",
    "# Output: run_<TRANSFER_TAG>_seedXXXX folder with metrics.json + plots\n",
    "# -------------------------\n",
    "def run_seed(seed: int):\n",
    "    set_all_seeds(seed)\n",
    "\n",
    "    run_dir = ZS_ROOT / f\"run_{TRANSFER_TAG}_seed{seed}\"\n",
    "    run_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    best_heads_path = chosen_exp / f\"run_D4_seed{seed}\" / \"best_heads.pt\"\n",
    "\n",
    "    print(f\"\\n[seed={seed}] Loading SOURCE heads from:\")\n",
    "    print(\" \", str(best_heads_path))\n",
    "    print(f\"[seed={seed}] Evaluating TARGET D1 TEST @ fixed_thr={FIXED_THR:.6f}\")\n",
    "\n",
    "    model = Wav2Vec2TwoHeadClassifier(BACKBONE_CKPT, dropout_p=DROPOUT_P).to(DEVICE)\n",
    "    model = load_heads_into_model(model, best_heads_path)\n",
    "    model.eval()\n",
    "\n",
    "    yt, pt, st = run_inference(test_loader, model, desc=f\"[seed={seed}] D1 TEST (zero-shot)\")\n",
    "\n",
    "    test_auc = compute_auc(yt, pt)  # threshold-free\n",
    "    thr_metrics = compute_threshold_metrics(yt, pt, thr=FIXED_THR)\n",
    "\n",
    "    fnr_by_sex, delta_f_minus_m, delta_abs = compute_fnr_by_group_signed(yt, pt, st, thr=FIXED_THR)\n",
    "    confusion_by_sex = compute_confusion_by_group(yt, pt, st, thr=FIXED_THR)\n",
    "\n",
    "    # Plots: always save overall ROC + confusion matrix\n",
    "    roc_png = run_dir / \"roc_curve.png\"\n",
    "    cm_png  = run_dir / \"confusion_matrix.png\"\n",
    "    save_roc_curve_png(yt, pt, str(roc_png), title_suffix=f\"D1 Test (seed={seed})\")\n",
    "    save_confusion_png(yt, pt, str(cm_png), thr=FIXED_THR, title_suffix=f\"D1 Test (seed={seed})\")\n",
    "\n",
    "    # Sex-specific confusion matrices (only if that sex exists in D1 TEST)\n",
    "    cm_m_png = None\n",
    "    cm_f_png = None\n",
    "    mask_m = (st == \"M\")\n",
    "    mask_f = (st == \"F\")\n",
    "\n",
    "    if int(mask_m.sum()) > 0:\n",
    "        cm_m_png = run_dir / \"confusion_matrix_M.png\"\n",
    "        save_confusion_png(yt[mask_m], pt[mask_m], str(cm_m_png), thr=FIXED_THR, title_suffix=f\"D1 Test SEX=M (seed={seed})\")\n",
    "\n",
    "    if int(mask_f.sum()) > 0:\n",
    "        cm_f_png = run_dir / \"confusion_matrix_F.png\"\n",
    "        save_confusion_png(yt[mask_f], pt[mask_f], str(cm_f_png), thr=FIXED_THR, title_suffix=f\"D1 Test SEX=F (seed={seed})\")\n",
    "\n",
    "    # Structured metrics saved for later analysis/paper tables\n",
    "    metrics = {\n",
    "        \"transfer\": {\n",
    "            \"tag\": TRANSFER_TAG,\n",
    "            \"source_dataset\": \"D4\",\n",
    "            \"target_dataset\": \"D1\",\n",
    "            \"threshold_policy\": \"Fixed threshold = mean VAL-opt threshold from D4 monolingual test summary (Youden J on D4 VAL)\",\n",
    "            \"fixed_threshold_value\": float(FIXED_THR),\n",
    "            \"d4_mono_summary_path\": str(D4_MONO_SUMMARY),\n",
    "        },\n",
    "\n",
    "        \"target\": {\n",
    "            \"dataset\": \"D1\",\n",
    "            \"seed\": int(seed),\n",
    "            \"n_test\": int(len(test_df)),\n",
    "            \"label_counts_test\": test_df[\"label_num\"].value_counts(dropna=False).to_dict(),\n",
    "            \"sex_counts_test_norm\": test_df[\"sex_norm\"].value_counts(dropna=False).to_dict(),\n",
    "        },\n",
    "\n",
    "        \"test_auroc\": float(test_auc),\n",
    "\n",
    "        \"threshold_metrics_test\": thr_metrics,\n",
    "        \"test_threshold_used\": float(FIXED_THR),\n",
    "\n",
    "        \"fairness_test\": {\n",
    "            \"definition\": \"H3: ΔFNR = FNR(F) - FNR(M), where FNR(sex)=FN/(FN+TP) computed on PD-only true labels at fixed transfer threshold.\",\n",
    "            \"fnr_by_sex_norm\": fnr_by_sex,\n",
    "            \"delta_fnr_F_minus_M\": float(delta_f_minus_m),\n",
    "            \"delta_fnr_abs\": float(delta_abs),\n",
    "            \"note\": \"If n_PD for a sex is 0, its FNR is NaN and ΔFNR is NaN.\",\n",
    "            \"sex_normalization_note\": \"D1 mapping: 0->F, 1->M; otherwise UNK.\",\n",
    "        },\n",
    "\n",
    "        \"confusion_by_sex_norm\": confusion_by_sex,\n",
    "\n",
    "        \"artifacts\": {\n",
    "            \"roc_curve_png\": str(roc_png),\n",
    "            \"confusion_matrix_png\": str(cm_png),\n",
    "            \"confusion_matrix_M_png\": str(cm_m_png) if cm_m_png is not None else None,\n",
    "            \"confusion_matrix_F_png\": str(cm_f_png) if cm_f_png is not None else None,\n",
    "        },\n",
    "\n",
    "        \"paths\": {\n",
    "            \"d1_out_root\": str(D1_OUT_ROOT),\n",
    "            \"zero_shot_root\": str(ZS_ROOT),\n",
    "            \"run_dir\": str(run_dir),\n",
    "            \"source_trainval_experiment_used\": str(chosen_exp),\n",
    "            \"source_best_heads_path\": str(best_heads_path),\n",
    "        },\n",
    "\n",
    "        \"backbone_ckpt\": BACKBONE_CKPT,\n",
    "        \"dropout_p\": float(DROPOUT_P),\n",
    "        \"timestamp\": time.strftime(\"%Y%m%d_%H%M%S\"),\n",
    "    }\n",
    "\n",
    "    metrics_path = run_dir / \"metrics.json\"\n",
    "    with open(metrics_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(metrics, f, indent=2)\n",
    "\n",
    "    print(f\"[seed={seed}] DONE | test_AUROC={test_auc:.6f}\")\n",
    "\n",
    "    def _fmt_fnr(g):\n",
    "        d = fnr_by_sex.get(g, None)\n",
    "        if d is None:\n",
    "            return \"n/a\"\n",
    "        return f\"fnr={d['fnr']:.6f} (n_PD={d['n_pos']}, fn={d['fn']}, tp={d['tp']})\"\n",
    "\n",
    "    print(f\"[seed={seed}] TEST metrics @ fixed_thr={FIXED_THR:.6f}\")\n",
    "    print(f\"[seed={seed}] FAIRNESS (H3) @ fixed_thr={FIXED_THR:.6f}:\")\n",
    "    print(\"  M:\", _fmt_fnr(\"M\"))\n",
    "    print(\"  F:\", _fmt_fnr(\"F\"))\n",
    "    if \"UNK\" in fnr_by_sex:\n",
    "        print(\"  UNK:\", _fmt_fnr(\"UNK\"))\n",
    "    print(\"  ΔFNR (F-M):\", f\"{delta_f_minus_m:.6f}\" if not np.isnan(delta_f_minus_m) else \"nan\")\n",
    "    print(\"  |ΔFNR|:\", f\"{delta_abs:.6f}\" if not np.isnan(delta_abs) else \"nan\")\n",
    "\n",
    "    print(f\"[seed={seed}] WROTE:\")\n",
    "    print(\" \", str(metrics_path))\n",
    "    print(\" \", str(roc_png))\n",
    "    print(\" \", str(cm_png))\n",
    "    if cm_m_png is not None:\n",
    "        print(\" \", str(cm_m_png))\n",
    "    if cm_f_png is not None:\n",
    "        print(\" \", str(cm_f_png))\n",
    "\n",
    "    return {\n",
    "        \"seed\": int(seed),\n",
    "        \"test_auroc\": float(test_auc),\n",
    "        \"thr_metrics_test\": thr_metrics,\n",
    "        \"fnr_by_sex_test\": fnr_by_sex,\n",
    "        \"delta_f_minus_m_test\": float(delta_f_minus_m),\n",
    "        \"delta_abs_test\": float(delta_abs),\n",
    "        \"run_dir\": str(run_dir),\n",
    "    }\n",
    "\n",
    "# -------------------------\n",
    "# Aggregate across 3 seeds: AUROC mean ± 95% CI, others mean ± SD\n",
    "# Inputs: per-seed outputs from run_seed()\n",
    "# Output: summary_zero_shot.json + history_index.jsonl under ZS_ROOT\n",
    "# -------------------------\n",
    "seed_results = []\n",
    "for seed in SEEDS:\n",
    "    seed_results.append(run_seed(seed))\n",
    "\n",
    "aurocs = [r[\"test_auroc\"] for r in seed_results]\n",
    "t_crit = 4.302652729911275  # df=2, 95% CI\n",
    "n = len(aurocs)\n",
    "mean_auc = float(np.mean(aurocs))\n",
    "std_auc = float(np.std(aurocs, ddof=1)) if n > 1 else 0.0\n",
    "half_width = float(t_crit * (std_auc / math.sqrt(n))) if n > 1 else 0.0\n",
    "ci95 = [float(mean_auc - half_width), float(mean_auc + half_width)]\n",
    "\n",
    "def _mean_sd(vals):\n",
    "    vals = np.asarray(vals, dtype=np.float64)\n",
    "    return float(np.nanmean(vals)), float(np.nanstd(vals, ddof=1)) if np.sum(~np.isnan(vals)) > 1 else 0.0\n",
    "\n",
    "thr_list = [r[\"thr_metrics_test\"] for r in seed_results]\n",
    "keys = [\"accuracy\",\"precision\",\"recall\",\"f1_score\",\"sensitivity\",\"specificity\",\"mcc\",\"p_value_fisher_two_sided\"]\n",
    "\n",
    "agg = {}\n",
    "for k in keys:\n",
    "    v = [float(tm.get(k, float(\"nan\"))) for tm in thr_list]\n",
    "    mu, sd = _mean_sd(v)\n",
    "    agg[k] = {\n",
    "        \"mean\": mu,\n",
    "        \"sd\": sd,\n",
    "        \"values_by_seed\": {str(r[\"seed\"]): float(tm.get(k, float(\"nan\"))) for r, tm in zip(seed_results, thr_list)}\n",
    "    }\n",
    "\n",
    "cm_by_seed = {str(r[\"seed\"]): r[\"thr_metrics_test\"][\"confusion_matrix\"] for r in seed_results}\n",
    "\n",
    "# FAIRNESS aggregation (H3)\n",
    "fnr_by_seed = {str(r[\"seed\"]): r[\"fnr_by_sex_test\"] for r in seed_results}\n",
    "delta_signed_by_seed = {str(r[\"seed\"]): float(r[\"delta_f_minus_m_test\"]) for r in seed_results}\n",
    "delta_abs_by_seed = {str(r[\"seed\"]): float(r[\"delta_abs_test\"]) for r in seed_results}\n",
    "\n",
    "fnr_m_vals, fnr_f_vals, n_pd_m_vals, n_pd_f_vals = [], [], [], []\n",
    "d_signed_vals, d_abs_vals = [], []\n",
    "\n",
    "for r in seed_results:\n",
    "    s = str(r[\"seed\"])\n",
    "    d = fnr_by_seed.get(s, {})\n",
    "    fnr_m_vals.append(float(d.get(\"M\", {}).get(\"fnr\", float(\"nan\"))))\n",
    "    fnr_f_vals.append(float(d.get(\"F\", {}).get(\"fnr\", float(\"nan\"))))\n",
    "    n_pd_m_vals.append(float(d.get(\"M\", {}).get(\"n_pos\", float(\"nan\"))))\n",
    "    n_pd_f_vals.append(float(d.get(\"F\", {}).get(\"n_pos\", float(\"nan\"))))\n",
    "    d_signed_vals.append(float(delta_signed_by_seed.get(s, float(\"nan\"))))\n",
    "    d_abs_vals.append(float(delta_abs_by_seed.get(s, float(\"nan\"))))\n",
    "\n",
    "fnr_m_mean, fnr_m_sd = _mean_sd(fnr_m_vals)\n",
    "fnr_f_mean, fnr_f_sd = _mean_sd(fnr_f_vals)\n",
    "d_signed_mean, d_signed_sd = _mean_sd(d_signed_vals)\n",
    "d_abs_mean, d_abs_sd = _mean_sd(d_abs_vals)\n",
    "\n",
    "print(\"\\nZero-shot TEST AUROC by seed:\")\n",
    "for r in seed_results:\n",
    "    print(f\"  seed {r['seed']}: {r['test_auroc']:.6f}\")\n",
    "print(f\"\\nMean Zero-shot TEST AUROC: {mean_auc:.6f}\")\n",
    "print(f\"95% CI (t, n=3): [{ci95[0]:.6f}, {ci95[1]:.6f}]\")\n",
    "\n",
    "print(f\"\\nFixed threshold used for ALL seeds (from D4 mono mean VAL-opt): {FIXED_THR:.6f}\")\n",
    "\n",
    "print(\"\\nThreshold metrics on D1 TEST @ fixed threshold (mean ± SD across seeds):\")\n",
    "for k in [\"accuracy\",\"precision\",\"sensitivity\",\"specificity\",\"f1_score\",\"mcc\"]:\n",
    "    mu = agg[k][\"mean\"]\n",
    "    sd = agg[k][\"sd\"]\n",
    "    print(f\"  {k}: {mu:.6f} ± {sd:.6f}\")\n",
    "print(\"  fisher_p_value_two_sided:\", f\"{agg['p_value_fisher_two_sided']['mean']:.6g} ± {agg['p_value_fisher_two_sided']['sd']:.6g}\")\n",
    "\n",
    "print(\"\\nFAIRNESS (H3) on D1 TEST @ fixed threshold (mean ± SD across seeds):\")\n",
    "print(f\"  FNR_M: {fnr_m_mean:.6f} ± {fnr_m_sd:.6f}\")\n",
    "print(f\"  FNR_F: {fnr_f_mean:.6f} ± {fnr_f_sd:.6f}\")\n",
    "print(f\"  ΔFNR (F-M): {d_signed_mean:.6f} ± {d_signed_sd:.6f}\")\n",
    "print(f\"  |ΔFNR|: {d_abs_mean:.6f} ± {d_abs_sd:.6f}\")\n",
    "\n",
    "summary = {\n",
    "    \"transfer\": {\n",
    "        \"tag\": TRANSFER_TAG,\n",
    "        \"source_dataset\": \"D4\",\n",
    "        \"target_dataset\": \"D1\",\n",
    "        \"fixed_threshold_value\": float(FIXED_THR),\n",
    "        \"fixed_threshold_source\": {\n",
    "            \"path\": str(D4_MONO_SUMMARY),\n",
    "            \"key\": \"val_opt_threshold_mean_sd.mean\",\n",
    "            \"note\": \"Mean across seeds of D4 VAL-opt thresholds (Youden J) as recorded by D4 monolingual test run.\",\n",
    "        },\n",
    "        \"source_trainval_experiment_used\": str(chosen_exp),\n",
    "    },\n",
    "\n",
    "    \"target\": {\n",
    "        \"dx_out_root\": str(D1_OUT_ROOT),\n",
    "        \"manifest_all\": str(D1_MANIFEST_ALL),\n",
    "        \"n_test\": int(len(test_df)),\n",
    "        \"label_counts_test\": test_df[\"label_num\"].value_counts(dropna=False).to_dict(),\n",
    "        \"sex_counts_test_norm\": test_df[\"sex_norm\"].value_counts(dropna=False).to_dict(),\n",
    "    },\n",
    "\n",
    "    \"seeds\": SEEDS,\n",
    "\n",
    "    \"test_aurocs_by_seed\": {str(r[\"seed\"]): float(r[\"test_auroc\"]) for r in seed_results},\n",
    "    \"mean_test_auroc\": float(mean_auc),\n",
    "    \"t_crit_df2_95\": float(t_crit),\n",
    "    \"half_width_95_ci\": float(half_width),\n",
    "    \"ci95_test_auroc\": ci95,\n",
    "\n",
    "    \"threshold_metrics_test_mean_sd\": agg,\n",
    "    \"confusion_matrix_by_seed\": cm_by_seed,\n",
    "\n",
    "    \"fairness_test\": {\n",
    "        \"definition\": \"H3: ΔFNR = FNR(F) - FNR(M), where FNR(sex)=FN/(FN+TP) computed on PD-only true labels at the fixed transfer threshold.\",\n",
    "        \"fnr_by_sex_norm_by_seed\": fnr_by_seed,\n",
    "        \"delta_fnr_F_minus_M_by_seed\": delta_signed_by_seed,\n",
    "        \"delta_fnr_abs_by_seed\": delta_abs_by_seed,\n",
    "        \"fnr_M_mean_sd\": {\"mean\": float(fnr_m_mean), \"sd\": float(fnr_m_sd), \"values_by_seed\": {str(s): float(v) for s, v in zip(SEEDS, fnr_m_vals)}},\n",
    "        \"fnr_F_mean_sd\": {\"mean\": float(fnr_f_mean), \"sd\": float(fnr_f_sd), \"values_by_seed\": {str(s): float(v) for s, v in zip(SEEDS, fnr_f_vals)}},\n",
    "        \"delta_fnr_F_minus_M_mean_sd\": {\"mean\": float(d_signed_mean), \"sd\": float(d_signed_sd), \"values_by_seed\": {str(s): float(v) for s, v in zip(SEEDS, d_signed_vals)}},\n",
    "        \"delta_fnr_abs_mean_sd\": {\"mean\": float(d_abs_mean), \"sd\": float(d_abs_sd), \"values_by_seed\": {str(s): float(v) for s, v in zip(SEEDS, d_abs_vals)}},\n",
    "        \"denominators_PD_by_seed\": {str(s): {\"n_PD_M\": float(n_pd_m_vals[i]), \"n_PD_F\": float(n_pd_f_vals[i])} for i, s in enumerate(SEEDS)},\n",
    "        \"sex_normalization_note\": \"D1 mapping: 0->F, 1->M; otherwise UNK. ΔFNR computed only when both M and F have defined FNR.\",\n",
    "    },\n",
    "\n",
    "    \"run_dirs\": [r[\"run_dir\"] for r in seed_results],\n",
    "    \"timestamp\": time.strftime(\"%Y%m%d_%H%M%S\"),\n",
    "}\n",
    "\n",
    "summary_path = ZS_ROOT / \"summary_zero_shot.json\"\n",
    "with open(summary_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "history_path = ZS_ROOT / \"history_index.jsonl\"\n",
    "with open(history_path, \"a\", encoding=\"utf-8\") as f:\n",
    "    f.write(json.dumps(summary) + \"\\n\")\n",
    "\n",
    "print(\"\\nWROTE summary:\", str(summary_path))\n",
    "print(\"APPENDED history index:\", str(history_path))\n",
    "print(\"Open this folder to access artifacts:\", str(ZS_ROOT))\n",
    "\n",
    "# -------------------------\n",
    "# Stop runtime (Colab)\n",
    "# Inputs: none\n",
    "# Output: attempts to release the GPU instance\n",
    "# -------------------------\n",
    "print(\"\\nAll done. Unassigning the runtime to stop the L4 instance...\")\n",
    "try:\n",
    "    from google.colab import runtime  # type: ignore\n",
    "    print(\"Calling runtime.unassign() now.\")\n",
    "    runtime.unassign()\n",
    "except Exception as e:\n",
    "    print(\"Could not unassign runtime automatically. You can stop the runtime manually in Colab.\")\n",
    "    print(\"Reason:\", repr(e))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "93f3f099a3c740c5995b1a4c05236b1f",
      "107b883e6c8b46a1b85c2f36c387809b",
      "fb8cd11d092f4b07ab116b23d094b2dc",
      "8ad7bad5a5d4425dbda50ca704edddf9",
      "74337012a1c746ea8736d6e63de1fc89",
      "310a18de0b374c3e891abbede027ba4f",
      "6b1a8498d2a54342a81e34b6b4c7f347",
      "633a543c6f8e4b8ba6a40931866888ca",
      "165e131430ac4fd387baa5839b4acbe8",
      "1b89938777294ea4850340b2e5e54809",
      "e1174b1dd0ed4c3a9a63cb5498a5be1f",
      "5ab23b7116ce4922aa624894879f8b77",
      "3f85af706bcf4d869fe77f126ef554b7",
      "ef963b9c20b64a1383face150e719110",
      "47fda5ae5a824e4d941c95e38b5998bc",
      "dd4b315958e44796a410076d7bcd1f53",
      "d2712a39c3ba410baf630768dd52c3d7",
      "1f97ecae39a04480a067625c8c13d407",
      "381b5a38019240a38b2930e0b20328f8",
      "f2a965e6286544439923bf273c49ef1d",
      "8d40e85f0dd5423aacecb65d4bab4f95",
      "e4a48ae5ab7546d881ec291f0fe1c513",
      "b442c79adf56418da1673196aa88d8ff",
      "5e79be0002bf4584bd5a9ac99b0e5c86",
      "0ec33cbf2b6243f794349237878e9ad8",
      "49dfa480902d4435af3b17b8c19c20c8",
      "f99b9e0df4f54575b12c34e8b98349e4",
      "d8faa0c6c4854f43bc275109a0f968a2",
      "543fba00f34a4ff3b52d16c65090fe4d",
      "7d642988080a40ef83bdb58679184efe",
      "9f9908d4f222483581960c59fd652488",
      "2eaf82d33c294e20adf6f6f7f814079a",
      "8675080af69244638c36de3e936ce387",
      "0ff9790360294d80b94221dc9ab4fa4b",
      "aa80591eae384d6790b3435277b12b1d",
      "1f073cdbdda44872b926f274e1f78c5f",
      "54c6a8cf0c06475a93cbb2967a024a26",
      "9d31ef07327941918a289d91fb5c2d4d",
      "633aeba3fa7f4080b591cf8ca2b88b9a",
      "b483b74cd45840209a3d9717e061ab0f",
      "73c55646f76b496da9635013bbdbc407",
      "ebb61ecdd41142559b7730145866dfdc",
      "fa18ccfcd9554f198e1992c3448f86ca",
      "aa0a4a4a72184bf492ae878b877073a4",
      "5929e2789e4e4f1ea7fb4d55c4b3e3be",
      "3306e0f55b8e418da4b75b0a02b959ff",
      "609bcecc6fc34ce583481cd739afa0a5",
      "36986ce93d214e2ab7dc13b9d6db7a96",
      "0017320c53be4556ae8094fd0411dd4c",
      "c8162e1194ca4fc5a82d4c4af8ce60a6",
      "e2d500185c8647eb8781c025d7591887",
      "90e4b5a9fa844b2a9f08fdba2fb1c42c",
      "0f146be5aeac41a1b2c2bf3011a71cde",
      "ac1b2d3000a94b17af2a8780cfd23883",
      "eb6e4aa5d6f847d4be6b4523e1688cd1",
      "66a22440e87242afbe323a9de1f368ad",
      "06c584ee47b4430ea69fbc85f95cd068",
      "f7f2119fb66e4a909c82443cac67ee82",
      "10d4ba4a298e4a4c9c4b63b9aed5538d",
      "82ef96e9e90a44f99b7e82c82bab46c7",
      "7f2b78f19bc14fa8ae967a8df3f9e83a",
      "0d83f9688ad9412da9322a3f2b74b226",
      "0e6e569ae6a049e5a027973e20991b12",
      "c8c1ad63079b47fdb8ed0bc1cf84a3b2",
      "caedc6d3e3554e2e8c7daaa163b0ab77",
      "a305775eda6a46078ea0024a32ea0244"
     ]
    },
    "id": "AkUeZlvd9xtO"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "The following cell runs a zero-shot transfer test from D2 (Slovak) to D5 (English). It checks how well model heads trained on D2 perform on D5 test clips, without any retraining on D5. The cell reads `manifest_all.csv` from D5, keeps only rows marked as `test`, verifies that all referenced audio files exist, and standardizes two fields used later in reporting: `task_group`, which separates vowel and other clips based on the `task` value, and `sex_norm`, which is set to M, F, or UNK.\n",
    "\n",
    "The classification threshold is not adjusted using D5 data. Instead, the cell loads a single fixed threshold from D2’s saved `monolingual_test_runs/summary_test.json`. It first looks for the newer key `threshold_selection.val_selected_threshold_mean_sd.mean`, and supports older summary formats as well. If no usable value is found, it falls back to extracting a stored threshold from the most recent D2 train and validation checkpoint files.\n",
    "\n",
    "The model setup matches the training configuration. It uses a frozen Wav2Vec2 backbone with two small task specific heads, one for vowel clips and one for other speech. The cell automatically finds the most recent D2 train and validation experiment folder that contains `best_heads.pt` for all three seeds, 1337, 2024, and 7777. For each seed, it loads the saved D2 heads, runs inference on the full D5 test set with progress bars and a short warm up, and produces Parkinson’s disease probabilities for each clip. It then computes AUROC as a threshold free measure, calculates threshold based metrics at the fixed threshold including accuracy, precision, sensitivity or recall, specificity, F1 score, MCC, and the Fisher exact test p value, and computes a fairness metric defined as ΔFNR = FNR(F) minus FNR(M). The false negative rate is calculated using only true Parkinson’s cases.\n",
    "\n",
    "For each seed, results are saved under a timestamped folder inside D5 at\n",
    "`<D5_OUT_ROOT>/zeroshot_transfer_runs/D2_to_D5_<timestamp>/run_D5_seedXXXX/`.\n",
    "Saved outputs include `metrics.json`, a ROC curve plot, an overall confusion matrix plot, and confusion matrices split by sex when enough data is available.\n",
    "\n",
    "After all three seeds complete, the cell combines results across seeds. It reports the mean AUROC with a 95 percent confidence interval using a t distribution with n equal to 3, computes the mean and standard deviation of the threshold based metrics, and summarizes fairness with mean and standard deviation for FNR by sex and for ΔFNR. A combined `summary_transfer.json` file and a `history_index.jsonl` file are written in the transfer run root. Finally, the Colab runtime is unassigned to stop the GPU session."
   ],
   "metadata": {
    "id": "l_IxlUSsyIzF"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Zero-Shot Transfer: D2 Heads on D5 Test (SK → EN)\n",
    "# Loads the most recent D2 trainval heads (3 seeds), applies a single fixed threshold from D2 monolingual testing,\n",
    "# and evaluates only the D5 test split with overall metrics and sex-based fairness (ΔFNR).\n",
    "\n",
    "# ============================================================\n",
    "# ZERO-SHOT TRANSFER TEST (CRASH-PROOF, WITH PROGRESS + STORED METRICS, SK to EN)\n",
    "# - TARGET: D5 TEST ONLY using <D5_OUT_ROOT>/manifests/manifest_all.csv (test split only)\n",
    "# - SOURCE: D2 trainval heads from MOST RECENT trainval experiment under:\n",
    "#     <D2_OUT_ROOT>/trainval_runs/exp_*/run_D2_seed{seed}/best_heads.pt\n",
    "# - FIXED THRESHOLD: read from D2 monolingual summary_test.json\n",
    "#     preferred key: threshold_selection.val_selected_threshold_mean_sd.mean\n",
    "#     (also supports older key variants + checkpoint fallback val_threshold)\n",
    "# - Evaluates 3 seeds separately (1337, 2024, 7777)\n",
    "# - Reports: mean Target Test AUROC ± 95% CI (t, n=3)\n",
    "# - Reports: threshold metrics at FIXED threshold (mean ± SD across seeds)\n",
    "# - Fairness (H3): ΔFNR = FNR(F) - FNR(M) at FIXED threshold (per seed + aggregated)\n",
    "# - Saves confusion charts overall + by sex at FIXED threshold\n",
    "# - Writes ONLY under:\n",
    "#     <D5_OUT_ROOT>/zeroshot_transfer_runs/D2_to_D5_<timestamp>/run_D5_seedXXXX/\n",
    "#   plus summary_transfer.json and history_index.jsonl in that transfer root\n",
    "# - Unassigns runtime at end (L4)\n",
    "# ============================================================\n",
    "\n",
    "import os, json, math, random, time, warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import soundfile as sf\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from transformers import Wav2Vec2Model\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, confusion_matrix, matthews_corrcoef\n",
    "from scipy.stats import fisher_exact\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Mount Drive (Colab)\n",
    "# Inputs: Colab runtime state\n",
    "# Output: /content/drive/MyDrive available for reading manifests and writing results\n",
    "# -------------------------\n",
    "if not os.path.isdir(\"/content/drive/MyDrive\"):\n",
    "    from google.colab import drive  # type: ignore\n",
    "    drive.mount(\"/content/drive\")\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Target and source roots\n",
    "# Inputs: D5 preprocessed root (target) and D2 preprocessed root (source)\n",
    "# Output: key file paths (D5 manifest_all.csv, D2 monolingual summary_test.json)\n",
    "# -------------------------\n",
    "D5_OUT_ROOT = \"/content/drive/MyDrive/AI_PD_Project/Datasets/D5-English (MDVR-KCL)/preprocessed_v2\"\n",
    "D5_MANIFEST_ALL = f\"{D5_OUT_ROOT}/manifests/manifest_all.csv\"\n",
    "\n",
    "D2_SOURCE_ROOT = \"/content/drive/MyDrive/AI_PD_Project/Datasets/D2-Slovak (EWA-DB)/EWA-DB/\"\n",
    "D2_OUT_ROOT = \"/content/drive/MyDrive/AI_PD_Project/Datasets/D2-Slovak (EWA-DB)/EWA-DB/preprocessed_v1\"\n",
    "D2_MONO_SUMMARY = Path(D2_OUT_ROOT) / \"monolingual_test_runs\" / \"summary_test.json\"\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Fixed runtime settings\n",
    "# Inputs: constants for evaluation\n",
    "# Output: consistent inference setup across seeds (sample rate, batch size, AMP, device)\n",
    "# -------------------------\n",
    "SEEDS          = [1337, 2024, 7777]\n",
    "BACKBONE_CKPT  = \"facebook/wav2vec2-base\"   # fallback if ckpt does not override\n",
    "SR_EXPECTED    = 16000\n",
    "TINY_THRESH    = 1e-4\n",
    "\n",
    "EFFECTIVE_BS   = 64\n",
    "PER_DEVICE_BS  = 16\n",
    "GRAD_ACCUM     = max(1, EFFECTIVE_BS // PER_DEVICE_BS)  # printed only\n",
    "\n",
    "NUM_WORKERS    = 0\n",
    "PIN_MEMORY     = False\n",
    "\n",
    "USE_AMP        = True\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "try:\n",
    "    torch.set_float32_matmul_precision(\"high\")\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", message=\"None of the inputs have requires_grad=True. Gradients will be None\")\n",
    "\n",
    "\n",
    "print(\"D5_OUT_ROOT (target):\", D5_OUT_ROOT)\n",
    "print(\"D5_MANIFEST_ALL:\", D5_MANIFEST_ALL)\n",
    "print(\"D2_SOURCE_ROOT (source root):\", D2_SOURCE_ROOT)\n",
    "print(\"D2_OUT_ROOT (source preprocessed):\", D2_OUT_ROOT)\n",
    "print(\"D2_MONO_SUMMARY:\", str(D2_MONO_SUMMARY))\n",
    "print(\"DEVICE:\", DEVICE)\n",
    "if DEVICE.type == \"cuda\":\n",
    "    print(\"GPU:\", torch.cuda.get_device_name(0))\n",
    "print(\"PER_DEVICE_BS:\", PER_DEVICE_BS, \"| EFFECTIVE_BS:\", PER_DEVICE_BS * GRAD_ACCUM)\n",
    "print(\"NUM_WORKERS:\", NUM_WORKERS, \"| PIN_MEMORY:\", PIN_MEMORY)\n",
    "print(\"USE_AMP:\", bool(USE_AMP and DEVICE.type == \"cuda\"))\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Fixed threshold loader (from D2 monolingual summary)\n",
    "# Inputs: D2 summary_test.json (new or older formats)\n",
    "# Outputs: single scalar FIXED_THR plus a string describing which key was used\n",
    "# -------------------------\n",
    "def _dig(d, path):\n",
    "    cur = d\n",
    "    for k in path:\n",
    "        if not isinstance(cur, dict) or k not in cur:\n",
    "            return None\n",
    "        cur = cur[k]\n",
    "    return cur\n",
    "\n",
    "def _as_finite_float(x):\n",
    "    try:\n",
    "        v = float(x)\n",
    "        return v if np.isfinite(v) else None\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def _load_fixed_thr_from_summary(summary: dict):\n",
    "    # Candidate paths are checked in priority order to support older runs.\n",
    "    candidates = [\n",
    "        ([\"threshold_selection\", \"val_selected_threshold_mean_sd\", \"mean\"], \"threshold_selection.val_selected_threshold_mean_sd.mean\"),\n",
    "        ([\"threshold_selection\", \"val_selected_threshold_by_seed\"], \"threshold_selection.val_selected_threshold_by_seed (mean)\"),\n",
    "\n",
    "        ([\"val_selected_threshold_mean_sd\", \"mean\"], \"val_selected_threshold_mean_sd.mean\"),\n",
    "        ([\"val_selected_threshold_by_seed\"], \"val_selected_threshold_by_seed (mean)\"),\n",
    "\n",
    "        ([\"val_opt_threshold_mean_sd\", \"mean\"], \"val_opt_threshold_mean_sd.mean\"),\n",
    "        ([\"val_opt_threshold_by_seed\"], \"val_opt_threshold_by_seed (mean)\"),\n",
    "\n",
    "        ([\"val_optimal_threshold\", \"mean_sd\", \"mean\"], \"val_optimal_threshold.mean_sd.mean\"),\n",
    "    ]\n",
    "\n",
    "    for path, key_name in candidates:\n",
    "        v = _dig(summary, path)\n",
    "        if v is None:\n",
    "            continue\n",
    "\n",
    "        fv = _as_finite_float(v)\n",
    "        if fv is not None:\n",
    "            return float(fv), key_name\n",
    "\n",
    "        # If the field is a dict (seed → threshold), average finite values.\n",
    "        if isinstance(v, dict) and len(v) > 0:\n",
    "            vals = []\n",
    "            for _, vv in v.items():\n",
    "                fvv = _as_finite_float(vv)\n",
    "                if fvv is not None:\n",
    "                    vals.append(fvv)\n",
    "            if len(vals) > 0:\n",
    "                return float(np.mean(vals)), key_name\n",
    "\n",
    "    return None, \"NOT_FOUND\"\n",
    "\n",
    "def _find_latest_exp_with_all_seeds(trainval_root: Path, seeds):\n",
    "    # Picks the most recently modified exp_* that contains best_heads.pt for all seeds.\n",
    "    exp_dirs = sorted([p for p in trainval_root.glob(\"exp_*\") if p.is_dir()],\n",
    "                      key=lambda p: p.stat().st_mtime, reverse=True)\n",
    "    if not exp_dirs:\n",
    "        raise FileNotFoundError(f\"No exp_* folders found under: {trainval_root}\")\n",
    "\n",
    "    def has_all(exp_path: Path):\n",
    "        for s in seeds:\n",
    "            p = exp_path / f\"run_D2_seed{s}\" / \"best_heads.pt\"\n",
    "            if not p.exists():\n",
    "                return False\n",
    "        return True\n",
    "\n",
    "    for ed in exp_dirs:\n",
    "        if has_all(ed):\n",
    "            return ed\n",
    "\n",
    "    raise FileNotFoundError(\n",
    "        f\"Could not find an exp_* folder that has all best_heads.pt files for seeds={seeds} under {trainval_root}\"\n",
    "    )\n",
    "\n",
    "def _fallback_thr_from_best_heads(d2_out_root: str, seeds):\n",
    "    # Last-resort fallback: read a threshold-like field directly from the checkpoints and average across seeds.\n",
    "    trainval_root = Path(d2_out_root) / \"trainval_runs\"\n",
    "    chosen_exp = _find_latest_exp_with_all_seeds(trainval_root, seeds)\n",
    "\n",
    "    thr_by_seed = {}\n",
    "    for s in seeds:\n",
    "        ckpt_path = chosen_exp / f\"run_D2_seed{s}\" / \"best_heads.pt\"\n",
    "        state = torch.load(str(ckpt_path), map_location=\"cpu\")\n",
    "        if not isinstance(state, dict):\n",
    "            continue\n",
    "\n",
    "        for k in [\"val_threshold\", \"val_thr\", \"threshold\", \"thr\"]:\n",
    "            if k in state:\n",
    "                fv = _as_finite_float(state[k])\n",
    "                if fv is not None:\n",
    "                    thr_by_seed[str(s)] = float(fv)\n",
    "                    break\n",
    "\n",
    "    if len(thr_by_seed) == 0:\n",
    "        return None, \"NO_THRESHOLD_IN_BEST_HEADS\", str(chosen_exp), thr_by_seed\n",
    "\n",
    "    return float(np.mean(list(thr_by_seed.values()))), \"best_heads.pt:val_threshold(mean)\", str(chosen_exp), thr_by_seed\n",
    "\n",
    "if not D2_MONO_SUMMARY.exists():\n",
    "    raise FileNotFoundError(\n",
    "        \"Missing D2 monolingual summary_test.json.\\n\"\n",
    "        f\"Expected: {str(D2_MONO_SUMMARY)}\\n\"\n",
    "        \"Run the D2 monolingual test cell first (the one that writes summary_test.json).\"\n",
    "    )\n",
    "\n",
    "with open(D2_MONO_SUMMARY, \"r\", encoding=\"utf-8\") as f:\n",
    "    d2_summary = json.load(f)\n",
    "\n",
    "FIXED_THR, THR_SRC_KEY = _load_fixed_thr_from_summary(d2_summary)\n",
    "\n",
    "if FIXED_THR is None:\n",
    "    FIXED_THR, THR_SRC_KEY, chosen_exp_fb, thr_by_seed_fb = _fallback_thr_from_best_heads(D2_OUT_ROOT, SEEDS)\n",
    "    if FIXED_THR is None:\n",
    "        raise RuntimeError(\n",
    "            \"Could not read a finite fixed threshold from D2 monolingual summary_test.json and fallback failed.\\n\"\n",
    "            f\"Summary path: {str(D2_MONO_SUMMARY)}\\n\"\n",
    "            f\"Fallback exp: {chosen_exp_fb}\\n\"\n",
    "            f\"thr_by_seed: {thr_by_seed_fb}\"\n",
    "        )\n",
    "\n",
    "print(f\"\\nFixed transfer threshold (from D2 monolingual mean VAL-selected): {FIXED_THR:.6f}\")\n",
    "print(\"Threshold source key:\", THR_SRC_KEY)\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Load target manifest (D5) and keep TEST only\n",
    "# Inputs: D5 manifest_all.csv\n",
    "# Outputs: test_df table with clip paths, labels, task, sex, and other light metadata\n",
    "# -------------------------\n",
    "if not os.path.exists(D5_MANIFEST_ALL):\n",
    "    raise FileNotFoundError(f\"Missing D5 manifest_all.csv: {D5_MANIFEST_ALL}\")\n",
    "\n",
    "m = pd.read_csv(D5_MANIFEST_ALL)\n",
    "\n",
    "req_cols = {\"split\", \"clip_path\", \"label_num\", \"task\", \"sex\", \"age\"}\n",
    "missing = [c for c in sorted(req_cols) if c not in m.columns]\n",
    "if missing:\n",
    "    raise ValueError(f\"D5 manifest missing required columns: {missing}. Found: {list(m.columns)}\")\n",
    "\n",
    "# TEST only (target reporting is test only)\n",
    "m = m[m[\"split\"].isin([\"test\"])].copy()\n",
    "if len(m) == 0:\n",
    "    raise RuntimeError(\"After filtering to split == 'test', D5 manifest has 0 rows.\")\n",
    "\n",
    "# dataset_id is used for naming output folders only.\n",
    "if \"dataset\" in m.columns and m[\"dataset\"].notna().any():\n",
    "    dataset_id = str(m[\"dataset\"].value_counts(dropna=True).idxmax())\n",
    "    m = m[m[\"dataset\"].astype(str) == dataset_id].copy()\n",
    "else:\n",
    "    dataset_id = \"D5\"\n",
    "\n",
    "keep_cols = [\"clip_path\", \"label_num\", \"task\", \"speaker_id\", \"sex\", \"age\", \"duration_sec\", \"split\"]\n",
    "for c in keep_cols:\n",
    "    if c not in m.columns:\n",
    "        m[c] = np.nan\n",
    "m = m[keep_cols].copy()\n",
    "\n",
    "test_df = m.reset_index(drop=True)\n",
    "\n",
    "print(f\"\\nTarget dataset inferred: {dataset_id}\")\n",
    "print(f\"TEST rows: {len(test_df)}\")\n",
    "print(\"TEST label counts:\", test_df[\"label_num\"].value_counts(dropna=False).to_dict())\n",
    "print(\"TEST sex counts (raw):\", test_df[\"sex\"].value_counts(dropna=False).to_dict())\n",
    "\n",
    "if len(test_df) == 0:\n",
    "    raise RuntimeError(\"Target TEST split has 0 rows.\")\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Fail-fast: target clip files must exist\n",
    "# Inputs: test_df clip_path values\n",
    "# Output: early error with a few missing examples (prevents long runs that fail later)\n",
    "# -------------------------\n",
    "def _fail_fast_missing_paths(df: pd.DataFrame, name: str):\n",
    "    missing_paths = []\n",
    "    for p in tqdm(df[\"clip_path\"].astype(str).tolist(), desc=f\"Check {name} clip_path exists\", dynamic_ncols=True):\n",
    "        if not os.path.exists(p):\n",
    "            missing_paths.append(p)\n",
    "            if len(missing_paths) >= 10:\n",
    "                break\n",
    "    if missing_paths:\n",
    "        raise FileNotFoundError(f\"{name}: missing clip_path(s). Examples: {missing_paths}\")\n",
    "\n",
    "_fail_fast_missing_paths(test_df, \"TEST\")\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Task routing (vowel vs other)\n",
    "# Inputs: manifest task values\n",
    "# Output: task_group column used to select the correct head at inference\n",
    "# -------------------------\n",
    "def _task_group(task_val) -> str:\n",
    "    return \"vowel\" if str(task_val) == \"vowl\" else \"other\"\n",
    "\n",
    "test_df[\"task_group\"] = test_df[\"task\"].apply(_task_group)\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Sex normalization for fairness and plots\n",
    "# Inputs: raw sex values from the manifest\n",
    "# Output: sex_norm in {M, F, UNK} for group metrics and per-sex confusion charts\n",
    "# -------------------------\n",
    "def normalize_sex(val) -> str:\n",
    "    if pd.isna(val):\n",
    "        return \"UNK\"\n",
    "    s = str(val).strip().lower()\n",
    "    if s in {\"m\", \"male\", \"man\", \"masc\", \"masculine\"}:\n",
    "        return \"M\"\n",
    "    if s in {\"f\", \"female\", \"woman\", \"fem\", \"feminine\"}:\n",
    "        return \"F\"\n",
    "    return \"UNK\"\n",
    "\n",
    "test_df[\"sex_norm\"] = test_df[\"sex\"].apply(normalize_sex)\n",
    "print(\"TEST sex counts (normalized):\", test_df[\"sex_norm\"].value_counts(dropna=False).to_dict())\n",
    "if (test_df[\"sex_norm\"] == \"UNK\").any():\n",
    "    print(\"NOTE: D5 v2 'sex' is blank or unmapped for some rows; they were counted as 'UNK' for fairness and sex charts.\")\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Audio dataset and collator (pads to batch max length)\n",
    "# Inputs: test_df with clip_path, label_num, task_group, sex_norm\n",
    "# Outputs: batched tensors (input_values, attention_mask, labels) plus task_group and sex_norm lists\n",
    "# -------------------------\n",
    "class AudioManifestDataset(Dataset):\n",
    "    def __init__(self, df: pd.DataFrame):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        row = self.df.iloc[idx]\n",
    "        clip_path = str(row[\"clip_path\"])\n",
    "        label = int(row[\"label_num\"])\n",
    "        task_group = str(row[\"task_group\"])\n",
    "        sex_norm = str(row[\"sex_norm\"])\n",
    "\n",
    "        if not os.path.exists(clip_path):\n",
    "            raise FileNotFoundError(f\"Missing clip_path: {clip_path}\")\n",
    "\n",
    "        y, sr = sf.read(clip_path, always_2d=False)\n",
    "        if isinstance(y, np.ndarray) and y.ndim > 1:\n",
    "            y = np.mean(y, axis=1)\n",
    "        y = np.asarray(y, dtype=np.float32)\n",
    "\n",
    "        if int(sr) != SR_EXPECTED:\n",
    "            raise ValueError(f\"Sample rate mismatch (expected {SR_EXPECTED}): {clip_path} has sr={sr}\")\n",
    "\n",
    "        # Attention mask rule:\n",
    "        # - vowel: mask trailing near-zero padding (prevents scoring silence)\n",
    "        # - other: keep full attention\n",
    "        attn = np.ones((len(y),), dtype=np.int64)\n",
    "        if task_group == \"vowel\":\n",
    "            k = -1\n",
    "            for j in range(len(y) - 1, -1, -1):\n",
    "                if abs(float(y[j])) > TINY_THRESH:\n",
    "                    k = j\n",
    "                    break\n",
    "            if k >= 0:\n",
    "                attn[:k+1] = 1\n",
    "                attn[k+1:] = 0\n",
    "            else:\n",
    "                attn[:] = 1\n",
    "        else:\n",
    "            attn[:] = 1\n",
    "\n",
    "        return {\n",
    "            \"input_values\": torch.from_numpy(y),\n",
    "            \"attention_mask\": torch.from_numpy(attn),\n",
    "            \"labels\": torch.tensor(label, dtype=torch.long),\n",
    "            \"task_group\": task_group,\n",
    "            \"sex_norm\": sex_norm,\n",
    "        }\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"Pads waveforms and attention masks to the longest clip in the batch.\"\"\"\n",
    "    max_len = int(max(b[\"input_values\"].numel() for b in batch))\n",
    "    input_vals, attn_masks, labels, task_groups, sex_norms = [], [], [], [], []\n",
    "    for b in batch:\n",
    "        x = b[\"input_values\"]\n",
    "        a = b[\"attention_mask\"]\n",
    "        pad = max_len - x.numel()\n",
    "        if pad > 0:\n",
    "            x = torch.cat([x, torch.zeros(pad, dtype=x.dtype)], dim=0)\n",
    "            a = torch.cat([a, torch.zeros(pad, dtype=a.dtype)], dim=0)\n",
    "        input_vals.append(x)\n",
    "        attn_masks.append(a)\n",
    "        labels.append(b[\"labels\"])\n",
    "        task_groups.append(b[\"task_group\"])\n",
    "        sex_norms.append(b[\"sex_norm\"])\n",
    "    return {\n",
    "        \"input_values\": torch.stack(input_vals, dim=0),\n",
    "        \"attention_mask\": torch.stack(attn_masks, dim=0),\n",
    "        \"labels\": torch.stack(labels, dim=0),\n",
    "        \"task_group\": task_groups,\n",
    "        \"sex_norm\": sex_norms,\n",
    "    }\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Transfer model (frozen backbone + two heads)\n",
    "# Inputs: backbone checkpoint name + dropout\n",
    "# Output: logits for class 0/1, routing by task_group (vowel vs other)\n",
    "# -------------------------\n",
    "class FrozenW2V2TwoHeadTest(nn.Module):\n",
    "    def __init__(self, ckpt: str, dropout_p: float):\n",
    "        super().__init__()\n",
    "        self.backbone = Wav2Vec2Model.from_pretrained(ckpt)\n",
    "        for p in self.backbone.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "        H = int(self.backbone.config.hidden_size)\n",
    "        self.head_vowel = nn.Sequential(\n",
    "            nn.LayerNorm(H),\n",
    "            nn.Dropout(float(dropout_p)),\n",
    "            nn.Linear(H, 2),\n",
    "        )\n",
    "        self.head_other = nn.Sequential(\n",
    "            nn.LayerNorm(H),\n",
    "            nn.Dropout(float(dropout_p)),\n",
    "            nn.Linear(H, 2),\n",
    "        )\n",
    "\n",
    "    def masked_mean_pool(self, last_hidden, attn_mask_samples):\n",
    "        feat_mask = self.backbone._get_feature_vector_attention_mask(last_hidden.shape[1], attn_mask_samples)\n",
    "        feat_mask = feat_mask.to(last_hidden.device).unsqueeze(-1).type_as(last_hidden)\n",
    "        masked = last_hidden * feat_mask\n",
    "        denom = feat_mask.sum(dim=1).clamp(min=1.0)\n",
    "        return masked.sum(dim=1) / denom\n",
    "\n",
    "    def _heads_fp32(self, x_any: torch.Tensor, head: nn.Module) -> torch.Tensor:\n",
    "        # Heads run in float32 to avoid dtype edge cases during AMP inference.\n",
    "        x = x_any.float()\n",
    "        if x.is_cuda:\n",
    "            with torch.amp.autocast(device_type=\"cuda\", enabled=False):\n",
    "                return head(x)\n",
    "        return head(x)\n",
    "\n",
    "    def forward_logits(self, input_values, attention_mask, task_group):\n",
    "        # Backbone is frozen and runs under no_grad.\n",
    "        with torch.no_grad():\n",
    "            out = self.backbone(input_values=input_values, attention_mask=attention_mask)\n",
    "            last_hidden = out.last_hidden_state\n",
    "\n",
    "        pooled = self.masked_mean_pool(last_hidden, attention_mask)\n",
    "\n",
    "        logits_v = self._heads_fp32(pooled, self.head_vowel)\n",
    "        logits_o = self._heads_fp32(pooled, self.head_other)\n",
    "\n",
    "        # Route each sample to the correct head.\n",
    "        is_vowel = torch.tensor([tg == \"vowel\" for tg in task_group], device=pooled.device, dtype=torch.bool)\n",
    "        logits = logits_o.clone()\n",
    "        if is_vowel.any():\n",
    "            logits[is_vowel] = logits_v[is_vowel]\n",
    "        return logits\n",
    "\n",
    "\n",
    "def load_heads_into_model(model: FrozenW2V2TwoHeadTest, best_heads_path: Path, state: dict):\n",
    "    \"\"\"\n",
    "    Loads head weights from best_heads.pt.\n",
    "    Inputs: checkpoint dict and expected head keys\n",
    "    Output: model with head_vowel and head_other weights populated\n",
    "    \"\"\"\n",
    "    if not best_heads_path.exists():\n",
    "        raise FileNotFoundError(f\"Missing best_heads.pt: {str(best_heads_path)}\")\n",
    "\n",
    "    if not isinstance(state, dict):\n",
    "        raise ValueError(f\"Unexpected best_heads.pt type: {type(state)}\")\n",
    "\n",
    "    if \"head_vowel\" in state and \"head_other\" in state and isinstance(state[\"head_vowel\"], dict) and isinstance(state[\"head_other\"], dict):\n",
    "        model.head_vowel.load_state_dict(state[\"head_vowel\"], strict=True)\n",
    "        model.head_other.load_state_dict(state[\"head_other\"], strict=True)\n",
    "        return model\n",
    "\n",
    "    for wrap_key in [\"state_dict\", \"model_state_dict\", \"model\"]:\n",
    "        if wrap_key in state and isinstance(state[wrap_key], dict):\n",
    "            sd = state[wrap_key]\n",
    "            _missing, _unexpected = model.load_state_dict(sd, strict=False)\n",
    "            has_head = any(k.startswith(\"head_vowel.\") or k.startswith(\"head_other.\") for k in sd.keys())\n",
    "            if not has_head:\n",
    "                raise KeyError(f\"Checkpoint wrapper '{wrap_key}' did not contain head keys. First keys: {list(sd.keys())[:25]}\")\n",
    "            return model\n",
    "\n",
    "    raise KeyError(\n",
    "        \"best_heads.pt did not match expected formats.\\n\"\n",
    "        f\"Top-level keys: {list(state.keys())[:50]}\"\n",
    "    )\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Metrics and plots\n",
    "# Inputs: y_true and y_score (probability of class 1)\n",
    "# Outputs: AUROC, threshold metrics, ROC and confusion PNGs\n",
    "# -------------------------\n",
    "def compute_auc(y_true, y_prob):\n",
    "    y_true = np.asarray(y_true, dtype=np.int64)\n",
    "    y_prob = np.asarray(y_prob, dtype=np.float64)\n",
    "    if len(np.unique(y_true)) < 2:\n",
    "        return float(\"nan\")\n",
    "    return float(roc_auc_score(y_true, y_prob))\n",
    "\n",
    "def compute_threshold_metrics(y_true, y_prob, thr):\n",
    "    # Computes confusion matrix and common classification metrics at a fixed threshold.\n",
    "    y_true = np.asarray(y_true, dtype=np.int64)\n",
    "    y_prob = np.asarray(y_prob, dtype=np.float64)\n",
    "    y_pred = (y_prob >= float(thr)).astype(np.int64)\n",
    "\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=[0, 1])\n",
    "    TN, FP, FN, TP = int(cm[0, 0]), int(cm[0, 1]), int(cm[1, 0]), int(cm[1, 1])\n",
    "\n",
    "    eps = 1e-12\n",
    "    acc = (TP + TN) / max(1, (TP + TN + FP + FN))\n",
    "    prec = TP / (TP + FP + eps)\n",
    "    rec = TP / (TP + FN + eps)     # sensitivity\n",
    "    f1 = 2 * prec * rec / (prec + rec + eps)\n",
    "    spec = TN / (TN + FP + eps)\n",
    "\n",
    "    try:\n",
    "        mcc = float(matthews_corrcoef(y_true, y_pred)) if len(np.unique(y_pred)) > 1 else float(\"nan\")\n",
    "    except Exception:\n",
    "        mcc = float(\"nan\")\n",
    "\n",
    "    try:\n",
    "        _, pval = fisher_exact([[TN, FP], [FN, TP]], alternative=\"two-sided\")\n",
    "        pval = float(pval)\n",
    "    except Exception:\n",
    "        pval = float(\"nan\")\n",
    "\n",
    "    return {\n",
    "        \"threshold\": float(thr),\n",
    "        \"confusion_matrix\": {\"TN\": TN, \"FP\": FP, \"FN\": FN, \"TP\": TP},\n",
    "        \"accuracy\": float(acc),\n",
    "        \"precision\": float(prec),\n",
    "        \"recall\": float(rec),\n",
    "        \"f1_score\": float(f1),\n",
    "        \"sensitivity\": float(rec),\n",
    "        \"specificity\": float(spec),\n",
    "        \"mcc\": float(mcc),\n",
    "        \"p_value_fisher_two_sided\": float(pval),\n",
    "    }\n",
    "\n",
    "def save_roc_curve_png(y_true, y_prob, out_png, title_suffix=\"Set\"):\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_prob)\n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr)\n",
    "    plt.plot([0, 1], [0, 1], linestyle=\"--\")\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(f\"ROC Curve ({title_suffix})\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_png, dpi=150)\n",
    "    plt.close()\n",
    "\n",
    "def save_confusion_png(y_true, y_prob, out_png, thr, title_suffix=\"Set\"):\n",
    "    y_true = np.asarray(y_true, dtype=np.int64)\n",
    "    y_prob = np.asarray(y_prob, dtype=np.float64)\n",
    "    y_pred = (y_prob >= float(thr)).astype(int)\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=[0, 1])\n",
    "    plt.figure()\n",
    "    plt.imshow(cm)\n",
    "    plt.xticks([0, 1], [\"Healthy(0)\", \"PD(1)\"])\n",
    "    plt.yticks([0, 1], [\"Healthy(0)\", \"PD(1)\"])\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"True\")\n",
    "    plt.title(f\"Confusion Matrix ({title_suffix}, thr={thr:.4f})\")\n",
    "    for (i, j), v in np.ndenumerate(cm):\n",
    "        plt.text(j, i, str(v), ha=\"center\", va=\"center\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_png, dpi=150)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Fairness: FNR by sex and ΔFNR (F − M)\n",
    "# Inputs: labels, scores, sex_norm, fixed threshold\n",
    "# Outputs: per-group FNR plus ΔFNR and |ΔFNR|\n",
    "# -------------------------\n",
    "def compute_fnr_by_group_signed(y_true, y_prob, groups, thr):\n",
    "    y_true = np.asarray(y_true, dtype=np.int64)\n",
    "    y_prob = np.asarray(y_prob, dtype=np.float64)\n",
    "    groups = np.asarray(list(groups), dtype=object)\n",
    "    y_pred = (y_prob >= float(thr)).astype(np.int64)\n",
    "\n",
    "    out = {}\n",
    "    for g in sorted(set(groups.tolist())):\n",
    "        mask_g = (groups == g)\n",
    "        if int(mask_g.sum()) == 0:\n",
    "            continue\n",
    "\n",
    "        pos_mask = mask_g & (y_true == 1)\n",
    "        n_pos = int(pos_mask.sum())\n",
    "        if n_pos == 0:\n",
    "            out[g] = {\"n_total\": int(mask_g.sum()), \"n_pos\": 0, \"tp\": 0, \"fn\": 0, \"fnr\": float(\"nan\")}\n",
    "            continue\n",
    "\n",
    "        tp = int(((y_pred == 1) & pos_mask).sum())\n",
    "        fn = int(((y_pred == 0) & pos_mask).sum())\n",
    "        fnr = float(fn / max(1, (fn + tp)))\n",
    "\n",
    "        out[g] = {\"n_total\": int(mask_g.sum()), \"n_pos\": int(n_pos), \"tp\": int(tp), \"fn\": int(fn), \"fnr\": float(fnr)}\n",
    "\n",
    "    fnr_m = out.get(\"M\", {}).get(\"fnr\", float(\"nan\"))\n",
    "    fnr_f = out.get(\"F\", {}).get(\"fnr\", float(\"nan\"))\n",
    "\n",
    "    if (not np.isnan(fnr_m)) and (not np.isnan(fnr_f)):\n",
    "        delta_signed = float(fnr_f - fnr_m)   # H3\n",
    "        delta_abs = float(abs(delta_signed))\n",
    "    else:\n",
    "        delta_signed = float(\"nan\")\n",
    "        delta_abs = float(\"nan\")\n",
    "\n",
    "    return out, delta_signed, delta_abs\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Confusion counts by sex group (for reporting)\n",
    "# Inputs: labels, scores, sex_norm, fixed threshold\n",
    "# Outputs: per-group confusion counts (TN/FP/FN/TP)\n",
    "# -------------------------\n",
    "def compute_confusion_counts(y_true, y_prob, thr):\n",
    "    y_true = np.asarray(y_true, dtype=np.int64)\n",
    "    y_prob = np.asarray(y_prob, dtype=np.float64)\n",
    "    y_pred = (y_prob >= float(thr)).astype(np.int64)\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=[0, 1])\n",
    "    TN, FP, FN, TP = int(cm[0, 0]), int(cm[0, 1]), int(cm[1, 0]), int(cm[1, 1])\n",
    "    return {\"TN\": TN, \"FP\": FP, \"FN\": FN, \"TP\": TP}\n",
    "\n",
    "def compute_confusion_by_group(y_true, y_prob, groups, thr):\n",
    "    y_true = np.asarray(y_true, dtype=np.int64)\n",
    "    y_prob = np.asarray(y_prob, dtype=np.float64)\n",
    "    groups = np.asarray(list(groups), dtype=object)\n",
    "    out = {}\n",
    "    for g in sorted(set(groups.tolist())):\n",
    "        mask = (groups == g)\n",
    "        if int(mask.sum()) == 0:\n",
    "            continue\n",
    "        out[g] = {\"n\": int(mask.sum()), \"confusion\": compute_confusion_counts(y_true[mask], y_prob[mask], thr=thr)}\n",
    "    return out\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Reproducible seeding (per seed run)\n",
    "# Inputs: seed integer\n",
    "# Output: deterministic torch/numpy/python RNG behavior\n",
    "# -------------------------\n",
    "def set_all_seeds(seed: int):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Choose the latest D2 trainval experiment that has all seeds\n",
    "# Inputs: D2 trainval_runs/exp_* folders\n",
    "# Output: chosen_exp used to load best_heads.pt for each seed\n",
    "# -------------------------\n",
    "D2_TRAINVAL_ROOT = Path(D2_OUT_ROOT) / \"trainval_runs\"\n",
    "if not D2_TRAINVAL_ROOT.exists():\n",
    "    raise FileNotFoundError(f\"Missing D2 trainval_runs folder: {str(D2_TRAINVAL_ROOT)}\")\n",
    "\n",
    "exp_dirs = sorted([p for p in D2_TRAINVAL_ROOT.glob(\"exp_*\") if p.is_dir()], key=lambda p: p.stat().st_mtime, reverse=True)\n",
    "if not exp_dirs:\n",
    "    raise FileNotFoundError(f\"No exp_* folders found under: {str(D2_TRAINVAL_ROOT)}\")\n",
    "\n",
    "def _has_all_seeds_d2(exp_path: Path, seeds: list):\n",
    "    for s in seeds:\n",
    "        p = exp_path / f\"run_D2_seed{s}\" / \"best_heads.pt\"\n",
    "        if not p.exists():\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "chosen_exp = None\n",
    "for ed in exp_dirs:\n",
    "    if _has_all_seeds_d2(ed, SEEDS):\n",
    "        chosen_exp = ed\n",
    "        break\n",
    "\n",
    "if chosen_exp is None:\n",
    "    sample = exp_dirs[0]\n",
    "    raise FileNotFoundError(\n",
    "        \"Could not find a D2 trainval experiment with all 3 best_heads.pt files.\\n\"\n",
    "        f\"Expected under: {str(D2_TRAINVAL_ROOT)}/exp_*/run_D2_seedXXXX/best_heads.pt\\n\"\n",
    "        f\"Most recent exp checked: {str(sample)}\"\n",
    "    )\n",
    "\n",
    "print(\"\\nUsing SOURCE (D2) Train+Val experiment folder:\")\n",
    "print(\" \", str(chosen_exp))\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Transfer output folder (one run root for all seeds)\n",
    "# Inputs: D5_OUT_ROOT + dataset_id + timestamp\n",
    "# Outputs: TRANSFER_ROOT with per-seed subfolders and summary files\n",
    "# -------------------------\n",
    "TRANSFER_ROOT = Path(D5_OUT_ROOT) / \"zeroshot_transfer_runs\" / f\"D2_to_{dataset_id}_{time.strftime('%Y%m%d_%H%M%S')}\"\n",
    "TRANSFER_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Target test DataLoader (D5 test only)\n",
    "# Inputs: test_df\n",
    "# Output: test_loader for batched inference\n",
    "# -------------------------\n",
    "test_ds = AudioManifestDataset(test_df)\n",
    "test_loader = DataLoader(\n",
    "    test_ds,\n",
    "    batch_size=PER_DEVICE_BS,\n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=PIN_MEMORY,\n",
    "    collate_fn=collate_fn,\n",
    ")\n",
    "\n",
    "def warmup_loader(loader, name: str, max_batches: int = 3):\n",
    "    \"\"\"Quick I/O warm-up to catch obvious read/shape issues early.\"\"\"\n",
    "    print(f\"\\nWarm-up: loading up to {max_batches} {name} batches...\")\n",
    "    t0 = time.time()\n",
    "    nb = len(loader)\n",
    "    wb = min(max_batches, nb)\n",
    "    if wb == 0:\n",
    "        raise RuntimeError(f\"{name} DataLoader has 0 batches. Check df length and PER_DEVICE_BS.\")\n",
    "    it = iter(loader)\n",
    "    for i in range(wb):\n",
    "        _ = next(it)\n",
    "        print(f\"  loaded {name} warmup batch {i+1}/{wb}\")\n",
    "    print(f\"Warm-up done in {time.time()-t0:.2f}s\")\n",
    "\n",
    "warmup_loader(test_loader, \"TEST\", max_batches=2)\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Inference loop (probability of PD)\n",
    "# Inputs: model, dataloader\n",
    "# Outputs: arrays of y_true, y_score, sex_norm for metrics and plots\n",
    "# -------------------------\n",
    "def run_inference(model: FrozenW2V2TwoHeadTest, loader: DataLoader, desc: str, use_amp: bool):\n",
    "    all_probs, all_true, all_sex = [], [], []\n",
    "    pbar = tqdm(loader, desc=desc, dynamic_ncols=True)\n",
    "    with torch.inference_mode():\n",
    "        for batch in pbar:\n",
    "            input_values = batch[\"input_values\"].to(DEVICE, non_blocking=False)\n",
    "            attention_mask = batch[\"attention_mask\"].to(DEVICE, non_blocking=False)\n",
    "            labels = batch[\"labels\"].to(DEVICE, non_blocking=False)\n",
    "            task_group = batch[\"task_group\"]\n",
    "            sex_norm = batch[\"sex_norm\"]\n",
    "\n",
    "            with torch.amp.autocast(device_type=\"cuda\", enabled=use_amp):\n",
    "                logits = model.forward_logits(input_values, attention_mask, task_group)\n",
    "\n",
    "            probs = torch.softmax(logits.float(), dim=-1)[:, 1].detach().cpu().numpy().astype(np.float64)\n",
    "            all_probs.extend(probs.tolist())\n",
    "            all_true.extend(labels.detach().cpu().numpy().astype(np.int64).tolist())\n",
    "            all_sex.extend(list(sex_norm))\n",
    "\n",
    "    return np.asarray(all_true, dtype=np.int64), np.asarray(all_probs, dtype=np.float64), np.asarray(all_sex, dtype=object)\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Single seed run: load D2 heads and evaluate D5 test at FIXED_THR\n",
    "# Inputs: seed, chosen_exp checkpoints, fixed threshold, D5 test_loader\n",
    "# Outputs: metrics.json + ROC/confusion PNGs in run_{dataset}_seedXXXX/\n",
    "# -------------------------\n",
    "def run_seed(seed: int):\n",
    "    set_all_seeds(seed)\n",
    "\n",
    "    run_dir = TRANSFER_ROOT / f\"run_{dataset_id}_seed{seed}\"\n",
    "    run_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    best_heads_path = chosen_exp / f\"run_D2_seed{seed}\" / \"best_heads.pt\"\n",
    "\n",
    "    print(f\"\\n[seed={seed}] Loading SOURCE heads from:\")\n",
    "    print(\" \", str(best_heads_path))\n",
    "    print(f\"[seed={seed}] Evaluating TARGET {dataset_id} TEST @ fixed_thr={FIXED_THR:.6f}\")\n",
    "\n",
    "    state = torch.load(str(best_heads_path), map_location=\"cpu\")\n",
    "\n",
    "    # Optionally inherit backbone/dropout values from the checkpoint for consistency.\n",
    "    ckpt_backbone = str(state.get(\"backbone_ckpt\", BACKBONE_CKPT)) if isinstance(state, dict) else BACKBONE_CKPT\n",
    "    ckpt_dropout  = float(state.get(\"dropout_p\", 0.10)) if isinstance(state, dict) else 0.10\n",
    "\n",
    "    model = FrozenW2V2TwoHeadTest(ckpt_backbone, dropout_p=ckpt_dropout).to(DEVICE)\n",
    "    model = load_heads_into_model(model, best_heads_path, state)\n",
    "    model.eval()\n",
    "\n",
    "    use_amp = bool(USE_AMP and DEVICE.type == \"cuda\")\n",
    "\n",
    "    # Target inference\n",
    "    yt, pt, st = run_inference(model, test_loader, desc=f\"[seed={seed}] Target TEST\", use_amp=use_amp)\n",
    "\n",
    "    test_auc = compute_auc(yt, pt)\n",
    "    thr_metrics = compute_threshold_metrics(yt, pt, thr=FIXED_THR)\n",
    "\n",
    "    # Fairness at the same fixed threshold used for all seeds.\n",
    "    fnr_by_sex, delta_f_minus_m, delta_abs = compute_fnr_by_group_signed(yt, pt, st, thr=FIXED_THR)\n",
    "\n",
    "    # Confusion by sex (uses all labels, not PD-only).\n",
    "    confusion_by_sex = compute_confusion_by_group(yt, pt, st, thr=FIXED_THR)\n",
    "\n",
    "    # Plots saved under the per-seed folder.\n",
    "    roc_png = run_dir / \"roc_curve.png\"\n",
    "    cm_png  = run_dir / \"confusion_matrix.png\"\n",
    "    save_roc_curve_png(yt, pt, str(roc_png), title_suffix=f\"Target Test (seed={seed})\")\n",
    "    save_confusion_png(yt, pt, str(cm_png), thr=FIXED_THR, title_suffix=f\"Target Test (seed={seed})\")\n",
    "\n",
    "    cm_m_png = None\n",
    "    cm_f_png = None\n",
    "\n",
    "    mask_m = (st == \"M\")\n",
    "    mask_f = (st == \"F\")\n",
    "\n",
    "    if int(mask_m.sum()) > 0:\n",
    "        cm_m_png = run_dir / \"confusion_matrix_M.png\"\n",
    "        save_confusion_png(yt[mask_m], pt[mask_m], str(cm_m_png), thr=FIXED_THR, title_suffix=f\"Target SEX=M (seed={seed})\")\n",
    "\n",
    "    if int(mask_f.sum()) > 0:\n",
    "        cm_f_png = run_dir / \"confusion_matrix_F.png\"\n",
    "        save_confusion_png(yt[mask_f], pt[mask_f], str(cm_f_png), thr=FIXED_THR, title_suffix=f\"Target SEX=F (seed={seed})\")\n",
    "\n",
    "    # metrics.json is the per-seed record used by the final summary.\n",
    "    metrics = {\n",
    "        \"source_dataset\": \"D2\",\n",
    "        \"target_dataset\": dataset_id,\n",
    "        \"seed\": int(seed),\n",
    "\n",
    "        \"fixed_threshold\": float(FIXED_THR),\n",
    "        \"fixed_threshold_source_key\": THR_SRC_KEY,\n",
    "        \"fixed_threshold_source_summary\": str(D2_MONO_SUMMARY),\n",
    "\n",
    "        \"n_test_target\": int(len(test_df)),\n",
    "        \"label_counts_test_target\": test_df[\"label_num\"].value_counts(dropna=False).to_dict(),\n",
    "        \"sex_counts_test_target_norm\": test_df[\"sex_norm\"].value_counts(dropna=False).to_dict(),\n",
    "\n",
    "        \"target_test_auroc\": float(test_auc),\n",
    "        \"threshold_metrics_target_test\": thr_metrics,\n",
    "\n",
    "        \"fairness_target_test\": {\n",
    "            \"definition\": \"H3: ΔFNR = FNR(F) - FNR(M), where FNR(sex)=FN/(FN+TP) computed on PD-only true labels at fixed_threshold.\",\n",
    "            \"fnr_by_sex_norm\": fnr_by_sex,\n",
    "            \"delta_fnr_F_minus_M\": float(delta_f_minus_m),\n",
    "            \"delta_fnr_abs\": float(delta_abs),\n",
    "            \"note\": \"If n_PD for a sex is 0, its FNR is NaN and ΔFNR is NaN.\",\n",
    "            \"sex_normalization_note\": \"sex_norm in {M,F,UNK}. Values not mapped to M/F counted as UNK.\",\n",
    "        },\n",
    "\n",
    "        \"confusion_by_sex_norm\": confusion_by_sex,\n",
    "\n",
    "        \"artifacts\": {\n",
    "            \"roc_curve_png\": str(roc_png),\n",
    "            \"confusion_matrix_png\": str(cm_png),\n",
    "            \"confusion_matrix_M_png\": str(cm_m_png) if cm_m_png is not None else None,\n",
    "            \"confusion_matrix_F_png\": str(cm_f_png) if cm_f_png is not None else None,\n",
    "        },\n",
    "\n",
    "        \"d5_out_root\": D5_OUT_ROOT,\n",
    "        \"d5_manifest_all\": D5_MANIFEST_ALL,\n",
    "\n",
    "        \"d2_out_root\": D2_OUT_ROOT,\n",
    "        \"d2_source_root\": D2_SOURCE_ROOT,\n",
    "        \"d2_trainval_experiment_used\": str(chosen_exp),\n",
    "        \"best_heads_path_used\": str(best_heads_path),\n",
    "\n",
    "        \"backbone_ckpt_used\": ckpt_backbone,\n",
    "        \"dropout_p_used\": float(ckpt_dropout),\n",
    "\n",
    "        \"timestamp\": time.strftime(\"%Y%m%d_%H%M%S\"),\n",
    "    }\n",
    "\n",
    "    metrics_path = run_dir / \"metrics.json\"\n",
    "    with open(metrics_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(metrics, f, indent=2)\n",
    "\n",
    "    def _fmt_fnr(g):\n",
    "        d = fnr_by_sex.get(g, None)\n",
    "        if d is None:\n",
    "            return \"n/a\"\n",
    "        return f\"fnr={d['fnr']:.6f} (n_PD={d['n_pos']}, fn={d['fn']}, tp={d['tp']})\"\n",
    "\n",
    "    print(f\"[seed={seed}] DONE | target_test_AUROC={test_auc:.6f} | fixed_thr={FIXED_THR:.6f}\")\n",
    "    print(f\"[seed={seed}] FAIRNESS (H3) @ fixed_thr={FIXED_THR:.6f}:\")\n",
    "    print(\"  M:\", _fmt_fnr(\"M\"))\n",
    "    print(\"  F:\", _fmt_fnr(\"F\"))\n",
    "    if \"UNK\" in fnr_by_sex:\n",
    "        print(\"  UNK:\", _fmt_fnr(\"UNK\"))\n",
    "    print(\"  ΔFNR (F-M):\", f\"{delta_f_minus_m:.6f}\" if not np.isnan(delta_f_minus_m) else \"nan\")\n",
    "    print(\"  |ΔFNR|:\", f\"{delta_abs:.6f}\" if not np.isnan(delta_abs) else \"nan\")\n",
    "\n",
    "    print(f\"[seed={seed}] WROTE:\")\n",
    "    print(\" \", str(metrics_path))\n",
    "    print(\" \", str(roc_png))\n",
    "    print(\" \", str(cm_png))\n",
    "    if cm_m_png is not None:\n",
    "        print(\" \", str(cm_m_png))\n",
    "    if cm_f_png is not None:\n",
    "        print(\" \", str(cm_f_png))\n",
    "\n",
    "    return {\n",
    "        \"seed\": int(seed),\n",
    "        \"target_test_auroc\": float(test_auc),\n",
    "        \"thr_metrics\": thr_metrics,\n",
    "        \"fnr_by_sex\": fnr_by_sex,\n",
    "        \"delta_signed\": float(delta_f_minus_m),\n",
    "        \"delta_abs\": float(delta_abs),\n",
    "        \"confusion_by_sex\": confusion_by_sex,\n",
    "        \"run_dir\": str(run_dir),\n",
    "    }\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Run all seeds and write transfer summary\n",
    "# Inputs: 3 per-seed results\n",
    "# Outputs: summary_transfer.json + history_index.jsonl under TRANSFER_ROOT\n",
    "# -------------------------\n",
    "seed_results = []\n",
    "for seed in SEEDS:\n",
    "    seed_results.append(run_seed(seed))\n",
    "\n",
    "aurocs = [r[\"target_test_auroc\"] for r in seed_results]\n",
    "t_crit = 4.302652729911275  # df=2, 95% CI\n",
    "n = len(aurocs)\n",
    "mean_auc = float(np.mean(aurocs))\n",
    "std_auc = float(np.std(aurocs, ddof=1)) if n > 1 else 0.0\n",
    "half_width = float(t_crit * (std_auc / math.sqrt(n))) if n > 1 else 0.0\n",
    "ci95 = [float(mean_auc - half_width), float(mean_auc + half_width)]\n",
    "\n",
    "def _mean_sd(vals):\n",
    "    vals = np.asarray(vals, dtype=np.float64)\n",
    "    return float(np.nanmean(vals)), float(np.nanstd(vals, ddof=1)) if np.sum(~np.isnan(vals)) > 1 else 0.0\n",
    "\n",
    "keys = [\"accuracy\",\"precision\",\"recall\",\"f1_score\",\"sensitivity\",\"specificity\",\"mcc\",\"p_value_fisher_two_sided\"]\n",
    "agg = {}\n",
    "for k in keys:\n",
    "    v = [float(r[\"thr_metrics\"].get(k, float(\"nan\"))) for r in seed_results]\n",
    "    mu, sd = _mean_sd(v)\n",
    "    agg[k] = {\n",
    "        \"mean\": mu,\n",
    "        \"sd\": sd,\n",
    "        \"values_by_seed\": {str(r[\"seed\"]): float(r[\"thr_metrics\"].get(k, float(\"nan\"))) for r in seed_results},\n",
    "    }\n",
    "\n",
    "cm_by_seed = {str(r[\"seed\"]): r[\"thr_metrics\"][\"confusion_matrix\"] for r in seed_results}\n",
    "\n",
    "fnr_m_vals, fnr_f_vals, n_pd_m_vals, n_pd_f_vals = [], [], [], []\n",
    "d_signed_vals, d_abs_vals = [], []\n",
    "\n",
    "fnr_by_seed = {}\n",
    "delta_signed_by_seed = {}\n",
    "delta_abs_by_seed = {}\n",
    "confusion_by_sex_by_seed = {}\n",
    "run_dirs = []\n",
    "\n",
    "for r in seed_results:\n",
    "    s = str(r[\"seed\"])\n",
    "    fnr_by_seed[s] = r[\"fnr_by_sex\"]\n",
    "    delta_signed_by_seed[s] = float(r[\"delta_signed\"])\n",
    "    delta_abs_by_seed[s] = float(r[\"delta_abs\"])\n",
    "    confusion_by_sex_by_seed[s] = r[\"confusion_by_sex\"]\n",
    "    run_dirs.append(r[\"run_dir\"])\n",
    "\n",
    "    d = r[\"fnr_by_sex\"]\n",
    "    fnr_m_vals.append(float(d.get(\"M\", {}).get(\"fnr\", float(\"nan\"))))\n",
    "    fnr_f_vals.append(float(d.get(\"F\", {}).get(\"fnr\", float(\"nan\"))))\n",
    "    n_pd_m_vals.append(float(d.get(\"M\", {}).get(\"n_pos\", float(\"nan\"))))\n",
    "    n_pd_f_vals.append(float(d.get(\"F\", {}).get(\"n_pos\", float(\"nan\"))))\n",
    "    d_signed_vals.append(float(r[\"delta_signed\"]))\n",
    "    d_abs_vals.append(float(r[\"delta_abs\"]))\n",
    "\n",
    "fnr_m_mean, fnr_m_sd = _mean_sd(fnr_m_vals)\n",
    "fnr_f_mean, fnr_f_sd = _mean_sd(fnr_f_vals)\n",
    "d_signed_mean, d_signed_sd = _mean_sd(d_signed_vals)\n",
    "d_abs_mean, d_abs_sd = _mean_sd(d_abs_vals)\n",
    "\n",
    "print(\"\\nTARGET Test AUROC by seed:\")\n",
    "for r in seed_results:\n",
    "    print(f\"  seed {r['seed']}: {r['target_test_auroc']:.6f}\")\n",
    "print(f\"\\nMean TARGET Test AUROC: {mean_auc:.6f}\")\n",
    "print(f\"95% CI (t, n=3): [{ci95[0]:.6f}, {ci95[1]:.6f}]\")\n",
    "\n",
    "print(f\"\\nThreshold metrics on TARGET TEST @ fixed_thr={FIXED_THR:.6f} (mean ± SD across seeds):\")\n",
    "for k in [\"accuracy\",\"precision\",\"sensitivity\",\"specificity\",\"f1_score\",\"mcc\"]:\n",
    "    mu = agg[k][\"mean\"]\n",
    "    sd = agg[k][\"sd\"]\n",
    "    print(f\"  {k}: {mu:.6f} ± {sd:.6f}\")\n",
    "print(\"  fisher_p_value_two_sided:\", f\"{agg['p_value_fisher_two_sided']['mean']:.6g} ± {agg['p_value_fisher_two_sided']['sd']:.6g}\")\n",
    "\n",
    "print(f\"\\nFAIRNESS (H3) on TARGET TEST @ fixed_thr={FIXED_THR:.6f} (mean ± SD):\")\n",
    "print(f\"  FNR_M: {fnr_m_mean:.6f} ± {fnr_m_sd:.6f}\")\n",
    "print(f\"  FNR_F: {fnr_f_mean:.6f} ± {fnr_f_sd:.6f}\")\n",
    "print(f\"  ΔFNR (F-M): {d_signed_mean:.6f} ± {d_signed_sd:.6f}\")\n",
    "print(f\"  |ΔFNR|: {d_abs_mean:.6f} ± {d_abs_sd:.6f}\")\n",
    "\n",
    "summary = {\n",
    "    \"source_dataset\": \"D2\",\n",
    "    \"target_dataset\": dataset_id,\n",
    "\n",
    "    \"d5_out_root\": D5_OUT_ROOT,\n",
    "    \"d5_manifest_all\": D5_MANIFEST_ALL,\n",
    "\n",
    "    \"d2_source_root\": D2_SOURCE_ROOT,\n",
    "    \"d2_out_root\": D2_OUT_ROOT,\n",
    "    \"d2_monolingual_summary\": str(D2_MONO_SUMMARY),\n",
    "    \"d2_trainval_experiment_used\": str(chosen_exp),\n",
    "\n",
    "    \"seeds\": SEEDS,\n",
    "\n",
    "    \"fixed_threshold\": float(FIXED_THR),\n",
    "    \"fixed_threshold_source_key\": THR_SRC_KEY,\n",
    "\n",
    "    \"n_test_target\": int(len(test_df)),\n",
    "    \"label_counts_test_target\": test_df[\"label_num\"].value_counts(dropna=False).to_dict(),\n",
    "    \"sex_counts_test_target_norm\": test_df[\"sex_norm\"].value_counts(dropna=False).to_dict(),\n",
    "\n",
    "    \"test_aurocs_by_seed\": {str(r[\"seed\"]): float(r[\"target_test_auroc\"]) for r in seed_results},\n",
    "    \"mean_test_auroc\": float(mean_auc),\n",
    "    \"t_crit_df2_95\": float(t_crit),\n",
    "    \"half_width_95_ci\": float(half_width),\n",
    "    \"ci95_test_auroc\": ci95,\n",
    "\n",
    "    \"threshold_metrics_mean_sd\": agg,\n",
    "    \"confusion_matrix_by_seed\": cm_by_seed,\n",
    "    \"run_dirs\": run_dirs,\n",
    "\n",
    "    \"fairness_test\": {\n",
    "        \"definition\": \"H3: ΔFNR = FNR(F) - FNR(M), where FNR(sex)=FN/(FN+TP) computed on PD-only true labels at fixed_threshold.\",\n",
    "        \"fnr_by_sex_norm_by_seed\": fnr_by_seed,\n",
    "        \"delta_fnr_F_minus_M_by_seed\": delta_signed_by_seed,\n",
    "        \"delta_fnr_abs_by_seed\": delta_abs_by_seed,\n",
    "        \"fnr_M_mean_sd\": {\"mean\": float(fnr_m_mean), \"sd\": float(fnr_m_sd), \"values_by_seed\": {str(s): float(v) for s, v in zip(SEEDS, fnr_m_vals)}},\n",
    "        \"fnr_F_mean_sd\": {\"mean\": float(fnr_f_mean), \"sd\": float(fnr_f_sd), \"values_by_seed\": {str(s): float(v) for s, v in zip(SEEDS, fnr_f_vals)}},\n",
    "        \"delta_fnr_F_minus_M_mean_sd\": {\"mean\": float(d_signed_mean), \"sd\": float(d_signed_sd), \"values_by_seed\": {str(s): float(v) for s, v in zip(SEEDS, d_signed_vals)}},\n",
    "        \"delta_fnr_abs_mean_sd\": {\"mean\": float(d_abs_mean), \"sd\": float(d_abs_sd), \"values_by_seed\": {str(s): float(v) for s, v in zip(SEEDS, d_abs_vals)}},\n",
    "        \"denominators_PD_by_seed\": {str(s): {\"n_PD_M\": float(n_pd_m_vals[i]), \"n_PD_F\": float(n_pd_f_vals[i])} for i, s in enumerate(SEEDS)},\n",
    "        \"sex_normalization_note\": \"sex_norm in {M,F,UNK}. Values not mapped to M/F counted as UNK. ΔFNR computed only when both M and F have defined FNR.\",\n",
    "    },\n",
    "\n",
    "    \"confusion_by_sex_norm_by_seed\": confusion_by_sex_by_seed,\n",
    "\n",
    "    \"timestamp\": time.strftime(\"%Y%m%d_%H%M%S\"),\n",
    "}\n",
    "\n",
    "summary_path = TRANSFER_ROOT / \"summary_transfer.json\"\n",
    "with open(summary_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "history_path = TRANSFER_ROOT / \"history_index.jsonl\"\n",
    "with open(history_path, \"a\", encoding=\"utf-8\") as f:\n",
    "    f.write(json.dumps(summary) + \"\\n\")\n",
    "\n",
    "print(\"\\nWROTE summary:\", str(summary_path))\n",
    "print(\"APPENDED history index:\", str(history_path))\n",
    "print(\"Open this folder to access artifacts:\", str(TRANSFER_ROOT))\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Stop runtime (Colab L4)\n",
    "# Inputs: Colab runtime availability\n",
    "# Output: runtime unassigned (or a message explaining manual stop)\n",
    "# -------------------------\n",
    "print(\"\\nAll done. Unassigning the runtime to stop the L4 instance...\")\n",
    "try:\n",
    "    from google.colab import runtime  # type: ignore\n",
    "    print(\"Calling runtime.unassign() now.\")\n",
    "    runtime.unassign()\n",
    "except Exception as e:\n",
    "    print(\"Could not unassign runtime automatically. You can stop the runtime manually in Colab.\")\n",
    "    print(\"Reason:\", repr(e))"
   ],
   "metadata": {
    "id": "PyJUEODrpA6R",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "e4355a020cdc4217a0e9f3fdfb79b4d8",
      "8b5186278462485684c4adee51d1bdb4",
      "9250112f60314014a33f86ebf1941e6f",
      "86cdecfb64154dffa733b3bb8a319fc0",
      "35de5fdcc3fa44c39d31e08364349156",
      "6ff8a70f135548bd84eeb6502ee23a23",
      "cd0dd68f144e4f5e9d6d1706e2bd0286",
      "c2e073f57c8c480da3ecf4f862ac8b15",
      "6a31f0879bcf4d91b98a6e70d1e0d79f",
      "e1b50d135a7447b681f5278ced82fce0",
      "66925f83cda3405d93181450b725ec77",
      "6d4e895e9dec4f68977ef56ec3516921",
      "6dd8f21350904c7daf636dce09317713",
      "f593631cd6204d85985c671ff502a74e",
      "961b1a46729247a982dd9ea181a0dba0",
      "82943807b8f0449881ec7bd3c291209d",
      "dd0b3b29e70c4cc69ec997e8c06a56e3",
      "6cb70e64743a42d789f6fd55a0f7a967",
      "7d0ccf2900bf4f25a7dd7c89a7e8ffaa",
      "731fed8c2f584b13aab186560a786c33",
      "ef5d4088a46a44f3bdc5d6fd47e44c5d",
      "ccd06a46c1d24b6492788c91c38c66e4",
      "6751b925f64141a8a1e8fb6d87f3f6ed",
      "b368d928337c4c33b62dcbcd5846c851",
      "7ec237eb70f34de5af0837ca14951496",
      "ce705ec70442416699ee4b776b0130de",
      "fa16cc8b3e0c41869816fffa8e09535b",
      "72277ec1b5cf4eca819f6f517a69c5fa",
      "afc2334dd3b7478ca566b1330f710ffe",
      "8c2348861dfc4bb1bd835f1d2a531adb",
      "68e5f4ec36694313ba5f49d50f97fa7e",
      "03aa3064dcce4fd9baa75e791d9ef120",
      "1b85c2845a844effb81fe1aa608c9769",
      "969ba5c649864bddbaa26f2e6387f574",
      "18fa2c1f381c4506b0d515893d74d587",
      "5c1aa6de071e48bb8cf3e3ea77d5e899",
      "0a157cb60b9c4325bad5ffe97828c07a",
      "4c3dfd4bf76f47c5b7d973c65c773880",
      "70006c9c700a47e7b8d72763386f8de2",
      "f99ac916713747a1ade10b0a647e0bc8",
      "ee5ac1ebb187469d9f659317d2482c7f",
      "504ea6aafb6a46df92e7732be923c88b",
      "54d1f39c5e0a45158f8bf26c73e391bd",
      "a4f5f8955ba84affa6a4ef177854b87c",
      "cd050abd1c544219a6ddda195eca3f2a",
      "438fdaeeb9254a67bee61ebf6bacc1bd",
      "7554f028b1f34ca3a8d6b2b05f015acc",
      "4c26b0e3ba7f464baf20be96081687a2",
      "0a662e5df3df483a852a35b5bb0f3874",
      "4d15fb8b09e145a4bccc932f955e7692",
      "d20c1353c2d840a3ad61188fbed242fd",
      "110c3415963a4cb58e6fc131c0a4f6e9",
      "094ca5b9e29945aa8d8384d7d685ea4b",
      "87f120b5dac34f6c9cf4ec10cd1be37e",
      "62dcd3d2807e4279ad9ae6283c3bac80",
      "9250849491544a5fb0146006f78f7acf",
      "0374e64e7d3543fb80a67f9b40b1c642",
      "629dec5a064a46bca6cea2fe43555a36",
      "c89b2957cf1f45d494a661813c73c050",
      "35900d4c80364f62bc352045cf7188a2",
      "bcb372447a16452ba06a9984c9b2c078",
      "e1d83bfd4bcc4689b66d390b99efd17b",
      "b2def5793bdb46f99dc7fcc5d1c620cf",
      "3561ce4a18194f4299b7ae4e3d9486b3",
      "378726377d9546688b1345de5f65d1e0",
      "ee09f76c209c4262966d51448d3ff4b2",
      "e246dc192e064f83844f95f6cb8a46e3",
      "167884e645a5447990f648e1eb1f0299",
      "def77bc6f74c491b93c32c1e4f7e3a3f",
      "2e30e058d6904a65ae0e1f53e9120a97",
      "4dc3206355c84fccbe460f539fa8a4fe",
      "01ee189b649b494f88682f77d32f5b25",
      "c67b767d34fb46938bc079b87f0eb8f4",
      "340557ea92d2431da8ae3dc2e6b9c2a7",
      "e4f58fba88244712a0fd5c1f4a036f40",
      "753356b719be4e928ce868d213981207",
      "e05fdab05b0142408b6665366e514ced"
     ]
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "#Multilingual Model D7 using D1, D4, D5 and D6 Datasets"
   ],
   "metadata": {
    "id": "VNvynaLntChs"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The following cell builds the merged multilingual dataset D7 by combining four already preprocessed datasets, D1, D4, D5v2, and D6, into one standard and self contained folder. It reads each source dataset’s `manifests/manifest_all.csv` as the source of truth, keeps the original train, validation, and test split assignments, and does not perform any re splitting. Every referenced audio clip is then copied into the D7 folder so the merged dataset can be used on its own without depending on the original dataset locations.\n",
    "\n",
    "Before any files are copied, the cell applies strict rules to avoid unintended changes. It locks the manifest schema and column order to a fixed header, checks that all required columns are present, converts any literal `\"NaN\"` strings into true missing values, and normalizes key fields so they are consistent across all source datasets. The numeric label is forced to 0 or 1, the string label is forced to Healthy or Parkinson and kept consistent with the numeric label, and sex is restricted to M, F, or blank, with unknown values left blank instead of creating new categories. To avoid speaker ID collisions between datasets, the cell rewrites `speaker_key_rel` by prefixing it with the source dataset code, such as `D1__`. The dataset column is set to `\"D7\"` so downstream code treats the merged data as a single dataset.\n",
    "\n",
    "The cell then performs a full preflight inventory check before copying any audio. For each row, it computes a deterministic destination filename based on the source dataset, label, speaker, task, and split counter. It verifies that every source audio file exists, checks for destination filename collisions, and confirms that if a destination file already exists it matches the expected file size. This allows safe re runs, since correct existing files are not overwritten or recopied. Even if the preflight step finds problems, the cell still writes standard log and configuration files first so issues are easy to debug and no run ends without metadata.\n",
    "\n",
    "If the preflight step finds any fatal issues, the cell stops immediately without copying files or rewriting the manifest. In that case, it still writes `logs/preprocess_warnings.csv` with structured errors and warnings, `logs/dataset_summary.json` with counts and status information, and `config/run_config.json` describing the inputs, schema rules, and preflight results.\n",
    "\n",
    "If the preflight step passes, the cell copies only the missing audio clips into `D7/clips/<split>/` using a no overwrite policy, and logs any copy errors. It then updates the merged table so `clip_path` points to the new D7 locations and `sample_id` matches the new filename stem. Final checks are run to confirm the schema order is correct, there are no literal `\"NaN\"` strings, labels are valid, and sex values are valid. The cell then writes all required outputs atomically to avoid partial files if the run is interrupted. These outputs include the `clips/` folder with audio organized by split, `manifests/manifest_all.csv` with the merged data and D7 local paths, `config/run_config.json` with final run details and copy statistics, `logs/preprocess_warnings.csv` with the full warning and error log, `logs/dataset_summary.json` with final counts and success status, and an empty `trainval_runs/` folder created for future training outputs."
   ],
   "metadata": {
    "id": "XNscaDy5x4H7"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Build D7 Multilingual Dataset (Merge Builder)\n",
    "# Merges multiple already-preprocessed datasets into one self-contained D7 folder by copying clips into D7,\n",
    "# keeping the original split assignments, and writing a single locked-schema manifest plus logs and config.\n",
    "\n",
    "# =========================\n",
    "# D7 MERGE-BUILDER (CRASH-PROOF, STANDARDS-SAFE) — D1 + D4 + D5v2 + D6 → D7\n",
    "#\n",
    "# Purpose\n",
    "# - Create a multilingual training root (D7) that mirrors the standard dataset layout\n",
    "#   and is fully self-sufficient (clips stored under D7).\n",
    "#\n",
    "# Guarantees / Invariants\n",
    "# - Manifest schema is locked to the attached sample header; 'source_dataset' is appended at the end.\n",
    "# - Missing values remain true NaN in-memory and are written as blanks in CSV (never the literal string \"NaN\").\n",
    "# - label_num ∈ {0, 1} and label_str ∈ {Healthy, Parkinson}, and both are mutually consistent.\n",
    "# - sex ∈ {M, F} or blank/NaN.\n",
    "# - speaker_key_rel is prefixed with the source dataset code to prevent collisions across merged datasets.\n",
    "# - clip files are copied into D7 with collision-proof filenames; manifest clip_path is rewritten to D7 paths.\n",
    "#\n",
    "# Outputs written (and only these)\n",
    "# - clips/\n",
    "# - manifests/manifest_all.csv\n",
    "# - config/run_config.json\n",
    "# - logs/preprocess_warnings.csv\n",
    "# - logs/dataset_summary.json\n",
    "# - trainval_runs/ (folder only; training code writes inside later)\n",
    "# =========================\n",
    "\n",
    "import os, json, re, shutil\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# -------------------------\n",
    "# Mount Drive (Colab)\n",
    "# Inputs: Colab runtime state\n",
    "# Output: /content/drive available when running in Colab\n",
    "# -------------------------\n",
    "# Safe no-op outside Colab or if Drive is already mounted.\n",
    "try:\n",
    "    from google.colab import drive  # type: ignore\n",
    "    if not os.path.exists(\"/content/drive\"):\n",
    "        drive.mount(\"/content/drive\")\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# -------------------------\n",
    "# Merge inputs and output root\n",
    "# Inputs: source manifest_all.csv files (already preprocessed)\n",
    "# Output: D7_OUT_ROOT where the merged clips and manifest will be written\n",
    "# -------------------------\n",
    "D7_OUT_ROOT = Path(\"/content/drive/MyDrive/AI_PD_Project/Datasets/D7-Multilingual (D1_D4_D5v2_D6)/preprocessed_v1\")\n",
    "\n",
    "# Each source points to its already-preprocessed manifest_all.csv.\n",
    "# Clips referenced by these manifests will be copied into D7.\n",
    "SOURCES = [\n",
    "    {\"src\": \"D1\", \"manifest_all\": \"/content/drive/MyDrive/AI_PD_Project/Datasets/D1-NeuroVoz-Castillan Spanish/preprocessed_v1/manifests/manifest_all.csv\"},\n",
    "    {\"src\": \"D4\", \"manifest_all\": \"/content/drive/MyDrive/AI_PD_Project/Datasets/D4-Italian (IPVS)/preprocessed_v1/manifests/manifest_all.csv\"},\n",
    "    {\"src\": \"D5\", \"manifest_all\": \"/content/drive/MyDrive/AI_PD_Project/Datasets/D5-English (MDVR-KCL)/preprocessed_v2/manifests/manifest_all.csv\"},  # D5 v2\n",
    "    {\"src\": \"D6\", \"manifest_all\": \"/content/drive/MyDrive/AI_PD_Project/Datasets/D6-Ah Sound (Figshare)/preprocessed_v1/manifests/manifest_all.csv\"},\n",
    "]\n",
    "\n",
    "# Keep only these splits from each source; no split changes are made.\n",
    "D7_INCLUDE_SPLITS = [\"train\", \"val\", \"test\"]\n",
    "\n",
    "# -------------------------\n",
    "# Locked manifest schema\n",
    "# Inputs: expected column list and order\n",
    "# Output: fail-fast protection against schema drift across sources\n",
    "# -------------------------\n",
    "# This list defines the canonical column set and the exact order expected in every source manifest.\n",
    "# A source manifest missing any of these columns triggers a fail-fast error to prevent schema drift.\n",
    "CANON_COLS = [\n",
    "    \"split\",\n",
    "    \"dataset\",\n",
    "    \"task\",\n",
    "    \"speaker_id\",\n",
    "    \"sample_id\",\n",
    "    \"label_str\",\n",
    "    \"label_num\",\n",
    "    \"age\",\n",
    "    \"sex\",\n",
    "    \"speaker_key_rel\",\n",
    "    \"clip_path\",\n",
    "    \"duration_sec\",\n",
    "    \"source_path\",\n",
    "    \"clip_start_sec\",\n",
    "    \"clip_end_sec\",\n",
    "    \"sr_hz\",\n",
    "    \"channels\",\n",
    "    \"clip_is_contiguous\",\n",
    "]\n",
    "# D7 adds provenance at the end only (no reordering of canonical fields).\n",
    "FINAL_COLS = CANON_COLS + [\"source_dataset\"]\n",
    "\n",
    "# Redundant minimum set used by core operations.\n",
    "REQ = [\"split\", \"clip_path\", \"label_num\", \"label_str\", \"sex\", \"dataset\", \"speaker_id\", \"task\", \"sample_id\", \"speaker_key_rel\"]\n",
    "\n",
    "# -------------------------\n",
    "# Create D7 folder structure\n",
    "# Inputs: D7_OUT_ROOT and split list\n",
    "# Output: standard subfolders (clips/manifests/config/logs/trainval_runs)\n",
    "# -------------------------\n",
    "# Only these folders are created or written. No other directories are touched.\n",
    "clips_dir = D7_OUT_ROOT / \"clips\"\n",
    "manif_dir = D7_OUT_ROOT / \"manifests\"\n",
    "cfg_dir   = D7_OUT_ROOT / \"config\"\n",
    "logs_dir  = D7_OUT_ROOT / \"logs\"\n",
    "trainval_runs_dir = D7_OUT_ROOT / \"trainval_runs\"\n",
    "\n",
    "for sp in D7_INCLUDE_SPLITS:\n",
    "    (clips_dir / sp).mkdir(parents=True, exist_ok=True)\n",
    "manif_dir.mkdir(parents=True, exist_ok=True)\n",
    "cfg_dir.mkdir(parents=True, exist_ok=True)\n",
    "logs_dir.mkdir(parents=True, exist_ok=True)\n",
    "trainval_runs_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# -------------------------\n",
    "# Utilities: guardrails, atomic writes, normalization\n",
    "# Inputs: simple values and dataframes\n",
    "# Outputs: consistent CSV writing and strict field standardization\n",
    "# -------------------------\n",
    "def require(cond, msg):\n",
    "    \"\"\"Stops immediately when an invariant is violated.\"\"\"\n",
    "    if not cond:\n",
    "        raise RuntimeError(msg)\n",
    "\n",
    "def atomic_write_text(dst: Path, text: str):\n",
    "    \"\"\"Writes through a temp file to avoid partial output on interruption.\"\"\"\n",
    "    tmp = dst.with_suffix(dst.suffix + \".tmp\")\n",
    "    with open(tmp, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(text)\n",
    "    os.replace(tmp, dst)\n",
    "\n",
    "def atomic_write_csv(dst: Path, df: pd.DataFrame):\n",
    "    \"\"\"Writes CSV with blanks for NaN values (never the literal string 'NaN').\"\"\"\n",
    "    tmp = dst.with_suffix(dst.suffix + \".tmp\")\n",
    "    df.to_csv(tmp, index=False, na_rep=\"\")\n",
    "    os.replace(tmp, dst)\n",
    "\n",
    "def normalize_sex_to_MF(x):\n",
    "    \"\"\"\n",
    "    Normalizes sex to 'M' or 'F' when possible.\n",
    "    Unknown values are set to NaN to keep categories stable.\n",
    "\n",
    "    Update: also handle numeric 1/0 encodings that may appear in D1 as floats (1.0/0.0).\n",
    "    \"\"\"\n",
    "    if pd.isna(x):\n",
    "        return np.nan\n",
    "\n",
    "    # Handle numeric 1/0 directly (covers int/float like 1, 0, 1.0, 0.0)\n",
    "    if isinstance(x, (int, np.integer)):\n",
    "        if int(x) == 1:\n",
    "            return \"M\"\n",
    "        if int(x) == 0:\n",
    "            return \"F\"\n",
    "        return np.nan\n",
    "    if isinstance(x, (float, np.floating)):\n",
    "        if np.isfinite(x):\n",
    "            if float(x) == 1.0:\n",
    "                return \"M\"\n",
    "            if float(x) == 0.0:\n",
    "                return \"F\"\n",
    "        return np.nan\n",
    "\n",
    "    s = str(x).strip().lower()\n",
    "    if s in [\"m\", \"male\", \"1\", \"1.0\"]:\n",
    "        return \"M\"\n",
    "    if s in [\"f\", \"female\", \"0\", \"0.0\"]:\n",
    "        return \"F\"\n",
    "    if s in [\"\", \"nan\", \"none\", \"unknown\", \"u\"]:\n",
    "        return np.nan\n",
    "    return np.nan  # do not invent categories\n",
    "\n",
    "def normalize_label_num(x):\n",
    "    \"\"\"\n",
    "    Normalizes label_num to integers 0/1.\n",
    "    Allows common encodings; unknown values become NaN (then fail-fast later).\n",
    "    \"\"\"\n",
    "    if pd.isna(x):\n",
    "        return np.nan\n",
    "    if isinstance(x, str):\n",
    "        t = x.strip().lower()\n",
    "        if t in [\"0\", \"healthy\", \"hc\"]:\n",
    "            return 0\n",
    "        if t in [\"1\", \"parkinson\", \"pd\"]:\n",
    "            return 1\n",
    "        return np.nan\n",
    "    try:\n",
    "        return int(x)\n",
    "    except Exception:\n",
    "        return np.nan\n",
    "\n",
    "def label_str_from_num(v):\n",
    "    \"\"\"Maps label_num to canonical strings: Healthy or Parkinson.\"\"\"\n",
    "    if pd.isna(v):\n",
    "        return np.nan\n",
    "    v = int(v)\n",
    "    if v == 0:\n",
    "        return \"Healthy\"\n",
    "    if v == 1:\n",
    "        return \"Parkinson\"\n",
    "    return np.nan\n",
    "\n",
    "def hc_pd_from_label_num(v):\n",
    "    \"\"\"Short label for filenames only (manifest keeps the canonical label_str).\"\"\"\n",
    "    return \"PD\" if int(v) == 1 else \"HC\"\n",
    "\n",
    "def safe_token(s, max_len=32, default=\"NA\"):\n",
    "    \"\"\"\n",
    "    Creates a filename-safe token from a value (speaker_id, task).\n",
    "    Used only for output clip filenames.\n",
    "    \"\"\"\n",
    "    if pd.isna(s):\n",
    "        return default\n",
    "    s = str(s).strip()\n",
    "    s = re.sub(r\"\\s+\", \"_\", s)\n",
    "    s = re.sub(r\"[^A-Za-z0-9_]+\", \"\", s)\n",
    "    return (s[:max_len] if s else default)\n",
    "\n",
    "# -------------------------\n",
    "# Warning log accumulator\n",
    "# Inputs: event details during preflight/copy\n",
    "# Output: logs/preprocess_warnings.csv at multiple points\n",
    "# -------------------------\n",
    "warnings_rows = []\n",
    "def add_warn(src, level, code, message, **extra):\n",
    "    \"\"\"Appends a structured log row for later CSV export.\"\"\"\n",
    "    row = {\n",
    "        \"ts\": datetime.utcnow().isoformat(),\n",
    "        \"src\": src,\n",
    "        \"level\": level,\n",
    "        \"code\": code,\n",
    "        \"message\": message,\n",
    "    }\n",
    "    row.update(extra)\n",
    "    warnings_rows.append(row)\n",
    "\n",
    "def count_by_level(rows):\n",
    "    \"\"\"Counts INFO/WARN/ERROR entries for quick summaries.\"\"\"\n",
    "    out = {\"ERROR\": 0, \"WARN\": 0, \"INFO\": 0}\n",
    "    for r in rows:\n",
    "        lvl = str(r.get(\"level\", \"INFO\")).upper()\n",
    "        if lvl not in out:\n",
    "            out[lvl] = 0\n",
    "        out[lvl] += 1\n",
    "    return out\n",
    "\n",
    "# -------------------------\n",
    "# Load, validate, normalize, and merge manifests\n",
    "# Inputs: each source manifest_all.csv\n",
    "# Output: merged dataframe with locked columns plus source_dataset\n",
    "# -------------------------\n",
    "print(\"D7_OUT_ROOT:\", str(D7_OUT_ROOT))\n",
    "print(\"Sources:\")\n",
    "for s in SOURCES:\n",
    "    print(f\"  - {s['src']}: {s['manifest_all']}\")\n",
    "\n",
    "parts = []\n",
    "source_counts = {}\n",
    "\n",
    "for s in SOURCES:\n",
    "    src = s[\"src\"]\n",
    "    mpath = Path(s[\"manifest_all\"])\n",
    "    require(mpath.exists(), f\"{src} manifest not found: {mpath}\")\n",
    "\n",
    "    df = pd.read_csv(mpath)\n",
    "\n",
    "    # Schema lock: all canonical columns must exist and keep the expected meaning.\n",
    "    missing = [c for c in CANON_COLS if c not in df.columns]\n",
    "    require(len(missing) == 0, f\"{src} manifest missing required columns (locked schema): {missing}\")\n",
    "\n",
    "    # Keep only requested splits; no split mutation is performed.\n",
    "    df = df[df[\"split\"].isin(D7_INCLUDE_SPLITS)].copy()\n",
    "\n",
    "    # Convert literal \"NaN\" strings to true NaN for common fields.\n",
    "    for col in [\"sex\", \"age\", \"duration_sec\", \"clip_start_sec\", \"clip_end_sec\", \"speaker_key_rel\", \"speaker_id\", \"task\", \"sample_id\"]:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].replace(\"NaN\", np.nan)\n",
    "\n",
    "    # Normalize labels and enforce 0/1 + Healthy/Parkinson consistency.\n",
    "    df[\"label_num\"] = df[\"label_num\"].map(normalize_label_num)\n",
    "    bad_num = sorted(set(df[\"label_num\"].dropna().unique()) - {0, 1})\n",
    "    require(len(bad_num) == 0, f\"{src} label_num has values outside {{0,1}} after normalization: {bad_num}\")\n",
    "\n",
    "    df[\"label_str\"] = df[\"label_num\"].map(label_str_from_num)\n",
    "    bad_str = sorted(set(df[\"label_str\"].dropna().unique()) - {\"Healthy\", \"Parkinson\"})\n",
    "    require(len(bad_str) == 0, f\"{src} label_str has values outside {{Healthy, Parkinson}} after normalization: {bad_str}\")\n",
    "\n",
    "    # Normalize sex to M/F/blank only.\n",
    "    df[\"sex\"] = df[\"sex\"].map(normalize_sex_to_MF)\n",
    "    bad_sex = sorted(set(df[\"sex\"].dropna().unique()) - {\"M\", \"F\"})\n",
    "    require(len(bad_sex) == 0, f\"{src} sex has unexpected values after normalization: {bad_sex}\")\n",
    "\n",
    "    # Force dataset = D7 so training code writes runs using the merged dataset id.\n",
    "    df[\"dataset\"] = \"D7\"\n",
    "\n",
    "    # Prefix speaker keys to avoid collisions across datasets.\n",
    "    df[\"speaker_key_rel\"] = df[\"speaker_key_rel\"].astype(str).map(lambda x: f\"{src}__{x}\")\n",
    "\n",
    "    # Add provenance at the end only.\n",
    "    df[\"source_dataset\"] = src\n",
    "\n",
    "    out = df[FINAL_COLS].copy()\n",
    "    parts.append(out)\n",
    "    source_counts[src] = int(len(out))\n",
    "\n",
    "require(len(parts) > 0, \"No rows loaded after split filtering.\")\n",
    "merged = pd.concat(parts, axis=0, ignore_index=True)\n",
    "\n",
    "print(\"\\nMerged rows (after split filter) by source:\", source_counts)\n",
    "print(\"Total merged rows:\", int(len(merged)))\n",
    "print(\"Merged split counts:\", merged[\"split\"].value_counts(dropna=False).to_dict())\n",
    "print(\"Merged label counts:\", merged[\"label_num\"].value_counts(dropna=False).to_dict())\n",
    "\n",
    "# -------------------------\n",
    "# Preflight: plan destination filenames and check files before copying\n",
    "# Inputs: merged rows (source clip_path values)\n",
    "# Outputs: dest_paths/dest_names aligned to merged rows, plus preflight stats and warnings\n",
    "# -------------------------\n",
    "# This stage checks everything before touching outputs:\n",
    "# - every source clip exists\n",
    "# - destination name collisions are detected\n",
    "# - if a destination file already exists, its size must match the source\n",
    "# - copy plan is recorded for reproducibility\n",
    "print(\"\\nPreflight: computing destination filenames and verifying source clips exist...\")\n",
    "\n",
    "split_counter_pre = {sp: 0 for sp in D7_INCLUDE_SPLITS}\n",
    "dest_paths = []\n",
    "dest_names = []\n",
    "src_paths = []\n",
    "\n",
    "seen_dest = set()\n",
    "\n",
    "n_src_missing = 0\n",
    "n_dest_collision = 0\n",
    "n_dest_exists_ok = 0\n",
    "n_dest_exists_mismatch = 0\n",
    "n_will_copy = 0\n",
    "\n",
    "pbar = tqdm(total=len(merged), desc=\"D7: Preflight\", dynamic_ncols=True)\n",
    "for i, row in merged.iterrows():\n",
    "    sp = str(row[\"split\"])\n",
    "    src = str(row[\"source_dataset\"])\n",
    "\n",
    "    require(sp in D7_INCLUDE_SPLITS, f\"Unexpected split value in merged table: {sp}\")\n",
    "\n",
    "    split_counter_pre[sp] += 1\n",
    "    gi = split_counter_pre[sp]\n",
    "\n",
    "    # Filename label uses PD/HC/UNK; manifest keeps the canonical label fields.\n",
    "    y = row[\"label_num\"]\n",
    "    if pd.isna(y):\n",
    "        add_warn(\"D7\", \"ERROR\", \"LABEL_NAN\", \"label_num is NaN after normalization\", row_index=int(i))\n",
    "        hc_pd = \"UNK\"\n",
    "    else:\n",
    "        hc_pd = hc_pd_from_label_num(y)\n",
    "\n",
    "    spkid_tok = safe_token(row[\"speaker_id\"], max_len=32, default=\"NA\")\n",
    "    task_tok  = safe_token(row[\"task\"], max_len=12, default=\"0\")\n",
    "\n",
    "    # Deterministic output naming: stable within a run given the merged row order.\n",
    "    out_name = f\"D7_{src}_{hc_pd}_{spkid_tok}_{task_tok}_{gi:06d}.wav\"\n",
    "    out_path = clips_dir / sp / out_name\n",
    "\n",
    "    # Detect deterministic collisions (should not happen unless inputs collide unexpectedly).\n",
    "    if str(out_path) in seen_dest:\n",
    "        n_dest_collision += 1\n",
    "        add_warn(\"D7\", \"ERROR\", \"NAME_COLLISION\", \"Destination filename collision during preflight\", out_path=str(out_path), row_index=int(i))\n",
    "    else:\n",
    "        seen_dest.add(str(out_path))\n",
    "\n",
    "    src_path = Path(str(row[\"clip_path\"]))\n",
    "    src_paths.append(str(src_path))\n",
    "    dest_paths.append(str(out_path))\n",
    "    dest_names.append(out_name)\n",
    "\n",
    "    # Missing source clips are fatal (recorded first, copying is blocked later).\n",
    "    if not src_path.exists():\n",
    "        n_src_missing += 1\n",
    "        if n_src_missing <= 50:\n",
    "            add_warn(\"D7\", \"ERROR\", \"MISSING_SOURCE_CLIP\", \"Source clip file missing\", clip_path=str(src_path), row_index=int(i))\n",
    "        pbar.update(1)\n",
    "        continue\n",
    "\n",
    "    # No overwrite policy: if a destination exists, it must match the source size.\n",
    "    if out_path.exists():\n",
    "        try:\n",
    "            if out_path.stat().st_size == src_path.stat().st_size:\n",
    "                n_dest_exists_ok += 1\n",
    "            else:\n",
    "                n_dest_exists_mismatch += 1\n",
    "                add_warn(\n",
    "                    \"D7\", \"ERROR\", \"DEST_EXISTS_SIZE_MISMATCH\",\n",
    "                    \"Destination exists but file size differs from source\",\n",
    "                    src_path=str(src_path), dest_path=str(out_path),\n",
    "                    src_size=int(src_path.stat().st_size), dest_size=int(out_path.stat().st_size),\n",
    "                    row_index=int(i)\n",
    "                )\n",
    "        except Exception as e:\n",
    "            n_dest_exists_mismatch += 1\n",
    "            add_warn(\n",
    "                \"D7\", \"ERROR\", \"DEST_EXISTS_STAT_ERROR\",\n",
    "                \"Failed to stat source/destination file during preflight\",\n",
    "                src_path=str(src_path), dest_path=str(out_path),\n",
    "                error=repr(e), row_index=int(i)\n",
    "            )\n",
    "    else:\n",
    "        n_will_copy += 1\n",
    "\n",
    "    pbar.update(1)\n",
    "\n",
    "pbar.close()\n",
    "\n",
    "# Preflight stats are written to logs before copying starts.\n",
    "preflight_stats = {\n",
    "    \"total_rows\": int(len(merged)),\n",
    "    \"split_counters\": {k: int(v) for k, v in split_counter_pre.items()},\n",
    "    \"n_src_missing\": int(n_src_missing),\n",
    "    \"n_dest_collision\": int(n_dest_collision),\n",
    "    \"n_dest_exists_ok\": int(n_dest_exists_ok),\n",
    "    \"n_dest_exists_mismatch\": int(n_dest_exists_mismatch),\n",
    "    \"n_will_copy\": int(n_will_copy),\n",
    "    \"warnings_by_level\": count_by_level(warnings_rows),\n",
    "}\n",
    "\n",
    "print(\"\\nPreflight summary:\")\n",
    "print(\"  Source clips missing:\", n_src_missing)\n",
    "print(\"  Destination collisions:\", n_dest_collision)\n",
    "print(\"  Destination exists (size OK):\", n_dest_exists_ok)\n",
    "print(\"  Destination exists (size mismatch/stat error):\", n_dest_exists_mismatch)\n",
    "print(\"  Will copy (missing in D7):\", n_will_copy)\n",
    "print(\"  Warnings by level:\", preflight_stats[\"warnings_by_level\"])\n",
    "\n",
    "# -------------------------\n",
    "# Write early logs and config (always)\n",
    "# Inputs: merged table and preflight stats\n",
    "# Outputs: run_config.json, dataset_summary.json, preprocess_warnings.csv\n",
    "# -------------------------\n",
    "# These files are written before copying so a failed run still leaves a usable trace.\n",
    "def summarize(df):\n",
    "    \"\"\"Small count summary used in dataset_summary.json.\"\"\"\n",
    "    out = {}\n",
    "    out[\"total_rows\"] = int(len(df))\n",
    "    out[\"by_split\"] = {sp: int((df[\"split\"] == sp).sum()) for sp in D7_INCLUDE_SPLITS}\n",
    "    out[\"by_source_dataset\"] = {sd: int((df[\"source_dataset\"] == sd).sum()) for sd in sorted(df[\"source_dataset\"].unique())}\n",
    "    out[\"by_source_dataset_by_split\"] = {}\n",
    "    for sd in sorted(df[\"source_dataset\"].unique()):\n",
    "        out[\"by_source_dataset_by_split\"][sd] = {sp: int(((df[\"source_dataset\"]==sd) & (df[\"split\"]==sp)).sum()) for sp in D7_INCLUDE_SPLITS}\n",
    "    out[\"label_counts_total\"] = {str(k): int(v) for k, v in df[\"label_num\"].value_counts(dropna=False).to_dict().items()}\n",
    "    return out\n",
    "\n",
    "run_config = {\n",
    "    \"dataset\": \"D7\",\n",
    "    \"mode\": \"merge_builder\",\n",
    "    \"created_utc\": datetime.utcnow().isoformat(),\n",
    "    \"d7_out_root\": str(D7_OUT_ROOT),\n",
    "    \"include_splits\": D7_INCLUDE_SPLITS,\n",
    "    \"source_manifests\": {s[\"src\"]: s[\"manifest_all\"] for s in SOURCES},\n",
    "    \"manifest_schema_locked_to_attachment\": {\n",
    "        \"canon_cols\": CANON_COLS,\n",
    "        \"final_cols\": FINAL_COLS,\n",
    "        \"label_str_values\": [\"Healthy\", \"Parkinson\"],\n",
    "        \"label_num_values\": [0, 1],\n",
    "        \"sex_values\": [\"M\", \"F\"],\n",
    "        \"notes\": [\n",
    "            \"speaker_key_rel prefixed with {SRC}__ to avoid collisions\",\n",
    "            \"clips copied into D7; clip_path rewritten to D7 paths\",\n",
    "            \"sample_id set to the output filename stem for determinism\",\n",
    "            \"existing destination clips are not overwritten (copy skipped when size matches)\",\n",
    "            \"missing values written as blanks (true NaN), never literal 'NaN'\",\n",
    "        ],\n",
    "    },\n",
    "    \"preflight\": preflight_stats,\n",
    "}\n",
    "\n",
    "early_summary = {\n",
    "    \"dataset\": \"D7\",\n",
    "    \"created_utc\": datetime.utcnow().isoformat(),\n",
    "    \"d7_out_root\": str(D7_OUT_ROOT),\n",
    "    \"include_splits\": D7_INCLUDE_SPLITS,\n",
    "    \"sources\": [{\"src\": s[\"src\"], \"manifest_all\": s[\"manifest_all\"]} for s in SOURCES],\n",
    "    \"source_row_counts_after_split_filter\": source_counts,\n",
    "    \"counts\": summarize(merged),\n",
    "    \"status\": \"PRECHECK_COMPLETE\",\n",
    "    \"preflight\": preflight_stats,\n",
    "}\n",
    "\n",
    "atomic_write_csv(logs_dir / \"preprocess_warnings.csv\", pd.DataFrame(warnings_rows))\n",
    "atomic_write_text(cfg_dir / \"run_config.json\", json.dumps(run_config, indent=2))\n",
    "atomic_write_text(logs_dir / \"dataset_summary.json\", json.dumps(early_summary, indent=2))\n",
    "\n",
    "# If any ERROR was logged, stop before copying to avoid partial outputs.\n",
    "fatal_pre = [w for w in warnings_rows if str(w.get(\"level\", \"\")).upper() == \"ERROR\"]\n",
    "if fatal_pre:\n",
    "    print(\"\\n❌ Preflight found ERROR conditions. No copying performed.\")\n",
    "    print(\"   Logs written:\")\n",
    "    print(\"   -\", str(logs_dir / \"preprocess_warnings.csv\"))\n",
    "    print(\"   -\", str(logs_dir / \"dataset_summary.json\"))\n",
    "    print(\"   -\", str(cfg_dir / \"run_config.json\"))\n",
    "    raise RuntimeError(f\"Preflight failed with {len(fatal_pre)} ERROR(s). See logs/preprocess_warnings.csv.\")\n",
    "\n",
    "# -------------------------\n",
    "# Copy stage: copy missing clips only (no overwrite)\n",
    "# Inputs: src_paths + dest_paths planned in preflight\n",
    "# Outputs: D7 clips/ populated; warnings updated if a copy fails\n",
    "# -------------------------\n",
    "print(\"\\nCopy stage: copying only missing clips (no overwrite when destination exists)...\")\n",
    "\n",
    "copied = 0\n",
    "skipped_exists = 0\n",
    "\n",
    "pbar = tqdm(total=len(merged), desc=\"D7: Copying clips\", dynamic_ncols=True)\n",
    "for i in range(len(merged)):\n",
    "    src_path = Path(src_paths[i])\n",
    "    out_path = Path(dest_paths[i])\n",
    "\n",
    "    # Defensive guard in case a source file disappears after preflight.\n",
    "    if not src_path.exists():\n",
    "        add_warn(\"D7\", \"ERROR\", \"SOURCE_CLIP_DISAPPEARED\", \"Source clip missing during copy stage\", clip_path=str(src_path), row_index=int(i))\n",
    "        pbar.update(1)\n",
    "        continue\n",
    "\n",
    "    if out_path.exists():\n",
    "        # Skip and keep the existing file (preflight already checked size match).\n",
    "        skipped_exists += 1\n",
    "        pbar.update(1)\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        shutil.copy2(src_path, out_path)\n",
    "        copied += 1\n",
    "    except Exception as e:\n",
    "        add_warn(\n",
    "            \"D7\", \"ERROR\", \"COPY_FAILED\", \"Failed to copy source clip into D7\",\n",
    "            src_path=str(src_path), dest_path=str(out_path), error=repr(e), row_index=int(i)\n",
    "        )\n",
    "\n",
    "    pbar.update(1)\n",
    "\n",
    "pbar.close()\n",
    "\n",
    "fatal_copy = [w for w in warnings_rows if str(w.get(\"level\", \"\")).upper() == \"ERROR\"]\n",
    "copy_stats = {\n",
    "    \"copied\": int(copied),\n",
    "    \"skipped_exists\": int(skipped_exists),\n",
    "    \"total_rows\": int(len(merged)),\n",
    "    \"warnings_by_level\": count_by_level(warnings_rows),\n",
    "}\n",
    "print(\"\\nCopy summary:\")\n",
    "print(\"  Copied:\", copied)\n",
    "print(\"  Skipped (already exists):\", skipped_exists)\n",
    "print(\"  Warnings by level:\", copy_stats[\"warnings_by_level\"])\n",
    "\n",
    "# Always refresh warnings CSV after the copy attempt.\n",
    "atomic_write_csv(logs_dir / \"preprocess_warnings.csv\", pd.DataFrame(warnings_rows))\n",
    "\n",
    "# Stop if any copy-stage errors occurred (manifest is not rewritten in that case).\n",
    "if fatal_copy:\n",
    "    fail_summary = dict(early_summary)\n",
    "    fail_summary[\"status\"] = \"FAILED_DURING_COPY\"\n",
    "    fail_summary[\"copy_stats\"] = copy_stats\n",
    "    atomic_write_text(logs_dir / \"dataset_summary.json\", json.dumps(fail_summary, indent=2))\n",
    "    print(\"\\n❌ Copy stage encountered ERROR conditions.\")\n",
    "    print(\"   Logs written:\")\n",
    "    print(\"   -\", str(logs_dir / \"preprocess_warnings.csv\"))\n",
    "    print(\"   -\", str(logs_dir / \"dataset_summary.json\"))\n",
    "    print(\"   -\", str(cfg_dir / \"run_config.json\"))\n",
    "    raise RuntimeError(f\"Copy failed with {len(fatal_copy)} ERROR(s). See logs/preprocess_warnings.csv.\")\n",
    "\n",
    "# Now that copies are confirmed, rewrite manifest fields to point to D7 copies.\n",
    "merged[\"clip_path\"] = dest_paths\n",
    "merged[\"sample_id\"] = [n.replace(\".wav\", \"\") for n in dest_names]\n",
    "\n",
    "# -------------------------\n",
    "# Final checks before writing manifest\n",
    "# Inputs: merged table after rewrite\n",
    "# Output: hard stop if any standard rule is violated\n",
    "# -------------------------\n",
    "require(list(merged.columns) == FINAL_COLS, \"Internal error: merged columns are not in the locked FINAL_COLS order.\")\n",
    "\n",
    "for c in FINAL_COLS:\n",
    "    if merged[c].dtype == object and (merged[c] == \"NaN\").any():\n",
    "        raise RuntimeError(f\"Found literal string 'NaN' in column '{c}'. This violates standardization rules.\")\n",
    "\n",
    "require(set(merged[\"label_num\"].dropna().unique()).issubset({0, 1}), \"label_num contains values outside {0,1}.\")\n",
    "require(set(merged[\"label_str\"].dropna().unique()).issubset({\"Healthy\", \"Parkinson\"}), \"label_str contains values outside {Healthy, Parkinson}.\")\n",
    "require(((merged[\"label_num\"] == 0) == (merged[\"label_str\"] == \"Healthy\")).all(), \"Mismatch between label_num==0 and label_str!=Healthy.\")\n",
    "require(((merged[\"label_num\"] == 1) == (merged[\"label_str\"] == \"Parkinson\")).all(), \"Mismatch between label_num==1 and label_str!=Parkinson.\")\n",
    "\n",
    "bad_sex = sorted(set(merged[\"sex\"].dropna().unique()) - {\"M\", \"F\"})\n",
    "require(len(bad_sex) == 0, f\"sex contains unexpected values: {bad_sex}\")\n",
    "\n",
    "# -------------------------\n",
    "# Write final artifacts\n",
    "# Inputs: merged table + updated logs/config\n",
    "# Outputs: manifests/manifest_all.csv and updated dataset_summary.json, preprocess_warnings.csv, run_config.json\n",
    "# -------------------------\n",
    "manifest_path = manif_dir / \"manifest_all.csv\"\n",
    "atomic_write_csv(manifest_path, merged)\n",
    "\n",
    "final_summary = dict(early_summary)\n",
    "final_summary[\"status\"] = \"SUCCESS\"\n",
    "final_summary[\"copy_stats\"] = copy_stats\n",
    "atomic_write_text(logs_dir / \"dataset_summary.json\", json.dumps(final_summary, indent=2))\n",
    "\n",
    "atomic_write_csv(logs_dir / \"preprocess_warnings.csv\", pd.DataFrame(warnings_rows))\n",
    "\n",
    "run_config[\"copy_stats\"] = copy_stats\n",
    "atomic_write_text(cfg_dir / \"run_config.json\", json.dumps(run_config, indent=2))\n",
    "\n",
    "print(\"\\n✅ D7 merge complete.\")\n",
    "print(f\"- Manifest: {manifest_path}\")\n",
    "print(f\"- Clips root: {clips_dir}\")\n",
    "print(f\"- Summary: {logs_dir / 'dataset_summary.json'}\")\n",
    "print(f\"- Warnings: {logs_dir / 'preprocess_warnings.csv'}\")\n",
    "print(f\"- Config: {cfg_dir / 'run_config.json'}\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 795,
     "referenced_widgets": [
      "d788941153844185989fb69cdcb784d1",
      "5e443ff8914f4045a7b7b7e0e884bafb",
      "97a1362f8eb44f189aa27171e344ef08",
      "12d13b7d2fa84f75911d963bb1697990",
      "0110f457296e43888d333db2fdb761e1",
      "89c49f70071140e282d9b25c15cf96ca",
      "76202622136f40cd883ecfcea12eaf39",
      "c37546acaf8f4ad793e7ab8b6a276613",
      "bee99eced3054d63ad8e540c81b6ccbc",
      "e398bdc8d30d40b08649a9b37d5d5d89",
      "0995f5c9476847bea5fb3cedce8b4f85",
      "8edcefe4154042b6907d9c1c5006235a",
      "87ac4775c7c045d1b1db3801b75e27e6",
      "8ed34a9d306f43b0befdb85d9354f2fd",
      "33068ca7fe4f45da82f6e0e3d155cc63",
      "eefee868a4a740d8a241af9a15771f9e",
      "0a62ab8805c24368b1914b2b5d0d9b25",
      "30356cb4755d45249d125c9c7398a8f4",
      "c1e970c521fb447a818e4fa67bc1da5f",
      "1408d4c0c18749fe97c75079fdecddf5",
      "71cfe21bee624dd0bf0540939bcd8e1e",
      "897c4aaab7f14cb1bff740fbe10e2ac0"
     ]
    },
    "id": "IWYIk3RoI97B"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "The following cell trains and validates the D7 multilingual model heads only. It does not run any test evaluation. The cell uses the merged D7 manifest file (`<DX_OUT_ROOT>/manifests/manifest_all.csv`), keeps only rows marked as `train` or `val`, checks that all required columns are present, and stops early with clear errors if the manifest is missing or if any audio clip paths do not exist. A new experiment folder is created under `<DX_OUT_ROOT>/trainval_runs/exp_<tag>_<timestamp>/` so each run is saved separately and can be traced later.\n",
    "\n",
    "The model uses a frozen Wav2Vec2 backbone, meaning the feature extractor is not updated during training, and two small trainable heads. One head is used for vowel clips and the other is used for all remaining speech clips. A simple rule assigns clips to the vowel head only when `task == \"vowl\"`, otherwise the clip is treated as other. For vowel clips, the data loader builds an attention mask that ignores trailing near silence, which helps prevent learning from padded silence. Audio is loaded from disk, checked to be 16 kHz, and padded within each batch so batches can be processed efficiently.\n",
    "\n",
    "Training is run separately for three random seeds, 1337, 2024, and 7777. For each seed, the model trains for up to a fixed maximum number of epochs and uses early stopping when validation AUROC stops improving for a set number of epochs. After every epoch, the cell runs validation and computes validation AUROC. When a new best AUROC is reached, the cell saves a snapshot of the trainable components only, which includes the two heads and their small LayerNorm and dropout blocks. At this best validation epoch, the cell also computes a validation optimal probability threshold using Youden’s J statistic, which selects the point on the ROC curve that best balances true positives and false positives. For clarity, the cell records threshold based metrics on validation at both a fixed threshold of 0.5 and at the Youden optimal threshold. These metrics include accuracy, precision, sensitivity or recall, specificity, F1 score, MCC, and the Fisher exact test p value. A ROC curve and confusion matrix plots are also saved.\n",
    "\n",
    "Outputs are written at two levels. For each seed, results are saved under `<EXP_ROOT>/run_<DATASET>_seed<SEED>/`, which contains `best_heads.pt`, `metrics.json`, and the generated plots. At the experiment level, a single `<EXP_ROOT>/summary_trainval.json` file is written. This summary includes the run settings, validation AUROC results across seeds with the mean and a 95 percent t based confidence interval, and the canonical threshold information used later by test cells, including `val_optimal_threshold.by_seed`, `val_optimal_threshold.mean_sd.mean`, and `val_optimal_threshold.mean_sd.sd`.\n",
    "\n",
    "The experiment summary is also appended to the running history file at `<DX_OUT_ROOT>/trainval_runs/history_index.jsonl` so previous experiments remain easy to find. At the end of the cell, the Colab runtime is unassigned to cleanly shut down the GPU instance."
   ],
   "metadata": {
    "id": "5VRbnwnOxtJr"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# D7 Train + Val (Frozen Backbone, Two Heads)\n",
    "# Trains only the small task heads (vowel vs other) on the D7 train split, validates on the D7 val split,\n",
    "# saves the best heads per seed, computes a VAL-optimal threshold from the best AUROC epoch, writes per-seed\n",
    "# artifacts and a single experiment summary, then unassigns the runtime.\n",
    "\n",
    "# =========================\n",
    "# Train + Val ONLY (CRASH-PROOF, WITH PROGRESS + HISTORY) - D7 (Multilingual)\n",
    "# - Frozen Wav2Vec2 backbone\n",
    "# - Two task heads + tiny LayerNorm + Dropout pre-head (trainable heads only)\n",
    "# - Uses ONLY: <DX_OUT_ROOT>/manifests/manifest_all.csv\n",
    "# - Writes ONLY under: <DX_OUT_ROOT>/trainval_runs/exp_<tag>_<timestamp>/\n",
    "# - Saves best-epoch plots + metrics per seed, plus per-experiment summary + history_index.jsonl\n",
    "# - Adds additional metrics: accuracy, precision, recall/sensitivity, specificity, F1, MCC, Fisher p-value\n",
    "# - Determines VAL-opt threshold via Youden J at the BEST-AUROC epoch (per seed)\n",
    "# - Stores thresholds ONLY as the canonical aggregate in summary_trainval.json:\n",
    "#     val_optimal_threshold.by_seed\n",
    "#     val_optimal_threshold.mean_sd.mean\n",
    "#     val_optimal_threshold.mean_sd.sd\n",
    "# - Ends by unassigning Colab runtime (L4) with messages\n",
    "# =========================\n",
    "\n",
    "import os, json, math, random, time, warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import soundfile as sf\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from transformers import Wav2Vec2Model\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, roc_curve,\n",
    "    confusion_matrix, accuracy_score,\n",
    "    precision_recall_fscore_support,\n",
    "    matthews_corrcoef\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# -------------------------\n",
    "# Guard against local name conflicts\n",
    "# Inputs: local filesystem under /content/\n",
    "# Output: stops early with a clear error if a local file would break imports\n",
    "# -------------------------\n",
    "if os.path.exists(\"/content/torch.py\") or os.path.exists(\"/content/torch/__init__.py\"):\n",
    "    raise RuntimeError(\"Found local /content/torch.py or /content/torch/ that shadows PyTorch. Rename/remove it and restart runtime.\")\n",
    "if os.path.exists(\"/content/transformers.py\") or os.path.exists(\"/content/transformers/__init__.py\"):\n",
    "    raise RuntimeError(\"Found local /content/transformers.py or /content/transformers/ that shadows Hugging Face Transformers. Rename/remove it and restart runtime.\")\n",
    "\n",
    "# -------------------------\n",
    "# Google Drive mount (safe if already mounted)\n",
    "# Inputs: Colab runtime state\n",
    "# Output: ensures /content/drive/MyDrive is available when running in Colab\n",
    "# -------------------------\n",
    "try:\n",
    "    from google.colab import drive  # type: ignore\n",
    "    if not os.path.isdir(\"/content/drive/MyDrive\"):\n",
    "        drive.mount(\"/content/drive\")\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# -------------------------\n",
    "# Resolve run root and manifest path\n",
    "# Inputs: DX_OUT_ROOT (if already set) or fallback D7 path\n",
    "# Output: DX_OUT_ROOT and MANIFEST_ALL used by the rest of the cell\n",
    "# -------------------------\n",
    "# Rule:\n",
    "# - Prefer runtime variable DX_OUT_ROOT if already defined (allows reuse across cells).\n",
    "# - Otherwise use the D7 fallback path below.\n",
    "D7_OUT_ROOT_FALLBACK = \"/content/drive/MyDrive/AI_PD_Project/Datasets/D7-Multilingual (D1_D4_D5v2_D6)/preprocessed_v1\"\n",
    "DX_OUT_ROOT = str(globals().get(\"DX_OUT_ROOT\", D7_OUT_ROOT_FALLBACK))\n",
    "MANIFEST_ALL = f\"{DX_OUT_ROOT}/manifests/manifest_all.csv\"\n",
    "\n",
    "# Keep for later cells that only need DX_OUT_ROOT\n",
    "globals()[\"DX_OUT_ROOT\"] = DX_OUT_ROOT\n",
    "\n",
    "# -------------------------\n",
    "# Experiment folder setup\n",
    "# Inputs: EXPERIMENT_TAG + timestamp\n",
    "# Output: a new exp_* folder that keeps older runs intact\n",
    "# -------------------------\n",
    "EXPERIMENT_TAG = \"frozen_LNDO\"\n",
    "RUN_STAMP = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "TRAINVAL_ROOT = Path(DX_OUT_ROOT) / \"trainval_runs\"\n",
    "EXP_ROOT = TRAINVAL_ROOT / f\"exp_{EXPERIMENT_TAG}_{RUN_STAMP}\"\n",
    "EXP_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# -------------------------\n",
    "# Training settings\n",
    "# Inputs: fixed hyperparameters and routing rules\n",
    "# Output: consistent train/val behavior across seeds\n",
    "# -------------------------\n",
    "MAX_EPOCHS     = 10\n",
    "EFFECTIVE_BS   = 64\n",
    "PER_DEVICE_BS  = 16\n",
    "GRAD_ACCUM     = max(1, EFFECTIVE_BS // PER_DEVICE_BS)\n",
    "\n",
    "LR             = 1e-3\n",
    "PATIENCE       = 2\n",
    "SEEDS          = [1337, 2024, 7777]\n",
    "\n",
    "BACKBONE_CKPT  = \"facebook/wav2vec2-base\"\n",
    "SR_EXPECTED    = 16000\n",
    "TINY_THRESH    = 1e-4\n",
    "\n",
    "DROPOUT_P      = 0.2\n",
    "\n",
    "NUM_WORKERS    = 0\n",
    "PIN_MEMORY     = False\n",
    "\n",
    "# Task routing rule: tasks with this exact value use the vowel head\n",
    "VOWEL_TASK_VALUE = \"vowl\"\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "try:\n",
    "    torch.set_float32_matmul_precision(\"high\")\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", message=\"None of the inputs have requires_grad=True. Gradients will be None\")\n",
    "warnings.filterwarnings(\"ignore\", message=\"Passing `gradient_checkpointing` to a config initialization is deprecated\")\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"huggingface_hub\")\n",
    "\n",
    "print(\"DX_OUT_ROOT:\", DX_OUT_ROOT)\n",
    "print(\"MANIFEST_ALL:\", MANIFEST_ALL)\n",
    "print(\"DEVICE:\", DEVICE)\n",
    "if DEVICE.type == \"cuda\":\n",
    "    print(\"GPU:\", torch.cuda.get_device_name(0))\n",
    "print(\"PER_DEVICE_BS:\", PER_DEVICE_BS, \"| GRAD_ACCUM:\", GRAD_ACCUM, \"| EFFECTIVE_BS:\", PER_DEVICE_BS * GRAD_ACCUM)\n",
    "print(\"NUM_WORKERS:\", NUM_WORKERS, \"| PIN_MEMORY:\", PIN_MEMORY)\n",
    "print(\"EXPERIMENT_TAG:\", EXPERIMENT_TAG, \"| RUN_STAMP:\", RUN_STAMP)\n",
    "\n",
    "# -------------------------\n",
    "# Load manifest and split into train/val tables\n",
    "# Inputs: manifest_all.csv\n",
    "# Output: train_df and val_df with required columns only\n",
    "# -------------------------\n",
    "if not os.path.exists(MANIFEST_ALL):\n",
    "    raise FileNotFoundError(\n",
    "        \"Missing manifest_all.csv at:\\n\"\n",
    "        f\"  {MANIFEST_ALL}\\n\"\n",
    "        \"Confirm that the D7 merge-builder wrote manifests/manifest_all.csv under DX_OUT_ROOT.\"\n",
    "    )\n",
    "\n",
    "m = pd.read_csv(MANIFEST_ALL)\n",
    "\n",
    "req_cols = {\"split\", \"clip_path\", \"label_num\", \"task\"}\n",
    "missing = [c for c in sorted(req_cols) if c not in m.columns]\n",
    "if missing:\n",
    "    raise ValueError(f\"Manifest missing required columns: {missing}. Found: {list(m.columns)}\")\n",
    "\n",
    "m = m[m[\"split\"].isin([\"train\", \"val\"])].copy()\n",
    "if len(m) == 0:\n",
    "    raise RuntimeError(\"After filtering to split in {train,val}, manifest has 0 rows.\")\n",
    "\n",
    "# Infer dataset_id for naming only\n",
    "if \"dataset\" in m.columns and m[\"dataset\"].notna().any():\n",
    "    dataset_id = str(m[\"dataset\"].astype(str).value_counts(dropna=True).idxmax())\n",
    "    m = m[m[\"dataset\"].astype(str) == dataset_id].copy()\n",
    "else:\n",
    "    dataset_id = \"DX\"\n",
    "\n",
    "keep_cols = [\"clip_path\", \"label_num\", \"task\", \"speaker_id\", \"duration_sec\", \"split\"]\n",
    "for c in keep_cols:\n",
    "    if c not in m.columns:\n",
    "        m[c] = np.nan\n",
    "m = m[keep_cols].copy()\n",
    "\n",
    "train_df = m[m[\"split\"] == \"train\"].copy().reset_index(drop=True)\n",
    "val_df   = m[m[\"split\"] == \"val\"].copy().reset_index(drop=True)\n",
    "\n",
    "print(f\"\\nDataset inferred: {dataset_id}\")\n",
    "print(f\"Train rows: {len(train_df)} | Val rows: {len(val_df)}\")\n",
    "print(\"Train label counts:\", train_df[\"label_num\"].value_counts(dropna=False).to_dict())\n",
    "print(\"Val label counts:\",   val_df[\"label_num\"].value_counts(dropna=False).to_dict())\n",
    "\n",
    "if len(train_df) == 0 or len(val_df) == 0:\n",
    "    raise RuntimeError(\"Train or Val split has 0 rows.\")\n",
    "\n",
    "# -------------------------\n",
    "# Fail fast if audio files are missing\n",
    "# Inputs: clip_path values in train_df/val_df\n",
    "# Output: stops early with a few missing examples instead of failing mid-epoch\n",
    "# -------------------------\n",
    "def _fail_fast_missing_paths(df: pd.DataFrame, name: str):\n",
    "    missing_paths = []\n",
    "    for p in tqdm(df[\"clip_path\"].astype(str).tolist(), desc=f\"Check {name} clip_path exists\", dynamic_ncols=True):\n",
    "        if not os.path.exists(p):\n",
    "            missing_paths.append(p)\n",
    "            if len(missing_paths) >= 10:\n",
    "                break\n",
    "    if missing_paths:\n",
    "        raise FileNotFoundError(f\"{name}: missing clip_path(s). Examples: {missing_paths}\")\n",
    "\n",
    "_fail_fast_missing_paths(train_df, \"TRAIN\")\n",
    "_fail_fast_missing_paths(val_df, \"VAL\")\n",
    "\n",
    "# -------------------------\n",
    "# Add task group used for head routing\n",
    "# Inputs: task column in the manifest\n",
    "# Output: task_group column with values \"vowel\" or \"other\"\n",
    "# -------------------------\n",
    "def _task_group(task_val) -> str:\n",
    "    return \"vowel\" if str(task_val) == VOWEL_TASK_VALUE else \"other\"\n",
    "\n",
    "train_df[\"task_group\"] = train_df[\"task\"].apply(_task_group)\n",
    "val_df[\"task_group\"]   = val_df[\"task\"].apply(_task_group)\n",
    "\n",
    "# -------------------------\n",
    "# Dataset and collator\n",
    "# Inputs: tables + audio files\n",
    "# Output: padded batches with attention masks and per-item task_group\n",
    "# -------------------------\n",
    "class AudioManifestDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Loads a clip and creates an attention mask.\n",
    "\n",
    "    Mask rule:\n",
    "    - vowel clips: mask trailing near-zeros to reduce learning from padded silence.\n",
    "    - other clips: keep full attention.\n",
    "    \"\"\"\n",
    "    def __init__(self, df: pd.DataFrame):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        row = self.df.iloc[idx]\n",
    "        clip_path = str(row[\"clip_path\"])\n",
    "        label = int(row[\"label_num\"])\n",
    "        task_group = str(row[\"task_group\"])\n",
    "\n",
    "        if not os.path.exists(clip_path):\n",
    "            raise FileNotFoundError(f\"Missing clip_path: {clip_path}\")\n",
    "\n",
    "        y, sr = sf.read(clip_path, always_2d=False)\n",
    "        if isinstance(y, np.ndarray) and y.ndim > 1:\n",
    "            y = np.mean(y, axis=1)\n",
    "        y = np.asarray(y, dtype=np.float32)\n",
    "\n",
    "        if int(sr) != SR_EXPECTED:\n",
    "            raise ValueError(f\"Sample rate mismatch (expected {SR_EXPECTED}): {clip_path} has sr={sr}\")\n",
    "\n",
    "        attn = np.ones((len(y),), dtype=np.int64)\n",
    "\n",
    "        if task_group == \"vowel\":\n",
    "            # Find last non-trivial sample and mask the trailing near-zero region\n",
    "            k = -1\n",
    "            for j in range(len(y) - 1, -1, -1):\n",
    "                if abs(float(y[j])) > TINY_THRESH:\n",
    "                    k = j\n",
    "                    break\n",
    "            if k >= 0:\n",
    "                attn[:k+1] = 1\n",
    "                attn[k+1:] = 0\n",
    "            else:\n",
    "                attn[:] = 1\n",
    "        else:\n",
    "            attn[:] = 1\n",
    "\n",
    "        return {\n",
    "            \"input_values\": torch.from_numpy(y),                 # float32 [T]\n",
    "            \"attention_mask\": torch.from_numpy(attn),            # int64   [T]\n",
    "            \"labels\": torch.tensor(label, dtype=torch.long),     # int64   []\n",
    "            \"task_group\": task_group,                            # str\n",
    "        }\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"Pads waveforms and masks to the longest clip in the batch.\"\"\"\n",
    "    max_len = int(max(b[\"input_values\"].numel() for b in batch))\n",
    "    input_vals, attn_masks, labels, task_groups = [], [], [], []\n",
    "    for b in batch:\n",
    "        x = b[\"input_values\"]\n",
    "        a = b[\"attention_mask\"]\n",
    "        pad = max_len - x.numel()\n",
    "        if pad > 0:\n",
    "            x = torch.cat([x, torch.zeros(pad, dtype=x.dtype)], dim=0)\n",
    "            a = torch.cat([a, torch.zeros(pad, dtype=a.dtype)], dim=0)\n",
    "        input_vals.append(x)\n",
    "        attn_masks.append(a)\n",
    "        labels.append(b[\"labels\"])\n",
    "        task_groups.append(b[\"task_group\"])\n",
    "    return {\n",
    "        \"input_values\": torch.stack(input_vals, dim=0),      # [B,T]\n",
    "        \"attention_mask\": torch.stack(attn_masks, dim=0),    # [B,T]\n",
    "        \"labels\": torch.stack(labels, dim=0),                # [B]\n",
    "        \"task_group\": task_groups,                           # list[str]\n",
    "    }\n",
    "\n",
    "# -------------------------\n",
    "# Model definition\n",
    "# Inputs: backbone checkpoint + dropout rate\n",
    "# Output: frozen backbone model with two small, trainable heads\n",
    "# -------------------------\n",
    "class Wav2Vec2TwoHeadClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    Frozen backbone with two task heads.\n",
    "\n",
    "    Trainable parts:\n",
    "    - LayerNorm+Dropout blocks (pre_vowel, pre_other)\n",
    "    - Linear heads (head_vowel, head_other)\n",
    "    \"\"\"\n",
    "    def __init__(self, ckpt: str, dropout_p: float):\n",
    "        super().__init__()\n",
    "        self.backbone = Wav2Vec2Model.from_pretrained(\n",
    "            ckpt,\n",
    "            use_safetensors=True,\n",
    "            local_files_only=False\n",
    "        )\n",
    "        for p in self.backbone.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "        H = int(self.backbone.config.hidden_size)\n",
    "        self.pre_vowel = nn.Sequential(nn.LayerNorm(H), nn.Dropout(float(dropout_p)))\n",
    "        self.pre_other = nn.Sequential(nn.LayerNorm(H), nn.Dropout(float(dropout_p)))\n",
    "\n",
    "        self.head_vowel = nn.Linear(H, 2)\n",
    "        self.head_other = nn.Linear(H, 2)\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    def masked_mean_pool(self, last_hidden, attn_mask_samples):\n",
    "        # Convert sample-level mask to feature-level mask, then average only valid frames\n",
    "        feat_mask = self.backbone._get_feature_vector_attention_mask(last_hidden.shape[1], attn_mask_samples)\n",
    "        feat_mask = feat_mask.to(last_hidden.device).unsqueeze(-1).type_as(last_hidden)\n",
    "        masked = last_hidden * feat_mask\n",
    "        denom = feat_mask.sum(dim=1).clamp(min=1.0)\n",
    "        return masked.sum(dim=1) / denom\n",
    "\n",
    "    def forward(self, input_values, attention_mask, labels, task_group):\n",
    "        # Backbone stays frozen and runs without gradients\n",
    "        with torch.no_grad():\n",
    "            out = self.backbone(input_values=input_values, attention_mask=attention_mask)\n",
    "            last_hidden = out.last_hidden_state  # [B,T',H]\n",
    "\n",
    "        pooled = self.masked_mean_pool(last_hidden, attention_mask).float()  # [B,H]\n",
    "\n",
    "        z_v = self.pre_vowel(pooled)\n",
    "        z_o = self.pre_other(pooled)\n",
    "\n",
    "        logits_v = self.head_vowel(z_v)  # 2-class logits\n",
    "        logits_o = self.head_other(z_o)\n",
    "\n",
    "        # Route each item to the matching head\n",
    "        is_vowel = torch.tensor([tg == \"vowel\" for tg in task_group], device=pooled.device, dtype=torch.bool)\n",
    "        logits = logits_o.clone()\n",
    "        if is_vowel.any():\n",
    "            logits[is_vowel] = logits_v[is_vowel]\n",
    "\n",
    "        loss = self.loss_fn(logits, labels)\n",
    "        return loss, logits\n",
    "\n",
    "# -------------------------\n",
    "# Metric utilities\n",
    "# Inputs: true labels, probabilities, threshold\n",
    "# Output: AUROC, threshold metrics, plots, and simple mean±SD summaries\n",
    "# -------------------------\n",
    "def compute_auc(y_true, y_prob):\n",
    "    y_true = np.asarray(y_true, dtype=np.int64)\n",
    "    y_prob = np.asarray(y_prob, dtype=np.float64)\n",
    "    if len(np.unique(y_true)) < 2:\n",
    "        return float(\"nan\")\n",
    "    return float(roc_auc_score(y_true, y_prob))\n",
    "\n",
    "def compute_threshold_metrics(y_true, y_prob, thr=0.5):\n",
    "    \"\"\"\n",
    "    Converts probabilities into hard predictions using thr, then computes common metrics.\n",
    "    \"\"\"\n",
    "    y_true = np.asarray(y_true, dtype=np.int64)\n",
    "    y_prob = np.asarray(y_prob, dtype=np.float64)\n",
    "    y_pred = (y_prob >= float(thr)).astype(np.int64)\n",
    "\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=[0, 1])\n",
    "    tn, fp, fn, tp = (cm.ravel().tolist() if cm.size == 4 else [0, 0, 0, 0])\n",
    "\n",
    "    acc = float(accuracy_score(y_true, y_pred))\n",
    "    prec, rec, f1, _ = precision_recall_fscore_support(y_true, y_pred, average=\"binary\", zero_division=0)\n",
    "    mcc = float(matthews_corrcoef(y_true, y_pred)) if len(np.unique(y_true)) > 1 else float(\"nan\")\n",
    "\n",
    "    sensitivity = float(rec)\n",
    "    specificity = float(tn / (tn + fp)) if (tn + fp) > 0 else float(\"nan\")\n",
    "\n",
    "    p_value = float(\"nan\")\n",
    "    try:\n",
    "        from scipy.stats import fisher_exact  # type: ignore\n",
    "        _, p_value = fisher_exact([[tn, fp], [fn, tp]], alternative=\"two-sided\")\n",
    "        p_value = float(p_value)\n",
    "    except Exception:\n",
    "        p_value = float(\"nan\")\n",
    "\n",
    "    return {\n",
    "        \"threshold\": float(thr),\n",
    "        \"tn\": int(tn), \"fp\": int(fp), \"fn\": int(fn), \"tp\": int(tp),\n",
    "        \"accuracy\": float(acc),\n",
    "        \"precision\": float(prec),\n",
    "        \"recall\": float(rec),\n",
    "        \"f1\": float(f1),\n",
    "        \"sensitivity\": float(sensitivity),\n",
    "        \"specificity\": float(specificity),\n",
    "        \"mcc\": float(mcc),\n",
    "        \"p_value_fisher\": float(p_value),\n",
    "    }\n",
    "\n",
    "def compute_youden_j_threshold(y_true, y_prob):\n",
    "    \"\"\"\n",
    "    Finds the ROC threshold that maximizes Youden J (TPR - FPR).\n",
    "    Returns an optimal threshold plus a small details dict.\n",
    "    \"\"\"\n",
    "    y_true = np.asarray(y_true, dtype=np.int64)\n",
    "    y_prob = np.asarray(y_prob, dtype=np.float64)\n",
    "    if len(np.unique(y_true)) < 2:\n",
    "        return float(\"nan\"), {\"youden_j\": float(\"nan\"), \"tpr\": float(\"nan\"), \"fpr\": float(\"nan\")}\n",
    "    fpr, tpr, thr = roc_curve(y_true, y_prob)\n",
    "    j = tpr - fpr\n",
    "    idx = int(np.argmax(j))\n",
    "    return float(thr[idx]), {\"youden_j\": float(j[idx]), \"tpr\": float(tpr[idx]), \"fpr\": float(fpr[idx])}\n",
    "\n",
    "def save_roc_curve_png(y_true, y_prob, out_png):\n",
    "    # Writes a simple ROC plot for the best epoch (VAL only)\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_prob)\n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr)\n",
    "    plt.plot([0, 1], [0, 1], linestyle=\"--\")\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(\"ROC Curve (Val)\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_png, dpi=150)\n",
    "    plt.close()\n",
    "\n",
    "def save_confusion_png(y_true, y_prob, out_png, thr=0.5):\n",
    "    # Writes a confusion matrix image for a chosen threshold (VAL only)\n",
    "    y_true = np.asarray(y_true, dtype=np.int64)\n",
    "    y_prob = np.asarray(y_prob, dtype=np.float64)\n",
    "    y_pred = (y_prob >= float(thr)).astype(int)\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=[0, 1])\n",
    "\n",
    "    plt.figure()\n",
    "    plt.imshow(cm)\n",
    "    plt.xticks([0, 1], [\"Healthy(0)\", \"PD(1)\"])\n",
    "    plt.yticks([0, 1], [\"Healthy(0)\", \"PD(1)\"])\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"True\")\n",
    "    plt.title(f\"Confusion Matrix (Val, thr={thr:.4f})\")\n",
    "    for (i, j), v in np.ndenumerate(cm):\n",
    "        plt.text(j, i, str(v), ha=\"center\", va=\"center\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_png, dpi=150)\n",
    "    plt.close()\n",
    "\n",
    "def mean_sd(vals):\n",
    "    vals = np.asarray(vals, dtype=np.float64)\n",
    "    mu = float(np.nanmean(vals)) if np.any(~np.isnan(vals)) else float(\"nan\")\n",
    "    sd = float(np.nanstd(vals, ddof=1)) if np.sum(~np.isnan(vals)) > 1 else 0.0\n",
    "    return mu, sd\n",
    "\n",
    "# -------------------------\n",
    "# Seed control\n",
    "# Inputs: seed integer\n",
    "# Output: repeatable training and validation behavior for that seed\n",
    "# -------------------------\n",
    "def set_all_seeds(seed: int):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# -------------------------\n",
    "# One seed run: train, validate, keep best epoch\n",
    "# Inputs: train_df, val_df, and fixed settings\n",
    "# Outputs: best_heads.pt + plots + metrics.json under run_<dataset>_seedXXXX\n",
    "# -------------------------\n",
    "def run_trainval_once(seed: int):\n",
    "    set_all_seeds(seed)\n",
    "\n",
    "    run_dir = EXP_ROOT / f\"run_{dataset_id}_seed{seed}\"\n",
    "    run_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    train_ds = AudioManifestDataset(train_df)\n",
    "    val_ds   = AudioManifestDataset(val_df)\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_ds,\n",
    "        batch_size=PER_DEVICE_BS,\n",
    "        shuffle=True,\n",
    "        num_workers=NUM_WORKERS,\n",
    "        pin_memory=PIN_MEMORY,\n",
    "        collate_fn=collate_fn\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_ds,\n",
    "        batch_size=PER_DEVICE_BS,\n",
    "        shuffle=False,\n",
    "        num_workers=NUM_WORKERS,\n",
    "        pin_memory=PIN_MEMORY,\n",
    "        collate_fn=collate_fn\n",
    "    )\n",
    "\n",
    "    # Quick warm-up: catches basic data issues before the first full epoch\n",
    "    print(f\"\\n[seed={seed}] Warm-up: loading 3 train batches...\")\n",
    "    t0 = time.time()\n",
    "    it = iter(train_loader)\n",
    "    for i in range(3):\n",
    "        _ = next(it)\n",
    "        print(f\"  loaded warmup batch {i+1}/3\")\n",
    "    print(f\"[seed={seed}] Warm-up done in {time.time()-t0:.2f}s\")\n",
    "\n",
    "    model = Wav2Vec2TwoHeadClassifier(BACKBONE_CKPT, dropout_p=DROPOUT_P).to(DEVICE)\n",
    "\n",
    "    # Only train the small head blocks (backbone remains frozen)\n",
    "    trainable_params = (\n",
    "        list(model.pre_vowel.parameters()) + list(model.pre_other.parameters()) +\n",
    "        list(model.head_vowel.parameters()) + list(model.head_other.parameters())\n",
    "    )\n",
    "    opt = torch.optim.Adam(trainable_params, lr=LR)\n",
    "\n",
    "    best_auc = -1.0\n",
    "    best_epoch = -1\n",
    "    no_improve = 0\n",
    "\n",
    "    # Snapshot of the best epoch (used for saving heads and computing thresholds)\n",
    "    best_state = None\n",
    "    best_val_probs = None\n",
    "    best_val_true = None\n",
    "\n",
    "    best_thr_youden = float(\"nan\")\n",
    "    best_thr_youden_details = None\n",
    "    best_val_metrics_thr05 = None\n",
    "    best_val_metrics_thr_opt = None\n",
    "\n",
    "    for epoch in range(1, MAX_EPOCHS + 1):\n",
    "        model.train()\n",
    "        train_losses = []\n",
    "        opt.zero_grad(set_to_none=True)\n",
    "\n",
    "        pbar = tqdm(train_loader, desc=f\"[seed={seed}] Train epoch {epoch}\", dynamic_ncols=True)\n",
    "        step = 0\n",
    "        for batch in pbar:\n",
    "            step += 1\n",
    "            input_values = batch[\"input_values\"].to(DEVICE, non_blocking=False)\n",
    "            attention_mask = batch[\"attention_mask\"].to(DEVICE, non_blocking=False)\n",
    "            labels = batch[\"labels\"].to(DEVICE, non_blocking=False)\n",
    "            task_group = batch[\"task_group\"]\n",
    "\n",
    "            loss, _ = model(input_values, attention_mask, labels, task_group)\n",
    "            loss = loss / GRAD_ACCUM\n",
    "            loss.backward()\n",
    "\n",
    "            train_losses.append(float(loss.detach().cpu().item()) * GRAD_ACCUM)\n",
    "\n",
    "            if (step % GRAD_ACCUM) == 0:\n",
    "                opt.step()\n",
    "                opt.zero_grad(set_to_none=True)\n",
    "\n",
    "        # Final optimizer step if the epoch ends mid-accumulation\n",
    "        if (step % GRAD_ACCUM) != 0:\n",
    "            opt.step()\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "\n",
    "        avg_train_loss = float(np.mean(train_losses)) if train_losses else float(\"nan\")\n",
    "\n",
    "        # Validation: collect probabilities for the full VAL set\n",
    "        model.eval()\n",
    "        all_probs, all_true = [], []\n",
    "        vpbar = tqdm(val_loader, desc=f\"[seed={seed}] Val epoch {epoch}\", dynamic_ncols=True)\n",
    "        with torch.inference_mode():\n",
    "            for batch in vpbar:\n",
    "                input_values = batch[\"input_values\"].to(DEVICE, non_blocking=False)\n",
    "                attention_mask = batch[\"attention_mask\"].to(DEVICE, non_blocking=False)\n",
    "                labels = batch[\"labels\"].to(DEVICE, non_blocking=False)\n",
    "                task_group = batch[\"task_group\"]\n",
    "\n",
    "                _, logits = model(input_values, attention_mask, labels, task_group)\n",
    "                probs = torch.softmax(logits.float(), dim=-1)[:, 1].detach().cpu().numpy().astype(np.float64)\n",
    "                all_probs.extend(probs.tolist())\n",
    "                all_true.extend(labels.detach().cpu().numpy().astype(np.int64).tolist())\n",
    "\n",
    "        val_auc = compute_auc(all_true, all_probs)\n",
    "        print(f\"seed={seed} | epoch {epoch:02d}/{MAX_EPOCHS} | train_loss={avg_train_loss:.5f} | val_AUROC={val_auc:.5f}\")\n",
    "\n",
    "        # Keep the epoch with the best VAL AUROC\n",
    "        improved = (not math.isnan(val_auc)) and (val_auc > best_auc + 1e-12)\n",
    "        if improved:\n",
    "            best_auc = float(val_auc)\n",
    "            best_epoch = int(epoch)\n",
    "            no_improve = 0\n",
    "\n",
    "            # Save only the trainable parts (for later test-only reload)\n",
    "            best_state = {\n",
    "                \"pre_vowel\": {k: v.detach().cpu().clone() for k, v in model.pre_vowel.state_dict().items()},\n",
    "                \"pre_other\": {k: v.detach().cpu().clone() for k, v in model.pre_other.state_dict().items()},\n",
    "                \"head_vowel\": {k: v.detach().cpu().clone() for k, v in model.head_vowel.state_dict().items()},\n",
    "                \"head_other\": {k: v.detach().cpu().clone() for k, v in model.head_other.state_dict().items()},\n",
    "            }\n",
    "\n",
    "            best_val_probs = list(all_probs)\n",
    "            best_val_true  = list(all_true)\n",
    "\n",
    "            # Reference metrics at thr=0.5\n",
    "            best_val_metrics_thr05 = compute_threshold_metrics(best_val_true, best_val_probs, thr=0.5)\n",
    "\n",
    "            # VAL-opt threshold is computed only at the best AUROC epoch\n",
    "            thr_opt, details = compute_youden_j_threshold(best_val_true, best_val_probs)\n",
    "            best_thr_youden = float(thr_opt)\n",
    "            best_thr_youden_details = details\n",
    "            best_val_metrics_thr_opt = compute_threshold_metrics(best_val_true, best_val_probs, thr=best_thr_youden)\n",
    "        else:\n",
    "            no_improve += 1\n",
    "\n",
    "        # Early stopping when AUROC has not improved for PATIENCE epochs\n",
    "        if no_improve >= PATIENCE:\n",
    "            break\n",
    "\n",
    "    if best_state is None or best_val_probs is None or best_val_true is None:\n",
    "        raise RuntimeError(\n",
    "            \"No best epoch captured. Validation AUROC may be NaN due to single-class validation split \"\n",
    "            \"or earlier failures.\"\n",
    "        )\n",
    "\n",
    "    # Save best epoch outputs (heads + plots + metrics)\n",
    "    best_heads_path = run_dir / \"best_heads.pt\"\n",
    "    torch.save(best_state, str(best_heads_path))\n",
    "\n",
    "    roc_png = run_dir / \"roc_curve.png\"\n",
    "    cm_png_05 = run_dir / \"confusion_matrix_thr0p5.png\"\n",
    "    cm_png_opt = run_dir / \"confusion_matrix_thr_opt.png\"\n",
    "\n",
    "    save_roc_curve_png(np.asarray(best_val_true, dtype=np.int64), np.asarray(best_val_probs, dtype=np.float64), str(roc_png))\n",
    "    save_confusion_png(np.asarray(best_val_true, dtype=np.int64), np.asarray(best_val_probs, dtype=np.float64), str(cm_png_05), thr=0.5)\n",
    "    if not np.isnan(best_thr_youden):\n",
    "        save_confusion_png(np.asarray(best_val_true, dtype=np.int64), np.asarray(best_val_probs, dtype=np.float64), str(cm_png_opt), thr=float(best_thr_youden))\n",
    "\n",
    "    # Per-seed metrics file is detailed; the experiment summary stores the canonical threshold aggregate\n",
    "    metrics = {\n",
    "        \"dataset\": dataset_id,\n",
    "        \"seed\": int(seed),\n",
    "        \"best_val_auroc\": float(best_auc),\n",
    "        \"best_epoch\": int(best_epoch),\n",
    "\n",
    "        \"n_train\": int(len(train_df)),\n",
    "        \"n_val\": int(len(val_df)),\n",
    "        \"label_counts_train\": train_df[\"label_num\"].value_counts(dropna=False).to_dict(),\n",
    "        \"label_counts_val\": val_df[\"label_num\"].value_counts(dropna=False).to_dict(),\n",
    "\n",
    "        \"experiment_tag\": EXPERIMENT_TAG,\n",
    "        \"run_stamp\": RUN_STAMP,\n",
    "\n",
    "        \"dropout_p\": float(DROPOUT_P),\n",
    "        \"lr\": float(LR),\n",
    "        \"effective_batch_size\": int(PER_DEVICE_BS * GRAD_ACCUM),\n",
    "        \"per_device_batch_size\": int(PER_DEVICE_BS),\n",
    "        \"grad_accum_steps\": int(GRAD_ACCUM),\n",
    "\n",
    "        \"backbone_ckpt\": BACKBONE_CKPT,\n",
    "\n",
    "        \"val_opt_threshold_method\": \"Youden J (maximize TPR - FPR on VAL ROC curve)\",\n",
    "        \"val_opt_threshold\": float(best_thr_youden),\n",
    "        \"val_opt_details\": best_thr_youden_details,\n",
    "\n",
    "        \"thr_metrics_val_thr0p5\": best_val_metrics_thr05,\n",
    "        \"thr_metrics_val_thr_opt\": best_val_metrics_thr_opt,\n",
    "\n",
    "        \"artifacts\": {\n",
    "            \"roc_curve_png\": str(roc_png),\n",
    "            \"confusion_thr0p5_png\": str(cm_png_05),\n",
    "            \"confusion_thr_opt_png\": str(cm_png_opt),\n",
    "            \"best_heads_pt\": str(best_heads_path),\n",
    "        },\n",
    "    }\n",
    "\n",
    "    metrics_path = run_dir / \"metrics.json\"\n",
    "    with open(metrics_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(metrics, f, indent=2)\n",
    "\n",
    "    print(f\"[seed={seed}] VAL-opt threshold (Youden J): {float(best_thr_youden):.6f}\")\n",
    "    print(f\"[seed={seed}] WROTE:\")\n",
    "    print(\" \", str(metrics_path))\n",
    "    print(\" \", str(roc_png))\n",
    "    print(\" \", str(cm_png_05))\n",
    "    print(\" \", str(cm_png_opt))\n",
    "    print(\" \", str(best_heads_path))\n",
    "\n",
    "    return {\n",
    "        \"seed\": int(seed),\n",
    "        \"best_val_auroc\": float(best_auc),\n",
    "        \"best_epoch\": int(best_epoch),\n",
    "        \"val_opt_thr\": float(best_thr_youden),\n",
    "        \"run_dir\": str(run_dir),\n",
    "        \"seed_metrics\": metrics,\n",
    "    }\n",
    "\n",
    "# -------------------------\n",
    "# Run all seeds and write experiment summary\n",
    "# Inputs: SEEDS list\n",
    "# Outputs: summary_trainval.json in EXP_ROOT and history_index.jsonl in trainval_runs/\n",
    "# -------------------------\n",
    "results = []\n",
    "for seed in SEEDS:\n",
    "    results.append(run_trainval_once(seed))\n",
    "\n",
    "aucs = [r[\"best_val_auroc\"] for r in results]\n",
    "thr_vals = [r[\"val_opt_thr\"] for r in results]\n",
    "\n",
    "# Aggregate AUROC across seeds (CI is for n=3 only)\n",
    "t_crit = 4.302652729911275  # df=2, 95% CI\n",
    "n = len(aucs)\n",
    "mean_auc = float(np.mean(aucs))\n",
    "std_auc = float(np.std(aucs, ddof=1)) if n > 1 else 0.0\n",
    "half_width = float(t_crit * (std_auc / math.sqrt(n))) if n > 1 else 0.0\n",
    "ci95 = [float(mean_auc - half_width), float(mean_auc + half_width)]\n",
    "\n",
    "thr_mean, thr_sd = mean_sd(thr_vals)\n",
    "\n",
    "print(\"\\nAUROC by seed:\")\n",
    "for r in results:\n",
    "    print(f\"  seed {r['seed']}: {r['best_val_auroc']:.6f}\")\n",
    "print(f\"\\nMean AUROC: {mean_auc:.6f}\")\n",
    "print(f\"95% CI (t, n=3): [{ci95[0]:.6f}, {ci95[1]:.6f}]\")\n",
    "\n",
    "print(\"\\nVAL-opt thresholds (Youden J) by seed:\")\n",
    "for r in results:\n",
    "    print(f\"  seed {r['seed']}: {r['val_opt_thr']:.6f}\")\n",
    "print(f\"  mean ± SD: {thr_mean:.6f} ± {thr_sd:.6f}\")\n",
    "\n",
    "# Canonical threshold storage for test-only cells\n",
    "val_optimal_threshold_obj = {\n",
    "    \"method\": \"Youden J (maximize TPR - FPR on VAL ROC curve)\",\n",
    "    \"by_seed\": {str(r[\"seed\"]): float(r[\"val_opt_thr\"]) for r in results},\n",
    "    \"mean_sd\": {\"mean\": float(thr_mean), \"sd\": float(thr_sd)},\n",
    "}\n",
    "\n",
    "exp_summary = {\n",
    "    \"dataset\": dataset_id,\n",
    "    \"dx_out_root\": DX_OUT_ROOT,\n",
    "    \"manifest_all\": MANIFEST_ALL,\n",
    "\n",
    "    \"experiment_tag\": EXPERIMENT_TAG,\n",
    "    \"run_stamp\": RUN_STAMP,\n",
    "    \"exp_root\": str(EXP_ROOT),\n",
    "    \"run_dirs\": [r[\"run_dir\"] for r in results],\n",
    "    \"seeds\": SEEDS,\n",
    "\n",
    "    \"aurocs\": [float(x) for x in aucs],\n",
    "    \"mean_auroc\": float(mean_auc),\n",
    "    \"t_crit_df2_95\": float(t_crit),\n",
    "    \"half_width_95_ci\": float(half_width),\n",
    "    \"ci95\": ci95,\n",
    "\n",
    "    \"n_train\": int(len(train_df)),\n",
    "    \"n_val\": int(len(val_df)),\n",
    "    \"label_counts_train\": train_df[\"label_num\"].value_counts(dropna=False).to_dict(),\n",
    "    \"label_counts_val\": val_df[\"label_num\"].value_counts(dropna=False).to_dict(),\n",
    "\n",
    "    \"effective_batch_size\": int(PER_DEVICE_BS * GRAD_ACCUM),\n",
    "    \"per_device_batch_size\": int(PER_DEVICE_BS),\n",
    "    \"grad_accum_steps\": int(GRAD_ACCUM),\n",
    "\n",
    "    \"backbone_ckpt\": BACKBONE_CKPT,\n",
    "    \"dropout_p\": float(DROPOUT_P),\n",
    "    \"lr\": float(LR),\n",
    "\n",
    "    \"val_optimal_threshold\": val_optimal_threshold_obj,\n",
    "\n",
    "    # Full per-seed payload kept for traceability\n",
    "    \"per_seed_metrics\": [r[\"seed_metrics\"] for r in results],\n",
    "}\n",
    "\n",
    "summary_path = EXP_ROOT / \"summary_trainval.json\"\n",
    "with open(summary_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(exp_summary, f, indent=2)\n",
    "\n",
    "history_path = TRAINVAL_ROOT / \"history_index.jsonl\"\n",
    "TRAINVAL_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "with open(history_path, \"a\", encoding=\"utf-8\") as f:\n",
    "    f.write(json.dumps(exp_summary) + \"\\n\")\n",
    "\n",
    "print(\"\\nWROTE per-experiment summary:\", str(summary_path))\n",
    "print(\"APPENDED global history index:\", str(history_path))\n",
    "print(\"\\nOpen this folder to access artifacts:\", str(EXP_ROOT))\n",
    "\n",
    "# -------------------------\n",
    "# Stop the runtime to release the GPU\n",
    "# Inputs: Colab runtime module (if available)\n",
    "# Output: ends the session cleanly when running on paid GPU instances\n",
    "# -------------------------\n",
    "print(\"\\nAll done. Unassigning the runtime to stop the L4 instance...\")\n",
    "try:\n",
    "    from google.colab import runtime  # type: ignore\n",
    "    print(\"Calling runtime.unassign() now.\")\n",
    "    runtime.unassign()\n",
    "except Exception as e:\n",
    "    print(\"Could not unassign runtime automatically. Error:\", repr(e))\n",
    "    print(\"Manual stop: Runtime -> Disconnect and delete runtime.\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "621c141907214fd1a1e1f792e7775598",
      "6b2c7cdf18814de8910849b3ed311aeb",
      "a0f42a0701ce4e8c96d5e639565d0231",
      "df0f32c36475494c9f87c24318848093",
      "b73bd85433274a51923fb726acc6ad1e",
      "f7c90149218f43b2b0f3d1295ffaf881",
      "51338d4c30c248adb6982a257eab93d0",
      "d112f5630fd4462f85a9766c1f9425a0",
      "caffe868277f49048e5a6bf189761906",
      "2f0a9ef0fe874f3a90fcd818324cdfc7",
      "3e04abd01cd94aa2ac1855048a23e4b8",
      "f7cd8a1a046f4a9ead88f0a2c899bb02",
      "bfaa7e0c1e5c49428dcc10ba79b8a440",
      "9188a962ac03426d8a20c85076a55b82",
      "dfa1a2367832483d85394dbcfd8aeb0f",
      "df8cea7e9c54438ea94e882e2c3c7bdd",
      "43c6ea42db7548d4b3ae76859b81e682",
      "aeaeeb9eb7ee4d2dab409e97b7b98e45",
      "972fda01540f41079fc2c415c56edb0e",
      "c8cf8b4440564d2a8cb39ea9aaf97a34",
      "2ad05c75e0f94dfe9707b183dfde6396",
      "9b1ac9187b1a4b428b14494609aa7354",
      "17cda3a152c14f3fa7673b9d36fef6ab",
      "0a045ab7b63d4d3b9138e827efc402d0",
      "3f8c502e641e4da7b817b02f2ba4c145",
      "d74410423a6041d0997eac581e7e1156",
      "a60100743f0840f19c7e84ea98c75552",
      "864b117ad6ea4bde8784c4568c5e36f2",
      "25fb97e468884078a6583e844f945ed5",
      "cf87bab096574dedb84a7d8d03643eb1",
      "0b13a7d54eb64206920cea7817f6c4c3",
      "037ff9ea499e4ed7a979125e2b2b59fd",
      "58a9845e4ac4492e82ea020e20901068",
      "0debfcf32896421fbbb3bc71edbc55d2",
      "c133b769403f49f9976c570811f9c160",
      "d247d98cbf404641bc6fbe48d3933f1b",
      "e3588966332b441c8bb81f8d26885af1",
      "88a69e8a4ff24b9da1c6472cf9a4a087",
      "40a4a8b8538c4e8ba8e5da7baa10875b",
      "71b1e90a37d740f9877b0c688a7bb66b",
      "9b7925b9c6d341bca9a7caf0f3f27bea",
      "3bc637d6f5a7491d864e87eb99fe9f51",
      "5c3d1ac44fd6451ab8de55c0ca7b7c89",
      "d3982985bc6d4c72b85ea6b543c86d1b",
      "cbea64689a1548909279ac5cf6e23271",
      "9500b4b67c5f4f099d4bde5228f77598",
      "598669150bc24423b3eacea600eee971",
      "dbc5f40ffd674a35a49937e04e9a56bc",
      "3743b982dbee40c4a3cf447cbec495db",
      "5bfa31a2326a4a3e9becec50d2d1e71f",
      "5b59fb844d294ba0b758ca6bf9b26d54",
      "5bcff4f6ced2435199cb3a299803c15b",
      "47ba8714cfe943068f43cb509c0e57b5",
      "383b69e802b9454ba054d95e9594db2e",
      "c85d022c376e4e9c8113914dce1ebdde",
      "7e0e763293f543f08823133dde423e83",
      "b1eae4ff7c5c4474830c4b31036e6034",
      "ecc4f00b699940a480d45a5f3a8f1f05",
      "7567308d916e438493fd1ec31eaa05ce",
      "2211cd92ac3343738ef4a4cdadc6c19f",
      "c9362295fd7c4e3a8e0a9c652a07704b",
      "c7637cc11b574048afbc5239ce6d12d4",
      "ce0788c0944443a983f2b63a44fbcee3",
      "b8c47d4b7e9e41acb4cf0d0eb5b9bb1a",
      "48dd79adfa574917994470c64c59a9cc",
      "b7e62a0de07d42baa905d298c60eaaa3",
      "1b7d3d25fd3c41c7a8cd98c23850d000",
      "cdf38ada38884d379180b1978a03d18e",
      "54f1b1dafecc48c4b66ac54dded1c8a9",
      "fddb634a02554c20b7032244029048fe",
      "2275582dd5764faeb20857e8d7f5bc4b",
      "0f6b7469411e4087b24d35bf6bf2947a",
      "08896295a3724be9acf0e382828bbb49",
      "c323758a560641c4a4a86383962e976f",
      "93c530e3e9484ef9bece23e33b98d466",
      "ceaa97f020704a858822d75f71ff3178",
      "6461dc7d1ab940d1842335771255398d",
      "0ca05ad497db400e83165786b59914d5",
      "1ab055a9038a43dcbaf3aefad26dcea1",
      "216914fdca944905a1a261ddbbadb14a",
      "5c8fafa8de6646a487018714d972f462",
      "9eb046caede64515ab5f8f166022bf9e",
      "7e9015aae9834287b3540fe9b88417f1",
      "2c258490ac084970a86fb9f015fbda48",
      "574ef16fad144829857c531ed841a06b",
      "99852a224403450ca2ff354ae882281c",
      "f280a48e8fa04c9ea936cb3efdff8f88",
      "27f0ca50888746ccbe449de91c5e3c51",
      "2321b4d103cb400c97dce796a3d0f761",
      "f2836b0df44c4822b58bfd304e2763c5",
      "80de56761c1246a386e3b07f0694ae1d",
      "54ded84663b6481eb7f2201ccbdf1280",
      "c21baf8f9ea34063b26311950801c203",
      "6485f725433241208625766a0e4377ee",
      "40a385a35abd4d88b119c625a4093b64",
      "4d16fe828e504aa48c7ee1b2a45371ac",
      "9e0a11933c0444898695e1159f06e0ca",
      "75cf626d5c0c431ab8206234885e5377",
      "d6023147509e46bc96e2efbe03524288",
      "3747f345a0ce496e98760857c50638c7",
      "50f16e6438914d6086b97a58c18315be",
      "6464719cdf2a4a08a00a81c0ee00aa3a",
      "dfdb5e4ba94341cbb3c5237f79b2344d",
      "f86845e4361944f18ac759ef8a87cbd1",
      "6c689451bc2a41ee87b8b0afe4f35fa5",
      "eec47971c6c54321b584dcc57d82bc3c",
      "c0be7e2f19bc4f429975d05ad8225563",
      "1e60e2f7ee934f6b9765006489a7dce3",
      "c5b2997231604ce786442ddc933082f9",
      "2ebcf21db6384113af0b682ef761ce65",
      "a9bafc1a240c49018bfa7b6ff48d74f3",
      "8ec40426701a4fd79e9cec9b0a882689",
      "3258150cfd8e4e418a979fbfbcebcb33",
      "aedfb138dd694442aa32392ce31a4550",
      "565153b73b3543dc996f05d77f71ebdb",
      "c3958402e6e0443f8da3094df3a641ae",
      "4d89887feef74f8a86385d4a267ee7d8",
      "2af58b2ede67445191d3befbf0306824",
      "3e9d04be36f1465fbb3aeb8c6f2e8bd4",
      "ef9f231d21a743758d257e8e5281b40e",
      "3bb732fa887548538e13639d7966fe91",
      "dc664a8a560445baa78db2b7224b9b89",
      "785198bfe70d4f4483dd862e170cbaa6",
      "8486b3d392a645609b7e802ee9da93f7",
      "e58fd2a34b044a0581878ccb98d25e75",
      "bce2ab4ff38243a786fc565f31f3fc0f",
      "e012c7cf99a543c3a818cb51a31f2e52",
      "97cf69864da74a2384af547d1c81409d",
      "dc9e63b47b474f7b93d41cea4fe57c36",
      "13a447275c1745359d4f72ddd02b97a8",
      "3f825fa01f8b469d914f89ab61055574",
      "471edb1518044c72b39867add951be6a",
      "9dbc513953f5492789e5a500be9168a9",
      "8a2ecadd33ba4288a60fe76402545cb1",
      "6ef34136d4954eabb39197ea85b8b7f9",
      "7b4c29daf8394ddda0c60cbdd1da1240",
      "dac5d3ad76784e7a9a095ba74e0e9200",
      "b271bec312c842a8be99357432952ac4",
      "43080558cbc342ea99790c0bde0ed080",
      "36ea1d5bc0894e40b7d618f86154f288",
      "71dc996992ff48f2b50b4dc7ba8d4413",
      "0201c0be3acc485a9480d7e20b5a7bb7",
      "e935b0ab797d4748b792cae578e3b8b3",
      "726cf58d22624d459350f1714d92fc34",
      "7aba8afed5324a778c1e5fc9e50c1663",
      "a841e8987e8c4f3097363ed21fd3a598",
      "a1e073259c394ac3ac08cfd2005c0a35",
      "85598cc6fadc414ba37ee3b2a47be401",
      "63b4f34d98e043ad8e4ed8cd03735034",
      "5e77f351f3784ea4b336f68debef83fa",
      "e526b17aa8d24deaae43b1dd45c16644",
      "aa1f82068f06490790cc95d4107f21be",
      "672e707f72e1474daae706d29f74eaa4",
      "5d0aaff7e7d244b8811fd0a3a2df5443",
      "1602ea2a877f45e984b8cf034e91a74e",
      "e1cbdd96523f49f6b530539b9e367756",
      "4d17987a6752468eb4af08d7cb00baba",
      "1b35d84e9c6149d68c111974179867d7",
      "5500b47a43db4e76ab54a556093f8e8b",
      "4bd9d7b23e664ff899197e670e5d2e75",
      "d8c6b2dcc89b4241864f3592ccb31760",
      "782083e63e8f424cb5941949f3140de1",
      "1b3ce5504a704a359965e9e6fb61df64",
      "14b2ad52875e465d9ae3594dd936fa51",
      "ad69e0ade6db48909b1dc9db398568da",
      "77fb45c52f654d649c4fc2b265cd6fb2",
      "9252c21a8d2042ea96e5fb856ff432ef",
      "7cafe5eb355d4df594aab5dcde38853e",
      "264ccc7591c24086b49b85238a532127",
      "49940472445749e08355824c2d83eeda",
      "9da7f65794434ac7b0285b31b1d4bae1",
      "4adac7f985814287a64fb8ce2b00eae4",
      "47e432e4385248329c966171597511f7",
      "7dd6fc3fdbcf45bd8c8ad4738cb0e04e",
      "e46688942b10429887f5e09ab7b516b7",
      "fb55b9586ad046479b9e2bf12483faaa",
      "dccec497bb7742afaf06dcffea800250",
      "601271d5c8ee48c29332d370bffd926d",
      "aa84db5a966343f2ba9df4c00a5efdc8",
      "9b5efc086c7143e3902e40ae7e162da1",
      "a96b815513f446d598fbe566675177f0",
      "dfa0cac42b3d458f94553089df552ead",
      "77e5ada6be4c4e8caab4bf9e29eb0170",
      "0d981297acc64a54a6041ab05d513897",
      "39ca32ed2f8c4015a45265916e57bf48",
      "af8cdc5ffa2049a399b0f8f7ea858caf",
      "ca82c03c4d454de2ab7ba1729ed15777",
      "aa0983efedab4b65adb1ad6a9dcfa37e",
      "e6c2e1f9a5f5452ba73a579ebb14c4f1",
      "d31839c1505843f6b6c054b48623ab2d",
      "3852e15b57b942638d2db8e81edc0b40",
      "93d670e8c01240d8a679119d3ca50379",
      "16c2070378884a119797eac8cb2927d7",
      "fbb0dec052634c9c89db0e664ac4ce6f",
      "dd037228fd2d49b7908169c6a92de161",
      "59336edd9726464d8b41859c7e44a964",
      "522427242f164d0b84cfeb2d3df60638",
      "0812b82b040b4c4f82e6dc45118e6355",
      "5cce90408c344bf489acd83ab8fffeba",
      "100922acca404c568bb94b2c7e2af735",
      "bdbb9e8f736041359d7001309407c782",
      "c48e56cbee2b40cd80005ae2691c7d99",
      "fff5906cb7bd4ce0b7aac967ca58cc60",
      "00da883cb391405892a2f23e32eb68ba",
      "f8aa501c0b404f93ad8a9c4ea80d2599",
      "b23258d8926b4d779d731eadf60b163a",
      "b0be6d2e760a46ef9f7ea10a316e1c6c",
      "6ba7557de2d84f70baef144cb14bd29d",
      "07d7c4ab8448401188eca0d13545de29",
      "51de3ac5f5a745a1b534cebfba2ddf51",
      "7d8cd411f76f4437a74ef558c3b24a16",
      "aaeb220e7a43457f8cbdfbc982871ff2",
      "5d1aea7d5c2245e5817921eb6dd0f667",
      "f64287abb5114a23a31765a3d3a48abf",
      "e484866c1f124b92a374c609c5a3e608",
      "a799a460be5f4abdb271038b7d8b2e4d",
      "1dd4f00663a74cda942c83a8307a5558",
      "492496b5319a4822a09b5cb12feb80e8",
      "af0799e47b2a4908ab97b590cc7fda69",
      "743e0638b5f545cbb9f86d024f21f2ed",
      "fab5096c736e435cb78a2a1ee99dafa3",
      "9155120aa57748dc8c0af851bb64b50d",
      "e473414a4ba9459fa972b4f1d60c2e1f",
      "2854004fb8aa4c87a2725a542a3b4ee3",
      "9033fd39977b4f6aa05597057e4055a2",
      "4bee7b37b88e450eaa040ffed7066426",
      "005bb737651147ea90febcd5044f3554",
      "4ae31fe9c5ae44b4ba8b1753e1ac5d74",
      "314a6103f824477da64507ed316092e1",
      "79eb9212686a4d1683f3de5fbe81aa77",
      "17571af857384ed1af8ad7d9b2c71fd8",
      "bfe1abb732184ddc8d928439df3e0085",
      "99f067ce9f9744f49f29a6d11fd4a9de",
      "c9b8608ec1634d3b964925cc9ad4cc09",
      "67ca38b2b01e4c4e9740d1e520e37d11",
      "c3e07e8fe9b44585accd7dc4025d9f26",
      "1bb30244d815458db4bb294cacca32fa",
      "91053473226a40ec968e0ae1e51bd2e9",
      "9c7be7f0f2d84679ac54ec2ada0c6f3e",
      "4087c4adb4f74a8eabf484996939d99f",
      "9d97182fa3e04aa394ed3f2c8dc9e0cb",
      "72db9076de16433a8dedec3d58a1bdb7",
      "53c33449198f4ac080b2265c347a2305",
      "8b54946b4fa54bc4b2bb52f0b40c9870",
      "0ffd3e2d403a42d49af8d04ab8797b28",
      "c2cddef5cbda4a6cb89006999a6e8ef4",
      "ac73e57851814fb08eca9a4596ddd298",
      "f1793b1b9d3c4cd3a16e2ea89a5ae028",
      "20555104ff234189924199ec7314c8b6",
      "88fd910e64cf47718e47cc0c6c7d19b6",
      "412c64248517478cad45c69ef30ba8b9",
      "daf9cbc6b0254d2891867dba4a40a1ce",
      "653fdf7bee304556a961a293f68caf4a",
      "2ce11fb3565648fd95ab66eb88af4940",
      "25b1e55796744a32b8f4cd396f11b24d",
      "2a3681a8187f4f5682905a6a9cce7c37",
      "3cec97abfa2b420e9d53b3554cacf439",
      "4661d5eafc7649cbbc31a251b81d2184",
      "44c83441880946bd8fbd4b738b1ed409",
      "bbb9364a739d4b5bb01302294de49860",
      "49fa162724da4ece91c582eed65325dd",
      "b6668107210345508b40294d4db8625e",
      "e8f34573fc8645ba9c3297382d568c13",
      "fbdaa8ca5c4b4951900ddaf7d70c309b",
      "9fea987509414e80a42af893f5968f97",
      "7f536d6c9be245e9973de58e56ec3d1f",
      "42d093663a5742688401905ce162b7db",
      "2d75e82565ad4d8fa1a16f7abf941f70",
      "2b64a16c7b234e2cb42cc19d8cf637a5",
      "7a2eead1de7241c88d4dfe2f8ab544de",
      "a55d5b07c570442183d37a9e6030e44a",
      "c151572dc43f4d278619bc7f5e32481b",
      "58be252c2d6c4e7daf58d1a0e10e0304",
      "dd1d57369cc44dd9813780db9039bdd7",
      "569958baa76d42ffb67dbfcf8da81af9",
      "b013f59470ef44c9afb1beac3492fbdf",
      "8903d103b79b48679c9df9c4ab21312a",
      "682cfe0077c9401fbba1e3d1ab7ea0f6",
      "f963559a58d9498ba5ec8eb652b23fff",
      "edfe22af42b34c9f8cb6d8ca34a8b645",
      "0339502c94d74da39918e858f7f64824",
      "264a55ef36a74c36b63ef1785e3b6915",
      "428c6a5715e64e3392e16ce565a5bbef",
      "dad8bafb61394c399ad1b07f458b7686",
      "4f524736a5134afe92dbff9efc70a7b2",
      "141fb81af9e94a1192fecaa26ff5a143",
      "b0c621d983e14bc4ba3f469b21a3e34f",
      "ab20cc13b93a48f5b5ea98cec8cc3a6d",
      "de9f0bc1250a48698a3c242df5cf9afd",
      "0cf3c38dc6df4d3c81a4cf25879e432d",
      "542bd668212443319570905c5edd0c60",
      "e02e33766ff648f49a1112edaa72bcf2",
      "79110136210c4bdcbfc6614647048c2d",
      "5d2cbb0117da4d74ac62d116792bad3f",
      "bb46a9aaaa9b42d9af16993409530299",
      "fe5c58d79b5e4f93b4c4b4bf620facd2",
      "1b9fa58d97ca41b896f82334f0970e52",
      "788066b7d5774b57b94d95772a57f361",
      "840bbb39f06a4005ac351a62dd1da394",
      "0302334168da4f1aa38dc127b9f2e6b5",
      "60febdc4998b43b5be6d4465bc694eb2",
      "3c2cc84b8a834b59b318a1e081a27f1f",
      "4fef65dd24f0479581b1c79e04363ebf",
      "e22c46ff9f284896b3a0eb77cacb7fa0",
      "cc235e9eda3346798c09f6c86fe75719",
      "3e2ddef67a3e4e2c9b110541b8091e5c",
      "93d82b14872140a6bfe365819da394f7",
      "a60265fc57ec4520b3bf58700bb577c4",
      "06b464a03c304026b6f355c8d89392b5",
      "aecdd74cd711460e8f32923cac7c811a",
      "8f7b8abbbf634d6b967d8dbcc95fc2a0",
      "a96f5c36b4bb4410b58e8064bf63946d",
      "7e87bb737d55447295eb506b8bb4d788",
      "5fdea466ef4748b7889605ce2ddd72bf",
      "14e66b6a10be4d69acbfa7e4cbf2caa0",
      "6f5d6fb440314fdaa445f48104ce3231",
      "ead6ea29c94e4d6cab0410f9150fc508",
      "dfdbc59724584343bf48c248e9c17ded",
      "de0c52e9fb0e4dbba43cacd60bc7800a",
      "2eca6508c80b4da89a4dc4c79dae676b",
      "21d4526f9a95470db4b4209488a9bacf",
      "4ead3575d29f4404bad875ce061aa136",
      "520753e2ced341388c8e904b99c4b0a4",
      "4749864d254a42e2b04b65005547cbbb",
      "644324cfcf414b6ca9e3c4dcad304ad7",
      "1d0e495b12054ac0892c6d988fe3ea7d",
      "73dc1873e2dd451c8d07bb85b9a58677",
      "9052b521c97840298b4ce11ee0c0f200",
      "10abb8b7b8354639b779d97778406933",
      "de2f39e978134c4a952031daa0c7caff",
      "1fdbf214c35147658ea18d1c018550e5",
      "be0838d1389c4f2389843fefc36ee267",
      "c09c0f4a65cb43aeadaa05825ef68c7d",
      "03160012e734465ba900cc08598b1317",
      "bbeae1549a914eb59f830aaa6a774eb0",
      "461869d9e0d84ed0b4a8e3f5bb42cdf2",
      "1f078c3254ef4b6885cb2f24cfa10c13",
      "bea1206982b049f4a1962e8a6277281a",
      "35b6b14ef2e9455180f2a9af492c1ffb",
      "e686706fc3f045ff9abe3d2d1008d5bb",
      "c9119a80f0e846f0be8dda540a841ab7",
      "47ad56160e9d45bfa1396e798f6a349a",
      "eceb9b63b569457d9bf2ab97a4188cfe",
      "6818dc45715d453c94cba4ffb7476497",
      "a04df8e35e6940c8b21ac8acd06bf2a4",
      "33e0902da8ad472ca0478d543e2e4d0b",
      "fb03da30c5bf49d19f1a30a49f7670d1",
      "ac5cc51f3a4a456488804d21e262e3ac",
      "14e30c882c20414cb76884b0ca934c60",
      "f58b21a583e34672832de3aa56990646",
      "5a7b9015937e4340bab248455147a671",
      "573b90e40eb74b5aac0083b387052464",
      "4c1afb49473a4bb78a40e58539eebda0",
      "0b988536ae0749a8854ef8f775c22d42",
      "126a2af5fcfa43c5a38fdac0eb0fe068",
      "8384563ac31446d08883e962e7cad5ca",
      "7ec09eaefa674355ae308e5887d8aca3",
      "079af1912a3e46e2b9ab699425d9792e",
      "68d4952150c14f3884db541cbcdb4f1c",
      "b5000b9b296944efbf939183d6f2374b",
      "8ea2a9a4daac464cb52e98ab39236d25",
      "e97fbcdf684f46c9963d0d7d1a00bc83",
      "83e9a4a4903b45abae78377f42177c6e",
      "0d52b4fe796e4556b23e139e07fdc1a2",
      "dde4ebee9d71486a8d5ffb1bbcb5f2a5",
      "f9c593bbbafd4eef9816ade2d6240358",
      "49bbe8b1974347c399d756f259e11615",
      "e0ccfdf835f546818cd58a5782df4d2f",
      "ef52ce9bd69b4bcea6d091ff6813a24d",
      "831a31e92b2f4d9c9c03f43c803a4570",
      "f492cec4e40f4724b281b3f9d37abf16",
      "a694b6b5350a44c58dd54a2397d37e60",
      "ef61126ecaee4d47a5a2ec2bd363d7e2",
      "a6c762475dd147038e09851a3b243ec8",
      "689e9d402a3f433d890bf1c1267f0ad2",
      "f78214584c0141259a899e00bb54092e",
      "07baa1ef67894ce3beac35b76dd81d97",
      "f4effd513ec7431bbdb813cf31d7b602",
      "65db2423d9e9431e9b88fb4694d51135",
      "90baa26b68b644369d0cb85f7fe62b78",
      "6b9c23a45ce24cbda9bcac08097f870d",
      "369fe20004174b86863aa78b789f8b0e",
      "21f7f169bada4db9a01dfcd3e48c6b85",
      "9a0d4f8f7e834437b93b9fb927830f87",
      "6cb65241ccb24d8eb0707cea16531372",
      "c2255ed9b7bb4b6a87e58f2ba2a1d08b",
      "37f9646faf8b42c9bd063ab122436ee2",
      "c5cb962f7c094bfd8a65e524f863429a",
      "53804d85d2614c449ae9b4ac97893484",
      "f8782fd1f2524bc7b70209e22458c3c5",
      "33eb93b3eea8432f851314a7e74a4ffc",
      "31b952ea544f46509f04a40cf845f059",
      "f5a4b526862243579cfb193036c62e4b",
      "e95c487325f84a7b980c9f1e1303bc1a",
      "0711af2b69cc40a78d915458acb736fa",
      "c1b21cc3143546378ea010fa31103ed4",
      "f9f033f9f12846bfbcdcfde87eafe0b7",
      "e0bfd239968e464bba3af44b98346d97",
      "676e5fd40bc949bea39198cca10c6562",
      "7993e0a97c2e40ccadaec18a0c08a2a3",
      "2cd8b11dd4b244bfaf63e3ebbd72358d",
      "b820bd692c144eb6b30f31a5318e0920",
      "2e12e211e32544b9a34603b2ab880de5",
      "53aba9238be44200ad18348bdda6e3d7",
      "2a68bee5b2b1438c8ca76c5346173ec7",
      "1adc7e543b254d289d045557d81c63b2",
      "4705ae07d6e94928907357a7bedc738f",
      "eb6cf6d94e814f7491e7b12da981a2c4",
      "5d576820e16945b395ddb562ad3c4525",
      "6b978d6df3ed4e9eb0170df6da351490",
      "b1f3a03183cd4a21a37c9841356b1374",
      "17f6c656c32640719f227f2ed6fa94c7",
      "a7e2091477a047b793b4f0d3115bbbef",
      "079ada3a2b2e4ec6ae316cfb640606fc",
      "3027b571fda3440682c3e0ce46ce4803",
      "87c0846b0e2640f6bca8e903622fc966",
      "0ce2dafb9841414091817a2875d1779a",
      "e487189f006541868da872a78117eea7",
      "31b1c477c3434cbb8c18b774e7f49b49",
      "ef615f88ffbf4856b0f6ce85999b4037",
      "a313d042a9644efe8d1fbc4fcf601286",
      "475f0500e0b14089a3100fbac0dad4ca",
      "4a893cc44d114efeb7d4e0f6840f7a04",
      "6b5396a8933543f5ae3ad1a32482926a",
      "ab5c011c6fcd44ff9ceb35e2a8c6e8b7",
      "5f197052925e467fb6050cf3115dcb87",
      "195dca20dbd04365b785b122ac343b6e",
      "e2a2ab204d5d41b1a1cd50231efa387b",
      "80a326f32d0b4b429a485a16564b802e",
      "a7d2c9b1525f4c7c95bd4fa8fb2e7692",
      "ac977e526e3549948f33f219c32921f1",
      "50143ae110c84663a8b46ed78859db00",
      "c8542ed2e6cd437f85a908def276eeb2",
      "a7a532507a034dc289e02b9510071514",
      "7da8a597a8e44656a93ab78364f84f48",
      "3c59b05a279d45618f929c2ccb3cba66",
      "027dfd8585e74b7aa6e858e566391669",
      "686b4020106b4892a40785c8cdd70682",
      "2c22a23ae66f453c8e43b0cd798da74b",
      "b8eaaeb4efde461fb7d33eda8c4b4e3f",
      "ca98d6d9077a49a288c0fab4e74709e3",
      "8486218ac88f41c581689322eaed1a4f",
      "127a98cc583d4b6aaae9afd196858c6c",
      "a96b90f329ff42378493d2ca883de9ff",
      "5e29251443564fbb8212394376617612",
      "0527e5a93e5f444fbf80e591ea0b63e4",
      "891f1837dc614266b75a4aed1c84317d",
      "9743d04b94cd4b8fa78b941c104bbaef",
      "ebf292fc05174e37b0eb2e05cbd93d86",
      "8d53d998352a4e3a9f424e83cd9c2370",
      "fd20b4921cc647f0a161e641881f632b",
      "1ca373ff314f4b76a87c3be6b6a89e5f",
      "dafbbe531fad414db5e86b0277fea65d",
      "78d734a20d3047d4abc27f0af10c4a6f",
      "01864a0be3474e0f8ca498f53c5512bc",
      "15aa5290f9e647f7a2a404c7f8cc74b0",
      "f54456cec3224c0d9865b8fc53a93c83",
      "3cae9c24637a47ed8ee05e8defe27b03",
      "a7230ec4b91d464697f8747d30f65aee",
      "6b79ddd5e84e4310a5cfc09efd2d34ab",
      "c2c27c67aa6e4337887be5b45a95cb16",
      "d1f822fa516b49e0aa3c30d2919c6425",
      "0adcfc4c3dd04099a2fb310dda49a726",
      "6b3768a6ee2844baaa0461285fc587b6",
      "6c81632d68404536823efcb0b26a4d68",
      "edd0a164a68b4f2aa4bed719e30759bf",
      "49b67f4d66cf4ac59f80c3f2f0cbee42",
      "4a2ee789d18e4aa7a3ab3af6bc1e9047",
      "9f9789e38fa8436ba9ca7a91cecf863e",
      "af9b4822edc04b7e8b35a2c1c6f5318f",
      "99cabf82c72844f2a775c5c16a67661f",
      "cb5bf2737e1747a08093773eee4fa5d4",
      "5e8262a24bb1466c884b81b94984cd8d",
      "089e99bddfcc4af5b98fe4927c299775",
      "ba12bab9ab344016951562cad16cfdb0",
      "ff7cccfd97d049fc8d210f6911fd9602",
      "5765639a58864e6e9661671426d86d58",
      "070c4e040e554347a8cde06d8af0d834",
      "079822b73a214af1b60f7da0e526b99b",
      "e4bcf65ccc474e07836981d448ca0ae5",
      "1e6fb12e4dc0469495b2feb8a7c9cb21",
      "7490a15f9faa4fb481feaa0ddcc778ec",
      "32417a86428c47189fb386da514151fd",
      "64c7dcaba33045a8b52d890d2a07d086",
      "3340fd8f547940f5a4f94581251c3891",
      "aae79b0d49904d27881e325d9c4673ea",
      "f3b8ff75bc4842d2a5f2282e0cea5dc2",
      "4a65a895036a4c7888e6351851273025",
      "53bad8ae05da4dc1a62b2d960d2f390a",
      "79ddd07099fd4391af154ad1717c9a44",
      "8d7a494fa1314243aa0a72361489a654",
      "a28c6d6a57824b0f8e8f0a8632e41e3a",
      "835891bf854c4f4f8a4875f21486f49f",
      "0dabe94bd72948469077f8f1527f3afb",
      "23690dbf5f8d401787805150a4f34cf7",
      "612518047e5e43d4a9cd8da0de7ae5bf",
      "0984e651f1194537b2c4a99543fa3b69",
      "f2d265b277e04c47b3b7af8a2d46db02",
      "e83688d3e41a46469a480c6b815b8268",
      "ab53ffa8323c4128a25a5738607bd2eb",
      "c523a14c12844eed869131ed79758835",
      "17aa666540c84190b8ab0043d13cbd47",
      "72ded8ae12d241078d5cc0df72ac854a",
      "cbbee5f6d5394ffa8e88fb63fa2c3ff4",
      "f4d0b7f43f02451da8b0c6900ba7c707",
      "40e7c3c7f6b74034979aff49d257e13e",
      "8535d3af0bab4e10af944d3adb22df50",
      "ad567f0fe35f45a1a8a36cc1a3a7605c",
      "3cc2912ae44e4b6badd4020ef76f8c73",
      "1a97175acc674948a78e251308989737",
      "f0284f19e34443209acfaec6cf831dc1",
      "c8323939f37b497caf55ed2973427e3b",
      "06c5461f09e448f7b40812b97f010005",
      "b02b841625704cd7a789f7c03d5835d7",
      "7f6b1af938f94a57bdc396dc2000d24a",
      "40c15331be8e480aa9b3e354c8960d07",
      "9a3c4428755c4705b01eabe35f5ee5d7",
      "2d2856f1834c43499ad9b2d2912fe15b",
      "bdd6a4875d41420c8e906c98f3222e4f",
      "98591536a6be40c2b44bfa8ee772e5da",
      "aa7b036c38cb4aa0803f4a11e81f7a12",
      "9e0d64cdc60b4518860d5849cc78f86d",
      "f8564551b2354b34bd8b6ababbdfba60",
      "df277bebf1904dabb185b436dd70bbcb",
      "477c0408fa114968a8ad5c364c236be4",
      "075b382dbfd04ec48157e445be03b77f",
      "a02f3e5f8ce9417691f244fdcfcda9b4",
      "4a012da3699c4171aaaecea5a2dd1863",
      "f3b2a88f0dd347a0b11b15937a510bd9",
      "6e5b3cbf625b46e49b6ada88edacde89",
      "f8b95ddbd23543aeb02fb7dd06437664",
      "9c88da097c0943b19da2ad48652634cf",
      "4109f644ceee4b07bc38825a62a4fe61",
      "78e37a3879dd44d9b4c5cd1037100a2e",
      "ad156109020b4683a35e3badd997dfa1",
      "a324da641cfd47cca790deb68faf55ee",
      "ce6d19d85ecb480cbda6041efe6f649b",
      "2afefb33abe44fbfa7f2522c5600481f",
      "4f8c67439d6f4fb8b4254ee1f3330e0e",
      "46103540790e41a38da1d7974846a000",
      "ea019a75f65949b98befe8f5b4b8f103",
      "b90fba2462d14c218afc7b5e216df0f2",
      "edb1897969f64ebc8262035a408f8eb2",
      "3d93d92cb6544e03b1ab52f4240b7fe6",
      "f1a9b3b5cdb64ca58ea4b41305315a6b",
      "c84a439a69ae48c69d77e6e5c6528c92",
      "aa6e23e76aa04716bdf6e346aa134476",
      "4c5d344cf5cc43a79a13fd8bdb449afc",
      "27ca25e6b9f44f26b18a41534137524d",
      "f374b03f6d464929be7a457f187a4f5d",
      "f3d5e8f0c7c94953aa798b795dfc7fc5",
      "1ec1ee469c154c129ecfc1df0af6ba32",
      "bd1c442fae3d4bbd9c60d75ce57ec1eb",
      "20d6b36792f94702a3767f44afa83b06",
      "b9ad17b8151049cdb66f47052d42215f",
      "313a7d5d76334ef1a23b03e032d7739f",
      "d2b98f7d34da4142b6fe48a835843703",
      "1c9b0defc6004b7e8da2f9c5de9f5d2c",
      "92e4a1cdec5d4ece82d3b9663628ae87",
      "9601ce7075284184a7b35d07730f3199",
      "30b00fed5c624053a2fba29503dc264d",
      "814b06c7e36f4a6f9f709f754eeef863",
      "8faac3d15d1f42f59c59e0cfe44b2a8e",
      "c73449b919224c26b2f2e9be2a1ff6d0",
      "6f49126e50d14c38ab9c0cc318ac68f0",
      "90f3e6b40c8549b9b0c02994139e23c9",
      "e2d678847a934aff829106496ddd5394",
      "287c74d73de0444d8a27c48f5f8183e8",
      "f266ac501be8429daabe43484934bfc8",
      "72156c4e86c74bce9578f3e68bd59352",
      "d3f5182865a44f6ba19b8fc68c74411c",
      "d5f64cb176b34c6cb609e331d1ef54cc",
      "0d95afd7e6024a199c89d07684635b72",
      "fce5af1f322e470abb2896c6fe3fa270",
      "28e5f99dd53549e3a02783bb1ac02a9f",
      "907b33cb1f294f079dc1adb591d56ed6",
      "d517d2664fa04275bfe13df6fbebef9a",
      "b3086cc24cf543ba9d8c2464338711b9",
      "210cbeb660074373ae97372ed99927bb",
      "35e7cb619d3c4c318ce7f18e1e0fff28",
      "b6e9cde24dd94459b56734411f5749aa",
      "bd671ce21c824cde9bf8e9cdc0507a8d",
      "146abaefd02040e292bbb41c823c406d",
      "68fd9ff409874a84b7c9657228d9252c",
      "051c8e9a85f84e30aa429c749a26c9cd",
      "cddc5858c79841eb9ace66507d50790e",
      "5c0a670165a04896831b673283784f48",
      "02d2cc8623704d2b9c5aaec82a8d05bd",
      "579905ffb0dd4c6e8c07e942390e5a78",
      "165491d1cef44d99b61c13fa37c7f038",
      "b0682c647d244020b09449d3551285e6",
      "5490ef290c6641b1afea978742b0fd93",
      "144bb5860b334d888ade3391036f55ab",
      "c28ad84fbf68426a9f3efe90609998fc",
      "37255a633c5c46d49832d9023be3fc28",
      "ce9b26d1d418440d9ac0c74b25960ccd",
      "dbbc47bff7984f33b0b29fd6ab392b48",
      "d0daa83be902457f80e35cbab627dda6",
      "e8d35841d4924f169434d7c94d640596",
      "7330b907fd404fa2945bd84acf7bf586",
      "691dac6b4e7d4196b91a2d86873c5ff8",
      "b95b2f0fe83d48ffba1b7607c5da07d4",
      "b9e933626e014fff917800ae6edf6fd0",
      "d25814d90c0a4bd182214395cc265df3",
      "12913849424443379c0cba71821a39b1",
      "9ad9cba0d4a1408ab4c2f5f121d3412a",
      "9464644cc386410ca97dccfe09eeb751",
      "cada51103ed34081b56f5f3fc4c17e7f",
      "8db0598d749642988e96a9f9c2aa5fa7",
      "b6bd83edd87d42f68ad54023270b1e1d",
      "0d4e3c5db3d84819b418f5cf6d0e72a5",
      "8a434937226b43cbb433bc453955c545",
      "e3df77810d71444fb50be31d64a40686",
      "a86b7032131c445b9ac2d55085ba90d6",
      "f822dc9c2e1c4780ab4a7e4d044bd84a",
      "c9c6f1ebd5494428bb17fd64be7099ab"
     ]
    },
    "id": "k_6qDxZKTEEy"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "The following cell evaluates the D7 multilingual model on its own D7 test split. It does not perform any training or update model parameters. The cell reads the D7 manifest file (`<DX_OUT_ROOT>/manifests/manifest_all.csv`), filters the data to `split == \"test\"`, checks that all required columns are present, and verifies that every audio file listed in the manifest exists before continuing. It also adds two helper fields used for reporting: `task_group`, which is set to “vowel” only when `task == \"vowl\"` and “other” otherwise, and `sex_norm`, which is normalized to M, F, or UNK using a robust rule even if the source data is already clean.\n",
    "\n",
    "The model is rebuilt in the same way as in other evaluation cells. It uses a frozen Wav2Vec2 backbone with two small classification heads, one for vowel clips and one for other clips. For vowel clips, an attention mask is created to trim trailing near silence so padded silence has less influence on the output. For other clips, full attention is used. A DataLoader is created and briefly warmed up by loading a few batches to catch data issues early, such as missing files or incorrect sample rates.\n",
    "\n",
    "The cell then automatically finds the most recent successful D7 train and validation experiment under `<DX_OUT_ROOT>/trainval_runs/exp_*`. This experiment must contain `best_heads.pt` files for all three seeds, 1337, 2024, and 7777, and a readable `summary_trainval.json` that includes the validation optimal thresholds, both by seed and as an overall mean with standard deviation. From this summary, the cell selects a single global mean validation optimal threshold, with a defined fallback if the stored mean is missing.\n",
    "\n",
    "Using this shared threshold for all seeds, the cell runs inference on the D7 test set separately for each seed. For each seed, it loads the saved head weights, produces Parkinson’s disease probability scores, and computes AUROC for the full test set. It also computes threshold based metrics at the shared threshold, including confusion matrix counts, accuracy, precision, sensitivity or recall, specificity, F1 score, MCC, and the Fisher exact test p value. Fairness is evaluated using the H3 definition at the same threshold, where ΔFNR is calculated as FNR(F) minus FNR(M), and FNR is computed only among true Parkinson’s cases as FN divided by FN plus TP.\n",
    "\n",
    "For each seed, outputs are written to a dedicated folder at\n",
    "`<DX_OUT_ROOT>/multilingual_test_runs/run_<DATASET>_seed<SEED>/`.\n",
    "These outputs include `metrics.json`, a ROC curve image, an overall confusion matrix image, and confusion matrices split by sex for M and F when available. After all three seeds complete, the cell aggregates results across seeds. It reports the mean AUROC with a 95 percent confidence interval using a t distribution, along with the mean and standard deviation of the threshold based metrics and the fairness results, including ΔFNR and the underlying FNR values. A combined `summary_test.json` file is written and the same summary is appended to `history_index.jsonl` in the `multilingual_test_runs/` folder. Finally, the Colab runtime is unassigned to stop the GPU instance."
   ],
   "metadata": {
    "id": "98f8umgsxXg0"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Multilingual Test (D7 → D7)\n",
    "# Runs evaluation only (no training). Reads the D7 test split from manifest_all.csv, loads the most recent\n",
    "# train+val experiment that has finished heads for all 3 seeds, then scores the D7 test set using a single\n",
    "# shared threshold (the mean VAL-opt threshold from the train+val summary). Writes per-seed metrics and plots,\n",
    "# plus an aggregated summary across seeds, then unassigns the runtime.\n",
    "\n",
    "# =========================\n",
    "# TEST ONLY (CRASH-PROOF, WITH PROGRESS + STORED METRICS) — D7 (Multilingual) to D7\n",
    "# - Uses ONLY: <DX_OUT_ROOT>/manifests/manifest_all.csv (test split only)\n",
    "# - Loads finished heads from MOST RECENT trainval experiment under:\n",
    "#     <DX_OUT_ROOT>/trainval_runs/exp_*/run_<DATASET>_seed{seed}/best_heads.pt\n",
    "# - Reads VAL-opt thresholds ONLY from that trainval run's summary_trainval.json:\n",
    "#     summary_trainval.json -> val_optimal_threshold.by_seed\n",
    "#     summary_trainval.json -> val_optimal_threshold.mean_sd.mean   (canonical aggregate)\n",
    "#     summary_trainval.json -> val_optimal_threshold.mean_sd.sd\n",
    "# - Evaluates 3 seeds separately (1337, 2024, 7777)\n",
    "# - Reports:\n",
    "#     * mean Test AUROC ± 95% CI (t, n=3)\n",
    "#     * Threshold metrics on TEST @ MEAN VAL-opt threshold (shared across seeds) as mean ± SD\n",
    "#     * FAIRNESS (H3) on TEST @ shared threshold as mean ± SD:\n",
    "#         ΔFNR = FNR(F) - FNR(M), where FNR(sex)=FN/(FN+TP) on PD-only true labels\n",
    "#     * Confusion charts split by sex (M/F) on TEST @ shared threshold\n",
    "# - Writes ONLY under: <DX_OUT_ROOT>/multilingual_test_runs/run_<DATASET>_seedXXXX/\n",
    "#   plus summary_test.json + history_index.jsonl in multilingual_test_runs/\n",
    "# - Unassigns runtime at end (L4)\n",
    "# - This cell does NOT re-fit any model parameters.\n",
    "# - Threshold(s) are taken verbatim from summary_trainval.json (produced earlier).\n",
    "# =========================\n",
    "\n",
    "import os, json, math, random, time, warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import soundfile as sf\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from transformers import Wav2Vec2Model\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix, roc_curve, matthews_corrcoef\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# -------------------------\n",
    "# Guard against local name conflicts\n",
    "# Inputs: local filesystem under /content/\n",
    "# Output: stops early with a clear error if a local file would break imports\n",
    "# -------------------------\n",
    "if os.path.exists(\"/content/torch.py\") or os.path.exists(\"/content/torch/__init__.py\"):\n",
    "    raise RuntimeError(\"Found local /content/torch.py or /content/torch/ that shadows PyTorch. Rename/remove it and restart runtime.\")\n",
    "if os.path.exists(\"/content/transformers.py\") or os.path.exists(\"/content/transformers/__init__.py\"):\n",
    "    raise RuntimeError(\"Found local /content/transformers.py or /content/transformers/ that shadows Hugging Face Transformers. Rename/remove it and restart runtime.\")\n",
    "\n",
    "# -------------------------\n",
    "# Google Drive mount (safe if already mounted)\n",
    "# Inputs: Colab runtime state\n",
    "# Output: ensures /content/drive/MyDrive is available when running in Colab\n",
    "# -------------------------\n",
    "try:\n",
    "    from google.colab import drive  # type: ignore\n",
    "    if not os.path.isdir(\"/content/drive/MyDrive\"):\n",
    "        drive.mount(\"/content/drive\")\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# -------------------------\n",
    "# Resolve run root and manifest path\n",
    "# Inputs: DX_OUT_ROOT (if already set) or fallback D7 path\n",
    "# Output: DX_OUT_ROOT and MANIFEST_ALL used by the rest of the cell\n",
    "# -------------------------\n",
    "# Rule:\n",
    "# - Prefer runtime variable DX_OUT_ROOT if already defined (allows reuse across cells).\n",
    "# - Otherwise use the D7 fallback path below.\n",
    "D7_OUT_ROOT_FALLBACK = \"/content/drive/MyDrive/AI_PD_Project/Datasets/D7-Multilingual (D1_D4_D5v2_D6)/preprocessed_v1\"\n",
    "DX_OUT_ROOT = str(globals().get(\"DX_OUT_ROOT\", D7_OUT_ROOT_FALLBACK))\n",
    "MANIFEST_ALL = f\"{DX_OUT_ROOT}/manifests/manifest_all.csv\"\n",
    "globals()[\"DX_OUT_ROOT\"] = DX_OUT_ROOT  # keep consistent across cells\n",
    "\n",
    "# -------------------------\n",
    "# Evaluation settings\n",
    "# Inputs: fixed constants (seeds, model checkpoint, batch sizes)\n",
    "# Output: consistent evaluation behavior across seeds and runs\n",
    "# -------------------------\n",
    "SEEDS          = [1337, 2024, 7777]\n",
    "BACKBONE_CKPT  = \"facebook/wav2vec2-base\"\n",
    "SR_EXPECTED    = 16000\n",
    "TINY_THRESH    = 1e-4\n",
    "\n",
    "EFFECTIVE_BS   = 64\n",
    "PER_DEVICE_BS  = 16\n",
    "GRAD_ACCUM     = max(1, EFFECTIVE_BS // PER_DEVICE_BS)\n",
    "\n",
    "DROPOUT_P      = 0.2\n",
    "\n",
    "NUM_WORKERS    = 0\n",
    "PIN_MEMORY     = False\n",
    "\n",
    "USE_AMP        = True  # evaluation autocast only\n",
    "VOWEL_TASK_VALUE = \"vowl\"\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "try:\n",
    "    torch.set_float32_matmul_precision(\"high\")\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", message=\"None of the inputs have requires_grad=True. Gradients will be None\")\n",
    "warnings.filterwarnings(\"ignore\", message=\"Passing `gradient_checkpointing` to a config initialization is deprecated\")\n",
    "\n",
    "print(\"DX_OUT_ROOT:\", DX_OUT_ROOT)\n",
    "print(\"MANIFEST_ALL:\", MANIFEST_ALL)\n",
    "print(\"DEVICE:\", DEVICE)\n",
    "if DEVICE.type == \"cuda\":\n",
    "    print(\"GPU:\", torch.cuda.get_device_name(0))\n",
    "print(\"PER_DEVICE_BS:\", PER_DEVICE_BS, \"| EFFECTIVE_BS:\", PER_DEVICE_BS * GRAD_ACCUM)\n",
    "print(\"NUM_WORKERS:\", NUM_WORKERS, \"| PIN_MEMORY:\", PIN_MEMORY)\n",
    "print(\"USE_AMP:\", bool(USE_AMP and DEVICE.type == \"cuda\"))\n",
    "\n",
    "# -------------------------\n",
    "# Load manifest and build the TEST table\n",
    "# Inputs: manifest_all.csv\n",
    "# Output: test_df (rows where split == \"test\"), plus dataset_id label for folder naming\n",
    "# -------------------------\n",
    "if not os.path.exists(MANIFEST_ALL):\n",
    "    raise FileNotFoundError(f\"Missing manifest_all.csv: {MANIFEST_ALL}\")\n",
    "\n",
    "m_all = pd.read_csv(MANIFEST_ALL)\n",
    "\n",
    "req_cols_core = {\"split\", \"clip_path\", \"label_num\", \"task\"}\n",
    "missing_core = [c for c in sorted(req_cols_core) if c not in m_all.columns]\n",
    "if missing_core:\n",
    "    raise ValueError(f\"Manifest missing required columns: {missing_core}. Found: {list(m_all.columns)}\")\n",
    "\n",
    "# Infer dataset_id (naming only)\n",
    "if \"dataset\" in m_all.columns and m_all[\"dataset\"].notna().any():\n",
    "    dataset_id = str(m_all[\"dataset\"].astype(str).value_counts(dropna=True).idxmax())\n",
    "    m_all = m_all[m_all[\"dataset\"].astype(str) == dataset_id].copy()\n",
    "else:\n",
    "    dataset_id = \"DX\"\n",
    "\n",
    "print(\"\\nDataset inferred:\", dataset_id)\n",
    "\n",
    "keep_cols = [\"clip_path\", \"label_num\", \"task\", \"speaker_id\", \"sex\", \"age\", \"duration_sec\", \"split\"]\n",
    "for c in keep_cols:\n",
    "    if c not in m_all.columns:\n",
    "        m_all[c] = np.nan\n",
    "m_all = m_all[keep_cols].copy()\n",
    "\n",
    "test_df = m_all[m_all[\"split\"].isin([\"test\"])].reset_index(drop=True)\n",
    "\n",
    "print(\"TEST rows:\", len(test_df))\n",
    "print(\"TEST label counts:\", test_df[\"label_num\"].value_counts(dropna=False).to_dict())\n",
    "print(\"TEST sex counts (raw):\", test_df[\"sex\"].value_counts(dropna=False).to_dict() if \"sex\" in test_df.columns else {})\n",
    "\n",
    "if len(test_df) == 0:\n",
    "    raise RuntimeError(\"After filtering to split=='test', manifest has 0 rows.\")\n",
    "\n",
    "# -------------------------\n",
    "# Fail fast if audio files are missing\n",
    "# Inputs: test_df.clip_path\n",
    "# Output: stops early with a few missing examples instead of failing mid-run\n",
    "# -------------------------\n",
    "def _fail_fast_missing_paths(df: pd.DataFrame, name: str):\n",
    "    missing_paths = []\n",
    "    for p in tqdm(df[\"clip_path\"].astype(str).tolist(), desc=f\"Check {name} clip_path exists\", dynamic_ncols=True):\n",
    "        if not os.path.exists(p):\n",
    "            missing_paths.append(p)\n",
    "            if len(missing_paths) >= 10:\n",
    "                break\n",
    "    if missing_paths:\n",
    "        raise FileNotFoundError(f\"{name}: missing clip_path(s). Examples: {missing_paths}\")\n",
    "\n",
    "_fail_fast_missing_paths(test_df, \"TEST\")\n",
    "\n",
    "# -------------------------\n",
    "# Add task group and normalize sex labels\n",
    "# Inputs: task, sex columns in the manifest\n",
    "# Output: test_df gains task_group and sex_norm for routing heads and fairness metrics\n",
    "# -------------------------\n",
    "def _task_group(task_val) -> str:\n",
    "    return \"vowel\" if str(task_val) == VOWEL_TASK_VALUE else \"other\"\n",
    "\n",
    "test_df[\"task_group\"] = test_df[\"task\"].apply(_task_group)\n",
    "\n",
    "def normalize_sex(val) -> str:\n",
    "    \"\"\"\n",
    "    Returns 'M', 'F', or 'UNK'.\n",
    "\n",
    "    Handles:\n",
    "    - common strings (male/female, m/f, etc.)\n",
    "    - numeric encodings:\n",
    "        0 -> F\n",
    "        1 -> M\n",
    "    \"\"\"\n",
    "    if pd.isna(val):\n",
    "        return \"UNK\"\n",
    "\n",
    "    try:\n",
    "        fv = float(val)\n",
    "        if np.isfinite(fv) and abs(fv - round(fv)) < 1e-9:\n",
    "            iv = int(round(fv))\n",
    "            if iv == 0:\n",
    "                return \"F\"\n",
    "            if iv == 1:\n",
    "                return \"M\"\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    s = str(val).strip().lower()\n",
    "    if s in {\"m\", \"male\", \"man\", \"masc\", \"masculine\"}:\n",
    "        return \"M\"\n",
    "    if s in {\"f\", \"female\", \"woman\", \"fem\", \"feminine\"}:\n",
    "        return \"F\"\n",
    "    return \"UNK\"\n",
    "\n",
    "test_df[\"sex_norm\"] = test_df[\"sex\"].apply(normalize_sex) if \"sex\" in test_df.columns else \"UNK\"\n",
    "print(\"TEST sex counts (normalized):\", test_df[\"sex_norm\"].value_counts(dropna=False).to_dict())\n",
    "if (test_df[\"sex_norm\"] == \"UNK\").any():\n",
    "    print(\"NOTE: Some 'sex' values could not be normalized to M/F and were counted as 'UNK' for fairness and sex charts.\")\n",
    "\n",
    "# -------------------------\n",
    "# Dataset and batch collator\n",
    "# Inputs: test_df and audio files on disk\n",
    "# Output: DataLoader batches with padding + attention masks, plus labels and metadata\n",
    "# -------------------------\n",
    "class AudioManifestDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Loads a clip and builds an attention mask in sample space.\n",
    "\n",
    "    Attention mask rule:\n",
    "    - vowel clips: mask trailing near-zeros to reduce impact of padded silence.\n",
    "    - other clips: full attention (all ones).\n",
    "    \"\"\"\n",
    "    def __init__(self, df: pd.DataFrame):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        row = self.df.iloc[idx]\n",
    "        clip_path = str(row[\"clip_path\"])\n",
    "        label = int(row[\"label_num\"])\n",
    "        task_group = str(row[\"task_group\"])\n",
    "        sex_norm = str(row[\"sex_norm\"])\n",
    "\n",
    "        if not os.path.exists(clip_path):\n",
    "            raise FileNotFoundError(f\"Missing clip_path: {clip_path}\")\n",
    "\n",
    "        y, sr = sf.read(clip_path, always_2d=False)\n",
    "        if isinstance(y, np.ndarray) and y.ndim > 1:\n",
    "            y = np.mean(y, axis=1)\n",
    "        y = np.asarray(y, dtype=np.float32)\n",
    "\n",
    "        if int(sr) != SR_EXPECTED:\n",
    "            raise ValueError(f\"Sample rate mismatch (expected {SR_EXPECTED}): {clip_path} has sr={sr}\")\n",
    "\n",
    "        attn = np.ones((len(y),), dtype=np.int64)\n",
    "\n",
    "        if task_group == \"vowel\":\n",
    "            k = -1\n",
    "            for j in range(len(y) - 1, -1, -1):\n",
    "                if abs(float(y[j])) > TINY_THRESH:\n",
    "                    k = j\n",
    "                    break\n",
    "            if k >= 0:\n",
    "                attn[:k+1] = 1\n",
    "                attn[k+1:] = 0\n",
    "            else:\n",
    "                attn[:] = 1\n",
    "        else:\n",
    "            attn[:] = 1\n",
    "\n",
    "        return {\n",
    "            \"input_values\": torch.from_numpy(y),                 # float32 [T]\n",
    "            \"attention_mask\": torch.from_numpy(attn),            # int64   [T]\n",
    "            \"labels\": torch.tensor(label, dtype=torch.long),     # int64   []\n",
    "            \"task_group\": task_group,                            # str\n",
    "            \"sex_norm\": sex_norm,                                # str\n",
    "        }\n",
    "\n",
    "def collate_fn(batch):\n",
    "    # Pads to the longest clip in the batch and pads attention_mask with zeros\n",
    "    max_len = int(max(b[\"input_values\"].numel() for b in batch))\n",
    "    input_vals, attn_masks, labels, task_groups, sex_norms = [], [], [], [], []\n",
    "    for b in batch:\n",
    "        x = b[\"input_values\"]\n",
    "        a = b[\"attention_mask\"]\n",
    "        pad = max_len - x.numel()\n",
    "        if pad > 0:\n",
    "            x = torch.cat([x, torch.zeros(pad, dtype=x.dtype)], dim=0)\n",
    "            a = torch.cat([a, torch.zeros(pad, dtype=a.dtype)], dim=0)\n",
    "        input_vals.append(x)\n",
    "        attn_masks.append(a)\n",
    "        labels.append(b[\"labels\"])\n",
    "        task_groups.append(b[\"task_group\"])\n",
    "        sex_norms.append(b[\"sex_norm\"])\n",
    "    return {\n",
    "        \"input_values\": torch.stack(input_vals, dim=0),\n",
    "        \"attention_mask\": torch.stack(attn_masks, dim=0),\n",
    "        \"labels\": torch.stack(labels, dim=0),\n",
    "        \"task_group\": task_groups,\n",
    "        \"sex_norm\": sex_norms,\n",
    "    }\n",
    "\n",
    "# -------------------------\n",
    "# Two-head classifier (inference only)\n",
    "# Inputs: Wav2Vec2 backbone checkpoint + loaded head weights\n",
    "# Output: logits per clip routed through vowel vs other head\n",
    "# -------------------------\n",
    "class Wav2Vec2TwoHeadClassifier(nn.Module):\n",
    "    def __init__(self, ckpt: str, dropout_p: float):\n",
    "        super().__init__()\n",
    "        self.backbone = Wav2Vec2Model.from_pretrained(ckpt, use_safetensors=True)\n",
    "        for p in self.backbone.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "        H = int(self.backbone.config.hidden_size)\n",
    "        self.pre_vowel = nn.Sequential(nn.LayerNorm(H), nn.Dropout(float(dropout_p)))\n",
    "        self.pre_other = nn.Sequential(nn.LayerNorm(H), nn.Dropout(float(dropout_p)))\n",
    "        self.head_vowel = nn.Linear(H, 2)\n",
    "        self.head_other = nn.Linear(H, 2)\n",
    "\n",
    "    def masked_mean_pool(self, last_hidden, attn_mask_samples):\n",
    "        # Converts sample-level mask to feature-level mask, then averages only valid frames\n",
    "        feat_mask = self.backbone._get_feature_vector_attention_mask(last_hidden.shape[1], attn_mask_samples)\n",
    "        feat_mask = feat_mask.to(last_hidden.device).unsqueeze(-1).type_as(last_hidden)\n",
    "        masked = last_hidden * feat_mask\n",
    "        denom = feat_mask.sum(dim=1).clamp(min=1.0)\n",
    "        return masked.sum(dim=1) / denom\n",
    "\n",
    "    def _heads_fp32(self, x_any: torch.Tensor, head: nn.Module) -> torch.Tensor:\n",
    "        # Runs heads in float32 for stable probabilities even when AMP is enabled\n",
    "        x = x_any.float()\n",
    "        if x.is_cuda:\n",
    "            with torch.amp.autocast(device_type=\"cuda\", enabled=False):\n",
    "                return head(x)\n",
    "        return head(x)\n",
    "\n",
    "    def forward_logits(self, input_values, attention_mask, task_group):\n",
    "        with torch.no_grad():\n",
    "            out = self.backbone(input_values=input_values, attention_mask=attention_mask)\n",
    "            last_hidden = out.last_hidden_state\n",
    "\n",
    "        pooled = self.masked_mean_pool(last_hidden, attention_mask)  # [B,H]\n",
    "\n",
    "        z_v = self.pre_vowel(pooled.float())\n",
    "        z_o = self.pre_other(pooled.float())\n",
    "\n",
    "        logits_v = self._heads_fp32(z_v, self.head_vowel)\n",
    "        logits_o = self._heads_fp32(z_o, self.head_other)\n",
    "\n",
    "        # Route each item to its matching head\n",
    "        is_vowel = torch.tensor([tg == \"vowel\" for tg in task_group], device=pooled.device, dtype=torch.bool)\n",
    "        logits = logits_o.clone()\n",
    "        if is_vowel.any():\n",
    "            logits[is_vowel] = logits_v[is_vowel]\n",
    "        return logits\n",
    "\n",
    "# -------------------------\n",
    "# Metrics, plots, and summary helpers\n",
    "# Inputs: y_true, y_prob, threshold\n",
    "# Output: AUROC, threshold metrics, ROC/CM images, mean±SD utilities\n",
    "# -------------------------\n",
    "def compute_auc(y_true, y_prob):\n",
    "    y_true = np.asarray(y_true, dtype=np.int64)\n",
    "    y_prob = np.asarray(y_prob, dtype=np.float64)\n",
    "    if len(np.unique(y_true)) < 2:\n",
    "        return float(\"nan\")\n",
    "    return float(roc_auc_score(y_true, y_prob))\n",
    "\n",
    "def compute_threshold_metrics(y_true, y_prob, thr=0.5):\n",
    "    y_true = np.asarray(y_true, dtype=np.int64)\n",
    "    y_prob = np.asarray(y_prob, dtype=np.float64)\n",
    "    y_pred = (y_prob >= float(thr)).astype(np.int64)\n",
    "\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=[0, 1])\n",
    "    tn, fp, fn, tp = int(cm[0, 0]), int(cm[0, 1]), int(cm[1, 0]), int(cm[1, 1])\n",
    "\n",
    "    eps = 1e-12\n",
    "    acc = (tp + tn) / max(1, (tp + tn + fp + fn))\n",
    "    prec = tp / (tp + fp + eps)\n",
    "    rec = tp / (tp + fn + eps)     # sensitivity\n",
    "    f1 = 2 * prec * rec / (prec + rec + eps)\n",
    "    spec = tn / (tn + fp + eps)\n",
    "\n",
    "    try:\n",
    "        mcc = float(matthews_corrcoef(y_true, y_pred)) if len(np.unique(y_pred)) > 1 else float(\"nan\")\n",
    "    except Exception:\n",
    "        mcc = float(\"nan\")\n",
    "\n",
    "    pval = float(\"nan\")\n",
    "    try:\n",
    "        from scipy.stats import fisher_exact  # type: ignore\n",
    "        _, pval = fisher_exact([[tn, fp], [fn, tp]], alternative=\"two-sided\")\n",
    "        pval = float(pval)\n",
    "    except Exception:\n",
    "        pval = float(\"nan\")\n",
    "\n",
    "    return {\n",
    "        \"threshold\": float(thr),\n",
    "        \"tn\": tn, \"fp\": fp, \"fn\": fn, \"tp\": tp,\n",
    "        \"accuracy\": float(acc),\n",
    "        \"precision\": float(prec),\n",
    "        \"recall\": float(rec),\n",
    "        \"f1\": float(f1),\n",
    "        \"sensitivity\": float(rec),\n",
    "        \"specificity\": float(spec),\n",
    "        \"mcc\": float(mcc),\n",
    "        \"p_value_fisher\": float(pval),\n",
    "    }\n",
    "\n",
    "def save_roc_curve_png(y_true, y_prob, out_png, title_suffix=\"Test\"):\n",
    "    fpr, tpr, _ = roc_curve(np.asarray(y_true, dtype=np.int64), np.asarray(y_prob, dtype=np.float64))\n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr)\n",
    "    plt.plot([0, 1], [0, 1], linestyle=\"--\")\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(f\"ROC Curve ({title_suffix})\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_png, dpi=150)\n",
    "    plt.close()\n",
    "\n",
    "def save_confusion_png(y_true, y_prob, out_png, thr=0.5, title_suffix=\"Test\"):\n",
    "    y_true = np.asarray(y_true, dtype=np.int64)\n",
    "    y_prob = np.asarray(y_prob, dtype=np.float64)\n",
    "    y_pred = (y_prob >= float(thr)).astype(int)\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=[0, 1])\n",
    "\n",
    "    plt.figure()\n",
    "    plt.imshow(cm)\n",
    "    plt.xticks([0, 1], [\"Healthy(0)\", \"PD(1)\"])\n",
    "    plt.yticks([0, 1], [\"Healthy(0)\", \"PD(1)\"])\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"True\")\n",
    "    plt.title(f\"Confusion Matrix ({title_suffix}, thr={thr:.4f})\")\n",
    "    for (i, j), v in np.ndenumerate(cm):\n",
    "        plt.text(j, i, str(v), ha=\"center\", va=\"center\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_png, dpi=150)\n",
    "    plt.close()\n",
    "\n",
    "def mean_sd(vals):\n",
    "    vals = np.asarray(vals, dtype=np.float64)\n",
    "    mu = float(np.nanmean(vals)) if np.any(~np.isnan(vals)) else float(\"nan\")\n",
    "    sd = float(np.nanstd(vals, ddof=1)) if np.sum(~np.isnan(vals)) > 1 else 0.0\n",
    "    return mu, sd\n",
    "\n",
    "# -------------------------\n",
    "# Fairness metric: FNR by sex and ΔFNR\n",
    "# Inputs: y_true, y_prob, sex_norm, threshold\n",
    "# Output: per-sex PD-only FNR and the difference F minus M\n",
    "# -------------------------\n",
    "def compute_fnr_by_sex_and_delta(y_true, y_prob, sex_norm, thr):\n",
    "    \"\"\"\n",
    "    FNR is computed on PD-only true labels (y_true==1):\n",
    "      FNR(sex) = FN/(FN+TP)\n",
    "    Returns:\n",
    "      fnr_by_sex: per-sex counts and FNR\n",
    "      delta_f_minus_m: FNR(F) - FNR(M)\n",
    "    \"\"\"\n",
    "    y_true = np.asarray(y_true, dtype=np.int64)\n",
    "    y_prob = np.asarray(y_prob, dtype=np.float64)\n",
    "    sex_norm = np.asarray(list(sex_norm), dtype=object)\n",
    "    y_pred = (y_prob >= float(thr)).astype(np.int64)\n",
    "\n",
    "    out = {}\n",
    "    for g in [\"M\", \"F\", \"UNK\"]:\n",
    "        mask_g = (sex_norm == g)\n",
    "        if int(mask_g.sum()) == 0:\n",
    "            continue\n",
    "        pos_mask = mask_g & (y_true == 1)\n",
    "        n_pos = int(pos_mask.sum())\n",
    "        if n_pos == 0:\n",
    "            out[g] = {\"n_total\": int(mask_g.sum()), \"n_pos\": 0, \"tp\": 0, \"fn\": 0, \"fnr\": float(\"nan\")}\n",
    "            continue\n",
    "        tp = int(((y_pred == 1) & pos_mask).sum())\n",
    "        fn = int(((y_pred == 0) & pos_mask).sum())\n",
    "        fnr = float(fn / max(1, (fn + tp)))\n",
    "        out[g] = {\"n_total\": int(mask_g.sum()), \"n_pos\": int(n_pos), \"tp\": int(tp), \"fn\": int(fn), \"fnr\": float(fnr)}\n",
    "\n",
    "    fnr_m = out.get(\"M\", {}).get(\"fnr\", float(\"nan\"))\n",
    "    fnr_f = out.get(\"F\", {}).get(\"fnr\", float(\"nan\"))\n",
    "    if (not np.isnan(fnr_m)) and (not np.isnan(fnr_f)):\n",
    "        delta = float(fnr_f - fnr_m)\n",
    "    else:\n",
    "        delta = float(\"nan\")\n",
    "\n",
    "    return out, delta\n",
    "\n",
    "# -------------------------\n",
    "# Seed control\n",
    "# Inputs: seed integer\n",
    "# Output: repeatable dataloader order and model behavior for that seed\n",
    "# -------------------------\n",
    "def set_all_seeds(seed: int):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# -------------------------\n",
    "# Build the TEST DataLoader + quick warm-up\n",
    "# Inputs: test_df and batching settings\n",
    "# Output: test_loader ready for inference; warm-up catches shape/path issues early\n",
    "# -------------------------\n",
    "test_ds = AudioManifestDataset(test_df)\n",
    "test_loader = DataLoader(\n",
    "    test_ds,\n",
    "    batch_size=PER_DEVICE_BS,\n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=PIN_MEMORY,\n",
    "    collate_fn=collate_fn,\n",
    ")\n",
    "\n",
    "print(\"\\nWarm-up: loading up to 3 TEST batches...\")\n",
    "t0 = time.time()\n",
    "nb = len(test_loader)\n",
    "wb = min(3, nb)\n",
    "if wb == 0:\n",
    "    raise RuntimeError(\"TEST DataLoader has 0 batches. Check test_df length and PER_DEVICE_BS.\")\n",
    "it = iter(test_loader)\n",
    "for i in range(wb):\n",
    "    _ = next(it)\n",
    "    print(f\"  loaded warmup TEST batch {i+1}/{wb}\")\n",
    "print(f\"Warm-up done in {time.time()-t0:.2f}s\")\n",
    "\n",
    "# -------------------------\n",
    "# Choose the most recent finished train+val experiment\n",
    "# Inputs: trainval_runs/exp_* folders under DX_OUT_ROOT\n",
    "# Output: chosen_exp and chosen_summary with thresholds and head checkpoints for all seeds\n",
    "# -------------------------\n",
    "TRAINVAL_ROOT = Path(DX_OUT_ROOT) / \"trainval_runs\"\n",
    "if not TRAINVAL_ROOT.exists():\n",
    "    raise FileNotFoundError(f\"Missing trainval_runs folder: {str(TRAINVAL_ROOT)}\")\n",
    "\n",
    "exp_dirs = sorted([p for p in TRAINVAL_ROOT.glob(\"exp_*\") if p.is_dir()], key=lambda p: p.stat().st_mtime, reverse=True)\n",
    "if not exp_dirs:\n",
    "    raise FileNotFoundError(f\"No exp_* folders found under: {str(TRAINVAL_ROOT)}\")\n",
    "\n",
    "def _has_all_seeds(exp_path: Path, dataset_id: str, seeds: list):\n",
    "    # Checks that each seed has a saved best_heads.pt\n",
    "    for s in seeds:\n",
    "        p = exp_path / f\"run_{dataset_id}_seed{s}\" / \"best_heads.pt\"\n",
    "        if not p.exists():\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def _load_trainval_summary(exp_path: Path) -> dict:\n",
    "    # Loads summary_trainval.json if present and readable\n",
    "    sp = exp_path / \"summary_trainval.json\"\n",
    "    if not sp.exists():\n",
    "        return {}\n",
    "    try:\n",
    "        with open(sp, \"r\", encoding=\"utf-8\") as f:\n",
    "            return json.load(f)\n",
    "    except Exception:\n",
    "        return {}\n",
    "\n",
    "def _summary_has_threshold_keys(summary: dict) -> bool:\n",
    "    # Ensures the threshold fields needed for evaluation are present\n",
    "    try:\n",
    "        v = summary[\"val_optimal_threshold\"]\n",
    "        _ = v[\"by_seed\"]\n",
    "        _ = v[\"mean_sd\"][\"mean\"]\n",
    "        _ = v[\"mean_sd\"][\"sd\"]\n",
    "        return True\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "chosen_exp = None\n",
    "chosen_summary = None\n",
    "\n",
    "for ed in exp_dirs:\n",
    "    if not _has_all_seeds(ed, dataset_id, SEEDS):\n",
    "        continue\n",
    "    summ = _load_trainval_summary(ed)\n",
    "    if not _summary_has_threshold_keys(summ):\n",
    "        continue\n",
    "    chosen_exp = ed\n",
    "    chosen_summary = summ\n",
    "    break\n",
    "\n",
    "if chosen_exp is None or chosen_summary is None:\n",
    "    sample = exp_dirs[0]\n",
    "    raise FileNotFoundError(\n",
    "        \"Could not find a recent trainval experiment with:\\n\"\n",
    "        \"  - all 3 best_heads.pt files\\n\"\n",
    "        \"  - a readable summary_trainval.json\\n\"\n",
    "        \"  - val_optimal_threshold.by_seed + mean_sd.mean + mean_sd.sd\\n\"\n",
    "        f\"Expected under: {str(TRAINVAL_ROOT)}/exp_*/\\n\"\n",
    "        f\"Most recent exp checked: {str(sample)}\"\n",
    "    )\n",
    "\n",
    "print(\"\\nUsing Train+Val experiment folder:\")\n",
    "print(\" \", str(chosen_exp))\n",
    "\n",
    "# -------------------------\n",
    "# Load VAL-opt thresholds and choose the shared TEST threshold\n",
    "# Inputs: chosen_summary[\"val_optimal_threshold\"]\n",
    "# Output: TEST_THR_MEAN_VAL_OPT used for all seeds on test\n",
    "# -------------------------\n",
    "val_opt = chosen_summary[\"val_optimal_threshold\"]\n",
    "thr_by_seed = {str(k): float(v) for k, v in (val_opt.get(\"by_seed\") or {}).items()}\n",
    "thr_mean = float(val_opt.get(\"mean_sd\", {}).get(\"mean\", float(\"nan\")))\n",
    "thr_sd   = float(val_opt.get(\"mean_sd\", {}).get(\"sd\", float(\"nan\")))\n",
    "\n",
    "print(\"\\nVAL-opt thresholds loaded from trainval summary_trainval.json:\")\n",
    "print(\"  method:\", str(val_opt.get(\"method\", \"unknown\")))\n",
    "print(\"  by_seed:\", {k: (f\"{v:.6f}\" if not np.isnan(v) else \"nan\") for k, v in thr_by_seed.items()})\n",
    "print(\"  mean ± SD:\", (f\"{thr_mean:.6f} ± {thr_sd:.6f}\" if not np.isnan(thr_mean) else f\"nan ± {thr_sd:.6f}\"))\n",
    "\n",
    "TEST_THR_SOURCE = \"trainval summary_trainval.json -> val_optimal_threshold.mean_sd.mean (canonical aggregate)\"\n",
    "TEST_THR_MEAN_VAL_OPT = float(thr_mean)\n",
    "TEST_THR_NOTE = None\n",
    "\n",
    "if np.isnan(TEST_THR_MEAN_VAL_OPT):\n",
    "    # Fallback only if the canonical mean is missing\n",
    "    vals = []\n",
    "    for s in SEEDS:\n",
    "        v = thr_by_seed.get(str(s), float(\"nan\"))\n",
    "        if not np.isnan(v):\n",
    "            vals.append(float(v))\n",
    "    if len(vals) > 0:\n",
    "        TEST_THR_MEAN_VAL_OPT = float(np.mean(vals))\n",
    "        TEST_THR_NOTE = \"val_optimal_threshold.mean_sd.mean was NaN. Fallback used: mean(thr_by_seed over available seeds).\"\n",
    "    else:\n",
    "        TEST_THR_MEAN_VAL_OPT = 0.5\n",
    "        TEST_THR_NOTE = \"val_optimal_threshold.mean_sd.mean was NaN and thr_by_seed had no usable values. Fallback used: thr=0.5.\"\n",
    "\n",
    "print(\"\\nTEST threshold policy:\")\n",
    "print(\"  Using MEAN VAL-opt threshold for ALL seeds/tests:\", f\"{TEST_THR_MEAN_VAL_OPT:.6f}\")\n",
    "print(\"  Source:\", TEST_THR_SOURCE)\n",
    "if TEST_THR_NOTE is not None:\n",
    "    print(\"  NOTE:\", TEST_THR_NOTE)\n",
    "\n",
    "# -------------------------\n",
    "# Output folder for test runs\n",
    "# Inputs: DX_OUT_ROOT\n",
    "# Output: multilingual_test_runs/ with one folder per seed plus summary files\n",
    "# -------------------------\n",
    "TEST_ROOT = Path(DX_OUT_ROOT) / \"multilingual_test_runs\"\n",
    "TEST_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# -------------------------\n",
    "# Load saved heads into the model\n",
    "# Inputs: best_heads.pt from the chosen train+val run\n",
    "# Output: model with the trained head weights ready for inference\n",
    "# -------------------------\n",
    "def load_heads_into_model(model: Wav2Vec2TwoHeadClassifier, best_heads_path: Path):\n",
    "    if not best_heads_path.exists():\n",
    "        raise FileNotFoundError(f\"Missing best_heads.pt: {str(best_heads_path)}\")\n",
    "    state = torch.load(str(best_heads_path), map_location=\"cpu\")\n",
    "    model.pre_vowel.load_state_dict(state[\"pre_vowel\"], strict=True)\n",
    "    model.pre_other.load_state_dict(state[\"pre_other\"], strict=True)\n",
    "    model.head_vowel.load_state_dict(state[\"head_vowel\"], strict=True)\n",
    "    model.head_other.load_state_dict(state[\"head_other\"], strict=True)\n",
    "    return model\n",
    "\n",
    "# -------------------------\n",
    "# Inference loop\n",
    "# Inputs: DataLoader + model\n",
    "# Output: arrays of true labels, PD probabilities, and sex labels (for fairness splits)\n",
    "# -------------------------\n",
    "def infer_probs(loader, model, desc):\n",
    "    use_amp = bool(USE_AMP and DEVICE.type == \"cuda\")\n",
    "    all_probs, all_true, all_sex = [], [], []\n",
    "\n",
    "    pbar = tqdm(loader, desc=desc, dynamic_ncols=True)\n",
    "    with torch.inference_mode():\n",
    "        for batch in pbar:\n",
    "            input_values = batch[\"input_values\"].to(DEVICE, non_blocking=False)\n",
    "            attention_mask = batch[\"attention_mask\"].to(DEVICE, non_blocking=False)\n",
    "            labels = batch[\"labels\"].to(DEVICE, non_blocking=False)\n",
    "            task_group = batch[\"task_group\"]\n",
    "            sex_norm = batch[\"sex_norm\"]\n",
    "\n",
    "            with torch.amp.autocast(device_type=\"cuda\", enabled=use_amp):\n",
    "                logits = model.forward_logits(input_values, attention_mask, task_group)\n",
    "\n",
    "            probs = torch.softmax(logits.float(), dim=-1)[:, 1].detach().cpu().numpy().astype(np.float64)\n",
    "            all_probs.extend(probs.tolist())\n",
    "            all_true.extend(labels.detach().cpu().numpy().astype(np.int64).tolist())\n",
    "            all_sex.extend(list(sex_norm))\n",
    "\n",
    "    return (\n",
    "        np.asarray(all_true, dtype=np.int64),\n",
    "        np.asarray(all_probs, dtype=np.float64),\n",
    "        np.asarray(all_sex, dtype=object),\n",
    "    )\n",
    "\n",
    "# -------------------------\n",
    "# Per-seed evaluation and artifact writing\n",
    "# Inputs: a seed, chosen train+val experiment, shared threshold\n",
    "# Outputs: run_<dataset>_seedXXXX/metrics.json + ROC/CM plots (overall + by sex)\n",
    "# -------------------------\n",
    "def run_test_once(seed: int):\n",
    "    set_all_seeds(seed)\n",
    "\n",
    "    run_dir = TEST_ROOT / f\"run_{dataset_id}_seed{seed}\"\n",
    "    run_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    best_heads_path = chosen_exp / f\"run_{dataset_id}_seed{seed}\" / \"best_heads.pt\"\n",
    "    thr_used = float(TEST_THR_MEAN_VAL_OPT)\n",
    "\n",
    "    print(f\"\\n[seed={seed}] Loading model + heads from:\")\n",
    "    print(\" \", str(best_heads_path))\n",
    "    print(f\"[seed={seed}] Using TEST threshold (shared mean VAL-opt): {thr_used:.6f}\")\n",
    "    if TEST_THR_NOTE is not None:\n",
    "        print(f\"[seed={seed}] NOTE: {TEST_THR_NOTE}\")\n",
    "\n",
    "    model = Wav2Vec2TwoHeadClassifier(BACKBONE_CKPT, dropout_p=DROPOUT_P).to(DEVICE)\n",
    "    model = load_heads_into_model(model, best_heads_path)\n",
    "    model.eval()\n",
    "\n",
    "    yt_true, yt_prob, yt_sex = infer_probs(test_loader, model, desc=f\"[seed={seed}] Test\")\n",
    "    test_auc = compute_auc(yt_true, yt_prob)\n",
    "\n",
    "    thr_metrics_test = compute_threshold_metrics(yt_true, yt_prob, thr=thr_used)\n",
    "    fnr_by_sex, delta_f_minus_m = compute_fnr_by_sex_and_delta(yt_true, yt_prob, yt_sex, thr=thr_used)\n",
    "\n",
    "    # Plots (overall)\n",
    "    roc_png = run_dir / \"roc_curve.png\"\n",
    "    cm_png  = run_dir / \"confusion_matrix.png\"\n",
    "    save_roc_curve_png(yt_true, yt_prob, str(roc_png), title_suffix=f\"Test (seed={seed})\")\n",
    "    save_confusion_png(yt_true, yt_prob, str(cm_png), thr=thr_used, title_suffix=f\"Test (seed={seed})\")\n",
    "\n",
    "    # Plots (by sex)\n",
    "    cm_m_png = None\n",
    "    cm_f_png = None\n",
    "    mask_m = (yt_sex == \"M\")\n",
    "    mask_f = (yt_sex == \"F\")\n",
    "\n",
    "    if int(mask_m.sum()) > 0:\n",
    "        cm_m_png = run_dir / \"confusion_matrix_M.png\"\n",
    "        save_confusion_png(yt_true[mask_m], yt_prob[mask_m], str(cm_m_png), thr=thr_used, title_suffix=f\"Test SEX=M (seed={seed})\")\n",
    "\n",
    "    if int(mask_f.sum()) > 0:\n",
    "        cm_f_png = run_dir / \"confusion_matrix_F.png\"\n",
    "        save_confusion_png(yt_true[mask_f], yt_prob[mask_f], str(cm_f_png), thr=thr_used, title_suffix=f\"Test SEX=F (seed={seed})\")\n",
    "\n",
    "    metrics = {\n",
    "        \"dataset\": dataset_id,\n",
    "        \"seed\": int(seed),\n",
    "\n",
    "        \"dx_out_root\": DX_OUT_ROOT,\n",
    "        \"manifest_all\": MANIFEST_ALL,\n",
    "\n",
    "        \"trainval_experiment_used\": str(chosen_exp),\n",
    "        \"best_heads_path\": str(best_heads_path),\n",
    "\n",
    "        \"n_test\": int(len(test_df)),\n",
    "        \"label_counts_test\": test_df[\"label_num\"].value_counts(dropna=False).to_dict(),\n",
    "        \"sex_counts_test_norm\": test_df[\"sex_norm\"].value_counts(dropna=False).to_dict(),\n",
    "\n",
    "        \"test_auroc\": float(test_auc),\n",
    "\n",
    "        \"threshold_source\": TEST_THR_SOURCE,\n",
    "        \"val_optimal_threshold_canonical\": {\n",
    "            \"method\": str(val_opt.get(\"method\", \"unknown\")),\n",
    "            \"by_seed\": {k: float(v) for k, v in thr_by_seed.items()},\n",
    "            \"mean_sd\": {\"mean\": float(thr_mean), \"sd\": float(thr_sd)},\n",
    "        },\n",
    "        \"test_threshold_used\": float(thr_used),\n",
    "        \"test_threshold_note\": (TEST_THR_NOTE if TEST_THR_NOTE is not None else \"\"),\n",
    "\n",
    "        \"threshold_metrics_test_at_val_opt\": thr_metrics_test,\n",
    "\n",
    "        \"fairness_test_at_val_opt\": {\n",
    "            \"definition\": \"H3: ΔFNR = FNR(F) - FNR(M), where FNR(sex)=FN/(FN+TP) computed on PD-only true labels at test_threshold_used.\",\n",
    "            \"threshold_used\": float(thr_used),\n",
    "            \"fnr_by_sex_norm\": fnr_by_sex,\n",
    "            \"delta_fnr_F_minus_M\": float(delta_f_minus_m),\n",
    "            \"note\": \"If n_PD for a sex is 0, its FNR is NaN and ΔFNR is NaN.\",\n",
    "            \"sex_normalization_note\": \"Normalizer accepts numeric 0->F, 1->M and common strings; otherwise UNK.\",\n",
    "        },\n",
    "\n",
    "        \"artifacts\": {\n",
    "            \"roc_curve_png\": str(roc_png),\n",
    "            \"confusion_matrix_png\": str(cm_png),\n",
    "            \"confusion_matrix_M_png\": str(cm_m_png) if cm_m_png is not None else None,\n",
    "            \"confusion_matrix_F_png\": str(cm_f_png) if cm_f_png is not None else None,\n",
    "        },\n",
    "\n",
    "        \"backbone_ckpt\": BACKBONE_CKPT,\n",
    "        \"dropout_p\": float(DROPOUT_P),\n",
    "        \"use_amp\": bool(USE_AMP and DEVICE.type == \"cuda\"),\n",
    "        \"timestamp\": time.strftime(\"%Y%m%d_%H%M%S\"),\n",
    "    }\n",
    "\n",
    "    metrics_path = run_dir / \"metrics.json\"\n",
    "    with open(metrics_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(metrics, f, indent=2)\n",
    "\n",
    "    print(f\"[seed={seed}] DONE | test_AUROC={test_auc:.6f} | thr_used={thr_used:.6f}\")\n",
    "    print(f\"[seed={seed}] WROTE:\")\n",
    "    print(\" \", str(metrics_path))\n",
    "    print(\" \", str(roc_png))\n",
    "    print(\" \", str(cm_png))\n",
    "    if cm_m_png is not None:\n",
    "        print(\" \", str(cm_m_png))\n",
    "    if cm_f_png is not None:\n",
    "        print(\" \", str(cm_f_png))\n",
    "\n",
    "    return {\n",
    "        \"seed\": int(seed),\n",
    "        \"thr_used\": float(thr_used),\n",
    "        \"test_auc\": float(test_auc),\n",
    "        \"thr_metrics\": thr_metrics_test,\n",
    "        \"fnr_by_sex\": fnr_by_sex,\n",
    "        \"delta_f_minus_m\": float(delta_f_minus_m),\n",
    "        \"run_dir\": str(run_dir),\n",
    "    }\n",
    "\n",
    "# -------------------------\n",
    "# Run all seeds and aggregate results\n",
    "# Inputs: SEEDS list\n",
    "# Outputs: per-seed artifacts + summary_test.json + history_index.jsonl\n",
    "# -------------------------\n",
    "results = []\n",
    "for seed in SEEDS:\n",
    "    results.append(run_test_once(seed))\n",
    "\n",
    "# ---- AUROC aggregation (TEST)\n",
    "aurocs = [r[\"test_auc\"] for r in results]\n",
    "t_crit = 4.302652729911275  # df=2, 95% CI\n",
    "n = len(aurocs)\n",
    "mean_auc = float(np.mean(aurocs))\n",
    "std_auc = float(np.std(aurocs, ddof=1)) if n > 1 else 0.0\n",
    "half_width = float(t_crit * (std_auc / math.sqrt(n))) if n > 1 else 0.0\n",
    "ci95 = [float(mean_auc - half_width), float(mean_auc + half_width)]\n",
    "\n",
    "# ---- threshold metrics aggregation @ shared threshold\n",
    "metric_keys = [\"accuracy\",\"precision\",\"recall\",\"f1\",\"sensitivity\",\"specificity\",\"mcc\",\"p_value_fisher\"]\n",
    "metric_agg = {}\n",
    "for k in metric_keys:\n",
    "    vals = [float(r[\"thr_metrics\"].get(k, float(\"nan\"))) for r in results]\n",
    "    mu, sd = mean_sd(vals)\n",
    "    metric_agg[k] = {\n",
    "        \"mean\": float(mu),\n",
    "        \"sd\": float(sd),\n",
    "        \"by_seed\": {str(r[\"seed\"]): float(r[\"thr_metrics\"].get(k, float(\"nan\"))) for r in results},\n",
    "    }\n",
    "\n",
    "# ---- fairness aggregation @ shared threshold\n",
    "delta_vals = [float(r[\"delta_f_minus_m\"]) for r in results]\n",
    "delta_mean, delta_sd = mean_sd(delta_vals)\n",
    "\n",
    "fnr_m_vals = []\n",
    "fnr_f_vals = []\n",
    "n_pd_m_vals = []\n",
    "n_pd_f_vals = []\n",
    "for r in results:\n",
    "    d = r[\"fnr_by_sex\"] or {}\n",
    "    fnr_m_vals.append(float(d.get(\"M\", {}).get(\"fnr\", float(\"nan\"))))\n",
    "    fnr_f_vals.append(float(d.get(\"F\", {}).get(\"fnr\", float(\"nan\"))))\n",
    "    n_pd_m_vals.append(float(d.get(\"M\", {}).get(\"n_pos\", float(\"nan\"))))\n",
    "    n_pd_f_vals.append(float(d.get(\"F\", {}).get(\"n_pos\", float(\"nan\"))))\n",
    "\n",
    "fnr_m_mean, fnr_m_sd = mean_sd(fnr_m_vals)\n",
    "fnr_f_mean, fnr_f_sd = mean_sd(fnr_f_vals)\n",
    "\n",
    "print(\"\\nTest AUROC by seed:\")\n",
    "for r in results:\n",
    "    print(f\"  seed {r['seed']}: {r['test_auc']:.6f}\")\n",
    "print(\"\\nMean Test AUROC:\", f\"{mean_auc:.6f}\")\n",
    "print(f\"95% CI (t, n=3): [{ci95[0]:.6f}, {ci95[1]:.6f}]\")\n",
    "\n",
    "print(\"\\nTEST threshold used (shared mean VAL-opt):\", f\"{TEST_THR_MEAN_VAL_OPT:.6f}\")\n",
    "if TEST_THR_NOTE is not None:\n",
    "    print(\"NOTE:\", TEST_THR_NOTE)\n",
    "\n",
    "print(\"\\nThreshold metrics on TEST @ shared mean VAL-opt threshold (mean ± SD across seeds):\")\n",
    "for k in [\"accuracy\",\"precision\",\"sensitivity\",\"specificity\",\"f1\",\"mcc\"]:\n",
    "    print(f\"  {k}: {metric_agg[k]['mean']:.6f} ± {metric_agg[k]['sd']:.6f}\")\n",
    "\n",
    "print(\"\\nFAIRNESS (H3) on TEST @ shared threshold (mean ± SD across seeds):\")\n",
    "print(f\"  FNR_M: {fnr_m_mean:.6f} ± {fnr_m_sd:.6f}\")\n",
    "print(f\"  FNR_F: {fnr_f_mean:.6f} ± {fnr_f_sd:.6f}\")\n",
    "print(f\"  ΔFNR (F-M): {delta_mean:.6f} ± {delta_sd:.6f}\")\n",
    "\n",
    "summary = {\n",
    "    \"dataset\": dataset_id,\n",
    "    \"dx_out_root\": DX_OUT_ROOT,\n",
    "    \"manifest_all\": MANIFEST_ALL,\n",
    "    \"trainval_experiment_used\": str(chosen_exp),\n",
    "    \"seeds\": SEEDS,\n",
    "\n",
    "    \"n_test\": int(len(test_df)),\n",
    "    \"label_counts_test\": test_df[\"label_num\"].value_counts(dropna=False).to_dict(),\n",
    "    \"sex_counts_test_norm\": test_df[\"sex_norm\"].value_counts(dropna=False).to_dict(),\n",
    "\n",
    "    \"val_optimal_threshold\": {\n",
    "        \"source\": \"trainval summary_trainval.json\",\n",
    "        \"method\": str(val_opt.get(\"method\", \"unknown\")),\n",
    "        \"by_seed\": {str(k): float(v) for k, v in thr_by_seed.items()},\n",
    "        \"mean_sd\": {\"mean\": float(thr_mean), \"sd\": float(thr_sd)},\n",
    "    },\n",
    "\n",
    "    \"test_threshold_used\": {\n",
    "        \"policy\": \"mean_val_optimal_threshold_shared_across_seeds\",\n",
    "        \"threshold_used\": float(TEST_THR_MEAN_VAL_OPT),\n",
    "        \"note\": (TEST_THR_NOTE if TEST_THR_NOTE is not None else \"\"),\n",
    "        \"source\": TEST_THR_SOURCE,\n",
    "    },\n",
    "\n",
    "    \"test_aurocs_by_seed\": {str(r[\"seed\"]): float(r[\"test_auc\"]) for r in results},\n",
    "    \"mean_test_auroc\": float(mean_auc),\n",
    "    \"t_crit_df2_95\": float(t_crit),\n",
    "    \"half_width_95_ci\": float(half_width),\n",
    "    \"ci95_test_auroc\": ci95,\n",
    "\n",
    "    \"threshold_metrics_test_at_val_opt_mean_sd\": metric_agg,\n",
    "\n",
    "    \"fairness_test_at_val_opt\": {\n",
    "        \"definition\": \"H3: ΔFNR = FNR(F) - FNR(M), where FNR(sex)=FN/(FN+TP) computed on PD-only true labels at the shared mean VAL-opt threshold.\",\n",
    "        \"delta_fnr_F_minus_M_by_seed\": {str(r[\"seed\"]): float(r[\"delta_f_minus_m\"]) for r in results},\n",
    "        \"delta_fnr_F_minus_M_mean_sd\": {\"mean\": float(delta_mean), \"sd\": float(delta_sd)},\n",
    "        \"fnr_M_mean_sd\": {\"mean\": float(fnr_m_mean), \"sd\": float(fnr_m_sd), \"by_seed\": {str(SEEDS[i]): float(fnr_m_vals[i]) for i in range(len(SEEDS))}},\n",
    "        \"fnr_F_mean_sd\": {\"mean\": float(fnr_f_mean), \"sd\": float(fnr_f_sd), \"by_seed\": {str(SEEDS[i]): float(fnr_f_vals[i]) for i in range(len(SEEDS))}},\n",
    "        \"denominators_PD_by_seed\": {str(SEEDS[i]): {\"n_PD_M\": float(n_pd_m_vals[i]), \"n_PD_F\": float(n_pd_f_vals[i])} for i in range(len(SEEDS))},\n",
    "        \"sex_normalization_note\": \"Normalizer accepts numeric 0->F, 1->M and common strings; otherwise UNK.\",\n",
    "    },\n",
    "\n",
    "    \"run_dirs\": [r[\"run_dir\"] for r in results],\n",
    "    \"timestamp\": time.strftime(\"%Y%m%d_%H%M%S\"),\n",
    "}\n",
    "\n",
    "summary_path = TEST_ROOT / \"summary_test.json\"\n",
    "with open(summary_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "history_path = TEST_ROOT / \"history_index.jsonl\"\n",
    "with open(history_path, \"a\", encoding=\"utf-8\") as f:\n",
    "    f.write(json.dumps(summary) + \"\\n\")\n",
    "\n",
    "print(\"\\nWROTE summary:\", str(summary_path))\n",
    "print(\"APPENDED history index:\", str(history_path))\n",
    "print(\"Open this folder to access artifacts:\", str(TEST_ROOT))\n",
    "\n",
    "# -------------------------\n",
    "# Stop the runtime to release the GPU\n",
    "# Inputs: Colab runtime module (if available)\n",
    "# Output: ends the session cleanly when running on paid GPU instances\n",
    "# -------------------------\n",
    "print(\"\\nAll done. Unassigning the runtime to stop the L4 instance...\")\n",
    "try:\n",
    "    from google.colab import runtime  # type: ignore\n",
    "    print(\"Calling runtime.unassign() now.\")\n",
    "    runtime.unassign()\n",
    "except Exception as e:\n",
    "    print(\"Could not unassign runtime automatically. You can stop the runtime manually in Colab.\")\n",
    "    print(\"Reason:\", repr(e))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "972ca25fea0c4666aa0930ae712e53dc",
      "1fe5867f33a1473ab0e6528045a9cc0f",
      "666f01173a0b4b6082895a9c0ff3ee52",
      "1b0ad1593a4f4b2490f386a71836cc06",
      "8dc30088d5354279a2fd0598bc6e982e",
      "964d97e1f53f48308ded5ada41016172",
      "833ddf36c76e441f8f3a202ced808f0d",
      "d1bffd86035c4812bff7980ccd8214ff",
      "2fdb8eb1b0e34fdbb07384bb5a3be57c",
      "3383862d3fb540f9a4c0ae913de05093",
      "20feee8698e54ba4acb96c795cce1cf3",
      "96a41c4b1a404608a1f2a6805c456a7c",
      "6dd365ba336a43e8a2f4031eedadd1f8",
      "c886ebd0645b40b4b21ffa6a45e49053",
      "7f5eb51370484f04942e53c71502b4da",
      "939fe719354749a68e07a10167e45bee",
      "f3c8c181d2c04217b1768281d347367d",
      "fa99177f03a74ba39c1ce2948804cf01",
      "bb3e420a69e241ce90a9b2c16370fa40",
      "1a381edd3f1443c7ae1d6d5a10a5510c",
      "608ca8c851794e3897efc1527b4bcd87",
      "acf1378fa96344a5a291232b0d030d7e",
      "bc52c382a9a444378b1075977e482133",
      "eebcd5c0ed10480f82ad69453e69d22b",
      "dcc1efb0ba9c42d79dec8ec897d88bc4",
      "b250bd4bdf0e494fbf06ffb553a1ca41",
      "4986d7b37f2647afa53c5646bee7c406",
      "97382ba8afe941d38cf0bea75f8af8a5",
      "603ad376b707421f89b9202dc4949e08",
      "4b69e4d3bef34a56844fca47c24ba9c9",
      "1f328ffec95948679aa871395213e50b",
      "722145dd29c54984a0a5ed6f49b29e07",
      "12bd5c72c3db4505a8e23cd7788762eb",
      "0ff4d8abdd764440a08507d5f810d5e2",
      "027beb15aa5648b2b11025859b9bd013",
      "4a312808bf4b43b08946ffefef9be3b3",
      "340c647234144c69b1ea63a1f2fd4452",
      "4832a458fde54cbab7b3bf13fa49b460",
      "70a87e34427c42029266512694aad12b",
      "62c9e00711074ec19f775d70369f6d15",
      "2ff7e0693ab545bdb1610dfdcf35fd0f",
      "d0160ba20fe3429e8be992495274dd4a",
      "7cf80a6534f14294ba834c34461080ac",
      "eeb97865be424edd8a91ab2969621e09"
     ]
    },
    "id": "1hq5_j2FESvh"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "The following cell runs a test only evaluation of the fixed D7 Base model on the D2 test split and saves fully traceable outputs, including predictions, plots, and summary metrics. It reads `manifest_all.csv` from D2, keeps only rows where `split == \"test\"`, checks that required columns are present, including sex and age, and verifies that every audio file path exists. It also creates two simple grouping fields used later in reporting: `task_group`, which is set to “vowel” when the task is `vowl` and “other” otherwise, and `sex_norm`, which is normalized to M, F, or UNK using a robust rule.\n",
    "\n",
    "The model used here matches the training setup. The cell rebuilds a frozen Wav2Vec2 backbone with two small classification heads, one for vowel clips and one for other clips. It then loads the trained head weights from one fixed, preselected D7 train and validation experiment folder, using one `best_heads.pt` file per seed. The cell also loads a single global decision threshold from that experiment’s `summary_trainval.json`, using `val_optimal_threshold.mean_sd.mean`. This same threshold is applied to all three test runs, one per seed. If the stored mean threshold is missing, the cell falls back to averaging the available per seed thresholds, and only uses 0.5 as a last resort.\n",
    "\n",
    "Inference is then run on the D2 test set for each of the three seeds, 1337, 2024, and 7777, producing a Parkinson’s disease probability score for every clip. For each seed, the cell computes AUROC on the full test set, threshold based metrics at the shared threshold including the confusion matrix, accuracy, precision, sensitivity or recall, specificity, F1 score, MCC, and the Fisher exact test p value, and a fairness metric using the H3 definition. Fairness is reported as ΔFNR, defined as FNR(F) minus FNR(M), where the false negative rate is calculated only on true Parkinson’s cases as FN divided by FN plus TP, using the normalized sex labels.\n",
    "\n",
    "To support later analysis and writing, the cell saves both per clip outputs and visual artifacts. For each seed, it writes a `predictions.csv` file that includes the clip path, true label, predicted score, normalized sex, speaker ID, task group, seed, the source train and validation experiment tag, the run timestamp, and the global threshold used. It also saves ROC curve images and confusion matrix images, including confusion matrices split by sex when both M and F samples are available. All outputs are written to two locations: a stable tag based folder that always reflects the latest run for that tag, and a run stamped folder that preserves a snapshot of the specific execution.\n",
    "\n",
    "After all three seeds finish, the cell aggregates results across seeds and writes a single `summary_test.json` file. This summary reports the mean AUROC with a 95 percent confidence interval using a t distribution with n equal to 3, the mean and standard deviation of the threshold based metrics, and the mean and standard deviation of ΔFNR along with the underlying FNR values for M and F. The summary is appended to a `history_index.jsonl` log for tracking runs over time. The cell also writes small pointer JSON files to help locate the latest run, writes builder aligned config and log stubs with backup if existing files are found, prints the key output locations, and finally unassigns the Colab runtime to stop GPU usage."
   ],
   "metadata": {
    "id": "yUzC81mUxK9R"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# =========================\n",
    "# D7 BASE → D2 TEST (Test-only evaluation)\n",
    "# Purpose:\n",
    "# - Run D2 test inference using frozen D7 BASE heads (3 seeds)\n",
    "# - Use ONE shared threshold: mean VAL-opt threshold saved by the base trainval run\n",
    "# Outputs:\n",
    "# - Per-seed predictions.csv + metrics.json in a stable tag folder and a run-stamped folder\n",
    "# - summary_test.json + history_index.jsonl under multilingual_test_runs/\n",
    "# - Builder-aligned run records under config/ and logs/\n",
    "# =========================\n",
    "\n",
    "import os, json, math, random, time, warnings, shutil\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import soundfile as sf\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from transformers import Wav2Vec2Model\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, confusion_matrix, matthews_corrcoef\n",
    "from scipy.stats import fisher_exact\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# -------------------------\n",
    "# Shadowing guardrails\n",
    "# Inputs: local filesystem\n",
    "# Outputs: fail-fast error if a local file would override torch/transformers imports\n",
    "# -------------------------\n",
    "if os.path.exists(\"/content/torch.py\") or os.path.exists(\"/content/torch/__init__.py\"):\n",
    "    raise RuntimeError(\"Found local /content/torch.py or /content/torch/ that shadows PyTorch. Rename/remove it and restart runtime.\")\n",
    "if os.path.exists(\"/content/transformers.py\") or os.path.exists(\"/content/transformers/__init__.py\"):\n",
    "    raise RuntimeError(\"Found local /content/transformers.py or /content/transformers/ that shadows Hugging Face Transformers. Rename/remove it and restart runtime.\")\n",
    "\n",
    "# -------------------------\n",
    "# Drive mount (Colab only)\n",
    "# Inputs: none\n",
    "# Outputs: Drive mounted when needed; no-op otherwise\n",
    "# -------------------------\n",
    "try:\n",
    "    from google.colab import drive  # type: ignore\n",
    "    if not os.path.isdir(\"/content/drive/MyDrive\"):\n",
    "        drive.mount(\"/content/drive\")\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# -------------------------\n",
    "# Paths and run identifiers\n",
    "# Inputs: D7_OUT_ROOT, D2_OUT_ROOT\n",
    "# Outputs: a run stamp and a safe tag used for folder names and metadata\n",
    "# -------------------------\n",
    "D7_OUT_ROOT = \"/content/drive/MyDrive/AI_PD_Project/Datasets/D7-Multilingual (D1_D4_D5v2_D6)/preprocessed_v1\"\n",
    "D2_OUT_ROOT = \"/content/drive/MyDrive/AI_PD_Project/Datasets/D2-Slovak (EWA-DB)/EWA-DB/preprocessed_v1\"\n",
    "\n",
    "D2_MANIFEST_ALL = f\"{D2_OUT_ROOT}/manifests/manifest_all.csv\"\n",
    "\n",
    "# Keep DX_OUT_ROOT aligned with run root (D7)\n",
    "DX_OUT_ROOT = D7_OUT_ROOT\n",
    "globals()[\"DX_OUT_ROOT\"] = DX_OUT_ROOT\n",
    "globals()[\"D7_OUT_ROOT\"] = D7_OUT_ROOT\n",
    "globals()[\"D2_OUT_ROOT\"] = D2_OUT_ROOT\n",
    "\n",
    "RUN_STAMP = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "def _sanitize_tag(s: str) -> str:\n",
    "    \"\"\"Folder-safe tag: keep letters, numbers, dash, underscore; convert others to '_'.\"\"\"\n",
    "    s = str(s).strip()\n",
    "    out = []\n",
    "    for ch in s:\n",
    "        if ch.isalnum() or ch in [\"-\", \"_\"]:\n",
    "            out.append(ch)\n",
    "        else:\n",
    "            out.append(\"_\")\n",
    "    out = \"\".join(out).strip(\"_\")\n",
    "    return out if out else \"tag\"\n",
    "\n",
    "# -------------------------\n",
    "# Fixed BASE trainval experiment\n",
    "# Inputs: a single base trainval experiment folder\n",
    "# Outputs: FULL_TRAINVAL_EXP_TAG used in predictions and summaries\n",
    "# -------------------------\n",
    "_trainval_root = Path(D7_OUT_ROOT) / \"trainval_runs\"\n",
    "if not _trainval_root.exists():\n",
    "    raise FileNotFoundError(f\"Missing trainval_runs folder under D7_OUT_ROOT: {str(_trainval_root)}\")\n",
    "\n",
    "def _is_complete_base_exp_dir(exp_dir: Path) -> bool:\n",
    "    if not exp_dir.is_dir():\n",
    "        return False\n",
    "    if not (exp_dir / \"summary_trainval.json\").exists():\n",
    "        return False\n",
    "    for seed in [1337, 2024, 7777]:\n",
    "        if not (exp_dir / f\"run_D7_seed{seed}\" / \"best_heads.pt\").exists():\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "_exp_dirs = sorted([p for p in _trainval_root.glob(\"exp_*\") if p.is_dir()], key=lambda p: p.stat().st_mtime, reverse=True)\n",
    "BASE_TRAINVAL_EXP_DIR = None\n",
    "for p in _exp_dirs:\n",
    "    if _is_complete_base_exp_dir(p):\n",
    "        BASE_TRAINVAL_EXP_DIR = p\n",
    "        break\n",
    "\n",
    "if BASE_TRAINVAL_EXP_DIR is None:\n",
    "    raise FileNotFoundError(\n",
    "        \"No suitable base trainval experiment folder found under: \"\n",
    "        f\"{str(_trainval_root)}. Expected an exp_* folder containing summary_trainval.json \"\n",
    "        \"and run_D7_seed{1337,2024,7777}/best_heads.pt.\"\n",
    "    )\n",
    "\n",
    "FULL_TRAINVAL_EXP_TAG = BASE_TRAINVAL_EXP_DIR.name  # must match the actual exp folder name\n",
    "\n",
    "# Tag SAFE: build a readable tag that is unique per execution\n",
    "TAG_RAW = f\"exp_frozen_LNDO_base_initBaseline_{RUN_STAMP}\"\n",
    "TAG_SAFE = _sanitize_tag(TAG_RAW)\n",
    "\n",
    "# -------------------------\n",
    "# Runtime and evaluation settings\n",
    "# Inputs: constants (seeds, checkpoint, sample rate, batching)\n",
    "# Outputs: consistent inference behavior across runs\n",
    "# -------------------------\n",
    "SEEDS          = [1337, 2024, 7777]\n",
    "BACKBONE_CKPT  = \"facebook/wav2vec2-base\"\n",
    "SR_EXPECTED    = 16000\n",
    "TINY_THRESH    = 1e-4\n",
    "\n",
    "EFFECTIVE_BS   = 64\n",
    "PER_DEVICE_BS  = 16\n",
    "GRAD_ACCUM     = max(1, EFFECTIVE_BS // PER_DEVICE_BS)\n",
    "\n",
    "DROPOUT_P      = 0.2\n",
    "\n",
    "NUM_WORKERS    = 0\n",
    "PIN_MEMORY     = False\n",
    "\n",
    "USE_AMP        = True\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "try:\n",
    "    torch.set_float32_matmul_precision(\"high\")\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# Silence known non-critical warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"None of the inputs have requires_grad=True. Gradients will be None\")\n",
    "warnings.filterwarnings(\"ignore\", message=\"Passing `gradient_checkpointing` to a config initialization is deprecated\")\n",
    "\n",
    "# Run header (prints only; no logic changes)\n",
    "print(\"D7_OUT_ROOT:\", D7_OUT_ROOT)\n",
    "print(\"D2_OUT_ROOT:\", D2_OUT_ROOT)\n",
    "print(\"D2_MANIFEST_ALL:\", D2_MANIFEST_ALL)\n",
    "print(\"BASE_TRAINVAL_EXP_DIR:\", str(BASE_TRAINVAL_EXP_DIR))\n",
    "print(\"FULL_TRAINVAL_EXP_TAG:\", FULL_TRAINVAL_EXP_TAG)\n",
    "print(\"TAG_SAFE:\", TAG_SAFE)\n",
    "print(\"DEVICE:\", DEVICE)\n",
    "if DEVICE.type == \"cuda\":\n",
    "    print(\"GPU:\", torch.cuda.get_device_name(0))\n",
    "print(\"PER_DEVICE_BS:\", PER_DEVICE_BS, \"| EFFECTIVE_BS:\", PER_DEVICE_BS * GRAD_ACCUM)\n",
    "print(\"NUM_WORKERS:\", NUM_WORKERS, \"| PIN_MEMORY:\", PIN_MEMORY)\n",
    "print(\"USE_AMP:\", bool(USE_AMP and DEVICE.type == \"cuda\"))\n",
    "\n",
    "# -------------------------\n",
    "# Backup helper for builder-aligned logs/config\n",
    "# Inputs: destination path\n",
    "# Outputs: a timestamped backup copy if the destination already exists\n",
    "# -------------------------\n",
    "def _backup_if_exists(path: Path):\n",
    "    if not path.exists():\n",
    "        return None\n",
    "    ts = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "    bak = path.with_name(path.name + f\".bak_{ts}\")\n",
    "    if path.is_dir():\n",
    "        shutil.copytree(path, bak)\n",
    "    else:\n",
    "        shutil.copy2(path, bak)\n",
    "    return str(bak)\n",
    "\n",
    "# -------------------------\n",
    "# Load D2 manifest and build the TEST table\n",
    "# Inputs: D2 manifest_all.csv\n",
    "# Outputs: test_df with required columns + basic counts printed\n",
    "# -------------------------\n",
    "if not os.path.exists(D2_MANIFEST_ALL):\n",
    "    raise FileNotFoundError(f\"Missing D2 manifest_all.csv: {D2_MANIFEST_ALL}\")\n",
    "\n",
    "m_all = pd.read_csv(D2_MANIFEST_ALL)\n",
    "\n",
    "# Minimum fields needed for inference, grouping, and fairness reporting\n",
    "req_cols = {\"split\", \"clip_path\", \"label_num\", \"task\", \"sex\", \"age\"}\n",
    "missing = [c for c in sorted(req_cols) if c not in m_all.columns]\n",
    "if missing:\n",
    "    raise ValueError(f\"D2 manifest missing required columns: {missing}. Found: {list(m_all.columns)}\")\n",
    "\n",
    "# Infer dataset id from the manifest, then keep only that dataset\n",
    "if \"dataset\" in m_all.columns and m_all[\"dataset\"].notna().any():\n",
    "    d2_dataset_id = str(m_all[\"dataset\"].astype(str).value_counts(dropna=True).idxmax())\n",
    "    m_all = m_all[m_all[\"dataset\"].astype(str) == d2_dataset_id].copy()\n",
    "else:\n",
    "    d2_dataset_id = \"DX\"\n",
    "\n",
    "# Guard: this cell is strictly D2 test evaluation\n",
    "if d2_dataset_id != \"D2\":\n",
    "    raise RuntimeError(\n",
    "        f\"Expected D2 dataset_id=='D2' but got {d2_dataset_id!r}. \"\n",
    "        \"This usually means D2_OUT_ROOT is wrong or the manifest is not D2. \"\n",
    "        f\"D2_OUT_ROOT={D2_OUT_ROOT}\"\n",
    "    )\n",
    "\n",
    "# Keep a small set of columns; create missing optional fields as NaN\n",
    "keep_cols = [\"clip_path\", \"label_num\", \"task\", \"speaker_id\", \"sex\", \"age\", \"duration_sec\", \"split\"]\n",
    "for c in keep_cols:\n",
    "    if c not in m_all.columns:\n",
    "        m_all[c] = np.nan\n",
    "m_all = m_all[keep_cols].copy()\n",
    "\n",
    "test_df = m_all[m_all[\"split\"].astype(str) == \"test\"].reset_index(drop=True)\n",
    "\n",
    "print(f\"\\nD2 dataset inferred: {d2_dataset_id}\")\n",
    "print(f\"D2 TEST rows: {len(test_df)}\")\n",
    "print(\"D2 TEST label counts:\", test_df[\"label_num\"].value_counts(dropna=False).to_dict())\n",
    "print(\"D2 TEST sex counts (raw):\", test_df[\"sex\"].value_counts(dropna=False).to_dict())\n",
    "\n",
    "if len(test_df) == 0:\n",
    "    raise RuntimeError(\"After filtering to split=='test', D2 manifest has 0 rows.\")\n",
    "\n",
    "# -------------------------\n",
    "# Missing file check (fail-fast)\n",
    "# Inputs: test_df clip_path\n",
    "# Outputs: stops early if any referenced audio file is missing\n",
    "# -------------------------\n",
    "def _fail_fast_missing_paths(df: pd.DataFrame, name: str):\n",
    "    missing_paths = []\n",
    "    for p in tqdm(df[\"clip_path\"].astype(str).tolist(), desc=f\"Check {name} clip_path exists\", dynamic_ncols=True):\n",
    "        if not os.path.exists(p):\n",
    "            missing_paths.append(p)\n",
    "            if len(missing_paths) >= 10:\n",
    "                break\n",
    "    if missing_paths:\n",
    "        raise FileNotFoundError(f\"{name}: missing clip_path(s). Examples: {missing_paths}\")\n",
    "\n",
    "_fail_fast_missing_paths(test_df, \"D2 TEST\")\n",
    "\n",
    "# -------------------------\n",
    "# Task grouping and sex normalization\n",
    "# Inputs: D2 task and sex columns\n",
    "# Outputs: task_group and sex_norm columns used in metrics, plots, and predictions.csv\n",
    "# -------------------------\n",
    "def _task_group(task_val) -> str:\n",
    "    \"\"\"Map raw task to a 2-level group used to pick the correct head.\"\"\"\n",
    "    return \"vowel\" if str(task_val) == \"vowl\" else \"other\"\n",
    "\n",
    "test_df[\"task_group\"] = test_df[\"task\"].apply(_task_group)\n",
    "\n",
    "def normalize_sex(val) -> str:\n",
    "    \"\"\"\n",
    "    Returns 'M', 'F', or 'UNK'\n",
    "\n",
    "    Expected values: 'male'/'female', but supports common variants and numeric codes.\n",
    "    \"\"\"\n",
    "    if pd.isna(val):\n",
    "        return \"UNK\"\n",
    "\n",
    "    # numeric mapping (if present)\n",
    "    try:\n",
    "        fv = float(val)\n",
    "        if np.isfinite(fv) and abs(fv - round(fv)) < 1e-9:\n",
    "            iv = int(round(fv))\n",
    "            if iv == 0:\n",
    "                return \"F\"\n",
    "            if iv == 1:\n",
    "                return \"M\"\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    s = str(val).strip().lower()\n",
    "    if s in {\"m\", \"male\", \"man\", \"masc\", \"masculine\"}:\n",
    "        return \"M\"\n",
    "    if s in {\"f\", \"female\", \"woman\", \"fem\", \"feminine\"}:\n",
    "        return \"F\"\n",
    "    return \"UNK\"\n",
    "\n",
    "test_df[\"sex_norm\"] = test_df[\"sex\"].apply(normalize_sex)\n",
    "print(\"D2 TEST sex counts (normalized):\", test_df[\"sex_norm\"].value_counts(dropna=False).to_dict())\n",
    "if (test_df[\"sex_norm\"] == \"UNK\").any():\n",
    "    print(\"NOTE: Some 'sex' values could not be normalized to M/F and were counted as 'UNK' for fairness and sex charts.\")\n",
    "\n",
    "# -------------------------\n",
    "# Dataset and collator\n",
    "# Inputs: test_df + audio files (16 kHz expected)\n",
    "# Outputs: batches with audio tensors + per-clip metadata for predictions.csv\n",
    "# -------------------------\n",
    "class AudioManifestDataset(Dataset):\n",
    "    def __init__(self, df: pd.DataFrame):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        row = self.df.iloc[idx]\n",
    "        clip_path = str(row[\"clip_path\"])\n",
    "        label = int(row[\"label_num\"])\n",
    "        task_group = str(row[\"task_group\"])\n",
    "        sex_norm = str(row[\"sex_norm\"])\n",
    "        speaker_id = row[\"speaker_id\"] if \"speaker_id\" in row.index else np.nan\n",
    "\n",
    "        if not os.path.exists(clip_path):\n",
    "            raise FileNotFoundError(f\"Missing clip_path: {clip_path}\")\n",
    "\n",
    "        y, sr = sf.read(clip_path, always_2d=False)\n",
    "        if isinstance(y, np.ndarray) and y.ndim > 1:\n",
    "            y = np.mean(y, axis=1)\n",
    "        y = np.asarray(y, dtype=np.float32)\n",
    "\n",
    "        if int(sr) != SR_EXPECTED:\n",
    "            raise ValueError(f\"Sample rate mismatch (expected {SR_EXPECTED}): {clip_path} has sr={sr}\")\n",
    "\n",
    "        # attention_mask marks valid samples; vowel clips may include padded silence\n",
    "        attn = np.ones((len(y),), dtype=np.int64)\n",
    "        if task_group == \"vowel\":\n",
    "            k = -1\n",
    "            for j in range(len(y) - 1, -1, -1):\n",
    "                if abs(float(y[j])) > TINY_THRESH:\n",
    "                    k = j\n",
    "                    break\n",
    "            if k >= 0:\n",
    "                attn[:k+1] = 1\n",
    "                attn[k+1:] = 0\n",
    "            else:\n",
    "                attn[:] = 1\n",
    "        else:\n",
    "            attn[:] = 1\n",
    "\n",
    "        return {\n",
    "            \"input_values\": torch.from_numpy(y),\n",
    "            \"attention_mask\": torch.from_numpy(attn),\n",
    "            \"labels\": torch.tensor(label, dtype=torch.long),\n",
    "            \"task_group\": task_group,\n",
    "            \"sex_norm\": sex_norm,\n",
    "            \"clip_path\": clip_path,\n",
    "            \"speaker_id\": speaker_id,\n",
    "        }\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"Pad audio and attention masks to the max length in the batch; keep metadata lists.\"\"\"\n",
    "    max_len = int(max(b[\"input_values\"].numel() for b in batch))\n",
    "    input_vals, attn_masks, labels = [], [], []\n",
    "    task_groups, sex_norms, clip_paths, speaker_ids = [], [], [], []\n",
    "\n",
    "    for b in batch:\n",
    "        x = b[\"input_values\"]\n",
    "        a = b[\"attention_mask\"]\n",
    "        pad = max_len - x.numel()\n",
    "        if pad > 0:\n",
    "            x = torch.cat([x, torch.zeros(pad, dtype=x.dtype)], dim=0)\n",
    "            a = torch.cat([a, torch.zeros(pad, dtype=a.dtype)], dim=0)\n",
    "\n",
    "        input_vals.append(x)\n",
    "        attn_masks.append(a)\n",
    "        labels.append(b[\"labels\"])\n",
    "        task_groups.append(b[\"task_group\"])\n",
    "        sex_norms.append(b[\"sex_norm\"])\n",
    "        clip_paths.append(b[\"clip_path\"])\n",
    "        speaker_ids.append(b[\"speaker_id\"])\n",
    "\n",
    "    return {\n",
    "        \"input_values\": torch.stack(input_vals, dim=0),\n",
    "        \"attention_mask\": torch.stack(attn_masks, dim=0),\n",
    "        \"labels\": torch.stack(labels, dim=0),\n",
    "        \"task_group\": task_groups,\n",
    "        \"sex_norm\": sex_norms,\n",
    "        \"clip_path\": clip_paths,\n",
    "        \"speaker_id\": speaker_ids,\n",
    "    }\n",
    "\n",
    "# -------------------------\n",
    "# Model definition (frozen backbone, two heads)\n",
    "# Inputs: backbone checkpoint name + dropout\n",
    "# Outputs: logits for PD/Healthy using the head chosen by task_group\n",
    "# -------------------------\n",
    "class Wav2Vec2TwoHeadClassifier(nn.Module):\n",
    "    def __init__(self, ckpt: str, dropout_p: float):\n",
    "        super().__init__()\n",
    "        self.backbone = Wav2Vec2Model.from_pretrained(ckpt, use_safetensors=True)\n",
    "        for p in self.backbone.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "        H = int(self.backbone.config.hidden_size)\n",
    "        self.pre_vowel = nn.Sequential(nn.LayerNorm(H), nn.Dropout(float(dropout_p)))\n",
    "        self.pre_other = nn.Sequential(nn.LayerNorm(H), nn.Dropout(float(dropout_p)))\n",
    "        self.head_vowel = nn.Linear(H, 2)\n",
    "        self.head_other = nn.Linear(H, 2)\n",
    "\n",
    "    def masked_mean_pool(self, last_hidden, attn_mask_samples):\n",
    "        \"\"\"Mean pooling over non-masked frames (mask derived from the sample-level attention_mask).\"\"\"\n",
    "        feat_mask = self.backbone._get_feature_vector_attention_mask(last_hidden.shape[1], attn_mask_samples)\n",
    "        feat_mask = feat_mask.to(last_hidden.device).unsqueeze(-1).type_as(last_hidden)\n",
    "        masked = last_hidden * feat_mask\n",
    "        denom = feat_mask.sum(dim=1).clamp(min=1.0)\n",
    "        return masked.sum(dim=1) / denom\n",
    "\n",
    "    def _heads_fp32(self, x_fp_any: torch.Tensor, head: nn.Module) -> torch.Tensor:\n",
    "        \"\"\"Run heads in float32 for numerical stability.\"\"\"\n",
    "        x = x_fp_any.float()\n",
    "        if x.is_cuda:\n",
    "            with torch.amp.autocast(device_type=\"cuda\", enabled=False):\n",
    "                return head(x)\n",
    "        return head(x)\n",
    "\n",
    "    def forward_logits(self, input_values, attention_mask, task_group):\n",
    "        \"\"\"Backbone forward (no grad) + pooled features + head selection by task_group.\"\"\"\n",
    "        with torch.no_grad():\n",
    "            out = self.backbone(input_values=input_values, attention_mask=attention_mask)\n",
    "            last_hidden = out.last_hidden_state\n",
    "\n",
    "        pooled = self.masked_mean_pool(last_hidden, attention_mask)  # [B,H]\n",
    "\n",
    "        z_v = self.pre_vowel(pooled.float())\n",
    "        z_o = self.pre_other(pooled.float())\n",
    "\n",
    "        logits_v = self._heads_fp32(z_v, self.head_vowel)\n",
    "        logits_o = self._heads_fp32(z_o, self.head_other)\n",
    "\n",
    "        is_vowel = torch.tensor([tg == \"vowel\" for tg in task_group], device=pooled.device, dtype=torch.bool)\n",
    "        logits = logits_o.clone()\n",
    "        if is_vowel.any():\n",
    "            logits[is_vowel] = logits_v[is_vowel]\n",
    "        return logits\n",
    "\n",
    "# -------------------------\n",
    "# Metrics and plotting helpers\n",
    "# Inputs: y_true, y_prob (PD probability), threshold\n",
    "# Outputs: AUROC, threshold metrics dict, ROC/CM png files\n",
    "# -------------------------\n",
    "def compute_auc(y_true, y_prob):\n",
    "    y_true = np.asarray(y_true, dtype=np.int64)\n",
    "    y_prob = np.asarray(y_prob, dtype=np.float64)\n",
    "    if len(np.unique(y_true)) < 2:\n",
    "        return float(\"nan\")\n",
    "    return float(roc_auc_score(y_true, y_prob))\n",
    "\n",
    "def compute_threshold_metrics(y_true, y_prob, thr=0.5):\n",
    "    \"\"\"Compute confusion-derived metrics at a fixed threshold.\"\"\"\n",
    "    y_true = np.asarray(y_true, dtype=np.int64)\n",
    "    y_prob = np.asarray(y_prob, dtype=np.float64)\n",
    "    y_pred = (y_prob >= float(thr)).astype(np.int64)\n",
    "\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=[0, 1])\n",
    "    TN, FP, FN, TP = int(cm[0, 0]), int(cm[0, 1]), int(cm[1, 0]), int(cm[1, 1])\n",
    "\n",
    "    eps = 1e-12\n",
    "    acc = (TP + TN) / max(1, (TP + TN + FP + FN))\n",
    "    prec = TP / (TP + FP + eps)\n",
    "    rec = TP / (TP + FN + eps)     # sensitivity\n",
    "    f1 = 2 * prec * rec / (prec + rec + eps)\n",
    "    spec = TN / (TN + FP + eps)\n",
    "\n",
    "    try:\n",
    "        mcc = float(matthews_corrcoef(y_true, y_pred)) if len(np.unique(y_pred)) > 1 else float(\"nan\")\n",
    "    except Exception:\n",
    "        mcc = float(\"nan\")\n",
    "\n",
    "    try:\n",
    "        _, pval = fisher_exact([[TN, FP], [FN, TP]], alternative=\"two-sided\")\n",
    "        pval = float(pval)\n",
    "    except Exception:\n",
    "        pval = float(\"nan\")\n",
    "\n",
    "    return {\n",
    "        \"threshold\": float(thr),\n",
    "        \"confusion_matrix\": {\"TN\": TN, \"FP\": FP, \"FN\": FN, \"TP\": TP},\n",
    "        \"accuracy\": float(acc),\n",
    "        \"precision\": float(prec),\n",
    "        \"recall\": float(rec),\n",
    "        \"f1_score\": float(f1),\n",
    "        \"sensitivity\": float(rec),\n",
    "        \"specificity\": float(spec),\n",
    "        \"mcc\": float(mcc),\n",
    "        \"p_value_fisher_two_sided\": float(pval),\n",
    "    }\n",
    "\n",
    "def save_roc_curve_png(y_true, y_prob, out_png, title_suffix=\"Test\"):\n",
    "    \"\"\"Write a basic ROC curve plot.\"\"\"\n",
    "    y_true = np.asarray(y_true, dtype=np.int64)\n",
    "    y_prob = np.asarray(y_prob, dtype=np.float64)\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_prob)\n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr)\n",
    "    plt.plot([0, 1], [0, 1], linestyle=\"--\")\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(f\"ROC Curve ({title_suffix})\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_png, dpi=150)\n",
    "    plt.close()\n",
    "\n",
    "def save_confusion_png(y_true, y_prob, out_png, thr=0.5, title_suffix=\"Test\"):\n",
    "    \"\"\"Write a confusion matrix plot at the chosen threshold.\"\"\"\n",
    "    y_true = np.asarray(y_true, dtype=np.int64)\n",
    "    y_prob = np.asarray(y_prob, dtype=np.float64)\n",
    "    y_pred = (y_prob >= float(thr)).astype(int)\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=[0, 1])\n",
    "\n",
    "    plt.figure()\n",
    "    plt.imshow(cm)\n",
    "    plt.xticks([0, 1], [\"Healthy(0)\", \"PD(1)\"])\n",
    "    plt.yticks([0, 1], [\"Healthy(0)\", \"PD(1)\"])\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"True\")\n",
    "    plt.title(f\"Confusion Matrix ({title_suffix}, thr={thr:.4f})\")\n",
    "    for (i, j), v in np.ndenumerate(cm):\n",
    "        plt.text(j, i, str(v), ha=\"center\", va=\"center\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_png, dpi=150)\n",
    "    plt.close()\n",
    "\n",
    "def mean_sd(vals):\n",
    "    \"\"\"Mean and sample SD over seeds (NaN-safe).\"\"\"\n",
    "    vals = np.asarray(vals, dtype=np.float64)\n",
    "    mu = float(np.nanmean(vals)) if np.any(~np.isnan(vals)) else float(\"nan\")\n",
    "    sd = float(np.nanstd(vals, ddof=1)) if np.sum(~np.isnan(vals)) > 1 else 0.0\n",
    "    return mu, sd\n",
    "\n",
    "# -------------------------\n",
    "# Fairness metric (H3): ΔFNR = FNR(F) - FNR(M)\n",
    "# Inputs: y_true, y_prob, sex_norm, threshold\n",
    "# Outputs: per-sex FNR details and ΔFNR (F minus M)\n",
    "# -------------------------\n",
    "def compute_fnr_by_sex_and_delta(y_true, y_prob, sex_norm, thr):\n",
    "    \"\"\"\n",
    "    PD-only FNR at the chosen threshold:\n",
    "      FNR(sex) = FN / (FN + TP) using only rows with y_true == 1\n",
    "    \"\"\"\n",
    "    y_true = np.asarray(y_true, dtype=np.int64)\n",
    "    y_prob = np.asarray(y_prob, dtype=np.float64)\n",
    "    sex_norm = np.asarray(list(sex_norm), dtype=object)\n",
    "    y_pred = (y_prob >= float(thr)).astype(np.int64)\n",
    "\n",
    "    out = {}\n",
    "    for g in [\"M\", \"F\", \"UNK\"]:\n",
    "        mask_g = (sex_norm == g)\n",
    "        if int(mask_g.sum()) == 0:\n",
    "            continue\n",
    "        pos_mask = mask_g & (y_true == 1)\n",
    "        n_pos = int(pos_mask.sum())\n",
    "        if n_pos == 0:\n",
    "            out[g] = {\"n_total\": int(mask_g.sum()), \"n_pos\": 0, \"tp\": 0, \"fn\": 0, \"fnr\": float(\"nan\")}\n",
    "            continue\n",
    "        tp = int(((y_pred == 1) & pos_mask).sum())\n",
    "        fn = int(((y_pred == 0) & pos_mask).sum())\n",
    "        fnr = float(fn / max(1, (fn + tp)))\n",
    "        out[g] = {\"n_total\": int(mask_g.sum()), \"n_pos\": int(n_pos), \"tp\": int(tp), \"fn\": int(fn), \"fnr\": float(fnr)}\n",
    "\n",
    "    fnr_m = out.get(\"M\", {}).get(\"fnr\", float(\"nan\"))\n",
    "    fnr_f = out.get(\"F\", {}).get(\"fnr\", float(\"nan\"))\n",
    "    if (not np.isnan(fnr_m)) and (not np.isnan(fnr_f)):\n",
    "        delta = float(fnr_f - fnr_m)\n",
    "    else:\n",
    "        delta = float(\"nan\")\n",
    "\n",
    "    return out, delta\n",
    "\n",
    "# -------------------------\n",
    "# Seed control\n",
    "# Inputs: integer seed\n",
    "# Outputs: deterministic CPU/GPU behavior for this run\n",
    "# -------------------------\n",
    "def set_all_seeds(seed: int):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# -------------------------\n",
    "# Build D2 TEST DataLoader + quick warm-up\n",
    "# Inputs: test_df\n",
    "# Outputs: test_loader ready for inference; warm-up checks read/decode path\n",
    "# -------------------------\n",
    "test_ds = AudioManifestDataset(test_df)\n",
    "test_loader = DataLoader(\n",
    "    test_ds,\n",
    "    batch_size=PER_DEVICE_BS,\n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=PIN_MEMORY,\n",
    "    collate_fn=collate_fn,\n",
    ")\n",
    "\n",
    "print(\"\\nWarm-up: loading up to 3 D2 TEST batches...\")\n",
    "t0 = time.time()\n",
    "nb = len(test_loader)\n",
    "wb = min(3, nb)\n",
    "if wb == 0:\n",
    "    raise RuntimeError(\"D2 TEST DataLoader has 0 batches. Check test_df length and PER_DEVICE_BS.\")\n",
    "it = iter(test_loader)\n",
    "for i in range(wb):\n",
    "    _ = next(it)\n",
    "    print(f\"  loaded warmup D2 TEST batch {i+1}/{wb}\")\n",
    "print(f\"Warm-up done in {time.time()-t0:.2f}s\")\n",
    "\n",
    "# -------------------------\n",
    "# Load the shared threshold from base trainval summary\n",
    "# Inputs: BASE_TRAINVAL_EXP_DIR/summary_trainval.json\n",
    "# Outputs: TEST_THR_MEAN_VAL_OPT used for ALL seeds on D2 test\n",
    "# -------------------------\n",
    "summary_path = BASE_TRAINVAL_EXP_DIR / \"summary_trainval.json\"\n",
    "if not summary_path.exists():\n",
    "    raise FileNotFoundError(f\"Missing summary_trainval.json in base exp: {str(summary_path)}\")\n",
    "\n",
    "with open(summary_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    chosen_summary = json.load(f)\n",
    "\n",
    "try:\n",
    "    val_opt = chosen_summary[\"val_optimal_threshold\"]\n",
    "    thr_mean = float(val_opt.get(\"mean_sd\", {}).get(\"mean\", float(\"nan\")))\n",
    "    thr_sd   = float(val_opt.get(\"mean_sd\", {}).get(\"sd\", float(\"nan\")))\n",
    "    thr_by_seed = {str(k): float(v) for k, v in (val_opt.get(\"by_seed\") or {}).items()}\n",
    "except Exception as e:\n",
    "    raise RuntimeError(f\"Could not parse val_optimal_threshold from {str(summary_path)}. Reason: {repr(e)}\")\n",
    "\n",
    "TEST_THR_SOURCE = \"trainval summary_trainval.json -> val_optimal_threshold.mean_sd.mean (canonical aggregate)\"\n",
    "TEST_THR_MEAN_VAL_OPT = float(thr_mean)\n",
    "TEST_THR_NOTE = None\n",
    "\n",
    "# Fallbacks only if the stored aggregate threshold is unusable\n",
    "if np.isnan(TEST_THR_MEAN_VAL_OPT):\n",
    "    vals = []\n",
    "    for s in SEEDS:\n",
    "        v = thr_by_seed.get(str(s), float(\"nan\"))\n",
    "        if not np.isnan(v):\n",
    "            vals.append(float(v))\n",
    "    if len(vals) > 0:\n",
    "        TEST_THR_MEAN_VAL_OPT = float(np.mean(vals))\n",
    "        TEST_THR_NOTE = \"val_optimal_threshold.mean_sd.mean was NaN. Fallback used: mean(thr_by_seed over available seeds).\"\n",
    "    else:\n",
    "        TEST_THR_MEAN_VAL_OPT = 0.5\n",
    "        TEST_THR_NOTE = \"val_optimal_threshold.mean_sd.mean was NaN and thr_by_seed had no usable values. Fallback used: thr=0.5.\"\n",
    "\n",
    "print(\"\\nVAL-opt thresholds loaded from BASE trainval summary_trainval.json:\")\n",
    "print(\"  method:\", str(val_opt.get(\"method\", \"unknown\")))\n",
    "print(\"  by_seed:\", {k: (f\"{v:.6f}\" if not np.isnan(v) else \"nan\") for k, v in thr_by_seed.items()})\n",
    "print(\"  mean ± SD:\", (f\"{thr_mean:.6f} ± {thr_sd:.6f}\" if not np.isnan(thr_mean) else f\"nan ± {thr_sd:.6f}\"))\n",
    "\n",
    "print(\"\\nTEST threshold policy:\")\n",
    "print(\"  Using MEAN VAL-opt threshold for ALL seeds/tests:\", f\"{TEST_THR_MEAN_VAL_OPT:.6f}\")\n",
    "print(\"  Source:\", TEST_THR_SOURCE)\n",
    "if TEST_THR_NOTE is not None:\n",
    "    print(\"  NOTE:\", TEST_THR_NOTE)\n",
    "\n",
    "# -------------------------\n",
    "# Output folder layout\n",
    "# Inputs: TAG_SAFE and RUN_STAMP\n",
    "# Outputs:\n",
    "# - TAG_ROOT: stable folder for this tag (accumulates per-seed outputs)\n",
    "# - STAMP_ROOT: unique folder for this execution\n",
    "# - CFG_DIR/LOG_DIR: builder-aligned run records\n",
    "# -------------------------\n",
    "TEST_ROOT = Path(D7_OUT_ROOT) / \"multilingual_test_runs\"\n",
    "TEST_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "TAG_ROOT = TEST_ROOT / f\"run_{TAG_SAFE}\"\n",
    "TAG_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "STAMP_ROOT = TEST_ROOT / f\"run_{TAG_SAFE}__{RUN_STAMP}\"\n",
    "STAMP_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "CFG_DIR = Path(D7_OUT_ROOT) / \"config\" / f\"D7_{TAG_SAFE}_on_D2_Test\"\n",
    "LOG_DIR = Path(D7_OUT_ROOT) / \"logs\"   / f\"D7_{TAG_SAFE}_on_D2_Test\"\n",
    "CFG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "LOG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# -------------------------\n",
    "# Write builder-aligned run records (config + dataset summary + empty warnings)\n",
    "# Inputs: test_df counts + threshold policy + run metadata\n",
    "# Outputs: run_config.json, dataset_summary.json, preprocess_warnings.csv\n",
    "# -------------------------\n",
    "run_config_path = CFG_DIR / \"run_config.json\"\n",
    "dataset_summary_path = LOG_DIR / \"dataset_summary.json\"\n",
    "preprocess_warn_path = LOG_DIR / \"preprocess_warnings.csv\"\n",
    "\n",
    "bak1 = _backup_if_exists(run_config_path)\n",
    "bak2 = _backup_if_exists(dataset_summary_path)\n",
    "bak3 = _backup_if_exists(preprocess_warn_path)\n",
    "\n",
    "if bak1: print(\"Backed up existing run_config.json to:\", bak1)\n",
    "if bak2: print(\"Backed up existing dataset_summary.json to:\", bak2)\n",
    "if bak3: print(\"Backed up existing preprocess_warnings.csv to:\", bak3)\n",
    "\n",
    "run_config = {\n",
    "    \"run_type\": \"test_only\",\n",
    "    \"model_family\": \"D7_multilingual_base\",\n",
    "    \"d7_out_root\": D7_OUT_ROOT,\n",
    "    \"d2_out_root\": D2_OUT_ROOT,\n",
    "    \"d2_manifest_all\": D2_MANIFEST_ALL,\n",
    "    \"base_trainval_exp_dir\": str(BASE_TRAINVAL_EXP_DIR),\n",
    "    \"trainval_exp_tag\": FULL_TRAINVAL_EXP_TAG,\n",
    "    \"tag_safe\": TAG_SAFE,\n",
    "    \"run_stamp\": RUN_STAMP,\n",
    "    \"seeds\": SEEDS,\n",
    "    \"backbone_ckpt\": BACKBONE_CKPT,\n",
    "    \"dropout_p\": float(DROPOUT_P),\n",
    "    \"batching\": {\"per_device_bs\": int(PER_DEVICE_BS), \"effective_bs\": int(PER_DEVICE_BS * GRAD_ACCUM)},\n",
    "    \"threshold_policy\": {\n",
    "        \"source\": TEST_THR_SOURCE,\n",
    "        \"threshold_used_global\": float(TEST_THR_MEAN_VAL_OPT),\n",
    "        \"note\": (TEST_THR_NOTE if TEST_THR_NOTE is not None else \"\")\n",
    "    }\n",
    "}\n",
    "with open(run_config_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(run_config, f, indent=2)\n",
    "\n",
    "ds_summary = {\n",
    "    \"dataset_id\": \"D2\",\n",
    "    \"split\": \"test\",\n",
    "    \"n_rows\": int(len(test_df)),\n",
    "    \"label_counts\": test_df[\"label_num\"].value_counts(dropna=False).to_dict(),\n",
    "    \"sex_counts_raw\": test_df[\"sex\"].value_counts(dropna=False).to_dict(),\n",
    "    \"sex_counts_norm\": test_df[\"sex_norm\"].value_counts(dropna=False).to_dict(),\n",
    "    \"task_group_counts\": test_df[\"task_group\"].value_counts(dropna=False).to_dict(),\n",
    "    \"manifest_path\": D2_MANIFEST_ALL,\n",
    "    \"generated_at\": RUN_STAMP,\n",
    "}\n",
    "with open(dataset_summary_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(ds_summary, f, indent=2)\n",
    "\n",
    "# Test-only run: warnings file is a structured placeholder\n",
    "pd.DataFrame(columns=[\"warning_type\", \"clip_path\", \"detail\"]).to_csv(preprocess_warn_path, index=False)\n",
    "\n",
    "print(\"\\nWROTE builder-aligned artifacts:\")\n",
    "print(\" \", str(run_config_path))\n",
    "print(\" \", str(dataset_summary_path))\n",
    "print(\" \", str(preprocess_warn_path))\n",
    "\n",
    "# -------------------------\n",
    "# Load heads into the model\n",
    "# Inputs: best_heads.pt from the base trainval run for a given seed\n",
    "# Outputs: model with pre-blocks and heads loaded (backbone stays frozen)\n",
    "# -------------------------\n",
    "def load_heads_into_model(model: Wav2Vec2TwoHeadClassifier, best_heads_path: Path):\n",
    "    if not best_heads_path.exists():\n",
    "        raise FileNotFoundError(f\"Missing best_heads.pt: {str(best_heads_path)}\")\n",
    "    state = torch.load(str(best_heads_path), map_location=\"cpu\")\n",
    "    model.pre_vowel.load_state_dict(state[\"pre_vowel\"], strict=True)\n",
    "    model.pre_other.load_state_dict(state[\"pre_other\"], strict=True)\n",
    "    model.head_vowel.load_state_dict(state[\"head_vowel\"], strict=True)\n",
    "    model.head_other.load_state_dict(state[\"head_other\"], strict=True)\n",
    "    return model\n",
    "\n",
    "# -------------------------\n",
    "# Inference loop (collect per-clip metadata)\n",
    "# Inputs: test_loader + model\n",
    "# Outputs: arrays for y_true, y_score, sex_norm plus clip_path/speaker_id/task_group lists\n",
    "# -------------------------\n",
    "def infer_probs_and_meta(loader, model, desc):\n",
    "    use_amp = bool(USE_AMP and DEVICE.type == \"cuda\")\n",
    "    all_probs, all_true, all_sex = [], [], []\n",
    "    all_clip, all_spk, all_tg = [], [], []\n",
    "\n",
    "    pbar = tqdm(loader, desc=desc, dynamic_ncols=True)\n",
    "    with torch.inference_mode():\n",
    "        for batch in pbar:\n",
    "            input_values = batch[\"input_values\"].to(DEVICE, non_blocking=False)\n",
    "            attention_mask = batch[\"attention_mask\"].to(DEVICE, non_blocking=False)\n",
    "            labels = batch[\"labels\"].to(DEVICE, non_blocking=False)\n",
    "\n",
    "            task_group = batch[\"task_group\"]\n",
    "            sex_norm = batch[\"sex_norm\"]\n",
    "            clip_paths = batch[\"clip_path\"]\n",
    "            speaker_ids = batch[\"speaker_id\"]\n",
    "\n",
    "            with torch.amp.autocast(device_type=\"cuda\", enabled=use_amp):\n",
    "                logits = model.forward_logits(input_values, attention_mask, task_group)\n",
    "\n",
    "            probs = torch.softmax(logits.float(), dim=-1)[:, 1].detach().cpu().numpy().astype(np.float64)\n",
    "\n",
    "            all_probs.extend(probs.tolist())\n",
    "            all_true.extend(labels.detach().cpu().numpy().astype(np.int64).tolist())\n",
    "            all_sex.extend(list(sex_norm))\n",
    "\n",
    "            all_clip.extend(list(clip_paths))\n",
    "            all_spk.extend([(\"\" if pd.isna(x) else str(x)) for x in speaker_ids])\n",
    "            all_tg.extend(list(task_group))\n",
    "\n",
    "    return (\n",
    "        np.asarray(all_true, dtype=np.int64),\n",
    "        np.asarray(all_probs, dtype=np.float64),\n",
    "        np.asarray(all_sex, dtype=object),\n",
    "        list(all_clip),\n",
    "        list(all_spk),\n",
    "        list(all_tg),\n",
    "    )\n",
    "\n",
    "# -------------------------\n",
    "# Single-seed execution\n",
    "# Inputs: seed, shared threshold, base heads file for that seed\n",
    "# Outputs:\n",
    "# - predictions.csv and metrics.json in both TAG and STAMP folders\n",
    "# - ROC and confusion plots (overall + by sex when available)\n",
    "# -------------------------\n",
    "def run_test_once(seed: int):\n",
    "    set_all_seeds(seed)\n",
    "\n",
    "    per_tag_seed_dir = TAG_ROOT / f\"run_D7_on_D2test_seed{seed}\"\n",
    "    per_tag_seed_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    per_stamp_seed_dir = STAMP_ROOT / f\"run_D7_on_D2test_seed{seed}\"\n",
    "    per_stamp_seed_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    best_heads_path = BASE_TRAINVAL_EXP_DIR / f\"run_D7_seed{seed}\" / \"best_heads.pt\"\n",
    "    if not best_heads_path.exists():\n",
    "        raise FileNotFoundError(f\"Missing best_heads.pt for seed={seed}: {str(best_heads_path)}\")\n",
    "\n",
    "    thr_used = float(TEST_THR_MEAN_VAL_OPT)\n",
    "\n",
    "    print(f\"\\n[seed={seed}] Loading model + BASE heads from:\")\n",
    "    print(\" \", str(best_heads_path))\n",
    "    print(f\"[seed={seed}] Using TEST threshold (shared mean VAL-opt): {thr_used:.6f}\")\n",
    "    if TEST_THR_NOTE is not None:\n",
    "        print(f\"[seed={seed}] NOTE: {TEST_THR_NOTE}\")\n",
    "\n",
    "    model = Wav2Vec2TwoHeadClassifier(BACKBONE_CKPT, dropout_p=DROPOUT_P).to(DEVICE)\n",
    "    model = load_heads_into_model(model, best_heads_path)\n",
    "    model.eval()\n",
    "\n",
    "    yt_true, yt_prob, yt_sex, clip_paths, speaker_ids, task_groups = infer_probs_and_meta(\n",
    "        test_loader, model, desc=f\"[seed={seed}] D2 TEST\"\n",
    "    )\n",
    "\n",
    "    test_auc = compute_auc(yt_true, yt_prob)\n",
    "    thr_metrics_test = compute_threshold_metrics(yt_true, yt_prob, thr=thr_used)\n",
    "    fnr_by_sex, delta_f_minus_m = compute_fnr_by_sex_and_delta(yt_true, yt_prob, yt_sex, thr=thr_used)\n",
    "\n",
    "    # Write plots to both locations so the stable tag folder and the run-stamped folder are complete\n",
    "    for out_dir in [per_tag_seed_dir, per_stamp_seed_dir]:\n",
    "        roc_png = out_dir / \"roc_curve.png\"\n",
    "        cm_png  = out_dir / \"confusion_matrix.png\"\n",
    "        save_roc_curve_png(yt_true, yt_prob, str(roc_png), title_suffix=f\"D2 TEST (seed={seed})\")\n",
    "        save_confusion_png(yt_true, yt_prob, str(cm_png), thr=thr_used, title_suffix=f\"D2 TEST (seed={seed})\")\n",
    "\n",
    "        mask_m = (yt_sex == \"M\")\n",
    "        mask_f = (yt_sex == \"F\")\n",
    "\n",
    "        if int(mask_m.sum()) > 0:\n",
    "            save_confusion_png(yt_true[mask_m], yt_prob[mask_m], str(out_dir / \"confusion_matrix_M.png\"),\n",
    "                               thr=thr_used, title_suffix=f\"D2 TEST SEX=M (seed={seed})\")\n",
    "        if int(mask_f.sum()) > 0:\n",
    "            save_confusion_png(yt_true[mask_f], yt_prob[mask_f], str(out_dir / \"confusion_matrix_F.png\"),\n",
    "                               thr=thr_used, title_suffix=f\"D2 TEST SEX=F (seed={seed})\")\n",
    "\n",
    "    # predictions.csv: one row per clip with score and metadata\n",
    "    pred_df = pd.DataFrame({\n",
    "        \"clip_path\": clip_paths,\n",
    "        \"y_true\": yt_true.astype(int),\n",
    "        \"y_score\": yt_prob.astype(float),\n",
    "        \"sex_norm\": [str(x) for x in yt_sex.tolist()],\n",
    "        \"speaker_id\": speaker_ids,\n",
    "        \"task_group\": task_groups,\n",
    "        \"seed\": int(seed),\n",
    "        \"trainval_exp_tag\": FULL_TRAINVAL_EXP_TAG,\n",
    "        \"run_stamp\": RUN_STAMP,\n",
    "        \"threshold_used_global\": float(thr_used),\n",
    "    })\n",
    "\n",
    "    pred_path_tag = per_tag_seed_dir / \"predictions.csv\"\n",
    "    pred_path_stamp = per_stamp_seed_dir / \"predictions.csv\"\n",
    "    pred_df.to_csv(pred_path_tag, index=False)\n",
    "    pred_df.to_csv(pred_path_stamp, index=False)\n",
    "\n",
    "    # metrics.json: per-seed snapshot used by the final summary\n",
    "    seed_metrics = {\n",
    "        \"seed\": int(seed),\n",
    "        \"d7_out_root\": D7_OUT_ROOT,\n",
    "        \"d2_out_root\": D2_OUT_ROOT,\n",
    "        \"d2_manifest_all\": D2_MANIFEST_ALL,\n",
    "        \"base_trainval_exp_dir\": str(BASE_TRAINVAL_EXP_DIR),\n",
    "        \"trainval_exp_tag\": FULL_TRAINVAL_EXP_TAG,\n",
    "        \"best_heads_path\": str(best_heads_path),\n",
    "\n",
    "        \"tag_safe\": TAG_SAFE,\n",
    "        \"run_stamp\": RUN_STAMP,\n",
    "\n",
    "        \"n_test\": int(len(test_df)),\n",
    "        \"label_counts_test\": test_df[\"label_num\"].value_counts(dropna=False).to_dict(),\n",
    "        \"sex_counts_test_norm\": test_df[\"sex_norm\"].value_counts(dropna=False).to_dict(),\n",
    "\n",
    "        \"test_auroc\": float(test_auc),\n",
    "\n",
    "        \"threshold_source\": TEST_THR_SOURCE,\n",
    "        \"val_optimal_threshold_canonical\": {\n",
    "            \"method\": str(val_opt.get(\"method\", \"unknown\")),\n",
    "            \"by_seed\": {k: float(v) for k, v in thr_by_seed.items()},\n",
    "            \"mean_sd\": {\"mean\": float(thr_mean), \"sd\": float(thr_sd)},\n",
    "        },\n",
    "        \"test_threshold_used\": float(thr_used),\n",
    "        \"test_threshold_note\": (TEST_THR_NOTE if TEST_THR_NOTE is not None else \"\"),\n",
    "\n",
    "        \"threshold_metrics_test_at_val_opt\": thr_metrics_test,\n",
    "\n",
    "        \"fairness_test_at_val_opt\": {\n",
    "            \"definition\": \"H3: ΔFNR = FNR(F) - FNR(M), where FNR(sex)=FN/(FN+TP) computed on PD-only true labels at test_threshold_used.\",\n",
    "            \"threshold_used\": float(thr_used),\n",
    "            \"fnr_by_sex_norm\": fnr_by_sex,\n",
    "            \"delta_fnr_F_minus_M\": float(delta_f_minus_m),\n",
    "            \"note\": \"If n_PD for a sex is 0, its FNR is NaN and ΔFNR is NaN.\",\n",
    "        },\n",
    "\n",
    "        \"artifacts\": {\n",
    "            \"tag_seed_dir\": str(per_tag_seed_dir),\n",
    "            \"stamp_seed_dir\": str(per_stamp_seed_dir),\n",
    "            \"predictions_csv_tag\": str(pred_path_tag),\n",
    "            \"predictions_csv_stamp\": str(pred_path_stamp),\n",
    "        },\n",
    "    }\n",
    "\n",
    "    for out_dir in [per_tag_seed_dir, per_stamp_seed_dir]:\n",
    "        mp = out_dir / \"metrics.json\"\n",
    "        with open(mp, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(seed_metrics, f, indent=2)\n",
    "\n",
    "    print(f\"[seed={seed}] DONE | test_AUROC={test_auc:.6f} | thr_used={thr_used:.6f}\")\n",
    "    print(f\"[seed={seed}] WROTE predictions:\")\n",
    "    print(\" \", str(pred_path_tag))\n",
    "    print(\" \", str(pred_path_stamp))\n",
    "\n",
    "    return {\n",
    "        \"seed\": int(seed),\n",
    "        \"thr_used\": float(thr_used),\n",
    "        \"test_auc\": float(test_auc),\n",
    "        \"thr_metrics\": thr_metrics_test,\n",
    "        \"fnr_by_sex\": fnr_by_sex,\n",
    "        \"delta_f_minus_m\": float(delta_f_minus_m),\n",
    "        \"tag_seed_dir\": str(per_tag_seed_dir),\n",
    "        \"stamp_seed_dir\": str(per_stamp_seed_dir),\n",
    "    }\n",
    "\n",
    "# -------------------------\n",
    "# Run all seeds and aggregate results\n",
    "# Inputs: SEEDS\n",
    "# Outputs: printed per-seed results + mean/CI + aggregated metrics\n",
    "# -------------------------\n",
    "results = []\n",
    "for seed in SEEDS:\n",
    "    results.append(run_test_once(seed))\n",
    "\n",
    "aurocs = [r[\"test_auc\"] for r in results]\n",
    "t_crit = 4.302652729911275  # df=2, 95% CI\n",
    "n = len(aurocs)\n",
    "mean_auc = float(np.mean(aurocs))\n",
    "std_auc = float(np.std(aurocs, ddof=1)) if n > 1 else 0.0\n",
    "half_width = float(t_crit * (std_auc / math.sqrt(n))) if n > 1 else 0.0\n",
    "ci95 = [float(mean_auc - half_width), float(mean_auc + half_width)]\n",
    "\n",
    "metric_keys = [\"accuracy\",\"precision\",\"sensitivity\",\"specificity\",\"f1_score\",\"mcc\",\"p_value_fisher_two_sided\"]\n",
    "metric_agg = {}\n",
    "for k in metric_keys:\n",
    "    vals = [float(r[\"thr_metrics\"].get(k, float(\"nan\"))) for r in results]\n",
    "    mu, sd = mean_sd(vals)\n",
    "    metric_agg[k] = {\n",
    "        \"mean\": float(mu),\n",
    "        \"sd\": float(sd),\n",
    "        \"by_seed\": {str(r[\"seed\"]): float(r[\"thr_metrics\"].get(k, float(\"nan\"))) for r in results},\n",
    "    }\n",
    "\n",
    "delta_vals = [float(r[\"delta_f_minus_m\"]) for r in results]\n",
    "delta_mean, delta_sd = mean_sd(delta_vals)\n",
    "\n",
    "fnr_m_vals = []\n",
    "fnr_f_vals = []\n",
    "n_pd_m_vals = []\n",
    "n_pd_f_vals = []\n",
    "for r in results:\n",
    "    d = r[\"fnr_by_sex\"] or {}\n",
    "    fnr_m_vals.append(float(d.get(\"M\", {}).get(\"fnr\", float(\"nan\"))))\n",
    "    fnr_f_vals.append(float(d.get(\"F\", {}).get(\"fnr\", float(\"nan\"))))\n",
    "    n_pd_m_vals.append(float(d.get(\"M\", {}).get(\"n_pos\", float(\"nan\"))))\n",
    "    n_pd_f_vals.append(float(d.get(\"F\", {}).get(\"n_pos\", float(\"nan\"))))\n",
    "\n",
    "fnr_m_mean, fnr_m_sd = mean_sd(fnr_m_vals)\n",
    "fnr_f_mean, fnr_f_sd = mean_sd(fnr_f_vals)\n",
    "\n",
    "# Console summary (prints only)\n",
    "print(\"\\nD2 TEST AUROC by seed:\")\n",
    "for r in results:\n",
    "    print(f\"  seed {r['seed']}: {r['test_auc']:.6f}\")\n",
    "print(\"\\nMean D2 TEST AUROC:\", f\"{mean_auc:.6f}\")\n",
    "print(f\"95% CI (t, n=3): [{ci95[0]:.6f}, {ci95[1]:.6f}]\")\n",
    "\n",
    "print(\"\\nD2 TEST threshold used (shared mean VAL-opt):\", f\"{TEST_THR_MEAN_VAL_OPT:.6f}\")\n",
    "if TEST_THR_NOTE is not None:\n",
    "    print(\"NOTE:\", TEST_THR_NOTE)\n",
    "\n",
    "print(\"\\nThreshold metrics on D2 TEST @ shared mean VAL-opt threshold (mean ± SD across seeds):\")\n",
    "for k in [\"accuracy\",\"precision\",\"sensitivity\",\"specificity\",\"f1_score\",\"mcc\"]:\n",
    "    print(f\"  {k}: {metric_agg[k]['mean']:.6f} ± {metric_agg[k]['sd']:.6f}\")\n",
    "\n",
    "print(\"\\nFAIRNESS (H3) on D2 TEST @ shared threshold (mean ± SD across seeds):\")\n",
    "print(f\"  FNR_M: {fnr_m_mean:.6f} ± {fnr_m_sd:.6f}\")\n",
    "print(f\"  FNR_F: {fnr_f_mean:.6f} ± {fnr_f_sd:.6f}\")\n",
    "print(f\"  ΔFNR (F-M): {delta_mean:.6f} ± {delta_sd:.6f}\")\n",
    "\n",
    "# -------------------------\n",
    "# Write summary_test.json and append history_index.jsonl\n",
    "# Inputs: results + test_df counts + threshold policy\n",
    "# Outputs: summary_test.json, history_index.jsonl, and pointer json files\n",
    "# -------------------------\n",
    "summary = {\n",
    "    \"run_type\": \"D7_base_on_D2_test\",\n",
    "    \"d7_out_root\": D7_OUT_ROOT,\n",
    "    \"d2_out_root\": D2_OUT_ROOT,\n",
    "    \"d2_manifest_all\": D2_MANIFEST_ALL,\n",
    "\n",
    "    \"base_trainval_exp_dir\": str(BASE_TRAINVAL_EXP_DIR),\n",
    "    \"trainval_exp_tag\": FULL_TRAINVAL_EXP_TAG,\n",
    "\n",
    "    \"tag_safe\": TAG_SAFE,\n",
    "    \"run_stamp\": RUN_STAMP,\n",
    "    \"seeds\": SEEDS,\n",
    "\n",
    "    \"n_test\": int(len(test_df)),\n",
    "    \"label_counts_test\": test_df[\"label_num\"].value_counts(dropna=False).to_dict(),\n",
    "    \"sex_counts_test_norm\": test_df[\"sex_norm\"].value_counts(dropna=False).to_dict(),\n",
    "    \"task_group_counts_test\": test_df[\"task_group\"].value_counts(dropna=False).to_dict(),\n",
    "\n",
    "    \"val_optimal_threshold\": {\n",
    "        \"source\": \"trainval summary_trainval.json\",\n",
    "        \"method\": str(val_opt.get(\"method\", \"unknown\")),\n",
    "        \"by_seed\": {str(k): float(v) for k, v in thr_by_seed.items()},\n",
    "        \"mean_sd\": {\"mean\": float(thr_mean), \"sd\": float(thr_sd)},\n",
    "    },\n",
    "\n",
    "    \"test_threshold_used\": {\n",
    "        \"policy\": \"mean_val_optimal_threshold_shared_across_seeds\",\n",
    "        \"threshold_used_global\": float(TEST_THR_MEAN_VAL_OPT),\n",
    "        \"note\": (TEST_THR_NOTE if TEST_THR_NOTE is not None else \"\"),\n",
    "        \"source\": TEST_THR_SOURCE,\n",
    "    },\n",
    "\n",
    "    \"test_aurocs_by_seed\": {str(r[\"seed\"]): float(r[\"test_auc\"]) for r in results},\n",
    "    \"mean_test_auroc\": float(mean_auc),\n",
    "    \"t_crit_df2_95\": float(t_crit),\n",
    "    \"half_width_95_ci\": float(half_width),\n",
    "    \"ci95_test_auroc\": ci95,\n",
    "\n",
    "    \"threshold_metrics_test_at_val_opt_mean_sd\": metric_agg,\n",
    "\n",
    "    \"fairness_test_at_val_opt\": {\n",
    "        \"definition\": \"H3: ΔFNR = FNR(F) - FNR(M), where FNR(sex)=FN/(FN+TP) computed on PD-only true labels at the shared mean VAL-opt threshold.\",\n",
    "        \"delta_fnr_F_minus_M_by_seed\": {str(r[\"seed\"]): float(r[\"delta_f_minus_m\"]) for r in results},\n",
    "        \"delta_fnr_F_minus_M_mean_sd\": {\"mean\": float(delta_mean), \"sd\": float(delta_sd)},\n",
    "        \"fnr_M_mean_sd\": {\"mean\": float(fnr_m_mean), \"sd\": float(fnr_m_sd), \"by_seed\": {str(SEEDS[i]): float(fnr_m_vals[i]) for i in range(len(SEEDS))}},\n",
    "        \"fnr_F_mean_sd\": {\"mean\": float(fnr_f_mean), \"sd\": float(fnr_f_sd), \"by_seed\": {str(SEEDS[i]): float(fnr_f_vals[i]) for i in range(len(SEEDS))}},\n",
    "        \"denominators_PD_by_seed\": {str(SEEDS[i]): {\"n_PD_M\": float(n_pd_m_vals[i]), \"n_PD_F\": float(n_pd_f_vals[i])} for i in range(len(SEEDS))},\n",
    "    },\n",
    "\n",
    "    \"tag_run_dir\": str(TAG_ROOT),\n",
    "    \"stamp_run_dir\": str(STAMP_ROOT),\n",
    "    \"tag_seed_dirs\": {str(r[\"seed\"]): r[\"tag_seed_dir\"] for r in results},\n",
    "    \"stamp_seed_dirs\": {str(r[\"seed\"]): r[\"stamp_seed_dir\"] for r in results},\n",
    "\n",
    "    \"builder_aligned\": {\n",
    "        \"run_config_json\": str(run_config_path),\n",
    "        \"dataset_summary_json\": str(dataset_summary_path),\n",
    "        \"preprocess_warnings_csv\": str(preprocess_warn_path),\n",
    "    },\n",
    "}\n",
    "\n",
    "summary_path = TEST_ROOT / \"summary_test.json\"\n",
    "with open(summary_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "history_path = TEST_ROOT / \"history_index.jsonl\"\n",
    "with open(history_path, \"a\", encoding=\"utf-8\") as f:\n",
    "    f.write(json.dumps(summary) + \"\\n\")\n",
    "\n",
    "# Small pointer files for quick navigation (safe to overwrite)\n",
    "with open(TEST_ROOT / \"last_run_pointer.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump({\"tag_safe\": TAG_SAFE, \"run_stamp\": RUN_STAMP, \"stamp_run_dir\": str(STAMP_ROOT)}, f, indent=2)\n",
    "\n",
    "with open(TAG_ROOT / \"tag_run_pointer.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump({\"tag_safe\": TAG_SAFE, \"run_stamp\": RUN_STAMP, \"stamp_run_dir\": str(STAMP_ROOT)}, f, indent=2)\n",
    "\n",
    "print(\"\\nWROTE summary:\", str(summary_path))\n",
    "print(\"APPENDED history index:\", str(history_path))\n",
    "print(\"TAG folder (stable):\", str(TAG_ROOT))\n",
    "print(\"STAMP folder (this execution):\", str(STAMP_ROOT))\n",
    "print(\"Open this folder to access artifacts:\", str(TEST_ROOT))\n",
    "\n",
    "# -------------------------\n",
    "# Stop the Colab runtime (GPU release)\n",
    "# Inputs: none\n",
    "# Outputs: runtime unassigned when supported\n",
    "# -------------------------\n",
    "print(\"\\nAll done. Unassigning the runtime to stop the L4 instance...\")\n",
    "try:\n",
    "    from google.colab import runtime  # type: ignore\n",
    "    print(\"Calling runtime.unassign() now.\")\n",
    "    runtime.unassign()\n",
    "except Exception as e:\n",
    "    print(\"Could not unassign runtime automatically. You can stop the runtime manually in Colab.\")\n",
    "    print(\"Reason:\", repr(e))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "f532fa80eb7c4a5b9140b24921a8a42a",
      "e52e30178126478ca8d2fa6e638c1fd5",
      "61b8b4826573401091edc7587e7e4aa8",
      "ae701becac3b47278edd675318e8e4ad",
      "9ab47a1f22fd4a22a76d1e6292b6f227",
      "afd1cc043a79489a94c82f85226f3e15",
      "670d9cc7f33f41cd841eef008b0cc758",
      "032cb7fe5b9c473c898d2da41dbd22a7",
      "52c10bd84b27434ea75d95b2351e8216",
      "cf81133268904deb9a9d4aaede7759ae",
      "88dfe8420b4e400dacf601e3a3f54f28",
      "9468cd2407cb42ed9b570ba40096652e",
      "1571a0b95c024dfba36cc8fcf69e92b9",
      "013476e2a8894bf4b51865e64e937916",
      "617f97a79f1447d58a1c692227cfc453",
      "0d510cd3d5cc41298380a65671bf3df6",
      "cdaf22a946ef4616909e22eece4c228d",
      "22e9b59c5b0346b3af741c78a635d838",
      "3c231d212c5d47b6956e306fb7e36c30",
      "546ff866af5d4ca685fb9f5f72e5e423",
      "15ec2d920b7e479881a1aebcdd4f7b40",
      "1267d00f7fab4a4493fcd95e177dc82b",
      "72cbd8851a704fdfaed2698057daa3d4",
      "8f5cf1f6885046e0ac170d57e62f8ada",
      "93b0c9f2523d457c93334e81dc5e8abf",
      "e98d1cee9b9248c0871ab4463fb036dd",
      "488785a4879c4000aee958c1a13fa832",
      "ebf235bd52f4418680160e5ff1791acc",
      "412c14b7bd3146da9f40fbb94c5e93ba",
      "7f4c6c7e299f4a818294ae2159d67725",
      "42f0d58d8dc5486e82628e584592eaf7",
      "51cf28f8df444b1e999aee93f8e4aa5c",
      "b2d7761f1754412ca8a242db717ff471",
      "6134b39e7f2743048fb86de5e0fb959d",
      "b01c9807ca2e49679629f764a0220a8c",
      "a4d80db4bb624c4ba6ae2e18ebf0374d",
      "b9095a85cea840d3aa853b187a8661d8",
      "6eccbf25c4be4a279a39190ee5e1b0f4",
      "850e9d58ca6c4fecae08e5abd2a27e20",
      "d899011af9b04aad954ee4fe6d910978",
      "c807e724c2a44667add36e13a0c32dd9",
      "a9ca58cb8f3b487dbbd1bc6620caa7da",
      "bbede8031ad94f3b94efb6c813364801",
      "5c988735c4394f9fbdf1525edd67c2f2",
      "3fdf769d57f844e48dcc1021c460b29f",
      "c7a0818caadb4c19bb24becf5d779441",
      "9f96244cafd445229a7583203927da59",
      "2076e6ea5f594e4a8e6e4d0561124c99",
      "5fcf71bea0bf4b5dbc4e0ac01495ac5c",
      "4792de41e4c94b10adbb8637ee677abb",
      "3531acb5a25b4e0c832900a8f10bd071",
      "cd5d258891334aac96c51066d9ef4294",
      "8aec878b016a40deb94a3fed22b59841",
      "ee1c6fa22c1946788823e73d8238e00e",
      "46999a55ea80418eab448d5b91573f51",
      "8006e28f7cf64cffacdc94685e2b884a",
      "4e078c4dfb9849d589451f064d3f61e5",
      "6e26cb2bc2014d1c9a0d219ddbcd658d",
      "fc462487abaf456db8864f45efa1cfd2",
      "e8dd9529fe424d44a9f596303746cb12",
      "bfa49740a481405d89bb878d1437770b",
      "6861a2523599471b83276a29f14d7595",
      "ee92231e88ab41068e3afd6a6e9fd54a",
      "82a1fccfc9904351b5e9ef0d39ca6b0a",
      "05b70b4bb3fe42a5998ca3a35b40f8e3",
      "7774934e3f74414bb7e12fb744e5fab0"
     ]
    },
    "id": "M_dQTtjyRW8U"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "The following cell builds an enhanced D7 training split called **train_enh1** by combining the full original D7 training data with a small, carefully selected portion of D2. It starts by reading `manifest_all.csv` from both datasets and enforcing a fixed column structure so the output stays compatible with the rest of the pipeline. Early checks are applied to catch problems right away. Dataset identifiers must be exactly D7 or D2, labels must be Healthy (0) or Parkinson’s (1), and sex values are standardized to M or F, with D2 values explicitly mapped from “male” and “female”. Any literal `\"NaN\"` strings are converted to real missing values so they do not cause issues later.\n",
    "\n",
    "From the D2 training split, the cell selects 10 percent of speakers using speaker level sampling. The number of speakers is rounded up and adjusted to an even count. All training clips for each selected speaker are included, and a strict check confirms that each speaker has a single, consistent diagnosis label across all clips. The selected speakers are balanced by class so there are equal numbers of Healthy and Parkinson’s speakers. A fixed random seed is used so the same speakers are chosen every time, making the process repeatable.\n",
    "\n",
    "Before copying any audio files, the cell checks that every source file referenced by the selected D7 and D2 rows actually exists. It then prepares a copy plan and checks the destination folder under `clips/train_enh1/` to avoid overwriting files by mistake. Files are copied, not moved, so the original datasets are left unchanged. If a destination file already exists and has the same file size, it is skipped. If a file exists but the size does not match, the run stops immediately to prevent silent errors.\n",
    "\n",
    "Two filename rules are used to keep everything traceable and avoid name conflicts. Original D7 training clips keep their existing filenames. D2 clips are renamed in a deterministic way that encodes the class, speaker identity, task type, and a stable index. During the process, structured outputs are written early and safely, including a run configuration file, a preprocessing warnings log, and a dataset summary file. Backups are created automatically so earlier results are not overwritten.\n",
    "\n",
    "After all files are copied, the cell creates `manifest_train_enh1.csv` using the locked schema and adds a final `source_dataset` column. All rows are marked with `split = \"train_enh1\"` and keep `dataset = \"D7\"` so the enhanced data can be used directly by the standard D7 training code. File paths and sample IDs are updated to match the copied files, and the source of each clip, either D7 or D2, is clearly recorded. Final checks confirm that labels and sex values are valid, there are no literal `\"NaN\"` strings, and every audio file listed in the manifest exists. The cell finishes by writing a success summary with counts by clip, speaker, label, sex, and source, and prints the main output locations for reference."
   ],
   "metadata": {
    "id": "Bhd8b0uLwv9K"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# =========================\n",
    "# D7 TRAIN_ENH1 BUILDER — Add ceil(10% of D2 TRAIN speakers)\n",
    "# Purpose:\n",
    "# - Create an augmented D7 training clip folder by copying:\n",
    "#   1) all existing D7 train clips (keep original filenames)\n",
    "#   2) plus a speaker-balanced 10% sample of D2 train speakers (copy all their train clips)\n",
    "# Outputs:\n",
    "# - New clips folder: clips/train_enh1/\n",
    "# - New manifest: manifests/manifest_train_enh1.csv (locked schema)\n",
    "# - Run records: config/.../run_config.json and logs/.../dataset_summary.json + preprocess_warnings.csv\n",
    "# Notes:\n",
    "# - Copy only (never move)\n",
    "# - No overwrite: if destination exists and size matches, skip; if size differs, fail-fast\n",
    "# =========================\n",
    "\n",
    "import os, json, re, shutil, math\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# -------------------------\n",
    "# Mount Drive (Colab only)\n",
    "# Inputs: none\n",
    "# Outputs: Drive mounted at /content/drive when needed\n",
    "# -------------------------\n",
    "try:\n",
    "    from google.colab import drive  # type: ignore\n",
    "    if not os.path.exists(\"/content/drive\"):\n",
    "        drive.mount(\"/content/drive\")\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# -------------------------\n",
    "# Paths and output locations\n",
    "# Inputs: D7_OUT_ROOT + D2_OUT_ROOT (preprocessed dataset roots)\n",
    "# Outputs: standard folders created if missing (clips, manifests, config, logs)\n",
    "# -------------------------\n",
    "D7_OUT_ROOT = Path(\"/content/drive/MyDrive/AI_PD_Project/Datasets/D7-Multilingual (D1_D4_D5v2_D6)/preprocessed_v1\")\n",
    "D2_OUT_ROOT = Path(\"/content/drive/MyDrive/AI_PD_Project/Datasets/D2-Slovak (EWA-DB)/EWA-DB/preprocessed_v1\")\n",
    "\n",
    "D7_MANIFEST_ALL = D7_OUT_ROOT / \"manifests\" / \"manifest_all.csv\"\n",
    "D2_MANIFEST_ALL = D2_OUT_ROOT / \"manifests\" / \"manifest_all.csv\"\n",
    "\n",
    "# train_enh → train_enh1 (folder + manifest)\n",
    "TRAIN_ENH_DIR = D7_OUT_ROOT / \"clips\" / \"train_enh1\"\n",
    "MANIFEST_TRAIN_ENH = D7_OUT_ROOT / \"manifests\" / \"manifest_train_enh1.csv\"\n",
    "\n",
    "# Standard layout dirs (match merge-builder style)\n",
    "clips_dir = D7_OUT_ROOT / \"clips\"\n",
    "manif_dir = D7_OUT_ROOT / \"manifests\"\n",
    "cfg_dir   = D7_OUT_ROOT / \"config\" / \"D7_Enh1_on_D2_Test\"\n",
    "logs_dir  = D7_OUT_ROOT / \"logs\" / \"D7_Enh1_on_D2_Test\"\n",
    "\n",
    "for d in [clips_dir, manif_dir, cfg_dir, logs_dir]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "TRAIN_ENH_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# All rows written into manifest_train_enh1.csv use split=\"train_enh1\"\n",
    "TRAIN_ENH_SPLIT_NAME = \"train_enh1\"\n",
    "\n",
    "# -------------------------\n",
    "# Locked manifest schema\n",
    "# Inputs: D7/D2 manifest_all.csv\n",
    "# Outputs: train_enh manifest with the same column order + a source_dataset column\n",
    "# -------------------------\n",
    "CANON_COLS = [\n",
    "    \"split\",\n",
    "    \"dataset\",\n",
    "    \"task\",\n",
    "    \"speaker_id\",\n",
    "    \"sample_id\",\n",
    "    \"label_str\",\n",
    "    \"label_num\",\n",
    "    \"age\",\n",
    "    \"sex\",\n",
    "    \"speaker_key_rel\",\n",
    "    \"clip_path\",\n",
    "    \"duration_sec\",\n",
    "    \"source_path\",\n",
    "    \"clip_start_sec\",\n",
    "    \"clip_end_sec\",\n",
    "    \"sr_hz\",\n",
    "    \"channels\",\n",
    "    \"clip_is_contiguous\",\n",
    "]\n",
    "FINAL_COLS = CANON_COLS + [\"source_dataset\"]\n",
    "\n",
    "# -------------------------\n",
    "# Sampling policy (10% of D2 train speakers)\n",
    "# Inputs: D2 train speakers + labels\n",
    "# Outputs: a fixed, repeatable speaker sample (seeded)\n",
    "# -------------------------\n",
    "TEN_PCT         = 0.10\n",
    "ROUNDING_POLICY = \"ceil_then_make_even_by_rounding_down_if_needed\"\n",
    "BALANCE_POLICY  = \"speaker-balanced\"  # equal speakers HC/PD\n",
    "SPEAKER_ID_COL  = \"speaker_id\"\n",
    "RNG_SEED        = 1337\n",
    "\n",
    "# Label mapping used everywhere\n",
    "LABEL_MAP_NOTE  = \"label_num mapping: 0=Healthy, 1=Parkinson\"\n",
    "\n",
    "# -------------------------\n",
    "# Guardrails and file writing helpers\n",
    "# Inputs: runtime checks + planned outputs\n",
    "# Outputs: warnings table + atomic writes for csv/json\n",
    "# -------------------------\n",
    "warnings_rows = []\n",
    "\n",
    "def require(cond: bool, msg: str):\n",
    "    \"\"\"Fail-fast check for required conditions.\"\"\"\n",
    "    if not cond:\n",
    "        raise RuntimeError(msg)\n",
    "\n",
    "def add_warn(src: str, level: str, code: str, message: str, **extra):\n",
    "    \"\"\"Structured warning log row saved to preprocess_warnings.csv.\"\"\"\n",
    "    row = {\n",
    "        \"ts\": datetime.utcnow().isoformat(),\n",
    "        \"src\": src,\n",
    "        \"level\": str(level).upper(),\n",
    "        \"code\": code,\n",
    "        \"message\": message,\n",
    "    }\n",
    "    row.update(extra)\n",
    "    warnings_rows.append(row)\n",
    "\n",
    "def count_by_level(rows):\n",
    "    \"\"\"Count INFO/WARN/ERROR in the warnings table.\"\"\"\n",
    "    out = {\"ERROR\": 0, \"WARN\": 0, \"INFO\": 0}\n",
    "    for r in rows:\n",
    "        lvl = str(r.get(\"level\", \"INFO\")).upper()\n",
    "        out[lvl] = out.get(lvl, 0) + 1\n",
    "    return out\n",
    "\n",
    "def _maybe_backup_existing(dst: Path):\n",
    "    \"\"\"\n",
    "    Backup an existing output file before rewriting it.\n",
    "    Used for builder outputs (manifest/config/logs).\n",
    "    \"\"\"\n",
    "    if dst.exists():\n",
    "        ts = datetime.utcnow().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        bak = dst.with_suffix(dst.suffix + f\".bak_{ts}\")\n",
    "        try:\n",
    "            shutil.copy2(dst, bak)\n",
    "        except Exception:\n",
    "            raise RuntimeError(f\"Failed to create backup for existing file: {str(dst)}\")\n",
    "\n",
    "def atomic_write_text(dst: Path, text: str):\n",
    "    \"\"\"Write a text file via a temp file to avoid partial results.\"\"\"\n",
    "    _maybe_backup_existing(dst)\n",
    "    tmp = dst.with_suffix(dst.suffix + \".tmp\")\n",
    "    with open(tmp, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(text)\n",
    "    os.replace(tmp, dst)\n",
    "\n",
    "def atomic_write_csv(dst: Path, df: pd.DataFrame):\n",
    "    \"\"\"Write a CSV via a temp file; missing values are blank, not the string 'NaN'.\"\"\"\n",
    "    _maybe_backup_existing(dst)\n",
    "    tmp = dst.with_suffix(dst.suffix + \".tmp\")\n",
    "    df.to_csv(tmp, index=False, na_rep=\"\")\n",
    "    os.replace(tmp, dst)\n",
    "\n",
    "def safe_token(s, max_len=32, default=\"NA\"):\n",
    "    \"\"\"\n",
    "    Make a short filename-safe token.\n",
    "    Used only for new D2-derived filenames.\n",
    "    \"\"\"\n",
    "    if pd.isna(s):\n",
    "        return default\n",
    "    s = str(s).strip()\n",
    "    s = re.sub(r\"\\s+\", \"_\", s)\n",
    "    s = re.sub(r\"[^A-Za-z0-9_]+\", \"\", s)\n",
    "    return (s[:max_len] if s else default)\n",
    "\n",
    "def label_str_from_num(v):\n",
    "    \"\"\"Convert numeric labels into canonical strings.\"\"\"\n",
    "    if pd.isna(v):\n",
    "        return np.nan\n",
    "    iv = int(v)\n",
    "    if iv == 0:\n",
    "        return \"Healthy\"\n",
    "    if iv == 1:\n",
    "        return \"Parkinson\"\n",
    "    return np.nan\n",
    "\n",
    "def hc_pd_from_label_num(v):\n",
    "    \"\"\"Short class tag used only in new filenames.\"\"\"\n",
    "    return \"PD\" if int(v) == 1 else \"HC\"\n",
    "\n",
    "def normalize_sex_to_MF_D7(x):\n",
    "    \"\"\"D7 sex normalization to M/F with NaN for unknown.\"\"\"\n",
    "    if pd.isna(x):\n",
    "        return np.nan\n",
    "    s = str(x).strip().lower()\n",
    "    if s in [\"m\", \"male\", \"1\"]:\n",
    "        return \"M\"\n",
    "    if s in [\"f\", \"female\", \"0\"]:\n",
    "        return \"F\"\n",
    "    if s in [\"\", \"nan\", \"none\", \"unknown\", \"u\"]:\n",
    "        return np.nan\n",
    "    return np.nan\n",
    "\n",
    "def normalize_sex_to_MF_D2(x):\n",
    "    \"\"\"D2 sex normalization from male/female to M/F.\"\"\"\n",
    "    if pd.isna(x):\n",
    "        return np.nan\n",
    "    s = str(x).strip().lower()\n",
    "    if s in [\"male\", \"m\"]:\n",
    "        return \"M\"\n",
    "    if s in [\"female\", \"f\"]:\n",
    "        return \"F\"\n",
    "    if s in [\"\", \"nan\", \"none\", \"unknown\", \"u\"]:\n",
    "        return np.nan\n",
    "    return np.nan\n",
    "\n",
    "def summarize_counts(df: pd.DataFrame, split_name: str):\n",
    "    \"\"\"Small summary block written to dataset_summary.json.\"\"\"\n",
    "    out = {}\n",
    "    out[\"total_rows\"] = int(len(df))\n",
    "    out[\"split\"] = split_name\n",
    "    out[\"label_counts_total\"] = {str(k): int(v) for k, v in df[\"label_num\"].value_counts(dropna=False).to_dict().items()}\n",
    "    out[\"by_source_dataset\"] = {sd: int((df[\"source_dataset\"] == sd).sum()) for sd in sorted(df[\"source_dataset\"].dropna().unique())}\n",
    "    out[\"sex_counts\"] = {str(k): int(v) for k, v in df[\"sex\"].value_counts(dropna=False).to_dict().items()}\n",
    "    out[\"n_unique_speakers\"] = int(df[\"speaker_id\"].astype(str).nunique()) if \"speaker_id\" in df.columns else int(0)\n",
    "    return out\n",
    "\n",
    "# -------------------------\n",
    "# Load manifests and validate schema\n",
    "# Inputs: D7 manifest_all.csv and D2 manifest_all.csv\n",
    "# Outputs: in-memory tables with cleaned NaNs, validated labels, normalized sex\n",
    "# -------------------------\n",
    "print(\"D7_OUT_ROOT:\", str(D7_OUT_ROOT))\n",
    "print(\"D2_OUT_ROOT:\", str(D2_OUT_ROOT))\n",
    "print(\"D7_MANIFEST_ALL:\", str(D7_MANIFEST_ALL))\n",
    "print(\"D2_MANIFEST_ALL:\", str(D2_MANIFEST_ALL))\n",
    "print(\"TRAIN_ENH_DIR:\", str(TRAIN_ENH_DIR))\n",
    "print(\"MANIFEST_TRAIN_ENH:\", str(MANIFEST_TRAIN_ENH))\n",
    "print(\"cfg_dir:\", str(cfg_dir))\n",
    "print(\"logs_dir:\", str(logs_dir))\n",
    "print(\"TRAIN_ENH_SPLIT_NAME:\", TRAIN_ENH_SPLIT_NAME)\n",
    "\n",
    "require(D7_MANIFEST_ALL.exists(), f\"Missing D7 manifest_all.csv: {str(D7_MANIFEST_ALL)}\")\n",
    "require(D2_MANIFEST_ALL.exists(), f\"Missing D2 manifest_all.csv: {str(D2_MANIFEST_ALL)}\")\n",
    "\n",
    "d7 = pd.read_csv(D7_MANIFEST_ALL)\n",
    "d2 = pd.read_csv(D2_MANIFEST_ALL)\n",
    "\n",
    "missing_d7 = [c for c in CANON_COLS if c not in d7.columns]\n",
    "missing_d2 = [c for c in CANON_COLS if c not in d2.columns]\n",
    "require(len(missing_d7) == 0, f\"D7 manifest missing required columns (locked schema): {missing_d7}\")\n",
    "require(len(missing_d2) == 0, f\"D2 manifest missing required columns (locked schema): {missing_d2}\")\n",
    "\n",
    "# Add source_dataset if missing so the final merged manifest can always track provenance.\n",
    "if \"source_dataset\" not in d7.columns:\n",
    "    d7[\"source_dataset\"] = \"D7\"\n",
    "if \"source_dataset\" not in d2.columns:\n",
    "    d2[\"source_dataset\"] = \"D2\"\n",
    "\n",
    "# Replace the literal text \"NaN\" with true missing values for consistent typing.\n",
    "for df in [d7, d2]:\n",
    "    for col in [\"sex\", \"age\", \"duration_sec\", \"clip_start_sec\", \"clip_end_sec\", \"speaker_key_rel\", \"speaker_id\", \"task\", \"sample_id\"]:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].replace(\"NaN\", np.nan)\n",
    "\n",
    "# Confirm each manifest really corresponds to the intended dataset ID.\n",
    "def infer_dataset_id(df: pd.DataFrame, fallback: str) -> str:\n",
    "    if \"dataset\" in df.columns and df[\"dataset\"].notna().any():\n",
    "        return str(df[\"dataset\"].astype(str).value_counts(dropna=True).idxmax())\n",
    "    return fallback\n",
    "\n",
    "d7_dataset_id = infer_dataset_id(d7, \"DX\")\n",
    "d2_dataset_id = infer_dataset_id(d2, \"DX\")\n",
    "\n",
    "require(d7_dataset_id == \"D7\", f\"Expected D7 manifest dataset=='D7' but inferred {d7_dataset_id!r}. Check D7_OUT_ROOT/manifest.\")\n",
    "require(d2_dataset_id == \"D2\", f\"Expected D2 manifest dataset=='D2' but inferred {d2_dataset_id!r}. Check D2_OUT_ROOT/manifest.\")\n",
    "\n",
    "# Keep only the intended dataset rows (extra safety if manifests were ever merged).\n",
    "d7 = d7[d7[\"dataset\"].astype(str) == \"D7\"].copy()\n",
    "d2 = d2[d2[\"dataset\"].astype(str) == \"D2\"].copy()\n",
    "\n",
    "# Enforce labels are strictly binary and set canonical label_str.\n",
    "for name, df in [(\"D7\", d7), (\"D2\", d2)]:\n",
    "    bad = sorted(set(df[\"label_num\"].dropna().unique()) - {0, 1})\n",
    "    require(len(bad) == 0, f\"{name} label_num contains values outside {{0,1}}: {bad}\")\n",
    "    df[\"label_str\"] = df[\"label_num\"].map(label_str_from_num)\n",
    "    bad_str = sorted(set(df[\"label_str\"].dropna().unique()) - {\"Healthy\", \"Parkinson\"})\n",
    "    require(len(bad_str) == 0, f\"{name} label_str contains unexpected values: {bad_str}\")\n",
    "\n",
    "# Normalize sex to M/F for consistent downstream metrics.\n",
    "d7[\"sex\"] = d7[\"sex\"].map(normalize_sex_to_MF_D7)\n",
    "d2[\"sex\"] = d2[\"sex\"].map(normalize_sex_to_MF_D2)\n",
    "\n",
    "bad_sex_d7 = sorted(set(d7[\"sex\"].dropna().unique()) - {\"M\", \"F\"})\n",
    "bad_sex_d2 = sorted(set(d2[\"sex\"].dropna().unique()) - {\"M\", \"F\"})\n",
    "require(len(bad_sex_d7) == 0, f\"D7 sex contains unexpected values after normalization: {bad_sex_d7}\")\n",
    "require(len(bad_sex_d2) == 0, f\"D2 sex contains unexpected values after normalization: {bad_sex_d2}\")\n",
    "\n",
    "# -------------------------\n",
    "# D7 train rows to include (baseline portion)\n",
    "# Inputs: D7 manifest rows where split == \"train\"\n",
    "# Outputs: d7_train table used for copying + manifest building\n",
    "# -------------------------\n",
    "d7_train = d7[d7[\"split\"].astype(str) == \"train\"].copy()\n",
    "require(len(d7_train) > 0, \"D7 train split has 0 rows. Expected split=='train' to exist in D7 manifest.\")\n",
    "\n",
    "print(\"\\nD7 train rows:\", int(len(d7_train)))\n",
    "print(\"D7 train label counts:\", d7_train[\"label_num\"].value_counts(dropna=False).to_dict())\n",
    "\n",
    "# -------------------------\n",
    "# D2 speaker-balanced sampling (10% of train speakers)\n",
    "# Inputs: D2 manifest rows where split == \"train\"\n",
    "# Outputs: selected_speakers list + d2_sel (all train clips for those speakers)\n",
    "# -------------------------\n",
    "d2_train = d2[d2[\"split\"].astype(str) == \"train\"].copy()\n",
    "require(len(d2_train) > 0, \"D2 train split has 0 rows. Expected split=='train' to exist in D2 manifest.\")\n",
    "\n",
    "print(\"\\nD2 train rows:\", int(len(d2_train)))\n",
    "print(\"D2 train label counts:\", d2_train[\"label_num\"].value_counts(dropna=False).to_dict())\n",
    "\n",
    "require(SPEAKER_ID_COL in d2_train.columns, f\"D2 manifest missing speaker id column: {SPEAKER_ID_COL}\")\n",
    "\n",
    "# Check that each speaker belongs to exactly one class (required for speaker-level sampling).\n",
    "speaker_labels = (\n",
    "    d2_train\n",
    "    .groupby(SPEAKER_ID_COL)[\"label_num\"]\n",
    "    .apply(lambda s: sorted(set(s.dropna().astype(int).tolist())))\n",
    ")\n",
    "mixed = speaker_labels[speaker_labels.apply(lambda x: len(x) != 1)]\n",
    "if len(mixed) > 0:\n",
    "    sample_mixed = mixed.head(10).to_dict()\n",
    "    raise RuntimeError(\n",
    "        \"D2 train has speakers with mixed label_num values across clips. \"\n",
    "        \"Speaker-level sampling requires each speaker to belong to exactly one class.\\n\"\n",
    "        f\"Examples (speaker_id -> labels): {sample_mixed}\"\n",
    "    )\n",
    "\n",
    "speaker_to_label = speaker_labels.apply(lambda x: int(x[0])).to_dict()\n",
    "all_speakers = sorted(list(speaker_to_label.keys()))\n",
    "total_speakers = len(all_speakers)\n",
    "\n",
    "hc_speakers = sorted([spk for spk, y in speaker_to_label.items() if y == 0])\n",
    "pd_speakers = sorted([spk for spk, y in speaker_to_label.items() if y == 1])\n",
    "\n",
    "require(len(hc_speakers) > 0 and len(pd_speakers) > 0, \"D2 train does not contain both HC and PD speakers; cannot do balanced sampling.\")\n",
    "\n",
    "# Compute target speaker count: ceil(10%), then force even so HC/PD can be equal.\n",
    "target_total = int(math.ceil(TEN_PCT * total_speakers))\n",
    "if target_total % 2 != 0:\n",
    "    target_total -= 1  # make even\n",
    "\n",
    "# Ensure at least one speaker per class.\n",
    "if target_total < 2:\n",
    "    target_total = 2\n",
    "\n",
    "target_per_class = target_total // 2\n",
    "\n",
    "require(target_per_class <= len(hc_speakers), f\"Not enough HC speakers in D2 train for target_per_class={target_per_class}. HC speakers={len(hc_speakers)}\")\n",
    "require(target_per_class <= len(pd_speakers), f\"Not enough PD speakers in D2 train for target_per_class={target_per_class}. PD speakers={len(pd_speakers)}\")\n",
    "\n",
    "# Sample speakers deterministically from the fixed seed.\n",
    "rng = np.random.default_rng(RNG_SEED)\n",
    "sel_hc = sorted(rng.choice(hc_speakers, size=target_per_class, replace=False).tolist())\n",
    "sel_pd = sorted(rng.choice(pd_speakers, size=target_per_class, replace=False).tolist())\n",
    "selected_speakers = sorted(sel_hc + sel_pd)\n",
    "\n",
    "# Keep all D2 train clips for the selected speakers.\n",
    "d2_sel = d2_train[d2_train[SPEAKER_ID_COL].astype(str).isin([str(x) for x in selected_speakers])].copy()\n",
    "\n",
    "print(\"\\nD2 speaker sampling:\")\n",
    "print(\"  speaker_id_col used:\", SPEAKER_ID_COL)\n",
    "print(\"  total D2 train speakers:\", total_speakers)\n",
    "print(f\"  target speakers total (10% ceil, balanced, odd->down): {target_total} => {target_per_class} HC + {target_per_class} PD\")\n",
    "print(\"  selected D2 rows (all clips for selected speakers):\", int(len(d2_sel)))\n",
    "print(\"  selected label counts:\", d2_sel[\"label_num\"].value_counts(dropna=False).to_dict())\n",
    "\n",
    "# -------------------------\n",
    "# Verify every referenced clip exists (before any copying)\n",
    "# Inputs: d7_train clip_path and d2_sel clip_path\n",
    "# Outputs: fail-fast error if any source audio is missing\n",
    "# -------------------------\n",
    "def fail_fast_missing_paths(df: pd.DataFrame, name: str):\n",
    "    missing_paths = []\n",
    "    for p in tqdm(df[\"clip_path\"].astype(str).tolist(), desc=f\"Check {name} clip_path exists\", dynamic_ncols=True):\n",
    "        if not os.path.exists(p):\n",
    "            missing_paths.append(p)\n",
    "            if len(missing_paths) >= 25:\n",
    "                break\n",
    "    if missing_paths:\n",
    "        raise FileNotFoundError(f\"{name}: missing clip_path(s). Examples (up to 25): {missing_paths}\")\n",
    "\n",
    "fail_fast_missing_paths(d7_train, \"D7 TRAIN\")\n",
    "fail_fast_missing_paths(d2_sel, \"D2 TRAIN (selected speakers)\")\n",
    "\n",
    "# -------------------------\n",
    "# Plan destination filenames (D7 keeps names, D2 gets collision-proof names)\n",
    "# Inputs: d7_train + d2_sel\n",
    "# Outputs: copy_plan list (source path → destination path)\n",
    "# -------------------------\n",
    "# Rules:\n",
    "# - D7 train clips: copy into train_enh1 using the same basename\n",
    "# - D2 selected clips: rename deterministically to avoid collisions in the shared folder\n",
    "copy_plan = []  # dicts: src_path, dst_path, origin, source_dataset, row_index\n",
    "\n",
    "# D7 → train_enh1 (same filenames)\n",
    "for i, row in d7_train.reset_index(drop=True).iterrows():\n",
    "    src_path = Path(str(row[\"clip_path\"]))\n",
    "    dst_path = TRAIN_ENH_DIR / src_path.name\n",
    "    copy_plan.append({\n",
    "        \"src_path\": str(src_path),\n",
    "        \"dst_path\": str(dst_path),\n",
    "        \"origin\": \"D7_train_existing\",\n",
    "        \"source_dataset\": str(row.get(\"source_dataset\", \"D7\")),\n",
    "        \"row_index\": int(i),\n",
    "    })\n",
    "\n",
    "# D2 selected → train_enh1 (deterministic order and names)\n",
    "d2_sel_reset = d2_sel.reset_index(drop=True).copy()\n",
    "\n",
    "# Sort so the numbering stays stable across reruns.\n",
    "d2_sel_reset[\"_speaker_tok\"] = d2_sel_reset[\"speaker_id\"].map(lambda x: safe_token(x, 32, \"NA\"))\n",
    "d2_sel_reset[\"_task_tok\"] = d2_sel_reset[\"task\"].map(lambda x: safe_token(x, 12, \"0\"))\n",
    "d2_sel_reset[\"_clip_path_str\"] = d2_sel_reset[\"clip_path\"].astype(str)\n",
    "d2_sel_reset = d2_sel_reset.sort_values(by=[\"_speaker_tok\", \"_task_tok\", \"_clip_path_str\"]).reset_index(drop=True)\n",
    "\n",
    "for j, row in d2_sel_reset.iterrows():\n",
    "    src_path = Path(str(row[\"clip_path\"]))\n",
    "    hc_pd = hc_pd_from_label_num(row[\"label_num\"])\n",
    "    spk_tok = safe_token(row[\"speaker_id\"], 32, \"NA\")\n",
    "    task_tok = safe_token(row[\"task\"], 12, \"0\")\n",
    "\n",
    "    out_name = f\"D7_D2add_{hc_pd}_{spk_tok}_{task_tok}_{j+1:06d}.wav\"\n",
    "    dst_path = TRAIN_ENH_DIR / out_name\n",
    "\n",
    "    copy_plan.append({\n",
    "        \"src_path\": str(src_path),\n",
    "        \"dst_path\": str(dst_path),\n",
    "        \"origin\": \"D2_train_selected\",\n",
    "        \"source_dataset\": \"D2\",\n",
    "        \"row_index\": int(j),\n",
    "    })\n",
    "\n",
    "# -------------------------\n",
    "# Preflight checks (no overwrite policy)\n",
    "# Inputs: copy_plan + filesystem\n",
    "# Outputs: preflight stats + warnings; stops early on size mismatches\n",
    "# -------------------------\n",
    "n_dest_exists_ok = 0\n",
    "n_dest_exists_mismatch = 0\n",
    "n_will_copy = 0\n",
    "\n",
    "print(\"\\nPreflight: destination existence and size checks (no overwrite)...\")\n",
    "for item in tqdm(copy_plan, desc=\"Preflight (dest checks)\", dynamic_ncols=True):\n",
    "    sp = Path(item[\"src_path\"])\n",
    "    dp = Path(item[\"dst_path\"])\n",
    "    require(sp.exists(), f\"Source clip missing at preflight (should have been caught earlier): {str(sp)}\")\n",
    "\n",
    "    if dp.exists():\n",
    "        try:\n",
    "            if dp.stat().st_size == sp.stat().st_size:\n",
    "                n_dest_exists_ok += 1\n",
    "            else:\n",
    "                n_dest_exists_mismatch += 1\n",
    "                add_warn(\n",
    "                    \"D7_TRAIN_ENH\", \"ERROR\", \"DEST_EXISTS_SIZE_MISMATCH\",\n",
    "                    \"Destination exists but file size differs from source\",\n",
    "                    src_path=str(sp), dest_path=str(dp),\n",
    "                    src_size=int(sp.stat().st_size), dest_size=int(dp.stat().st_size),\n",
    "                )\n",
    "        except Exception as e:\n",
    "            n_dest_exists_mismatch += 1\n",
    "            add_warn(\n",
    "                \"D7_TRAIN_ENH\", \"ERROR\", \"DEST_EXISTS_STAT_ERROR\",\n",
    "                \"Failed to stat source/destination during preflight\",\n",
    "                src_path=str(sp), dest_path=str(dp), error=repr(e),\n",
    "            )\n",
    "    else:\n",
    "        n_will_copy += 1\n",
    "\n",
    "preflight_stats = {\n",
    "    \"total_planned_files\": int(len(copy_plan)),\n",
    "    \"n_dest_exists_ok\": int(n_dest_exists_ok),\n",
    "    \"n_dest_exists_mismatch\": int(n_dest_exists_mismatch),\n",
    "    \"n_will_copy\": int(n_will_copy),\n",
    "    \"warnings_by_level\": count_by_level(warnings_rows),\n",
    "}\n",
    "\n",
    "print(\"\\nPreflight summary:\")\n",
    "print(\"  Planned files:\", int(len(copy_plan)))\n",
    "print(\"  Destination exists (size OK):\", n_dest_exists_ok)\n",
    "print(\"  Destination exists (size mismatch/stat error):\", n_dest_exists_mismatch)\n",
    "print(\"  Will copy:\", n_will_copy)\n",
    "print(\"  Warnings by level:\", preflight_stats[\"warnings_by_level\"])\n",
    "\n",
    "# -------------------------\n",
    "# Write run_config + early summary (before copying)\n",
    "# Inputs: selection details + preflight stats\n",
    "# Outputs: config/run_config.json, logs/dataset_summary.json, logs/preprocess_warnings.csv\n",
    "# -------------------------\n",
    "run_config = {\n",
    "    \"dataset\": \"D7\",\n",
    "    \"mode\": \"train_enh_builder\",\n",
    "    \"created_utc\": datetime.utcnow().isoformat(),\n",
    "\n",
    "    \"d7_out_root\": str(D7_OUT_ROOT),\n",
    "    \"d2_out_root\": str(D2_OUT_ROOT),\n",
    "    \"d7_manifest_all\": str(D7_MANIFEST_ALL),\n",
    "    \"d2_manifest_all\": str(D2_MANIFEST_ALL),\n",
    "\n",
    "    \"train_enh_dir\": str(TRAIN_ENH_DIR),\n",
    "    \"manifest_train_enh\": str(MANIFEST_TRAIN_ENH),\n",
    "    \"train_enh_split_name\": TRAIN_ENH_SPLIT_NAME,\n",
    "\n",
    "    \"policy\": {\n",
    "        \"definition\": \"Add ceil(10%) of D2 TRAIN by speaker into D7 training clips folder\",\n",
    "        \"pct_speakers\": float(TEN_PCT),\n",
    "        \"rounding_policy\": ROUNDING_POLICY,\n",
    "        \"balance_policy\": BALANCE_POLICY,\n",
    "        \"speaker_id_col_used\": SPEAKER_ID_COL,\n",
    "        \"rng_seed\": int(RNG_SEED),\n",
    "        \"label_note\": LABEL_MAP_NOTE,\n",
    "        \"file_operation\": \"copy\",\n",
    "        \"no_overwrite_rule\": \"skip if dest exists with matching size; error if size differs\",\n",
    "    },\n",
    "\n",
    "    \"inputs\": {\n",
    "        \"d7_train_rows\": int(len(d7_train)),\n",
    "        \"d7_train_label_counts\": d7_train[\"label_num\"].value_counts(dropna=False).to_dict(),\n",
    "        \"d2_train_rows\": int(len(d2_train)),\n",
    "        \"d2_train_label_counts\": d2_train[\"label_num\"].value_counts(dropna=False).to_dict(),\n",
    "        \"d2_train_total_speakers\": int(total_speakers),\n",
    "        \"d2_train_speakers_hc\": int(len(hc_speakers)),\n",
    "        \"d2_train_speakers_pd\": int(len(pd_speakers)),\n",
    "    },\n",
    "\n",
    "    \"selection\": {\n",
    "        \"target_total_speakers\": int(target_total),\n",
    "        \"target_per_class\": int(target_per_class),\n",
    "        \"selected_speakers_hc\": sel_hc,\n",
    "        \"selected_speakers_pd\": sel_pd,\n",
    "        \"selected_speakers_all\": selected_speakers,\n",
    "        \"selected_d2_rows\": int(len(d2_sel)),\n",
    "        \"selected_d2_label_counts\": d2_sel[\"label_num\"].value_counts(dropna=False).to_dict(),\n",
    "    },\n",
    "\n",
    "    \"preflight\": preflight_stats,\n",
    "}\n",
    "\n",
    "early_summary = {\n",
    "    \"dataset\": \"D7\",\n",
    "    \"created_utc\": datetime.utcnow().isoformat(),\n",
    "    \"status\": \"PRECHECK_COMPLETE\",\n",
    "    \"d7_out_root\": str(D7_OUT_ROOT),\n",
    "    \"train_enh_dir\": str(TRAIN_ENH_DIR),\n",
    "    \"train_enh_split_name\": TRAIN_ENH_SPLIT_NAME,\n",
    "    \"preflight\": preflight_stats,\n",
    "    \"selection\": run_config[\"selection\"],\n",
    "}\n",
    "\n",
    "atomic_write_csv(logs_dir / \"preprocess_warnings.csv\", pd.DataFrame(warnings_rows))\n",
    "atomic_write_text(cfg_dir / \"run_config.json\", json.dumps(run_config, indent=2))\n",
    "atomic_write_text(logs_dir / \"dataset_summary.json\", json.dumps(early_summary, indent=2))\n",
    "\n",
    "# Stop here if preflight found any ERROR-level warnings.\n",
    "fatal_pre = [w for w in warnings_rows if str(w.get(\"level\", \"\")).upper() == \"ERROR\"]\n",
    "if fatal_pre:\n",
    "    print(\"\\n❌ Preflight found ERROR conditions. No copying performed.\")\n",
    "    print(\"   Logs written:\")\n",
    "    print(\"   -\", str(logs_dir / \"preprocess_warnings.csv\"))\n",
    "    print(\"   -\", str(logs_dir / \"dataset_summary.json\"))\n",
    "    print(\"   -\", str(cfg_dir / \"run_config.json\"))\n",
    "    raise RuntimeError(f\"Preflight failed with {len(fatal_pre)} ERROR(s). See logs/preprocess_warnings.csv.\")\n",
    "\n",
    "# -------------------------\n",
    "# Copy clips into clips/train_enh1 (skip existing)\n",
    "# Inputs: copy_plan\n",
    "# Outputs: copied audio files under clips/train_enh1/ + updated warnings log\n",
    "# -------------------------\n",
    "print(\"\\nCopy stage: copying into train_enh1 (no overwrite)...\")\n",
    "\n",
    "copied = 0\n",
    "skipped_exists = 0\n",
    "copy_errors = 0\n",
    "\n",
    "for item in tqdm(copy_plan, desc=\"Copying clips\", dynamic_ncols=True):\n",
    "    sp = Path(item[\"src_path\"])\n",
    "    dp = Path(item[\"dst_path\"])\n",
    "\n",
    "    if not sp.exists():\n",
    "        copy_errors += 1\n",
    "        add_warn(\"D7_TRAIN_ENH\", \"ERROR\", \"SOURCE_CLIP_DISAPPEARED\", \"Source clip missing during copy stage\", clip_path=str(sp))\n",
    "        continue\n",
    "\n",
    "    if dp.exists():\n",
    "        skipped_exists += 1\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        shutil.copy2(sp, dp)\n",
    "        copied += 1\n",
    "    except Exception as e:\n",
    "        copy_errors += 1\n",
    "        add_warn(\"D7_TRAIN_ENH\", \"ERROR\", \"COPY_FAILED\", \"Failed to copy file\", src_path=str(sp), dest_path=str(dp), error=repr(e))\n",
    "\n",
    "copy_stats = {\n",
    "    \"copied\": int(copied),\n",
    "    \"skipped_exists\": int(skipped_exists),\n",
    "    \"copy_errors\": int(copy_errors),\n",
    "    \"total_planned_files\": int(len(copy_plan)),\n",
    "    \"warnings_by_level\": count_by_level(warnings_rows),\n",
    "}\n",
    "\n",
    "print(\"\\nCopy summary:\")\n",
    "print(\"  Copied:\", copied)\n",
    "print(\"  Skipped (already exists):\", skipped_exists)\n",
    "print(\"  Copy errors:\", copy_errors)\n",
    "print(\"  Warnings by level:\", copy_stats[\"warnings_by_level\"])\n",
    "\n",
    "# Persist warnings after copying so failures still leave a readable trail.\n",
    "atomic_write_csv(logs_dir / \"preprocess_warnings.csv\", pd.DataFrame(warnings_rows))\n",
    "\n",
    "# Stop if any ERROR occurred during copying.\n",
    "fatal_copy = [w for w in warnings_rows if str(w.get(\"level\", \"\")).upper() == \"ERROR\"]\n",
    "if fatal_copy:\n",
    "    fail_summary = dict(early_summary)\n",
    "    fail_summary[\"status\"] = \"FAILED_DURING_COPY\"\n",
    "    fail_summary[\"copy_stats\"] = copy_stats\n",
    "    atomic_write_text(logs_dir / \"dataset_summary.json\", json.dumps(fail_summary, indent=2))\n",
    "    run_config[\"copy_stats\"] = copy_stats\n",
    "    atomic_write_text(cfg_dir / \"run_config.json\", json.dumps(run_config, indent=2))\n",
    "\n",
    "    print(\"\\n❌ Copy stage encountered ERROR conditions.\")\n",
    "    print(\"   Logs written:\")\n",
    "    print(\"   -\", str(logs_dir / \"preprocess_warnings.csv\"))\n",
    "    print(\"   -\", str(logs_dir / \"dataset_summary.json\"))\n",
    "    print(\"   -\", str(cfg_dir / \"run_config.json\"))\n",
    "    raise RuntimeError(f\"Copy failed with {len(fatal_copy)} ERROR(s). See logs/preprocess_warnings.csv.\")\n",
    "\n",
    "# -------------------------\n",
    "# Build manifest_train_enh1.csv (locked schema)\n",
    "# Inputs: D7 train rows + selected D2 train rows\n",
    "# Outputs: merged train_enh manifest pointing to clips/train_enh1/ paths\n",
    "# -------------------------\n",
    "# All rows are marked split=\"train_enh1\".\n",
    "# dataset is kept as \"D7\" so D7 training code treats this as a D7 training split.\n",
    "\n",
    "# D7 existing train rows → train_enh manifest (paths rewritten to train_enh1 folder)\n",
    "d7_enh = d7_train[CANON_COLS].copy()\n",
    "d7_enh[\"split\"] = TRAIN_ENH_SPLIT_NAME\n",
    "d7_enh[\"dataset\"] = \"D7\"\n",
    "d7_enh[\"clip_path\"] = d7_train[\"clip_path\"].astype(str).map(lambda p: str(TRAIN_ENH_DIR / Path(p).name))\n",
    "d7_enh[\"sample_id\"] = d7_enh[\"clip_path\"].map(lambda p: Path(p).stem)\n",
    "d7_enh[\"source_dataset\"] = d7_train.get(\"source_dataset\", pd.Series([\"D7\"] * len(d7_train))).astype(str).tolist()\n",
    "\n",
    "# D2 selected rows → train_enh manifest (paths rewritten to the renamed files in train_enh1)\n",
    "d2_enh = d2_sel_reset[CANON_COLS].copy()\n",
    "d2_enh[\"split\"] = TRAIN_ENH_SPLIT_NAME\n",
    "d2_enh[\"dataset\"] = \"D7\"\n",
    "\n",
    "new_paths = []\n",
    "new_ids = []\n",
    "for j, row in d2_sel_reset.iterrows():\n",
    "    hc_pd = hc_pd_from_label_num(row[\"label_num\"])\n",
    "    spk_tok = safe_token(row[\"speaker_id\"], 32, \"NA\")\n",
    "    task_tok = safe_token(row[\"task\"], 12, \"0\")\n",
    "    out_name = f\"D7_D2add_{hc_pd}_{spk_tok}_{task_tok}_{j+1:06d}.wav\"\n",
    "    out_path = TRAIN_ENH_DIR / out_name\n",
    "    new_paths.append(str(out_path))\n",
    "    new_ids.append(out_path.stem)\n",
    "\n",
    "d2_enh[\"clip_path\"] = new_paths\n",
    "d2_enh[\"sample_id\"] = new_ids\n",
    "d2_enh[\"source_dataset\"] = \"D2\"\n",
    "\n",
    "# Combine into one training split and enforce exact column order.\n",
    "train_enh = pd.concat([d7_enh, d2_enh], axis=0, ignore_index=True)\n",
    "train_enh = train_enh[FINAL_COLS].copy()\n",
    "\n",
    "# Validate no literal \"NaN\" strings slipped into the final manifest.\n",
    "for c in FINAL_COLS:\n",
    "    if train_enh[c].dtype == object and (train_enh[c] == \"NaN\").any():\n",
    "        raise RuntimeError(f\"Found literal string 'NaN' in column '{c}'. This violates NaN policy.\")\n",
    "\n",
    "# Sanity checks for labels and sex.\n",
    "require(set(train_enh[\"label_num\"].dropna().unique()).issubset({0, 1}), \"train_enh manifest label_num contains values outside {0,1}.\")\n",
    "require(set(train_enh[\"label_str\"].dropna().unique()).issubset({\"Healthy\", \"Parkinson\"}), \"train_enh manifest label_str contains values outside {Healthy, Parkinson}.\")\n",
    "require(((train_enh[\"label_num\"] == 0) == (train_enh[\"label_str\"] == \"Healthy\")).all(), \"train_enh manifest mismatch label_num==0 vs label_str.\")\n",
    "require(((train_enh[\"label_num\"] == 1) == (train_enh[\"label_str\"] == \"Parkinson\")).all(), \"train_enh manifest mismatch label_num==1 vs label_str.\")\n",
    "bad_sex = sorted(set(train_enh[\"sex\"].dropna().unique()) - {\"M\", \"F\"})\n",
    "require(len(bad_sex) == 0, f\"train_enh manifest sex contains unexpected values: {bad_sex}\")\n",
    "\n",
    "# Verify the manifest points only to files that exist under clips/train_enh1.\n",
    "missing_enh = []\n",
    "for p in tqdm(train_enh[\"clip_path\"].astype(str).tolist(), desc=\"Check TRAIN_ENH clip_path exists\", dynamic_ncols=True):\n",
    "    if not os.path.exists(p):\n",
    "        missing_enh.append(p)\n",
    "        if len(missing_enh) >= 25:\n",
    "            break\n",
    "require(len(missing_enh) == 0, f\"train_enh manifest points to missing files. Examples (up to 25): {missing_enh}\")\n",
    "\n",
    "# -------------------------\n",
    "# Write final artifacts and final summary\n",
    "# Inputs: train_enh dataframe + copy_stats + selection info\n",
    "# Outputs: manifests/manifest_train_enh1.csv and logs/config JSON summaries\n",
    "# -------------------------\n",
    "atomic_write_csv(MANIFEST_TRAIN_ENH, train_enh)\n",
    "\n",
    "final_summary = {\n",
    "    \"dataset\": \"D7\",\n",
    "    \"created_utc\": datetime.utcnow().isoformat(),\n",
    "    \"status\": \"SUCCESS\",\n",
    "    \"d7_out_root\": str(D7_OUT_ROOT),\n",
    "    \"train_enh_dir\": str(TRAIN_ENH_DIR),\n",
    "    \"train_enh_split_name\": TRAIN_ENH_SPLIT_NAME,\n",
    "    \"manifest_train_enh\": str(MANIFEST_TRAIN_ENH),\n",
    "    \"selection\": run_config[\"selection\"],\n",
    "    \"copy_stats\": copy_stats,\n",
    "    \"counts_train_enh\": summarize_counts(train_enh, split_name=TRAIN_ENH_SPLIT_NAME),\n",
    "}\n",
    "\n",
    "atomic_write_text(logs_dir / \"dataset_summary.json\", json.dumps(final_summary, indent=2))\n",
    "atomic_write_csv(logs_dir / \"preprocess_warnings.csv\", pd.DataFrame(warnings_rows))\n",
    "\n",
    "run_config[\"copy_stats\"] = copy_stats\n",
    "run_config[\"outputs\"] = {\n",
    "    \"train_enh_dir\": str(TRAIN_ENH_DIR),\n",
    "    \"manifest_train_enh\": str(MANIFEST_TRAIN_ENH),\n",
    "    \"dataset_summary_json\": str(logs_dir / \"dataset_summary.json\"),\n",
    "    \"preprocess_warnings_csv\": str(logs_dir / \"preprocess_warnings.csv\"),\n",
    "    \"run_config_json\": str(cfg_dir / \"run_config.json\"),\n",
    "}\n",
    "atomic_write_text(cfg_dir / \"run_config.json\", json.dumps(run_config, indent=2))\n",
    "\n",
    "print(\"\\n✅ D7 train_enh build complete.\")\n",
    "print(\"- Train_enh folder:\", str(TRAIN_ENH_DIR))\n",
    "print(\"- Manifest:\", str(MANIFEST_TRAIN_ENH))\n",
    "print(\"- Summary:\", str(logs_dir / \"dataset_summary.json\"))\n",
    "print(\"- Warnings:\", str(logs_dir / \"preprocess_warnings.csv\"))\n",
    "print(\"- Config:\", str(cfg_dir / \"run_config.json\"))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "941903461056439db2726a04fba82b57",
      "31278e912cb8479ba03f97f2cd8bb3a6",
      "d83996810d76475c872af326fcc17540",
      "36ab79c47903442bb632a2f698ee756e",
      "0f6056425a094b2686341ed8257d342e",
      "5f9b22e126d24ff584eec93b7b31f54e",
      "31aae0087c504ec7920549763328c4cb",
      "28a0928f845a4c7bbbdd80d7a63b367b",
      "4d5012146edf4dffb02160e8f1f1e965",
      "d402fd6e853741a8a084a3e5cdb2f1dc",
      "cbc4592667e44c4d832f528496f23143",
      "9c43b1d7a3d64bbf9f8bfeeaabe31f6c",
      "2f77c0d7b106437ba4acefe825d587c5",
      "bade9819ed97474aa8cefb71c08c724e",
      "694795cd18e54d8cb0b0c9dd2a11c09b",
      "4f57c927e2b24b248106a8c5996d624d",
      "e086878927314a6982126a191600f200",
      "75906b8866e84d5cad6555bd2e942ef6",
      "9c048a5d3525485dac7964fe14418215",
      "9167568564c54ce4a3a37c588f694363",
      "af3028134e764b878cae07e17457eda5",
      "decddfa1117242ccbb896b7e7612e083",
      "350490c6900645089d23c078619cf94c",
      "1fa1ae07762f4fed99d9ad892ef6dd9d",
      "a4c13d594b4f4032a5703aafda629c23",
      "a6d0a90e882348aeb14377692f2be3b2",
      "1bd731f09386429ebad718b074801ea9",
      "254d2828affb479186bf154899398319",
      "6aa6732fc4c04a8994946610ab65004f",
      "851ce6de35c5401f898b91dfa1038a5e",
      "1620976f176544ed809908817575dd57",
      "f3a0321f2d424ed0becbc76cc7073d81",
      "94e2892268e841f58cb910c993419f0e",
      "a9d7968f7cdd43589b18d57488717e1c",
      "eecd8b775e5049bdadf386f88d8f93a2",
      "0da5e1a48e9443f19eaf68840bcd004c",
      "4b4139c5d5584be19b1596c34c4be529",
      "73bf5600868d4fefb90e562f4f6253dc",
      "2cf2e3d4ed484150902310843ac0349d",
      "8c32ecd80ac74d11b4663d43298fe0e2",
      "2a1ea78e073947568308f469238f770d",
      "0c06bbed129c40b78aca6e2b8194a6c2",
      "772b3f42f3d24509aa5572dc3ed99078",
      "c0849711314f42f08569b8c75288cf54",
      "cdcab67385a9476ba9554d5ee8f99494",
      "953a88372b7e4b4f8d013f447ce9134d",
      "5d51d8cdf71a45b1a9e63bc4ecc8dade",
      "1f8468a4265a4c93a94ab99818e26e2c",
      "a9187b188eb3467997380eea422e3143",
      "b40771296bbf4350a88a55d156af8aed",
      "9000f58b56594d15845c14cdc48dc08b",
      "215619193ae34e53884feebc7d96c1ca",
      "6a73777c7f1b4e719c69cac5f15f0c6f",
      "438b07ad64ae4870ab8625daefa5574f",
      "6e187b24d1f8465a991adf9e6efcffea"
     ]
    },
    "id": "V1SEAdOXWcFS"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "The following cell trains and validates the **D7 enhanced model (train_enh1)** using three fixed random seeds, while keeping the Wav2Vec2 backbone frozen and training only the small head layers. Training data comes from `manifests/manifest_train_enh1.csv` (rows where `split == \"train_enh1\"`), and validation data comes from the standard D7 `manifests/manifest_all.csv` (rows where `split == \"val\"`). The code does not reshuffle or recreate any splits and relies entirely on the existing manifest files. Before training starts, it checks that both manifests exist, confirms required columns are present, verifies that the validation data belongs to dataset D7, and stops immediately if any audio file listed in the train or validation splits is missing.\n",
    "\n",
    "To handle clips of different lengths, the cell creates a `task_group` field using a simple rule. Clips with `task == \"vowl\"` are treated as vowel clips, and all others are treated as other. A custom dataset loader reads each audio file, converts it to mono if needed, checks that the sample rate is 16 kHz, and builds an attention mask. For vowel clips, the attention mask hides trailing near-silent samples so the model does not learn from padded silence. For other clips, the attention mask remains fully active. A collate function pads each batch to the longest clip in that batch and pads both the waveform and the attention mask in a consistent way.\n",
    "\n",
    "The model is a two-head classifier built on top of a frozen `facebook/wav2vec2-base` backbone. It includes two small trainable pre-head blocks (LayerNorm and Dropout), one for vowel clips and one for other clips, followed by two linear classification heads. During the forward pass, the backbone produces features, a masked mean pooling step is applied using the attention mask, and each sample is routed to the vowel head or the other head based on its `task_group`. Only the pre-head blocks and the linear heads are trained. The backbone weights are never updated.\n",
    "\n",
    "A key feature of this cell is initialization from a prior baseline model. Before training the enhanced model, the code searches `trainval_runs/exp_*/` and automatically selects the most recent experiment folder that contains a valid `summary_trainval.json` and all three `best_heads.pt` files, one for each seed. For each seed run, it loads that seed’s baseline `best_heads.pt` into the enhanced model’s heads and then continues training on the `train_enh1` data from that starting point. This helps keep results comparable and reduces sensitivity to random initialization.\n",
    "\n",
    "For each seed (1337, 2024, 7777), the cell runs a full training and validation loop with progress bars and early stopping, using a patience of two epochs based on validation AUROC. Gradient accumulation is used so the effective batch size matches the intended value of 64, using a per-device batch size of 16. After each epoch, the model is evaluated on the validation set and AUROC is computed. If AUROC improves, the current heads are saved as the best snapshot and the validation probabilities are stored. When training for a seed finishes, the best heads are written to `best_heads.pt`, a validation ROC plot is saved, and confusion matrix plots are generated at both threshold 0.5 and the chosen optimal threshold.\n",
    "\n",
    "The optimal validation threshold is computed only at the best-AUROC epoch using Youden’s J statistic, which maximizes TPR minus FPR on the validation ROC curve. For that best epoch, the cell computes and stores threshold-based validation metrics at both threshold 0.5 and the Youden-optimal threshold. These metrics include accuracy, precision, recall or sensitivity, specificity, F1 score, MCC, and Fisher exact test p-value. Each seed writes a `metrics.json` file that records the data sources used, details of the baseline initialization, the best epoch, best AUROC, optimal threshold information, and paths to all saved outputs.\n",
    "\n",
    "After all three seeds finish, the cell combines results across seeds and writes an experiment-level `summary_trainval.json` under a new folder named `trainval_runs/exp_<EXPERIMENT_TAG>_<timestamp>/`. This summary includes AUROC values for each seed, the mean AUROC with a 95 percent t-based confidence interval using three seeds, and the validation-optimal thresholds stored in a standard format with per-seed values and overall mean and standard deviation. The same summary is appended as one line to the global `trainval_runs/history_index.jsonl` file for long-term tracking. The cell also mirrors small run configuration and dataset summary files into the builder-style `config/` and `logs/` folders for consistent record keeping, without changing the main training outputs. Finally, it prints the main output locations and unassigns the Colab runtime to stop the GPU instance."
   ],
   "metadata": {
    "id": "5cQ_I7sOwhls"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# =========================\n",
    "# D7 Enhanced Train + Val (train_enh1), Baseline-Initialized Heads\n",
    "# Inputs: D7 enhanced-train manifest (train_enh1 split) + D7 full manifest (val split) + baseline best_heads.pt (per seed)\n",
    "# Outputs: Per-seed best heads, metrics, ROC/confusion plots, plus an experiment summary + history index entries\n",
    "# =========================\n",
    "# Train + Val ONLY (CRASH-PROOF, WITH PROGRESS + HISTORY) — D7 ENHANCED (train_enh1)\n",
    "# - Frozen Wav2Vec2 backbone\n",
    "# - Two task heads + small LayerNorm + Dropout blocks (only these parts train)\n",
    "# - Uses ONLY two manifests:\n",
    "#     * Enhanced TRAIN manifest: train_enh1 split\n",
    "#     * Standard VAL manifest: val split\n",
    "# - Initializes heads from the most recent BASELINE D7 train+val experiment (same 3 seeds),\n",
    "#   then continues training on train_enh1\n",
    "# - Saves per-seed best artifacts and a per-experiment summary + append-only history index\n",
    "# - Adds extra threshold-based metrics and computes a VAL-optimal threshold (Youden J) at the best-AUROC epoch\n",
    "# - Ends by unassigning the Colab runtime (GPU stop)\n",
    "#\n",
    "# NOTES\n",
    "# - Train comes from the enhanced train manifest (already points to enhanced clips)\n",
    "# - Val comes from the standard D7 manifest\n",
    "# - dataset_id is inferred from val (expected \"D7\"); no re-splitting happens here\n",
    "# =========================\n",
    "\n",
    "import os, json, math, random, time, warnings\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import soundfile as sf\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from transformers import Wav2Vec2Model\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, roc_curve,\n",
    "    confusion_matrix, accuracy_score,\n",
    "    precision_recall_fscore_support,\n",
    "    matthews_corrcoef\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# -------------------------\n",
    "# 0) Safety checks: avoid importing a local file that masks real libraries\n",
    "# -------------------------\n",
    "if os.path.exists(\"/content/torch.py\") or os.path.exists(\"/content/torch/__init__.py\"):\n",
    "    raise RuntimeError(\"Found local /content/torch.py or /content/torch/ that shadows PyTorch. Rename/remove it and restart runtime.\")\n",
    "if os.path.exists(\"/content/transformers.py\") or os.path.exists(\"/content/transformers/__init__.py\"):\n",
    "    raise RuntimeError(\"Found local /content/transformers.py or /content/transformers/ that shadows Hugging Face Transformers. Rename/remove it and restart runtime.\")\n",
    "\n",
    "# -------------------------\n",
    "# 1) Drive access (Colab): mount only if not already available\n",
    "# -------------------------\n",
    "try:\n",
    "    from google.colab import drive  # type: ignore\n",
    "    if not os.path.isdir(\"/content/drive/MyDrive\"):\n",
    "        drive.mount(\"/content/drive\")\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# -------------------------\n",
    "# 2) Root paths and manifest names\n",
    "# Inputs: a single D7 preprocessed root (DX_OUT_ROOT)\n",
    "# Outputs: resolved manifest file paths and stable config/log folders\n",
    "# -------------------------\n",
    "D7_OUT_ROOT_FALLBACK = \"/content/drive/MyDrive/AI_PD_Project/Datasets/D7-Multilingual (D1_D4_D5v2_D6)/preprocessed_v1\"\n",
    "DX_OUT_ROOT = str(globals().get(\"DX_OUT_ROOT\", D7_OUT_ROOT_FALLBACK))\n",
    "globals()[\"DX_OUT_ROOT\"] = DX_OUT_ROOT\n",
    "\n",
    "MANIFEST_ALL = f\"{DX_OUT_ROOT}/manifests/manifest_all.csv\"\n",
    "MANIFEST_TRAIN_ENH = f\"{DX_OUT_ROOT}/manifests/manifest_train_enh1.csv\"\n",
    "TRAIN_ENH_SPLIT_NAME = \"train_enh1\"\n",
    "\n",
    "# Builder-style mirror folders (lightweight copies of key run info)\n",
    "cfg_dir  = Path(DX_OUT_ROOT) / \"config\" / \"D7_Enh1_on_D2_Test\"\n",
    "logs_dir = Path(DX_OUT_ROOT) / \"logs\" / \"D7_Enh1_on_D2_Test\"\n",
    "cfg_dir.mkdir(parents=True, exist_ok=True)\n",
    "logs_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# -------------------------\n",
    "# 3) Experiment identity and output folder for this run\n",
    "# Output: a new exp_... folder under trainval_runs/ (never overwrites older experiments)\n",
    "# -------------------------\n",
    "EXPERIMENT_TAG = \"frozen_LNDO_trainEnh1_initBaseline\"\n",
    "RUN_STAMP = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "TRAINVAL_ROOT = Path(DX_OUT_ROOT) / \"trainval_runs\"\n",
    "EXP_ROOT = TRAINVAL_ROOT / f\"exp_{EXPERIMENT_TAG}_{RUN_STAMP}\"\n",
    "EXP_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# -------------------------\n",
    "# 4) Training settings and runtime options\n",
    "# Inputs: fixed hyperparameters, 3 seeds, and expected audio format\n",
    "# -------------------------\n",
    "MAX_EPOCHS     = 10\n",
    "EFFECTIVE_BS   = 64\n",
    "PER_DEVICE_BS  = 16\n",
    "GRAD_ACCUM     = max(1, EFFECTIVE_BS // PER_DEVICE_BS)\n",
    "\n",
    "LR             = 1e-3\n",
    "PATIENCE       = 2\n",
    "SEEDS          = [1337, 2024, 7777]\n",
    "\n",
    "BACKBONE_CKPT  = \"facebook/wav2vec2-base\"\n",
    "SR_EXPECTED    = 16000\n",
    "TINY_THRESH    = 1e-4\n",
    "\n",
    "DROPOUT_P      = 0.2\n",
    "\n",
    "NUM_WORKERS    = 0\n",
    "PIN_MEMORY     = False\n",
    "\n",
    "VOWEL_TASK_VALUE = \"vowl\"\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "try:\n",
    "    torch.set_float32_matmul_precision(\"high\")\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# Quiet known warnings to keep logs readable\n",
    "warnings.filterwarnings(\"ignore\", message=\"None of the inputs have requires_grad=True. Gradients will be None\")\n",
    "warnings.filterwarnings(\"ignore\", message=\"Passing `gradient_checkpointing` to a config initialization is deprecated\")\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"huggingface_hub\")\n",
    "\n",
    "# Quick run summary (prints only; does not change any artifacts)\n",
    "print(\"DX_OUT_ROOT:\", DX_OUT_ROOT)\n",
    "print(\"MANIFEST_TRAIN_ENH:\", MANIFEST_TRAIN_ENH)\n",
    "print(\"TRAIN_ENH_SPLIT_NAME:\", TRAIN_ENH_SPLIT_NAME)\n",
    "print(\"MANIFEST_ALL (val source):\", MANIFEST_ALL)\n",
    "print(\"DEVICE:\", DEVICE)\n",
    "if DEVICE.type == \"cuda\":\n",
    "    print(\"GPU:\", torch.cuda.get_device_name(0))\n",
    "print(\"PER_DEVICE_BS:\", PER_DEVICE_BS, \"| GRAD_ACCUM:\", GRAD_ACCUM, \"| EFFECTIVE_BS:\", PER_DEVICE_BS * GRAD_ACCUM)\n",
    "print(\"NUM_WORKERS:\", NUM_WORKERS, \"| PIN_MEMORY:\", PIN_MEMORY)\n",
    "print(\"EXPERIMENT_TAG:\", EXPERIMENT_TAG, \"| RUN_STAMP:\", RUN_STAMP)\n",
    "print(\"EXP_ROOT:\", str(EXP_ROOT))\n",
    "print(\"cfg_dir:\", str(cfg_dir))\n",
    "print(\"logs_dir:\", str(logs_dir))\n",
    "\n",
    "# -------------------------\n",
    "# 5) Read manifests and build the train/val tables\n",
    "# Inputs: enhanced TRAIN manifest + standard ALL manifest\n",
    "# Outputs: train_df (train_enh1) and val_df (val) with the same core columns\n",
    "# -------------------------\n",
    "if not os.path.exists(MANIFEST_TRAIN_ENH):\n",
    "    raise FileNotFoundError(\n",
    "        \"Missing manifest_train_enh1.csv at:\\n\"\n",
    "        f\"  {MANIFEST_TRAIN_ENH}\\n\"\n",
    "        \"Run the train_enh builder first.\"\n",
    "    )\n",
    "if not os.path.exists(MANIFEST_ALL):\n",
    "    raise FileNotFoundError(\n",
    "        \"Missing manifest_all.csv at:\\n\"\n",
    "        f\"  {MANIFEST_ALL}\\n\"\n",
    "        \"Confirm D7 merge-builder wrote manifests/manifest_all.csv under DX_OUT_ROOT.\"\n",
    "    )\n",
    "\n",
    "m_train = pd.read_csv(MANIFEST_TRAIN_ENH)\n",
    "m_all   = pd.read_csv(MANIFEST_ALL)\n",
    "\n",
    "# Minimum required columns for training and validation\n",
    "req_cols = {\"split\", \"clip_path\", \"label_num\", \"task\"}\n",
    "for name, df in [(\"manifest_train_enh1\", m_train), (\"manifest_all\", m_all)]:\n",
    "    missing = [c for c in sorted(req_cols) if c not in df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"{name} missing required columns: {missing}. Found: {list(df.columns)}\")\n",
    "\n",
    "# Split selection: enhanced train split + standard val split\n",
    "m_train = m_train[m_train[\"split\"].astype(str) == TRAIN_ENH_SPLIT_NAME].copy()\n",
    "m_val   = m_all[m_all[\"split\"].astype(str) == \"val\"].copy()\n",
    "\n",
    "if len(m_train) == 0:\n",
    "    raise RuntimeError(f\"After filtering manifest_train_enh1.csv to split=={TRAIN_ENH_SPLIT_NAME!r}, 0 rows remain.\")\n",
    "if len(m_val) == 0:\n",
    "    raise RuntimeError(\"After filtering manifest_all.csv to split=='val', 0 rows remain.\")\n",
    "\n",
    "# Infer dataset_id from validation data (used for naming outputs and as a guard)\n",
    "if \"dataset\" in m_val.columns and m_val[\"dataset\"].notna().any():\n",
    "    dataset_id = str(m_val[\"dataset\"].astype(str).value_counts(dropna=True).idxmax())\n",
    "    m_val = m_val[m_val[\"dataset\"].astype(str) == dataset_id].copy()\n",
    "else:\n",
    "    dataset_id = \"DX\"\n",
    "\n",
    "# Guard: this training script expects D7 validation data\n",
    "if dataset_id != \"D7\":\n",
    "    raise RuntimeError(f\"Dataset inferred from VAL manifest is {dataset_id!r}. Expected 'D7'. Check DX_OUT_ROOT/manifests/manifest_all.csv.\")\n",
    "\n",
    "# Keep a consistent, compact set of columns across train and val\n",
    "keep_cols = [\"clip_path\", \"label_num\", \"task\", \"speaker_id\", \"duration_sec\", \"split\"]\n",
    "for df in [m_train, m_val]:\n",
    "    for c in keep_cols:\n",
    "        if c not in df.columns:\n",
    "            df[c] = np.nan\n",
    "\n",
    "m_train = m_train[keep_cols].copy()\n",
    "m_val   = m_val[keep_cols].copy()\n",
    "\n",
    "train_df = m_train.copy().reset_index(drop=True)\n",
    "val_df   = m_val.copy().reset_index(drop=True)\n",
    "\n",
    "print(f\"\\nDataset inferred (from VAL): {dataset_id}\")\n",
    "print(f\"Train rows ({TRAIN_ENH_SPLIT_NAME}): {len(train_df)} | Val rows: {len(val_df)}\")\n",
    "print(\"Train label counts:\", train_df[\"label_num\"].value_counts(dropna=False).to_dict())\n",
    "print(\"Val label counts:\",   val_df[\"label_num\"].value_counts(dropna=False).to_dict())\n",
    "\n",
    "# -------------------------\n",
    "# 6) Fail-fast: confirm audio files exist before training starts\n",
    "# Input: train_df and val_df clip_path values\n",
    "# Output: raises early with example missing paths, instead of failing mid-epoch\n",
    "# -------------------------\n",
    "def _fail_fast_missing_paths(df: pd.DataFrame, name: str):\n",
    "    missing_paths = []\n",
    "    for p in tqdm(df[\"clip_path\"].astype(str).tolist(), desc=f\"Check {name} clip_path exists\", dynamic_ncols=True):\n",
    "        if not os.path.exists(p):\n",
    "            missing_paths.append(p)\n",
    "            if len(missing_paths) >= 10:\n",
    "                break\n",
    "    if missing_paths:\n",
    "        raise FileNotFoundError(f\"{name}: missing clip_path(s). Examples: {missing_paths}\")\n",
    "\n",
    "_fail_fast_missing_paths(train_df, \"TRAIN_ENH1\")\n",
    "_fail_fast_missing_paths(val_df, \"VAL\")\n",
    "\n",
    "# -------------------------\n",
    "# 7) Task grouping for the two-head model\n",
    "# Rule: task == \"vowl\" -> vowel head, otherwise -> other head\n",
    "# -------------------------\n",
    "def _task_group(task_val) -> str:\n",
    "    return \"vowel\" if str(task_val) == VOWEL_TASK_VALUE else \"other\"\n",
    "\n",
    "train_df[\"task_group\"] = train_df[\"task\"].apply(_task_group)\n",
    "val_df[\"task_group\"]   = val_df[\"task\"].apply(_task_group)\n",
    "\n",
    "# -------------------------\n",
    "# 8) Dataset + batch collation (waveforms + attention masks)\n",
    "# Inputs: manifest rows with clip_path, label_num, and task_group\n",
    "# Outputs: padded tensors for model input, plus per-item task_group routing\n",
    "# -------------------------\n",
    "class AudioManifestDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Loads a clip and creates an attention mask at the sample level.\n",
    "\n",
    "    Masking rule:\n",
    "    - vowel clips: mask trailing near-silence (often padding)\n",
    "    - other clips: keep all samples active\n",
    "    \"\"\"\n",
    "    def __init__(self, df: pd.DataFrame):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        row = self.df.iloc[idx]\n",
    "        clip_path = str(row[\"clip_path\"])\n",
    "        label = int(row[\"label_num\"])\n",
    "        task_group = str(row[\"task_group\"])\n",
    "\n",
    "        if not os.path.exists(clip_path):\n",
    "            raise FileNotFoundError(f\"Missing clip_path: {clip_path}\")\n",
    "\n",
    "        # Load audio and force mono float32\n",
    "        y, sr = sf.read(clip_path, always_2d=False)\n",
    "        if isinstance(y, np.ndarray) and y.ndim > 1:\n",
    "            y = np.mean(y, axis=1)\n",
    "        y = np.asarray(y, dtype=np.float32)\n",
    "\n",
    "        # Hard check: training assumes a single sample rate everywhere\n",
    "        if int(sr) != SR_EXPECTED:\n",
    "            raise ValueError(f\"Sample rate mismatch (expected {SR_EXPECTED}): {clip_path} has sr={sr}\")\n",
    "\n",
    "        # Default: attend to everything\n",
    "        attn = np.ones((len(y),), dtype=np.int64)\n",
    "\n",
    "        # Vowel: ignore trailing near-zeros so padding does not affect training\n",
    "        if task_group == \"vowel\":\n",
    "            k = -1\n",
    "            for j in range(len(y) - 1, -1, -1):\n",
    "                if abs(float(y[j])) > TINY_THRESH:\n",
    "                    k = j\n",
    "                    break\n",
    "            if k >= 0:\n",
    "                attn[:k+1] = 1\n",
    "                attn[k+1:] = 0\n",
    "            else:\n",
    "                attn[:] = 1\n",
    "        else:\n",
    "            attn[:] = 1\n",
    "\n",
    "        return {\n",
    "            \"input_values\": torch.from_numpy(y),                 # float32 [T]\n",
    "            \"attention_mask\": torch.from_numpy(attn),            # int64   [T]\n",
    "            \"labels\": torch.tensor(label, dtype=torch.long),     # int64   []\n",
    "            \"task_group\": task_group,                            # str\n",
    "        }\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"Pads waveforms and masks to the longest clip in the batch.\"\"\"\n",
    "    max_len = int(max(b[\"input_values\"].numel() for b in batch))\n",
    "    input_vals, attn_masks, labels, task_groups = [], [], [], []\n",
    "    for b in batch:\n",
    "        x = b[\"input_values\"]\n",
    "        a = b[\"attention_mask\"]\n",
    "        pad = max_len - x.numel()\n",
    "        if pad > 0:\n",
    "            x = torch.cat([x, torch.zeros(pad, dtype=x.dtype)], dim=0)\n",
    "            a = torch.cat([a, torch.zeros(pad, dtype=a.dtype)], dim=0)\n",
    "        input_vals.append(x)\n",
    "        attn_masks.append(a)\n",
    "        labels.append(b[\"labels\"])\n",
    "        task_groups.append(b[\"task_group\"])\n",
    "    return {\n",
    "        \"input_values\": torch.stack(input_vals, dim=0),      # [B,T]\n",
    "        \"attention_mask\": torch.stack(attn_masks, dim=0),    # [B,T]\n",
    "        \"labels\": torch.stack(labels, dim=0),                # [B]\n",
    "        \"task_group\": task_groups,                           # list[str]\n",
    "    }\n",
    "\n",
    "# -------------------------\n",
    "# 9) Two-head classifier with a frozen Wav2Vec2 backbone\n",
    "# Inputs: waveform + attention mask + task_group routing\n",
    "# Outputs: loss and logits (PD probability comes from softmax(logits)[:,1])\n",
    "# -------------------------\n",
    "class Wav2Vec2TwoHeadClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    Trainable parts:\n",
    "    - LayerNorm+Dropout blocks (one per task group)\n",
    "    - Linear heads (one per task group)\n",
    "    Backbone stays frozen.\n",
    "    \"\"\"\n",
    "    def __init__(self, ckpt: str, dropout_p: float):\n",
    "        super().__init__()\n",
    "        self.backbone = Wav2Vec2Model.from_pretrained(\n",
    "            ckpt,\n",
    "            use_safetensors=True,\n",
    "            local_files_only=False\n",
    "        )\n",
    "        for p in self.backbone.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "        H = int(self.backbone.config.hidden_size)\n",
    "        self.pre_vowel = nn.Sequential(nn.LayerNorm(H), nn.Dropout(float(dropout_p)))\n",
    "        self.pre_other = nn.Sequential(nn.LayerNorm(H), nn.Dropout(float(dropout_p)))\n",
    "\n",
    "        self.head_vowel = nn.Linear(H, 2)\n",
    "        self.head_other = nn.Linear(H, 2)\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    def masked_mean_pool(self, last_hidden, attn_mask_samples):\n",
    "        # Converts sample-level mask into feature-level mask, then average-pools valid frames\n",
    "        feat_mask = self.backbone._get_feature_vector_attention_mask(last_hidden.shape[1], attn_mask_samples)\n",
    "        feat_mask = feat_mask.to(last_hidden.device).unsqueeze(-1).type_as(last_hidden)\n",
    "        masked = last_hidden * feat_mask\n",
    "        denom = feat_mask.sum(dim=1).clamp(min=1.0)\n",
    "        return masked.sum(dim=1) / denom\n",
    "\n",
    "    def forward(self, input_values, attention_mask, labels, task_group):\n",
    "        # Backbone forward is wrapped in no_grad since it is frozen\n",
    "        with torch.no_grad():\n",
    "            out = self.backbone(input_values=input_values, attention_mask=attention_mask)\n",
    "            last_hidden = out.last_hidden_state  # [B,T',H]\n",
    "\n",
    "        pooled = self.masked_mean_pool(last_hidden, attention_mask).float()  # [B,H]\n",
    "\n",
    "        # Separate feature transforms for the two task groups\n",
    "        z_v = self.pre_vowel(pooled)\n",
    "        z_o = self.pre_other(pooled)\n",
    "\n",
    "        # Compute both heads, then select per item using task_group\n",
    "        logits_v = self.head_vowel(z_v)\n",
    "        logits_o = self.head_other(z_o)\n",
    "\n",
    "        is_vowel = torch.tensor([tg == \"vowel\" for tg in task_group], device=pooled.device, dtype=torch.bool)\n",
    "        logits = logits_o.clone()\n",
    "        if is_vowel.any():\n",
    "            logits[is_vowel] = logits_v[is_vowel]\n",
    "\n",
    "        loss = self.loss_fn(logits, labels)\n",
    "        return loss, logits\n",
    "\n",
    "# -------------------------\n",
    "# 9.5) Baseline head loading\n",
    "# Input: best_heads.pt saved from an earlier baseline train+val run (per seed)\n",
    "# Output: model heads set to baseline weights before enhanced training starts\n",
    "# -------------------------\n",
    "def load_heads_into_model(model: Wav2Vec2TwoHeadClassifier, best_heads_path: Path):\n",
    "    if not best_heads_path.exists():\n",
    "        raise FileNotFoundError(f\"Missing best_heads.pt: {str(best_heads_path)}\")\n",
    "    state = torch.load(str(best_heads_path), map_location=\"cpu\")\n",
    "    model.pre_vowel.load_state_dict(state[\"pre_vowel\"], strict=True)\n",
    "    model.pre_other.load_state_dict(state[\"pre_other\"], strict=True)\n",
    "    model.head_vowel.load_state_dict(state[\"head_vowel\"], strict=True)\n",
    "    model.head_other.load_state_dict(state[\"head_other\"], strict=True)\n",
    "    return model\n",
    "\n",
    "# -------------------------\n",
    "# 10) Metric helpers\n",
    "# Inputs: y_true labels and y_prob PD probabilities\n",
    "# Outputs: AUROC, threshold-based metrics, and a VAL-optimal threshold from ROC (Youden J)\n",
    "# -------------------------\n",
    "def compute_auc(y_true, y_prob):\n",
    "    y_true = np.asarray(y_true, dtype=np.int64)\n",
    "    y_prob = np.asarray(y_prob, dtype=np.float64)\n",
    "    if len(np.unique(y_true)) < 2:\n",
    "        return float(\"nan\")\n",
    "    return float(roc_auc_score(y_true, y_prob))\n",
    "\n",
    "def compute_threshold_metrics(y_true, y_prob, thr=0.5):\n",
    "    y_true = np.asarray(y_true, dtype=np.int64)\n",
    "    y_prob = np.asarray(y_prob, dtype=np.float64)\n",
    "    y_pred = (y_prob >= float(thr)).astype(np.int64)\n",
    "\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=[0, 1])\n",
    "    tn, fp, fn, tp = (cm.ravel().tolist() if cm.size == 4 else [0, 0, 0, 0])\n",
    "\n",
    "    # Common classification metrics at a fixed threshold\n",
    "    acc = float(accuracy_score(y_true, y_pred))\n",
    "    prec, rec, f1, _ = precision_recall_fscore_support(y_true, y_pred, average=\"binary\", zero_division=0)\n",
    "    mcc = float(matthews_corrcoef(y_true, y_pred)) if len(np.unique(y_true)) > 1 else float(\"nan\")\n",
    "\n",
    "    sensitivity = float(rec)\n",
    "    specificity = float(tn / (tn + fp)) if (tn + fp) > 0 else float(\"nan\")\n",
    "\n",
    "    # Fisher exact p-value for the 2x2 confusion table (when SciPy is available)\n",
    "    p_value = float(\"nan\")\n",
    "    try:\n",
    "        from scipy.stats import fisher_exact  # type: ignore\n",
    "        _, p_value = fisher_exact([[tn, fp], [fn, tp]], alternative=\"two-sided\")\n",
    "        p_value = float(p_value)\n",
    "    except Exception:\n",
    "        p_value = float(\"nan\")\n",
    "\n",
    "    return {\n",
    "        \"threshold\": float(thr),\n",
    "        \"tn\": int(tn), \"fp\": int(fp), \"fn\": int(fn), \"tp\": int(tp),\n",
    "        \"accuracy\": float(acc),\n",
    "        \"precision\": float(prec),\n",
    "        \"recall\": float(rec),\n",
    "        \"f1\": float(f1),\n",
    "        \"sensitivity\": float(sensitivity),\n",
    "        \"specificity\": float(specificity),\n",
    "        \"mcc\": float(mcc),\n",
    "        \"p_value_fisher\": float(p_value),\n",
    "    }\n",
    "\n",
    "def compute_youden_j_threshold(y_true, y_prob):\n",
    "    # Picks the threshold that maximizes (TPR - FPR) on the validation ROC curve\n",
    "    y_true = np.asarray(y_true, dtype=np.int64)\n",
    "    y_prob = np.asarray(y_prob, dtype=np.float64)\n",
    "    if len(np.unique(y_true)) < 2:\n",
    "        return float(\"nan\"), {\"youden_j\": float(\"nan\"), \"tpr\": float(\"nan\"), \"fpr\": float(\"nan\")}\n",
    "    fpr, tpr, thr = roc_curve(y_true, y_prob)\n",
    "    j = tpr - fpr\n",
    "    idx = int(np.argmax(j))\n",
    "    return float(thr[idx]), {\"youden_j\": float(j[idx]), \"tpr\": float(tpr[idx]), \"fpr\": float(fpr[idx])}\n",
    "\n",
    "def save_roc_curve_png(y_true, y_prob, out_png):\n",
    "    # Saves a simple ROC plot for the best validation epoch\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_prob)\n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr)\n",
    "    plt.plot([0, 1], [0, 1], linestyle=\"--\")\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(\"ROC Curve (Val)\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_png, dpi=150)\n",
    "    plt.close()\n",
    "\n",
    "def save_confusion_png(y_true, y_prob, out_png, thr=0.5):\n",
    "    # Saves a confusion matrix image for a chosen threshold\n",
    "    y_true = np.asarray(y_true, dtype=np.int64)\n",
    "    y_prob = np.asarray(y_prob, dtype=np.float64)\n",
    "    y_pred = (y_prob >= float(thr)).astype(int)\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=[0, 1])\n",
    "\n",
    "    plt.figure()\n",
    "    plt.imshow(cm)\n",
    "    plt.xticks([0, 1], [\"Healthy(0)\", \"PD(1)\"])\n",
    "    plt.yticks([0, 1], [\"Healthy(0)\", \"PD(1)\"])\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"True\")\n",
    "    plt.title(f\"Confusion Matrix (Val, thr={thr:.4f})\")\n",
    "    for (i, j), v in np.ndenumerate(cm):\n",
    "        plt.text(j, i, str(v), ha=\"center\", va=\"center\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_png, dpi=150)\n",
    "    plt.close()\n",
    "\n",
    "def mean_sd(vals):\n",
    "    # Small helper for reporting mean ± SD across the three seeds\n",
    "    vals = np.asarray(vals, dtype=np.float64)\n",
    "    mu = float(np.nanmean(vals)) if np.any(~np.isnan(vals)) else float(\"nan\")\n",
    "    sd = float(np.nanstd(vals, ddof=1)) if np.sum(~np.isnan(vals)) > 1 else 0.0\n",
    "    return mu, sd\n",
    "\n",
    "# -------------------------\n",
    "# 11) Reproducibility: set all random seeds (Python, NumPy, PyTorch)\n",
    "# -------------------------\n",
    "def set_all_seeds(seed: int):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# -------------------------\n",
    "# 11.5) Find the baseline experiment used to initialize heads\n",
    "# Inputs: trainval_runs/exp_* folders\n",
    "# Output: the most recent exp_* that contains summary_trainval.json and 3 best_heads.pt files\n",
    "# -------------------------\n",
    "BASELINE_TRAINVAL_ROOT = Path(DX_OUT_ROOT) / \"trainval_runs\"\n",
    "if not BASELINE_TRAINVAL_ROOT.exists():\n",
    "    raise FileNotFoundError(f\"Missing trainval_runs folder under DX_OUT_ROOT: {str(BASELINE_TRAINVAL_ROOT)}\")\n",
    "\n",
    "exp_dirs = sorted([p for p in BASELINE_TRAINVAL_ROOT.glob(\"exp_*\") if p.is_dir()],\n",
    "                  key=lambda p: p.stat().st_mtime, reverse=True)\n",
    "if not exp_dirs:\n",
    "    raise FileNotFoundError(f\"No exp_* folders found under: {str(BASELINE_TRAINVAL_ROOT)}\")\n",
    "\n",
    "train_dataset_id = \"D7\"\n",
    "\n",
    "def _has_all_seeds_and_summary(exp_path: Path, dataset_id: str, seeds: list):\n",
    "    summary_path = exp_path / \"summary_trainval.json\"\n",
    "    if not summary_path.exists():\n",
    "        return False\n",
    "    for s in seeds:\n",
    "        p = exp_path / f\"run_{dataset_id}_seed{s}\" / \"best_heads.pt\"\n",
    "        if not p.exists():\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "baseline_exp = None\n",
    "for ed in exp_dirs:\n",
    "    # Skip the current experiment folder to avoid self-selection\n",
    "    if ed.resolve() == EXP_ROOT.resolve():\n",
    "        continue\n",
    "    if _has_all_seeds_and_summary(ed, train_dataset_id, SEEDS):\n",
    "        baseline_exp = ed\n",
    "        break\n",
    "\n",
    "if baseline_exp is None:\n",
    "    sample = exp_dirs[0]\n",
    "    raise FileNotFoundError(\n",
    "        \"Could not find a baseline D7 trainval experiment with all 3 best_heads.pt files and summary_trainval.json.\\n\"\n",
    "        f\"Expected under: {str(BASELINE_TRAINVAL_ROOT)}/exp_*/run_D7_seedXXXX/best_heads.pt and summary_trainval.json\\n\"\n",
    "        f\"Most recent exp checked: {str(sample)}\"\n",
    "    )\n",
    "\n",
    "baseline_summary_path = baseline_exp / \"summary_trainval.json\"\n",
    "with open(baseline_summary_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    baseline_summary = json.load(f)\n",
    "\n",
    "print(\"\\nBaseline initialization experiment selected:\")\n",
    "print(\" \", str(baseline_exp))\n",
    "print(\" \", \"summary:\", str(baseline_summary_path))\n",
    "for s in SEEDS:\n",
    "    p = baseline_exp / f\"run_{train_dataset_id}_seed{s}\" / \"best_heads.pt\"\n",
    "    if not p.exists():\n",
    "        raise RuntimeError(f\"Baseline artifact missing after selection. Missing: {str(p)}\")\n",
    "\n",
    "# -------------------------\n",
    "# 12) Per-seed training loop with early stopping on validation AUROC\n",
    "# Inputs: train_df + val_df and baseline-initialized heads\n",
    "# Outputs: best_heads.pt + best-epoch plots + metrics.json for that seed\n",
    "# -------------------------\n",
    "def run_trainval_once(seed: int):\n",
    "    set_all_seeds(seed)\n",
    "\n",
    "    run_dir = EXP_ROOT / f\"run_{dataset_id}_seed{seed}\"\n",
    "    run_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Dataset objects keep I/O simple and consistent across train and val\n",
    "    train_ds = AudioManifestDataset(train_df)\n",
    "    val_ds   = AudioManifestDataset(val_df)\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_ds,\n",
    "        batch_size=PER_DEVICE_BS,\n",
    "        shuffle=True,\n",
    "        num_workers=NUM_WORKERS,\n",
    "        pin_memory=PIN_MEMORY,\n",
    "        collate_fn=collate_fn\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_ds,\n",
    "        batch_size=PER_DEVICE_BS,\n",
    "        shuffle=False,\n",
    "        num_workers=NUM_WORKERS,\n",
    "        pin_memory=PIN_MEMORY,\n",
    "        collate_fn=collate_fn\n",
    "    )\n",
    "\n",
    "    # Warm-up: confirm batches can be formed and read successfully\n",
    "    print(f\"\\n[seed={seed}] Warm-up: loading 3 train batches...\")\n",
    "    t0 = time.time()\n",
    "    it = iter(train_loader)\n",
    "    for i in range(3):\n",
    "        _ = next(it)\n",
    "        print(f\"  loaded warmup batch {i+1}/3\")\n",
    "    print(f\"[seed={seed}] Warm-up done in {time.time()-t0:.2f}s\")\n",
    "\n",
    "    # Build model and start from baseline heads for this seed\n",
    "    model = Wav2Vec2TwoHeadClassifier(BACKBONE_CKPT, dropout_p=DROPOUT_P).to(DEVICE)\n",
    "\n",
    "    baseline_heads_path = baseline_exp / f\"run_{train_dataset_id}_seed{seed}\" / \"best_heads.pt\"\n",
    "    print(f\"[seed={seed}] Initializing heads from baseline:\")\n",
    "    print(\" \", str(baseline_heads_path))\n",
    "    model = load_heads_into_model(model, baseline_heads_path)\n",
    "    model.train()\n",
    "\n",
    "    # Optimizer only sees trainable head parameters (backbone stays frozen)\n",
    "    trainable_params = (\n",
    "        list(model.pre_vowel.parameters()) + list(model.pre_other.parameters()) +\n",
    "        list(model.head_vowel.parameters()) + list(model.head_other.parameters())\n",
    "    )\n",
    "    opt = torch.optim.Adam(trainable_params, lr=LR)\n",
    "\n",
    "    # Track the best validation AUROC and stop after PATIENCE non-improving epochs\n",
    "    best_auc = -1.0\n",
    "    best_epoch = -1\n",
    "    no_improve = 0\n",
    "\n",
    "    # Save the best heads and the validation predictions from the best epoch\n",
    "    best_state = None\n",
    "    best_val_probs = None\n",
    "    best_val_true = None\n",
    "\n",
    "    # Threshold tracking for reporting at the best epoch\n",
    "    best_thr_youden = float(\"nan\")\n",
    "    best_thr_youden_details = None\n",
    "    best_val_metrics_thr05 = None\n",
    "    best_val_metrics_thr_opt = None\n",
    "\n",
    "    for epoch in range(1, MAX_EPOCHS + 1):\n",
    "        # ---- Train phase ----\n",
    "        model.train()\n",
    "        train_losses = []\n",
    "        opt.zero_grad(set_to_none=True)\n",
    "\n",
    "        pbar = tqdm(train_loader, desc=f\"[seed={seed}] Train epoch {epoch}\", dynamic_ncols=True)\n",
    "        step = 0\n",
    "        for batch in pbar:\n",
    "            step += 1\n",
    "            input_values = batch[\"input_values\"].to(DEVICE, non_blocking=False)\n",
    "            attention_mask = batch[\"attention_mask\"].to(DEVICE, non_blocking=False)\n",
    "            labels = batch[\"labels\"].to(DEVICE, non_blocking=False)\n",
    "            task_group = batch[\"task_group\"]\n",
    "\n",
    "            loss, _ = model(input_values, attention_mask, labels, task_group)\n",
    "            loss = loss / GRAD_ACCUM\n",
    "            loss.backward()\n",
    "\n",
    "            train_losses.append(float(loss.detach().cpu().item()) * GRAD_ACCUM)\n",
    "\n",
    "            # Gradient accumulation to reach the effective batch size\n",
    "            if (step % GRAD_ACCUM) == 0:\n",
    "                opt.step()\n",
    "                opt.zero_grad(set_to_none=True)\n",
    "\n",
    "        # Final step if the epoch ends mid-accumulation\n",
    "        if (step % GRAD_ACCUM) != 0:\n",
    "            opt.step()\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "\n",
    "        avg_train_loss = float(np.mean(train_losses)) if train_losses else float(\"nan\")\n",
    "\n",
    "        # ---- Validation phase ----\n",
    "        model.eval()\n",
    "        all_probs, all_true = [], []\n",
    "        vpbar = tqdm(val_loader, desc=f\"[seed={seed}] Val epoch {epoch}\", dynamic_ncols=True)\n",
    "        with torch.inference_mode():\n",
    "            for batch in vpbar:\n",
    "                input_values = batch[\"input_values\"].to(DEVICE, non_blocking=False)\n",
    "                attention_mask = batch[\"attention_mask\"].to(DEVICE, non_blocking=False)\n",
    "                labels = batch[\"labels\"].to(DEVICE, non_blocking=False)\n",
    "                task_group = batch[\"task_group\"]\n",
    "\n",
    "                _, logits = model(input_values, attention_mask, labels, task_group)\n",
    "                probs = torch.softmax(logits.float(), dim=-1)[:, 1].detach().cpu().numpy().astype(np.float64)\n",
    "                all_probs.extend(probs.tolist())\n",
    "                all_true.extend(labels.detach().cpu().numpy().astype(np.int64).tolist())\n",
    "\n",
    "        # Main selection metric for the best epoch\n",
    "        val_auc = compute_auc(all_true, all_probs)\n",
    "        print(f\"seed={seed} | epoch {epoch:02d}/{MAX_EPOCHS} | train_loss={avg_train_loss:.5f} | val_AUROC={val_auc:.5f}\")\n",
    "\n",
    "        # Save best epoch artifacts by AUROC\n",
    "        improved = (not math.isnan(val_auc)) and (val_auc > best_auc + 1e-12)\n",
    "        if improved:\n",
    "            best_auc = float(val_auc)\n",
    "            best_epoch = int(epoch)\n",
    "            no_improve = 0\n",
    "\n",
    "            # Save only the trainable parts (heads + small pre-head blocks)\n",
    "            best_state = {\n",
    "                \"pre_vowel\": {k: v.detach().cpu().clone() for k, v in model.pre_vowel.state_dict().items()},\n",
    "                \"pre_other\": {k: v.detach().cpu().clone() for k, v in model.pre_other.state_dict().items()},\n",
    "                \"head_vowel\": {k: v.detach().cpu().clone() for k, v in model.head_vowel.state_dict().items()},\n",
    "                \"head_other\": {k: v.detach().cpu().clone() for k, v in model.head_other.state_dict().items()},\n",
    "            }\n",
    "\n",
    "            # Save validation outputs for plots and threshold calculation\n",
    "            best_val_probs = list(all_probs)\n",
    "            best_val_true  = list(all_true)\n",
    "\n",
    "            # Baseline threshold report at 0.5\n",
    "            best_val_metrics_thr05 = compute_threshold_metrics(best_val_true, best_val_probs, thr=0.5)\n",
    "\n",
    "            # VAL-optimal threshold from ROC curve at the best epoch\n",
    "            thr_opt, details = compute_youden_j_threshold(best_val_true, best_val_probs)\n",
    "            best_thr_youden = float(thr_opt)\n",
    "            best_thr_youden_details = details\n",
    "            best_val_metrics_thr_opt = compute_threshold_metrics(best_val_true, best_val_probs, thr=best_thr_youden)\n",
    "        else:\n",
    "            no_improve += 1\n",
    "\n",
    "        # Early stopping: stop when validation AUROC no longer improves\n",
    "        if no_improve >= PATIENCE:\n",
    "            break\n",
    "\n",
    "    # Guard: ensure at least one best epoch was captured\n",
    "    if best_state is None or best_val_probs is None or best_val_true is None:\n",
    "        raise RuntimeError(\n",
    "            \"No best epoch captured. Validation AUROC may be NaN due to single-class validation split \"\n",
    "            \"or earlier failures.\"\n",
    "        )\n",
    "\n",
    "    # Save the best heads for reuse in later test-only code\n",
    "    best_heads_path = run_dir / \"best_heads.pt\"\n",
    "    torch.save(best_state, str(best_heads_path))\n",
    "\n",
    "    # Best-epoch plots (ROC + confusion matrices at 0.5 and at VAL-opt threshold)\n",
    "    roc_png = run_dir / \"roc_curve.png\"\n",
    "    cm_png_05 = run_dir / \"confusion_matrix_thr0p5.png\"\n",
    "    cm_png_opt = run_dir / \"confusion_matrix_thr_opt.png\"\n",
    "\n",
    "    save_roc_curve_png(np.asarray(best_val_true, dtype=np.int64), np.asarray(best_val_probs, dtype=np.float64), str(roc_png))\n",
    "    save_confusion_png(np.asarray(best_val_true, dtype=np.int64), np.asarray(best_val_probs, dtype=np.float64), str(cm_png_05), thr=0.5)\n",
    "    if not np.isnan(best_thr_youden):\n",
    "        save_confusion_png(np.asarray(best_val_true, dtype=np.int64), np.asarray(best_val_probs, dtype=np.float64), str(cm_png_opt), thr=float(best_thr_youden))\n",
    "\n",
    "    # Per-seed metrics file (used later for threshold lookup and provenance)\n",
    "    metrics = {\n",
    "        \"dataset\": dataset_id,\n",
    "        \"seed\": int(seed),\n",
    "        \"best_val_auroc\": float(best_auc),\n",
    "        \"best_epoch\": int(best_epoch),\n",
    "\n",
    "        # Data sources (train_enh1 + standard val)\n",
    "        \"train_manifest_used\": MANIFEST_TRAIN_ENH,\n",
    "        \"val_manifest_used\": MANIFEST_ALL,\n",
    "        \"train_split_name\": TRAIN_ENH_SPLIT_NAME,\n",
    "\n",
    "        # Baseline initialization details (where the starting heads came from)\n",
    "        \"init_heads\": {\n",
    "            \"mode\": \"baseline_best_heads\",\n",
    "            \"baseline_exp_used\": str(baseline_exp),\n",
    "            \"baseline_summary_path\": str(baseline_summary_path),\n",
    "            \"baseline_best_heads_path\": str(baseline_heads_path),\n",
    "        },\n",
    "\n",
    "        \"n_train\": int(len(train_df)),\n",
    "        \"n_val\": int(len(val_df)),\n",
    "        \"label_counts_train\": train_df[\"label_num\"].value_counts(dropna=False).to_dict(),\n",
    "        \"label_counts_val\": val_df[\"label_num\"].value_counts(dropna=False).to_dict(),\n",
    "\n",
    "        \"experiment_tag\": EXPERIMENT_TAG,\n",
    "        \"run_stamp\": RUN_STAMP,\n",
    "\n",
    "        \"dropout_p\": float(DROPOUT_P),\n",
    "        \"lr\": float(LR),\n",
    "        \"effective_batch_size\": int(PER_DEVICE_BS * GRAD_ACCUM),\n",
    "        \"per_device_batch_size\": int(PER_DEVICE_BS),\n",
    "        \"grad_accum_steps\": int(GRAD_ACCUM),\n",
    "\n",
    "        \"backbone_ckpt\": BACKBONE_CKPT,\n",
    "\n",
    "        # Threshold selection based on validation ROC at the best epoch\n",
    "        \"val_opt_threshold_method\": \"Youden J (maximize TPR - FPR on VAL ROC curve)\",\n",
    "        \"val_opt_threshold\": float(best_thr_youden),\n",
    "        \"val_opt_details\": best_thr_youden_details,\n",
    "\n",
    "        \"thr_metrics_val_thr0p5\": best_val_metrics_thr05,\n",
    "        \"thr_metrics_val_thr_opt\": best_val_metrics_thr_opt,\n",
    "\n",
    "        \"artifacts\": {\n",
    "            \"roc_curve_png\": str(roc_png),\n",
    "            \"confusion_thr0p5_png\": str(cm_png_05),\n",
    "            \"confusion_thr_opt_png\": str(cm_png_opt),\n",
    "            \"best_heads_pt\": str(best_heads_path),\n",
    "        },\n",
    "    }\n",
    "\n",
    "    metrics_path = run_dir / \"metrics.json\"\n",
    "    with open(metrics_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(metrics, f, indent=2)\n",
    "\n",
    "    print(f\"[seed={seed}] VAL-opt threshold (Youden J): {float(best_thr_youden):.6f}\")\n",
    "    print(f\"[seed={seed}] WROTE:\")\n",
    "    print(\" \", str(metrics_path))\n",
    "    print(\" \", str(roc_png))\n",
    "    print(\" \", str(cm_png_05))\n",
    "    print(\" \", str(cm_png_opt))\n",
    "    print(\" \", str(best_heads_path))\n",
    "\n",
    "    return {\n",
    "        \"seed\": int(seed),\n",
    "        \"best_val_auroc\": float(best_auc),\n",
    "        \"best_epoch\": int(best_epoch),\n",
    "        \"val_opt_thr\": float(best_thr_youden),\n",
    "        \"run_dir\": str(run_dir),\n",
    "        \"seed_metrics\": metrics,\n",
    "    }\n",
    "\n",
    "# -------------------------\n",
    "# 13) Run all seeds, then write a single experiment summary\n",
    "# Outputs: summary_trainval.json (experiment-level) + history_index.jsonl (append-only)\n",
    "# -------------------------\n",
    "results = []\n",
    "for seed in SEEDS:\n",
    "    results.append(run_trainval_once(seed))\n",
    "\n",
    "aucs = [r[\"best_val_auroc\"] for r in results]\n",
    "thr_vals = [r[\"val_opt_thr\"] for r in results]\n",
    "\n",
    "# 95% CI for mean AUROC across 3 seeds (t distribution, df=2)\n",
    "t_crit = 4.302652729911275  # df=2, 95% CI\n",
    "n = len(aucs)\n",
    "mean_auc = float(np.mean(aucs))\n",
    "std_auc = float(np.std(aucs, ddof=1)) if n > 1 else 0.0\n",
    "half_width = float(t_crit * (std_auc / math.sqrt(n))) if n > 1 else 0.0\n",
    "ci95 = [float(mean_auc - half_width), float(mean_auc + half_width)]\n",
    "\n",
    "thr_mean, thr_sd = mean_sd(thr_vals)\n",
    "\n",
    "print(\"\\nAUROC by seed:\")\n",
    "for r in results:\n",
    "    print(f\"  seed {r['seed']}: {r['best_val_auroc']:.6f}\")\n",
    "print(f\"\\nMean AUROC: {mean_auc:.6f}\")\n",
    "print(f\"95% CI (t, n=3): [{ci95[0]:.6f}, {ci95[1]:.6f}]\")\n",
    "\n",
    "print(\"\\nVAL-opt thresholds (Youden J) by seed:\")\n",
    "for r in results:\n",
    "    print(f\"  seed {r['seed']}: {r['val_opt_thr']:.6f}\")\n",
    "print(f\"  mean ± SD: {thr_mean:.6f} ± {thr_sd:.6f}\")\n",
    "\n",
    "# Store thresholds in one canonical place (by-seed + mean/sd)\n",
    "val_optimal_threshold_obj = {\n",
    "    \"method\": \"Youden J (maximize TPR - FPR on VAL ROC curve)\",\n",
    "    \"by_seed\": {str(r[\"seed\"]): float(r[\"val_opt_thr\"]) for r in results},\n",
    "    \"mean_sd\": {\"mean\": float(thr_mean), \"sd\": float(thr_sd)},\n",
    "}\n",
    "\n",
    "# Experiment-level summary that later test scripts can read\n",
    "exp_summary = {\n",
    "    \"dataset\": dataset_id,\n",
    "    \"dx_out_root\": DX_OUT_ROOT,\n",
    "\n",
    "    # Data sources\n",
    "    \"train_manifest_used\": MANIFEST_TRAIN_ENH,\n",
    "    \"val_manifest_used\": MANIFEST_ALL,\n",
    "    \"train_split_name\": TRAIN_ENH_SPLIT_NAME,\n",
    "\n",
    "    # Baseline initialization (shared baseline exp across all seeds)\n",
    "    \"init_heads\": {\n",
    "        \"mode\": \"baseline_best_heads\",\n",
    "        \"baseline_exp_used\": str(baseline_exp),\n",
    "        \"baseline_summary_path\": str(baseline_summary_path),\n",
    "        \"baseline_best_heads_by_seed\": {\n",
    "            str(s): str(baseline_exp / f\"run_{train_dataset_id}_seed{s}\" / \"best_heads.pt\") for s in SEEDS\n",
    "        },\n",
    "    },\n",
    "\n",
    "    \"experiment_tag\": EXPERIMENT_TAG,\n",
    "    \"run_stamp\": RUN_STAMP,\n",
    "    \"exp_root\": str(EXP_ROOT),\n",
    "    \"run_dirs\": [r[\"run_dir\"] for r in results],\n",
    "    \"seeds\": SEEDS,\n",
    "\n",
    "    # AUROC aggregation across seeds\n",
    "    \"aurocs\": [float(x) for x in aucs],\n",
    "    \"mean_auroc\": float(mean_auc),\n",
    "    \"t_crit_df2_95\": float(t_crit),\n",
    "    \"half_width_95_ci\": float(half_width),\n",
    "    \"ci95\": ci95,\n",
    "\n",
    "    # Basic dataset counts (train/val)\n",
    "    \"n_train\": int(len(train_df)),\n",
    "    \"n_val\": int(len(val_df)),\n",
    "    \"label_counts_train\": train_df[\"label_num\"].value_counts(dropna=False).to_dict(),\n",
    "    \"label_counts_val\": val_df[\"label_num\"].value_counts(dropna=False).to_dict(),\n",
    "\n",
    "    # Training settings that matter for reproducibility\n",
    "    \"effective_batch_size\": int(PER_DEVICE_BS * GRAD_ACCUM),\n",
    "    \"per_device_batch_size\": int(PER_DEVICE_BS),\n",
    "    \"grad_accum_steps\": int(GRAD_ACCUM),\n",
    "\n",
    "    \"backbone_ckpt\": BACKBONE_CKPT,\n",
    "    \"dropout_p\": float(DROPOUT_P),\n",
    "    \"lr\": float(LR),\n",
    "\n",
    "    # Canonical threshold block\n",
    "    \"val_optimal_threshold\": val_optimal_threshold_obj,\n",
    "    \"per_seed_metrics\": [r[\"seed_metrics\"] for r in results],\n",
    "}\n",
    "\n",
    "summary_path = EXP_ROOT / \"summary_trainval.json\"\n",
    "with open(summary_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(exp_summary, f, indent=2)\n",
    "\n",
    "# Append-only global history file (keeps a record of all experiments)\n",
    "history_path = TRAINVAL_ROOT / \"history_index.jsonl\"\n",
    "TRAINVAL_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "with open(history_path, \"a\", encoding=\"utf-8\") as f:\n",
    "    f.write(json.dumps(exp_summary) + \"\\n\")\n",
    "\n",
    "# -------------------------\n",
    "# 13.5) Builder-style mirror artifacts (small, stable files)\n",
    "# Outputs: run_config_trainval.json + dataset_summary_trainval.json + placeholder warnings CSV\n",
    "# -------------------------\n",
    "trainval_run_config = {\n",
    "    \"dataset\": dataset_id,\n",
    "    \"mode\": \"trainval_enh1\",\n",
    "    \"created_utc\": datetime.utcnow().isoformat(),\n",
    "    \"dx_out_root\": DX_OUT_ROOT,\n",
    "    \"train_manifest_used\": MANIFEST_TRAIN_ENH,\n",
    "    \"val_manifest_used\": MANIFEST_ALL,\n",
    "    \"train_split_name\": TRAIN_ENH_SPLIT_NAME,\n",
    "    \"experiment_tag\": EXPERIMENT_TAG,\n",
    "    \"run_stamp\": RUN_STAMP,\n",
    "    \"exp_root\": str(EXP_ROOT),\n",
    "    \"baseline_exp_used\": str(baseline_exp),\n",
    "    \"baseline_summary_path\": str(baseline_summary_path),\n",
    "    \"seeds\": SEEDS,\n",
    "}\n",
    "with open(cfg_dir / \"run_config_trainval.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(trainval_run_config, f, indent=2)\n",
    "\n",
    "trainval_dataset_summary = {\n",
    "    \"dataset\": dataset_id,\n",
    "    \"created_utc\": datetime.utcnow().isoformat(),\n",
    "    \"status\": \"SUCCESS\",\n",
    "    \"train_split_name\": TRAIN_ENH_SPLIT_NAME,\n",
    "    \"n_train\": int(len(train_df)),\n",
    "    \"n_val\": int(len(val_df)),\n",
    "    \"label_counts_train\": train_df[\"label_num\"].value_counts(dropna=False).to_dict(),\n",
    "    \"label_counts_val\": val_df[\"label_num\"].value_counts(dropna=False).to_dict(),\n",
    "    \"exp_root\": str(EXP_ROOT),\n",
    "    \"summary_trainval_json\": str(summary_path),\n",
    "}\n",
    "with open(logs_dir / \"dataset_summary_trainval.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(trainval_dataset_summary, f, indent=2)\n",
    "\n",
    "with open(logs_dir / \"trainval_warnings.csv\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"ts,level,message\\n\")  # placeholder: errors surface as exceptions in this cell\n",
    "\n",
    "print(\"\\nWROTE per-experiment summary:\", str(summary_path))\n",
    "print(\"APPENDED global history index:\", str(history_path))\n",
    "print(\"\\nWROTE (builder-aligned):\")\n",
    "print(\" \", str(cfg_dir / \"run_config_trainval.json\"))\n",
    "print(\" \", str(logs_dir / \"dataset_summary_trainval.json\"))\n",
    "print(\" \", str(logs_dir / \"trainval_warnings.csv\"))\n",
    "print(\"\\nOpen this folder to access artifacts:\", str(EXP_ROOT))\n",
    "\n",
    "# -------------------------\n",
    "# 14) Release runtime resources (stop L4 GPU)\n",
    "# -------------------------\n",
    "print(\"\\nAll done. Unassigning the runtime to stop the L4 instance...\")\n",
    "try:\n",
    "    from google.colab import runtime  # type: ignore\n",
    "    print(\"Calling runtime.unassign() now.\")\n",
    "    runtime.unassign()\n",
    "except Exception as e:\n",
    "    print(\"Could not unassign runtime automatically. Error:\", repr(e))\n",
    "    print(\"Manual stop: Runtime -> Disconnect and delete runtime.\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "da1dde15ec2f4742b34b8dde702b3b4f",
      "872bed64d21249358555a3e6b1c875f0",
      "df9f91bd333647cbb63ebf55173bf466",
      "bc7383034a924004b0172318f7960731",
      "bdd2f4be97a04c4b857d24df7f55c92b",
      "0ed0f9bb31474530b6e91990953b1b8f",
      "a8387612963d4161ab7f947ca7762489",
      "6d36e791917a4c729cf70c8b4958a9f4",
      "c116d097d45145a7b40a9db58fcf9483",
      "60590c37a4004ac8b24b84b754aa7c1c",
      "c30609011573491dbb0bfcb0856c0fbf",
      "3f79adc1003345f3a127973c9cd3f443",
      "cb2c4dd09e024c53aa7038417fbd36f3",
      "df73952c662b454798040fbb8d9753d1",
      "938519c5c642408a9bab9210180d3b87",
      "9da6f312ecd142a8a4229a44f172d09a",
      "bdb26376694e484dbff8a0ae5e8b4b3b",
      "dae61ab2f018495eae50ab54a2112a79",
      "beaa0f1409894f5381f42592053b1b50",
      "ba992439adb8481389a569690309962a",
      "5dff5df6d6c54ca3b703a7f2adef7492",
      "c1ff70a4912d43cca33edbac21a6256a",
      "7a7524b7438b418793f16153fb857683",
      "0b0a48cd4eaa48d4860a304de2774abc",
      "39529608b1e44742aca759b77af6870f",
      "29653bbdd2f04e9f8b28dde45b185bc1",
      "7e3599a20c684e7290a3c487f0a1c4f8",
      "da27e5033e0f427aa41724f213f4690c",
      "edd34906cca84f028ce742b3dda30b0d",
      "778194145f1c412d81a3acca7cb732cd",
      "7bfeea38722043f8a07c739af04267c3",
      "8c331941ea334fff8df6ec44484dc014",
      "5ac2aa76d3d642f998ae5f517ffec117",
      "bc8d22fb3d364adb8c39f92d5d207e63",
      "184499697758492aa9c11f93cc395e4e",
      "67c13786140c45718d446a2620540b0b",
      "dc5fb5488e374417b21d89257aa90b68",
      "74dcc8942e36416088f38efebe84ef70",
      "e39735391ad04f6580a0af8ac3c13acb",
      "985b36263a8b4e79a3d78c1bba2aaed1",
      "eda051b608614856a1d071aa113816e3",
      "9982ff8a593c4cf897ec766f6ff0b17d",
      "75efe5152a144456931546ccc61b622c",
      "5c7d61e39d0e4ddab5d24ce6764b59bd",
      "c49b6d981d444ed1abf63bd74c8bc353",
      "a2b908f800fe4ddc961b4fb9e6d725eb",
      "752a26131db0455da8df5ac916408c9a",
      "400c536807874acb856f71655ed0e589",
      "9d8e6739cfb546c2a16936abfe594833",
      "5910ccf53de348d29e168c5fc5e9f631",
      "43ee4ed33f2f49ff97079705804b7219",
      "f338971aadd7453d82bdca794f5f90ae",
      "05e123eae8bd4e329354d99a1846dcae",
      "3fc82891dc4c42d49126ed1920e13cf4",
      "1046aaa143a54c869dec15077f9331ba",
      "67a6aeee0c1244358e90208ff687eee5",
      "aba3b7527ab94c2eb9be5b883e72b0a9",
      "be4eb127d983456681c05e07331b9a71",
      "d2900fab9cff48a486f40b241ae98fc8",
      "6bc6521dbbd146999a1850bc8e6d2361",
      "4753f5e456bf4b6ba923ac903f25a066",
      "0be95f4ced764deeaa023d84aae9297c",
      "806e674a5fe54d24901eb4b07fdb30cc",
      "3a195d790d19453ba7af17509217c6bd",
      "7a8ee5845ba143ebacccb73a214773b3",
      "bd8f8fd7a9ae4643a507179fe1961810",
      "31b8d29eda894d319016e5408e56dd4f",
      "72371518cea349b8b31699b67665600d",
      "a3b8287996b2454b9855b1a5b06f07b3",
      "410e9e2d0a6d4aa780e449645277a701",
      "67f461a184a54f539d7f8594fca32a60",
      "496eba73686e4c8caa9d46d66b426838",
      "1b8614a06d5a46aa8d8d9942e60eb5f1",
      "9291eba00d0d4feebdc415f0900698c9",
      "9919b55ab75143cc801a4db0d1c92936",
      "2425905a86e54569afc995ac1731e526",
      "bbd3ea3ab3f14ce5bf5f9cd6edf6b972",
      "e6ab9eb52ff7446ab0229d19c2d19f35",
      "e3d1270edd3147f18fb057db9657b5c8",
      "1612cb06551e4da1b728b53825b07c60",
      "26be3a340c874567b601c60e77420859",
      "9032ff1ccc53459e948cb11f1465eaf7",
      "a887a352c0d5446a9c77186a9e702e32",
      "abb911755e4046e086febee86699e9b7",
      "ccbdaf0fc825453aab52307bbc8af25a",
      "00a82395bb484f4094d5632a13ae953e",
      "f1991be8f8ba4d38b6bda1f629255ac5",
      "f4af9e7d593f4de2ac84506fbc77c317",
      "8cd3894bf8864fa9bd071113e4ee6467",
      "ae1098075c044a32805f68e920331e55",
      "10f87bdb7ae64c608bdff212485da62a",
      "ab3ead6ab72841e4b3c9a40c9b26a1e4",
      "f4af2ed6c1c94b1789c1596c700ee35b",
      "1dea6ee241ca44e398c00e3fb57a1397",
      "91bfd8c1ba214727a72d51e5ced4e777",
      "5e71c2a622ef4b8292b9fe32a3ac9e71",
      "50f031401f614094af7a8311c299a43a",
      "22f6a5121fe84c41a5e78e72b1041682",
      "518e525cf2db46cb82b000b9e767ce89",
      "53e7e89f05034131a19800bbd82bc4bb",
      "6ea8b4d3e1f64aa4b497008d7b45b60a",
      "726703304712446babf9a50124047bf3",
      "0bbdd63b0873410f877cf15f7ba34ab7",
      "3cd4e00d198644ce8bc27738b01954fd",
      "7931c54bbeef4cba9d20345f0e71bfb5",
      "ceb3773dd5f747fabfa423386efeaf4a",
      "e95c2aff427c41139ad838705529f3d0",
      "fc18a83e8e1044aeba45bd08815aca79",
      "e8bed9e5ca9649d9873c486c58108f4e",
      "7d43410d8c8f4b42b6253cba9f7102fc",
      "027742885c3f4418b117fdb14d20f9f0",
      "3c6ef836f9fa40d0b02976c861d14e8f",
      "14829d93fbaa40419d3a4bdd3dff22d3",
      "f7719e21d4e24d08aa5a23c00a0b8d61",
      "d9a4b48f4fb84b61ba01967b1698eb80",
      "c415549693194ed2b28f9e4e8f658b89",
      "1b7a7eacd0404a27b77800abbd5b46e8",
      "e766dc2a22ff404e91e5e4a0930fdcf6",
      "8eae7d2937fe4130bb2147d07a9d12dd",
      "c6d0b00899f0459f8f9966d29bdf90aa",
      "00783712ac8a483e995af0b2fab7b0b0",
      "6fa067d7178b479ebc2322f602c6bdf8",
      "87c9f37a5a5f4d53ae7d8c01844c2923",
      "4b988f41277f468eac5255e14dc0a308",
      "168a34aa76d344f59b5cf9140f96c47f",
      "05008a251bdb47ddb5c52c71775a9614",
      "65812340aa4847e0a67ff52ad49d6fe5",
      "90fdf2f142bc4fa7b68fb8c2a7de2c33",
      "b91595ea22af4d1cb31aa4da640f7dd6",
      "8b6a763fa5ac4de8bc4b71b18fd47308",
      "41f704bf3046434ab51415e4d87737f9",
      "832f86c0fa884489a38e8d80ebd50346",
      "d2fbe6ea0d5c4cdc9fd43e7d87b2102c",
      "ea91d1b0f33e44079f3d3bd9c7332cd0",
      "f04b296fdce347f6a2be4e1f6158541b",
      "f2f87baf190f45b3a5cee0011242887d",
      "ae84f769415c4634af797eaa17831937",
      "4ab16bd5769c480fb573264df72b609c",
      "1d0b9ee9afd04cf5aa8e3cdbecd7fff6",
      "c8e0c671fff745c395812293b8c84702",
      "1da9866b6c0640bb9feb327957a13616",
      "321c7d202a5446d6b6b8664d04d6ce17",
      "a8040e46ccbc4d869d1c0ec8d6f63407",
      "479332215f2a4631b854090e994e675a",
      "0e552146d97c4d868c5e2cba967abba1",
      "8004fb18673448bcb1376b9acfc3f08b",
      "6c5a669002644f3e90be865f41e6828a",
      "5d3b73f4367c4870a83212a5a4bc414c",
      "76beb11458134854a222d2eff4f7cb6d",
      "6f64871d49ec46de931d6e4fd5bc1241",
      "9ddc1044b59b4af6a553034a312437af",
      "aa36ad6c538d4b11ba1067f2fbfcabf7",
      "788d4519ccaf43dc939fb1ec79836151",
      "a8fa0bcc86e14e7f95c44b1adf47eb66",
      "ae897e40603b46ec88f0c4732de290ab",
      "daa39961278d4a5da9e5ed8f8146dc0b",
      "30d0ecc7030a4dda8c9f574ca2f1c5c4",
      "e37743ec56884a1c953fa5926cc06cec",
      "4f3bc7d9bf404936b7268d6d0610a9d3",
      "5f7db7bd64ba47848a30ed8f8cbfe73a",
      "77d3c2740f7c445f9bdcc12850c45271",
      "374960a6c4134ce49839367c32ee8da2",
      "e7a3a84a0f844407b4db9e815db8f88d",
      "ae9de24c143749848bf190c93668eff1",
      "857356c1f0dc47a79b91126043f48589",
      "a994dd346bda4a84ac9afe1864562f19",
      "35fe61bd1ba941e09a9196d0d07e017e",
      "492f377cfc9644a29e90c95cf1de93f9",
      "8cd905dc229d42b3a3b269e573a8f33c",
      "ac5fd10e94f24b0394131f34fa81453e",
      "b6845eaefcfd4d748a2b3beebf3579d3",
      "bea7b6fdc1e54dc5a427a4c1c279dcfc",
      "144ec074433e445cbe15864dcf01ffe9",
      "7c95d2b2b1cf44b19a58ca5f4e0ba8e9",
      "5b41ccd56ef14d53b7af883ebbd49996",
      "6de95811b4234d0594fec9162d932817",
      "d8408d4892224576bc614158203faae7",
      "b7f1e7a78a4e467b95f838ea800a7836",
      "4eab3e54013e4d5eb1208dfb87f52b51",
      "95df6d4acd3742108549fe3cffeb05d7",
      "7ea1e53644d4464b8d638dc724344e58",
      "fd7db4d8157e461b821c62d325bac059",
      "e5e24f9f2b75419aaa56d3b0733f84cb",
      "c7a6d9ecc5cf4f448299b62e7c4c4fe0",
      "a9ba7224e493416cac52cd76d92b6d63",
      "13d21679c8f346b99d5238e6fb6d5c39",
      "90eb7fd18b9541c1b794131ac25e8d82",
      "165e85f0c97d4a0081f377ed7077b0de",
      "08803b0620b24cc4b3a7eecfed75129f",
      "50f56e7d9dd24d0b957d84ccf9ccd571",
      "81f763ced27c426e9a5f43df535ebe86",
      "66a5a42ed8fa4b9e99a88d8e151adc01",
      "3b909e3064f74a1a9b73da6d22c045d2",
      "e1b602d756164a90b3c5767e979e2e16",
      "bdae5a2954974cf4805eb9910605760f",
      "86c9b52dadd04f3f9f11848e074fdc02",
      "05fdb1a5f8b8470db00c5a36e9e7ccb6",
      "ac8246dcbb7e40ebab1342213011be66",
      "56ffcbb84eed4e9f8973241e262ac790",
      "80f166b3520d4e46b5882244c3c0fbac",
      "1cbc3894a95f419580c57c45cd236989",
      "4cffc0b8b2274554ab2d037475b871e5",
      "cca94a99a2394bf99585ddfea84e0f96",
      "6d7720daad30459ca410e1c374f2676f",
      "f83dc105d5ba4b998df74ee248e0c666",
      "985361fc66fd4bab9a658c532f8a0d6f",
      "72d705d9a2c34d97a2b423b156eb8e4d",
      "a8f05c1003184ca3ad9f7695d1bc7447",
      "681845a2dcc24f47a0df48cbaa84b68d",
      "058e89696f1b4f50b4639ebc5beeb2f6",
      "67030408c1c441a9b9e0288112cadf2e",
      "3dc1fd3f89794421a15d6dad10275b80",
      "b8b33939c5e849e398c529f4225c98a9",
      "f5073da78de9497bbb97de9c1a6b92f7",
      "1bfd65f3e4ff40a4815920021a01a63c",
      "0c7f24914e954213ae318ed5f094c2d8",
      "1292faeb8ce64486ad7c05b6d2b85a30",
      "e3c7d869dcc04ef482b22e439035c4c8",
      "0435de16a8114ec6bc44d93f4ba38518",
      "579fc59050504cf5973e64dfb8b349e2",
      "d8682c158a0d4f4abbb4725698a19c1e",
      "8a26956f329f4734a10ae34209f09841",
      "c0c385bf84844cfa9ef26db5e7e013e2",
      "5ebad03786c345ea8cdac58b6df59caf",
      "23da0115f21b4360b7d0004c3df440ed",
      "bb848e91420f4e16902ca29edc3d1862",
      "5c93a41d697142bf8305b2fe2d63be85",
      "6abec8f6e2bb4308a3b92b91594a6fc8",
      "2896058abbf14bb2ade61fc9982a31a3",
      "c3632d41df7e4bfaacc18bf9fa438a4e",
      "bfa6866d621c48ad8ac0cd970d2e53a2",
      "3c05ce93e129440a8be09adbd94c71c7",
      "bfa8c959a3fc40af85eecaae8f1398bd",
      "170e26ff18724a7f9d235f484c58ca71",
      "711f8dc8fc024d14a4688ba52db10c19",
      "fa94298d68e04277815653d21f5eee22",
      "78a17256a74645a3ad43cd54f40a090a",
      "5cbba9d16bb54b41bd28514d7be02eff",
      "cdd0f856d2cf4632a88117ccb4f98924",
      "b4c141c45db64aca967d8bb3f6e98418",
      "f43d6a46fafb4b3fb413b4dd7cd3ffef",
      "8a4e4623fb9a44c8b1677c51fb86c8a5",
      "62481bc1fd744ef3b4a56cb92fd7ec8d",
      "e98898cdbef04b34878b3581abd6d758",
      "f6af817968e7456a892745b358dd9bca",
      "43ed3a7949c4439c922dc07ae1312fb5",
      "1a2976a0346e434393680ed81580d3ed",
      "3dea287cf669456d8f97c4cefe446498",
      "aa81f70012b14b548b9a8631bc413e81",
      "af3443010abd4969861319c86c8ef961",
      "8247aba38709404286bf97fc4281d901",
      "a8f4035cd2da484b8dd2fe6730664433",
      "8a38805362934534b262cef4bdee30e7",
      "e82eb4a9752a403e90fad1a338b944fa",
      "c7996ee43ff3418ba05cff557240ec3a",
      "11699bfad04d47f6b8e6e805f0a8b5cf",
      "bf9d9db345634863b0fb771592c15a10",
      "e6369bafb25f465f84cd179dc89a8afc",
      "59513e5991514992b9567e668a4e4404",
      "b3fc703a3129496b96c75c207b2dd2ae",
      "02e5dc4ffb0a47f5bca076ca59c70f7d",
      "6cc013e63d3f45229357b5834f0e4dbf",
      "f1ed73e526054b4eabd462f8c7f2befa",
      "ed76bea5e7bf468ba8d9eb9ed9d403e9",
      "0ffc7598dba74bfab4161eee565f3e01",
      "3817e070ba544c3eb4d7d5507d9a526b",
      "787454c05eab44a59d86ce83a254694f",
      "6c130ff517154561adddc9be406bb086",
      "cee4599d524a4300bebd45577711cf3c",
      "b650a52ef2664756be2727e980a5eb79",
      "cd2d72f60bc244158ef922e03fa60747",
      "56bf95434a124a0fa9032a7452babcf8",
      "0bf2c4e8d74d496ab0342468c4ddf99f",
      "01ced6ff9e324c979efee3db51cc3b30",
      "98aecbd91f6341b7a0d255e4d693cec5",
      "a2f53acdcc0747a88a54e2e8c45b02b9",
      "ba05a2943e47492d87009ab310f4b110",
      "544c1e70d06f483a877105251c5a63a9",
      "44137a412b38451cad99ef790b28941a",
      "ae575d954e8c472dabab2f8246cc4e91",
      "ddd83f1df8694d1b8195335105c6f429",
      "97e9437d25c848958819e0ae4be7db77",
      "04a0300d69494708b15d94e67e421c83",
      "f7962a2ab2a046ee8ecb2b6dcb95affe",
      "b99ab6fba77140b5a9250424a47d3d0e",
      "90b939807aa1446ebd867b599e194cf2",
      "4511effa4cd14404ada750612b97a32a",
      "1ba7c105de15480da6640bf7ff4da1cc",
      "de75b3eb8c8c422c98262a5313368f0b",
      "533e38e42daa494b9f78ab036071e485",
      "ea4c63e85afa4aecad3bb57acffd5e18",
      "e44f82fbab1547ba8594f9fe8d0fe249",
      "4cff02c654ec43d99f9f947e427617a9",
      "cf166b081023444a9acd47451b798ac5",
      "778a79e6c7504534ae59437184b41d8e",
      "f42b419918d844d0992a4ebf6e4f0a71",
      "2d4e7ba718fd4e6fb0a064f06deb31eb",
      "9de84ea75e974f7d920d5739bd890de0",
      "1b3d50883c3e469ca7dac428e4b1ffcd",
      "1d200bc194d24bb8a20e2c2c9f4b77db",
      "e82abdb5349d4bc9b2c717027bac6c70",
      "443aa1f269d243aa9fc128ec1880a43b",
      "9a35d1487d634eb7b47f42bd6ba80877",
      "942815cb2105479cbdb77f45c84e516f",
      "40f81343c6a343b59c1b74db9dab1604",
      "2878b51e53ee4781a5b6eb6bf99cd48a",
      "7c706776ef144a978b5a68f06849d924",
      "176e4bba6fdb42bfbfc8a55962f90f75",
      "3c02aece9f4647c5994488bf6e18d69b",
      "ab856b9043494ff1a24e1746f2c466bc",
      "c1b5f932560e493ebbcb601f116383fd",
      "acf64a87005e480188bab025b94f9944",
      "3917a42d4350404d860d9b4dd4da120d",
      "fde919a1c9df4a52a227d96c37cd5351",
      "e3e055ad8104418382967e1bcf8e4c36",
      "bbcc57f60722471a964ce892852044ae",
      "882f89c7e0d34b3dbb4fa859a2d727b8",
      "02944b77a091436db5d19723c8225f8c",
      "44f2ab6fcda949139818b1a5bcd1ad81",
      "b5280fd3d77d403986674dcf831a2e9e",
      "c20c914d8e6f40c49b56e679b0c45036",
      "4dd3955216404492bbf44f88c2b84a6c",
      "6f37d652285f4971be9ce1591576556d",
      "0358dbdd1dd94915a70751c66d92f5df",
      "63cc36d9e9874932833033a67b0b87b5",
      "d0446352e38a40cf910c3f6ec00190d6",
      "bdb93303d1ee4cd6960191bf380c4a6c",
      "353e17d1cefc4954b23e88f611d93cb1",
      "63d0d0421a154c2e9a81a748b7f93efb",
      "5280ecb0c32d441bbac73d8536860ab9",
      "958812fd91194b688461481bfb3783d0",
      "4982e21bd4ef491281fd99c4fde55589",
      "d5d4c03fee5747278e60122434a69301",
      "f86545c2f0c249ca83bfc4b90c83f224",
      "eedabedda97f4009ba5dd6008cc54a60",
      "5a6b5dc8e84545bb81cc349ab7965297",
      "3f5be12b419c44d48f2e652f7c2ecefd",
      "4dded7bbd69c4f8393460bbb4cdfd065",
      "8873ad29edf342ff8b5e8a9b8a0b3ef6",
      "f63a9f9c75534f69856886cfb12c87af",
      "9f7040fad6b34832ac4335f94195706e",
      "b6f08dacc3d74330948d18631283b1fa",
      "6894bb9551ac4f279b7f355ed752a6fc",
      "07cb9fcdb5374156a42a62898846010c",
      "bddd3fa5e34449618b07cdb4a6801b3c",
      "1de0733a04954d7c921d547eb0a2661f",
      "c8fa94b6bfea45d69c9a10f86427817e",
      "fa236cabbe9a4916ad4b5ab95892bf5b",
      "f6b63744f98e46edac2597ea6b7c1ba2",
      "65fa6c56d556424aad19b06e1e63699f",
      "fb7451c685c24da69a7d9f3f2d82e0f2",
      "df54bd0a6f34433abfb41486b330b26a"
     ]
    },
    "id": "0lsppflcdDMp"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "The following cell runs a **test-only evaluation** of the **D7 enhanced model (trainEnh1)** on the **D2 test split**, with no training involved. It reads only the D2 `manifest_all.csv`, keeps rows where `split == \"test\"`, confirms that the manifest truly belongs to D2 as a hard safety check, and verifies that every listed audio file exists before continuing. It also adds two simple helper columns used during evaluation: **task_group** (set to “vowel” when `task == \"vowl\"`, otherwise “other”) and a **normalized sex** label where D2 values `\"male\"` and `\"female\"` are mapped to `M` and `F`, and anything else is set to `UNK`.\n",
    "\n",
    "Next, the cell automatically finds the **most recent D7 enhanced train and validation experiment** under `<D7_OUT_ROOT>/trainval_runs/exp_*/` whose folder name includes “trainEnh1” (case-insensitive). It requires that this experiment contains a `summary_trainval.json` file and `best_heads.pt` files for all three seeds (1337, 2024, 7777). After selecting the experiment, it checks again that all three head files are present. From the experiment summary, it reads a **single global decision threshold** from `val_optimal_threshold.mean_sd.mean`. This same threshold is used for all seeds. If the value is missing or invalid, the code falls back to 0.5 and records that this fallback was used.\n",
    "\n",
    "For each seed, the cell rebuilds the model using a **frozen Wav2Vec2 backbone with two task-specific heads** (one for vowel clips and one for other clips), loads the saved head weights, and runs inference on the full D2 test set. For each seed it writes a dedicated output folder that includes a `predictions.csv` file with clip-level results and metadata (clip path, true label, PD probability, sex, speaker ID, task group, seed, run tag, and threshold used), a `metrics.json` file with AUROC, threshold-based metrics, fairness values, and artifact paths, and several plots. The plots include a ROC curve, an overall confusion matrix, and confusion matrices split by sex for M and F when enough data is available.\n",
    "\n",
    "After all three seeds finish, the cell prints and saves an overall summary. This includes the **mean test AUROC with a 95 percent t-based confidence interval (n=3)**, along with threshold-based metrics reported as **mean ± standard deviation** across seeds. It also computes the paper-ready fairness metric **H3** on the D2 test set at the same global threshold, defined as **ΔFNR = FNR(F) − FNR(M)** and its absolute value, where FNR is calculated only on true Parkinson’s cases. Finally, the cell writes results to a structured directory under `monolingual_test_runs/`, updates a pointer to the latest run, appends the summary to a history log, writes a stable tag-based pointer, mirrors key metadata into builder-style `config/` and `logs/` folders with backups if needed, and then unassigns the Colab runtime to stop the GPU."
   ],
   "metadata": {
    "id": "LJEV1LcdwQOe"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# =========================\n",
    "# D7 trainEnh1 Heads → D2 Test Evaluation\n",
    "# Inputs: D2 manifest (test split) and latest matching D7 trainval experiment (best_heads + summary_trainval)\n",
    "# Outputs: per-seed predictions and metrics, plots, run summary files, plus latest and pointer files\n",
    "# =========================\n",
    "# =========================\n",
    "# TEST ONLY (CRASH-PROOF, WITH PROGRESS + STORED METRICS) — D7 ENHANCED → D2 TEST\n",
    "# - Evaluates the D7 ENHANCED trained heads (trained on train_enh1) on the D2 TEST split only\n",
    "# - Uses ONLY D2: <D2_OUT_ROOT>/manifests/manifest_all.csv  (TEST split)\n",
    "# - Loads finished heads from MOST RECENT D7 *ENHANCED* trainval experiment under:\n",
    "#     <D7_OUT_ROOT>/trainval_runs/exp_*/run_D7_seed{seed}/best_heads.pt\n",
    "#   Selection rule: exp folder name must contain substring \"trainEnh1\" (case-insensitive)\n",
    "#   and must contain all three seeds + summary_trainval.json.\n",
    "# - Uses the SINGLE MEAN VAL-optimal threshold stored by that D7 trainval in:\n",
    "#     summary_trainval.json -> val_optimal_threshold.mean_sd.mean\n",
    "#   (No VAL threshold recomputation in this cell)\n",
    "# - Evaluates 3 seeds separately (1337, 2024, 7777)\n",
    "# - Reports:\n",
    "#     * mean Test AUROC ± 95% CI (t, n=3)\n",
    "#     * A single threshold used for ALL seeds (mean val-opt threshold) + note if fallback to 0.5\n",
    "#     * Threshold metrics on D2 TEST @ that single threshold as mean ± SD\n",
    "#     * FAIRNESS (H3) on D2 TEST @ that single threshold as mean ± SD\n",
    "#     * Confusion charts split by sex (M/F) on D2 TEST @ that single threshold\n",
    "# - Writes all artifacts under:\n",
    "#     <D7_OUT_ROOT>/monolingual_test_runs/run_<FULL_TRAINVAL_EXP_TAG>__<RUN_STAMP>/...\n",
    "#   plus:\n",
    "#     * monolingual_test_runs/last_run_pointer.json (intentional overwrite)\n",
    "#     * monolingual_test_runs/summary_latest.json (intentional overwrite)\n",
    "#     * monolingual_test_runs/history_index.jsonl (append-only)\n",
    "#     * monolingual_test_runs/run_<TAG>/tag_run_pointer.json (never overwritten across tags)\n",
    "#\n",
    "# D2-SPECIFIC NOTE (the manifest):\n",
    "# - sex is encoded as the exact strings \"male\" / \"female\" (case-sensitive)\n",
    "#   This code maps: \"male\" -> M, \"female\" -> F, and anything else -> UNK\n",
    "#\n",
    "# GUARDS:\n",
    "# A) Hard-assert D2 dataset_id == \"D2\" after inference from D2 manifest\n",
    "# B) Re-assert all best_heads.pt exist immediately after chosen_exp is selected\n",
    "# =========================\n",
    "\n",
    "import os, json, math, random, time, warnings\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import soundfile as sf\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from transformers import Wav2Vec2Model\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, confusion_matrix, matthews_corrcoef\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# -------------------------\n",
    "# 0) Environment safety checks\n",
    "# -------------------------\n",
    "# Prevents accidental import of local files named like core libraries.\n",
    "if os.path.exists(\"/content/torch.py\") or os.path.exists(\"/content/torch/__init__.py\"):\n",
    "    raise RuntimeError(\"Found local /content/torch.py or /content/torch/ that shadows PyTorch. Rename/remove it and restart runtime.\")\n",
    "if os.path.exists(\"/content/transformers.py\") or os.path.exists(\"/content/transformers/__init__.py\"):\n",
    "    raise RuntimeError(\"Found local /content/transformers.py or /content/transformers/ that shadows Hugging Face Transformers. Rename/remove it and restart runtime.\")\n",
    "\n",
    "# -------------------------\n",
    "# 1) Drive access (Colab-friendly)\n",
    "# -------------------------\n",
    "# Mounts Google Drive when needed; safe to skip outside Colab.\n",
    "try:\n",
    "    from google.colab import drive  # type: ignore\n",
    "    if not os.path.isdir(\"/content/drive/MyDrive\"):\n",
    "        drive.mount(\"/content/drive\")\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# -------------------------\n",
    "# 2) Resolve input roots and basic run helpers\n",
    "# -------------------------\n",
    "# Inputs:\n",
    "# - D2 manifest is read from the D2 root\n",
    "# - D7 trainval experiments are searched under the D7 root\n",
    "# Outputs:\n",
    "# - all test artifacts are written under the D7 root\n",
    "D7_OUT_ROOT_FALLBACK = \"/content/drive/MyDrive/AI_PD_Project/Datasets/D7-Multilingual (D1_D4_D5v2_D6)/preprocessed_v1\"\n",
    "D2_OUT_ROOT_FALLBACK = \"/content/drive/MyDrive/AI_PD_Project/Datasets/D2-Slovak (EWA-DB)/EWA-DB/preprocessed_v1\"\n",
    "\n",
    "D7_OUT_ROOT = str(globals().get(\"D7_OUT_ROOT\", D7_OUT_ROOT_FALLBACK))\n",
    "D2_OUT_ROOT = str(globals().get(\"D2_OUT_ROOT\", D2_OUT_ROOT_FALLBACK))\n",
    "\n",
    "D2_MANIFEST_ALL = f\"{D2_OUT_ROOT}/manifests/manifest_all.csv\"\n",
    "\n",
    "# Keep DX_OUT_ROOT aligned with the run root (D7), since style uses DX_OUT_ROOT.\n",
    "DX_OUT_ROOT = D7_OUT_ROOT\n",
    "globals()[\"DX_OUT_ROOT\"] = DX_OUT_ROOT\n",
    "globals()[\"D7_OUT_ROOT\"] = D7_OUT_ROOT\n",
    "globals()[\"D2_OUT_ROOT\"] = D2_OUT_ROOT\n",
    "\n",
    "# Timestamp used in output folder naming and backups.\n",
    "RUN_STAMP = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "def _backup_if_exists(p: Path):\n",
    "    # Moves an existing file aside before overwriting it.\n",
    "    if p.exists():\n",
    "        bak = p.with_suffix(p.suffix + f\".bak_{RUN_STAMP}\")\n",
    "        try:\n",
    "            p.rename(bak)\n",
    "            print(f\"BACKUP: {str(p)} -> {str(bak)}\")\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Could not backup existing file before overwrite: {str(p)}. Error: {repr(e)}\")\n",
    "\n",
    "def _sanitize_tag(s: str) -> str:\n",
    "    # Makes a filesystem-safe tag for folder names.\n",
    "    s = str(s).strip()\n",
    "    out = []\n",
    "    for ch in s:\n",
    "        if ch.isalnum() or ch in [\"-\", \"_\"]:\n",
    "            out.append(ch)\n",
    "        else:\n",
    "            out.append(\"_\")\n",
    "    out = \"\".join(out).strip(\"_\")\n",
    "    return out if out else \"tag\"\n",
    "\n",
    "# -------------------------\n",
    "# 3) Fixed evaluation settings\n",
    "# -------------------------\n",
    "# Keeps evaluation consistent across runs and seeds.\n",
    "SEEDS          = [1337, 2024, 7777]\n",
    "BACKBONE_CKPT  = \"facebook/wav2vec2-base\"\n",
    "SR_EXPECTED    = 16000\n",
    "TINY_THRESH    = 1e-4\n",
    "\n",
    "# Batch settings for inference (no training happens in this cell).\n",
    "EFFECTIVE_BS   = 64\n",
    "PER_DEVICE_BS  = 16\n",
    "GRAD_ACCUM     = max(1, EFFECTIVE_BS // PER_DEVICE_BS)\n",
    "\n",
    "# Head architecture settings (must match trainval).\n",
    "DROPOUT_P      = 0.2\n",
    "\n",
    "# Dataloader stability settings (safe defaults for Colab).\n",
    "NUM_WORKERS    = 0\n",
    "PIN_MEMORY     = False\n",
    "\n",
    "# Mixed precision speeds up GPU inference when available.\n",
    "USE_AMP        = True\n",
    "\n",
    "# Only experiments whose name includes this substring are considered.\n",
    "REQUIRED_EXP_SUBSTRING = \"trainEnh1\"  # case-insensitive\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "try:\n",
    "    torch.set_float32_matmul_precision(\"high\")\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# Quiet known, non-actionable warnings.\n",
    "warnings.filterwarnings(\"ignore\", message=\"None of the inputs have requires_grad=True. Gradients will be None\")\n",
    "warnings.filterwarnings(\"ignore\", message=\"Passing `gradient_checkpointing` to a config initialization is deprecated\")\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"huggingface_hub\")\n",
    "\n",
    "print(\"D7_OUT_ROOT:\", D7_OUT_ROOT)\n",
    "print(\"D2_OUT_ROOT:\", D2_OUT_ROOT)\n",
    "print(\"D2_MANIFEST_ALL:\", D2_MANIFEST_ALL)\n",
    "print(\"DEVICE:\", DEVICE)\n",
    "if DEVICE.type == \"cuda\":\n",
    "    print(\"GPU:\", torch.cuda.get_device_name(0))\n",
    "print(\"PER_DEVICE_BS:\", PER_DEVICE_BS, \"| EFFECTIVE_BS:\", PER_DEVICE_BS * GRAD_ACCUM)\n",
    "print(\"NUM_WORKERS:\", NUM_WORKERS, \"| PIN_MEMORY:\", PIN_MEMORY)\n",
    "print(\"USE_AMP:\", bool(USE_AMP and DEVICE.type == \"cuda\"))\n",
    "print(\"Enhanced exp required substring (case-insensitive):\", REQUIRED_EXP_SUBSTRING)\n",
    "\n",
    "# -------------------------\n",
    "# 4) Load D2 manifest and build the TEST table\n",
    "# -------------------------\n",
    "# Reads D2 manifest_all.csv, checks required columns, and keeps only split==\"test\".\n",
    "# Also confirms the manifest truly belongs to dataset \"D2\" (Guard A).\n",
    "if not os.path.exists(D2_MANIFEST_ALL):\n",
    "    raise FileNotFoundError(f\"Missing D2 manifest_all.csv: {D2_MANIFEST_ALL}\")\n",
    "\n",
    "m_all = pd.read_csv(D2_MANIFEST_ALL)\n",
    "\n",
    "# Required fields for evaluation and subgroup reporting.\n",
    "req_cols = {\"split\", \"clip_path\", \"label_num\", \"task\", \"sex\", \"age\"}\n",
    "missing = [c for c in sorted(req_cols) if c not in m_all.columns]\n",
    "if missing:\n",
    "    raise ValueError(f\"D2 manifest missing required columns: {missing}. Found: {list(m_all.columns)}\")\n",
    "\n",
    "# Infer dataset id from the manifest to avoid mixing roots by mistake.\n",
    "if \"dataset\" in m_all.columns and m_all[\"dataset\"].notna().any():\n",
    "    d2_dataset_id = str(m_all[\"dataset\"].astype(str).value_counts(dropna=True).idxmax())\n",
    "    m_all = m_all[m_all[\"dataset\"].astype(str) == d2_dataset_id].copy()\n",
    "else:\n",
    "    d2_dataset_id = \"DX\"\n",
    "\n",
    "# --------- GUARD A ----------\n",
    "# Stops immediately if the loaded manifest is not D2.\n",
    "if d2_dataset_id != \"D2\":\n",
    "    raise RuntimeError(\n",
    "        f\"Expected D2 dataset_id=='D2' but got {d2_dataset_id!r}. \"\n",
    "        \"This usually means D2_OUT_ROOT is wrong or the manifest is not D2. \"\n",
    "        f\"D2_OUT_ROOT={D2_OUT_ROOT}\"\n",
    "    )\n",
    "\n",
    "# Keep a minimal, consistent column set for inference + reporting.\n",
    "keep_cols = [\"clip_path\", \"label_num\", \"task\", \"speaker_id\", \"sex\", \"age\", \"duration_sec\", \"split\"]\n",
    "for c in keep_cols:\n",
    "    if c not in m_all.columns:\n",
    "        m_all[c] = np.nan\n",
    "m_all = m_all[keep_cols].copy()\n",
    "\n",
    "# IMPORTANT: TEST split only\n",
    "test_df = m_all[m_all[\"split\"].astype(str) == \"test\"].reset_index(drop=True)\n",
    "\n",
    "print(f\"\\nD2 dataset inferred: {d2_dataset_id}\")\n",
    "print(f\"D2 TEST rows: {len(test_df)}\")\n",
    "print(\"D2 TEST label counts:\", test_df[\"label_num\"].value_counts(dropna=False).to_dict())\n",
    "print(\"D2 TEST sex counts (raw):\", test_df[\"sex\"].value_counts(dropna=False).to_dict())\n",
    "\n",
    "if len(test_df) == 0:\n",
    "    raise RuntimeError(\"After filtering to split=='test', D2 manifest has 0 rows.\")\n",
    "\n",
    "# -------------------------\n",
    "# 5) Confirm all test audio files exist\n",
    "# -------------------------\n",
    "# Prevents running a long evaluation that will fail midway due to missing clips.\n",
    "def _fail_fast_missing_paths(df: pd.DataFrame, name: str):\n",
    "    missing_paths = []\n",
    "    for p in tqdm(df[\"clip_path\"].astype(str).tolist(), desc=f\"Check {name} clip_path exists\", dynamic_ncols=True):\n",
    "        if not os.path.exists(p):\n",
    "            missing_paths.append(p)\n",
    "            if len(missing_paths) >= 10:\n",
    "                break\n",
    "    if missing_paths:\n",
    "        raise FileNotFoundError(f\"{name}: missing clip_path(s). Examples: {missing_paths}\")\n",
    "\n",
    "_fail_fast_missing_paths(test_df, \"D2 TEST\")\n",
    "\n",
    "# -------------------------\n",
    "# 6) Add task grouping used by the two-head model\n",
    "# -------------------------\n",
    "# \"vowl\" clips go through the vowel head; everything else goes through the other head.\n",
    "def _task_group(task_val) -> str:\n",
    "    return \"vowel\" if str(task_val) == \"vowl\" else \"other\"\n",
    "\n",
    "test_df[\"task_group\"] = test_df[\"task\"].apply(_task_group)\n",
    "\n",
    "# -------------------------\n",
    "# 6.5) Normalize sex values for subgroup reporting\n",
    "# -------------------------\n",
    "# D2 uses exact strings \"male\"/\"female\"; everything else is treated as unknown.\n",
    "def normalize_sex_d2_case_sensitive(val) -> str:\n",
    "    if pd.isna(val):\n",
    "        return \"UNK\"\n",
    "    if val == \"male\":\n",
    "        return \"M\"\n",
    "    if val == \"female\":\n",
    "        return \"F\"\n",
    "    return \"UNK\"\n",
    "\n",
    "test_df[\"sex_norm\"] = test_df[\"sex\"].apply(normalize_sex_d2_case_sensitive)\n",
    "print(\"D2 TEST sex counts (normalized):\", test_df[\"sex_norm\"].value_counts(dropna=False).to_dict())\n",
    "if (test_df[\"sex_norm\"] == \"UNK\").any():\n",
    "    print(\"NOTE: Some D2 'sex' values were not exactly 'male'/'female' and were mapped to 'UNK'.\")\n",
    "\n",
    "# -------------------------\n",
    "# 7) Dataset and batching logic\n",
    "# -------------------------\n",
    "# Loads audio, checks sample rate, and builds attention masks:\n",
    "# - vowel: mask tries to ignore trailing silence\n",
    "# - other: mask keeps all samples\n",
    "class AudioManifestDataset(Dataset):\n",
    "    def __init__(self, df: pd.DataFrame):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        row = self.df.iloc[idx]\n",
    "        clip_path = str(row[\"clip_path\"])\n",
    "        label = int(row[\"label_num\"])\n",
    "        task_group = str(row[\"task_group\"])\n",
    "        sex_norm = str(row[\"sex_norm\"])\n",
    "        speaker_id = row[\"speaker_id\"] if \"speaker_id\" in row.index else np.nan\n",
    "\n",
    "        if not os.path.exists(clip_path):\n",
    "            raise FileNotFoundError(f\"Missing clip_path: {clip_path}\")\n",
    "\n",
    "        y, sr = sf.read(clip_path, always_2d=False)\n",
    "        if isinstance(y, np.ndarray) and y.ndim > 1:\n",
    "            y = np.mean(y, axis=1)\n",
    "        y = np.asarray(y, dtype=np.float32)\n",
    "\n",
    "        # Hard check to match the model’s expected input rate.\n",
    "        if int(sr) != SR_EXPECTED:\n",
    "            raise ValueError(f\"Sample rate mismatch (expected {SR_EXPECTED}): {clip_path} has sr={sr}\")\n",
    "\n",
    "        # Attention mask is used after feature extraction to ignore padded or silent regions.\n",
    "        attn = np.ones((len(y),), dtype=np.int64)\n",
    "        if task_group == \"vowel\":\n",
    "            k = -1\n",
    "            for j in range(len(y) - 1, -1, -1):\n",
    "                if abs(float(y[j])) > TINY_THRESH:\n",
    "                    k = j\n",
    "                    break\n",
    "            if k >= 0:\n",
    "                attn[:k+1] = 1\n",
    "                attn[k+1:] = 0\n",
    "            else:\n",
    "                attn[:] = 1\n",
    "        else:\n",
    "            attn[:] = 1\n",
    "\n",
    "        return {\n",
    "            \"input_values\": torch.from_numpy(y),\n",
    "            \"attention_mask\": torch.from_numpy(attn),\n",
    "            \"labels\": torch.tensor(label, dtype=torch.long),\n",
    "            \"task_group\": task_group,\n",
    "            \"sex_norm\": sex_norm,\n",
    "            \"clip_path\": clip_path,\n",
    "            \"speaker_id\": speaker_id,\n",
    "        }\n",
    "\n",
    "def collate_fn(batch):\n",
    "    # Pads variable-length waveforms to the longest clip in the batch.\n",
    "    max_len = int(max(b[\"input_values\"].numel() for b in batch))\n",
    "    input_vals, attn_masks, labels = [], [], []\n",
    "    task_groups, sex_norms, clip_paths, speaker_ids = [], [], [], []\n",
    "    for b in batch:\n",
    "        x = b[\"input_values\"]\n",
    "        a = b[\"attention_mask\"]\n",
    "        pad = max_len - x.numel()\n",
    "        if pad > 0:\n",
    "            x = torch.cat([x, torch.zeros(pad, dtype=x.dtype)], dim=0)\n",
    "            a = torch.cat([a, torch.zeros(pad, dtype=a.dtype)], dim=0)\n",
    "        input_vals.append(x)\n",
    "        attn_masks.append(a)\n",
    "        labels.append(b[\"labels\"])\n",
    "        task_groups.append(b[\"task_group\"])\n",
    "        sex_norms.append(b[\"sex_norm\"])\n",
    "        clip_paths.append(b[\"clip_path\"])\n",
    "        speaker_ids.append(b[\"speaker_id\"])\n",
    "    return {\n",
    "        \"input_values\": torch.stack(input_vals, dim=0),\n",
    "        \"attention_mask\": torch.stack(attn_masks, dim=0),\n",
    "        \"labels\": torch.stack(labels, dim=0),\n",
    "        \"task_group\": task_groups,\n",
    "        \"sex_norm\": sex_norms,\n",
    "        \"clip_path\": clip_paths,\n",
    "        \"speaker_id\": speaker_ids,\n",
    "    }\n",
    "\n",
    "# -------------------------\n",
    "# 8) Model definition (same structure as training)\n",
    "# -------------------------\n",
    "# Backbone is frozen; only the two heads are loaded from best_heads.pt.\n",
    "class Wav2Vec2TwoHeadClassifier(nn.Module):\n",
    "    def __init__(self, ckpt: str, dropout_p: float):\n",
    "        super().__init__()\n",
    "        self.backbone = Wav2Vec2Model.from_pretrained(ckpt, use_safetensors=True)\n",
    "        for p in self.backbone.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "        H = int(self.backbone.config.hidden_size)\n",
    "        self.pre_vowel = nn.Sequential(nn.LayerNorm(H), nn.Dropout(float(dropout_p)))\n",
    "        self.pre_other = nn.Sequential(nn.LayerNorm(H), nn.Dropout(float(dropout_p)))\n",
    "        self.head_vowel = nn.Linear(H, 2)\n",
    "        self.head_other = nn.Linear(H, 2)\n",
    "\n",
    "    def masked_mean_pool(self, last_hidden, attn_mask_samples):\n",
    "        # Pools time frames using the attention mask mapped into feature space.\n",
    "        feat_mask = self.backbone._get_feature_vector_attention_mask(last_hidden.shape[1], attn_mask_samples)\n",
    "        feat_mask = feat_mask.to(last_hidden.device).unsqueeze(-1).type_as(last_hidden)\n",
    "        masked = last_hidden * feat_mask\n",
    "        denom = feat_mask.sum(dim=1).clamp(min=1.0)\n",
    "        return masked.sum(dim=1) / denom\n",
    "\n",
    "    def _heads_fp32(self, x_any: torch.Tensor, head: nn.Module) -> torch.Tensor:\n",
    "        # Runs heads in float32 for numeric stability.\n",
    "        x = x_any.float()\n",
    "        if x.is_cuda:\n",
    "            with torch.amp.autocast(device_type=\"cuda\", enabled=False):\n",
    "                return head(x)\n",
    "        return head(x)\n",
    "\n",
    "    def forward_logits(self, input_values, attention_mask, task_group):\n",
    "        # Backbone forward is frozen and runs without gradients.\n",
    "        with torch.no_grad():\n",
    "            out = self.backbone(input_values=input_values, attention_mask=attention_mask)\n",
    "            last_hidden = out.last_hidden_state\n",
    "\n",
    "        pooled = self.masked_mean_pool(last_hidden, attention_mask)\n",
    "\n",
    "        z_v = self.pre_vowel(pooled.float())\n",
    "        z_o = self.pre_other(pooled.float())\n",
    "\n",
    "        logits_v = self._heads_fp32(z_v, self.head_vowel)\n",
    "        logits_o = self._heads_fp32(z_o, self.head_other)\n",
    "\n",
    "        # Routes each item to the correct head based on task_group.\n",
    "        is_vowel = torch.tensor([tg == \"vowel\" for tg in task_group], device=pooled.device, dtype=torch.bool)\n",
    "        logits = logits_o.clone()\n",
    "        if is_vowel.any():\n",
    "            logits[is_vowel] = logits_v[is_vowel]\n",
    "        return logits\n",
    "\n",
    "# -------------------------\n",
    "# 9) Metrics and plot writers\n",
    "# -------------------------\n",
    "# Computes AUROC, threshold metrics, ROC curves, and confusion matrix images.\n",
    "def compute_auc(y_true, y_prob):\n",
    "    y_true = np.asarray(y_true, dtype=np.int64)\n",
    "    y_prob = np.asarray(y_prob, dtype=np.float64)\n",
    "    if len(np.unique(y_true)) < 2:\n",
    "        return float(\"nan\")\n",
    "    return float(roc_auc_score(y_true, y_prob))\n",
    "\n",
    "def compute_threshold_metrics(y_true, y_prob, thr=0.5):\n",
    "    # Standard classification metrics at a fixed probability threshold.\n",
    "    y_true = np.asarray(y_true, dtype=np.int64)\n",
    "    y_prob = np.asarray(y_prob, dtype=np.float64)\n",
    "    y_pred = (y_prob >= float(thr)).astype(np.int64)\n",
    "\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=[0, 1])\n",
    "    TN, FP, FN, TP = int(cm[0, 0]), int(cm[0, 1]), int(cm[1, 0]), int(cm[1, 1])\n",
    "\n",
    "    eps = 1e-12\n",
    "    acc = (TP + TN) / max(1, (TP + TN + FP + FN))\n",
    "    prec = TP / (TP + FP + eps)\n",
    "    rec = TP / (TP + FN + eps)     # sensitivity\n",
    "    f1 = 2 * prec * rec / (prec + rec + eps)\n",
    "    spec = TN / (TN + FP + eps)\n",
    "\n",
    "    try:\n",
    "        mcc = float(matthews_corrcoef(y_true, y_pred)) if len(np.unique(y_pred)) > 1 else float(\"nan\")\n",
    "    except Exception:\n",
    "        mcc = float(\"nan\")\n",
    "\n",
    "    pval = float(\"nan\")\n",
    "    try:\n",
    "        from scipy.stats import fisher_exact  # type: ignore\n",
    "        _, pval = fisher_exact([[TN, FP], [FN, TP]], alternative=\"two-sided\")\n",
    "        pval = float(pval)\n",
    "    except Exception:\n",
    "        pval = float(\"nan\")\n",
    "\n",
    "    return {\n",
    "        \"threshold\": float(thr),\n",
    "        \"tn\": TN, \"fp\": FP, \"fn\": FN, \"tp\": TP,\n",
    "        \"accuracy\": float(acc),\n",
    "        \"precision\": float(prec),\n",
    "        \"recall\": float(rec),\n",
    "        \"f1\": float(f1),\n",
    "        \"sensitivity\": float(rec),\n",
    "        \"specificity\": float(spec),\n",
    "        \"mcc\": float(mcc),\n",
    "        \"p_value_fisher_two_sided\": float(pval),\n",
    "    }\n",
    "\n",
    "def save_roc_curve_png(y_true, y_prob, out_png, title_suffix=\"Test\"):\n",
    "    # Saves an ROC curve image for quick visual checks.\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_prob)\n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr)\n",
    "    plt.plot([0, 1], [0, 1], linestyle=\"--\")\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(f\"ROC Curve ({title_suffix})\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_png, dpi=150)\n",
    "    plt.close()\n",
    "\n",
    "def save_confusion_png(y_true, y_prob, out_png, thr=0.5, title_suffix=\"Test\"):\n",
    "    # Saves a confusion matrix image at the chosen threshold.\n",
    "    y_true = np.asarray(y_true, dtype=np.int64)\n",
    "    y_prob = np.asarray(y_prob, dtype=np.float64)\n",
    "    y_pred = (y_prob >= float(thr)).astype(int)\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=[0, 1])\n",
    "\n",
    "    plt.figure()\n",
    "    plt.imshow(cm)\n",
    "    plt.xticks([0, 1], [\"Healthy(0)\", \"PD(1)\"])\n",
    "    plt.yticks([0, 1], [\"Healthy(0)\", \"PD(1)\"])\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"True\")\n",
    "    plt.title(f\"Confusion Matrix ({title_suffix}, thr={thr:.4f})\")\n",
    "    for (i, j), v in np.ndenumerate(cm):\n",
    "        plt.text(j, i, str(v), ha=\"center\", va=\"center\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_png, dpi=150)\n",
    "    plt.close()\n",
    "\n",
    "# -------------------------\n",
    "# 9.5) Fairness metric (sex-based ΔFNR)\n",
    "# -------------------------\n",
    "# Computes FNR per sex and ΔFNR = FNR(F) - FNR(M) using PD-only true labels.\n",
    "def compute_fnr_by_group_signed(y_true, y_prob, groups, thr=0.5):\n",
    "    y_true = np.asarray(y_true, dtype=np.int64)\n",
    "    y_prob = np.asarray(y_prob, dtype=np.float64)\n",
    "    groups = np.asarray(list(groups), dtype=object)\n",
    "    y_pred = (y_prob >= float(thr)).astype(np.int64)\n",
    "\n",
    "    out = {}\n",
    "    for g in sorted(set(groups.tolist())):\n",
    "        mask_g = (groups == g)\n",
    "        if int(mask_g.sum()) == 0:\n",
    "            continue\n",
    "\n",
    "        pos_mask = mask_g & (y_true == 1)\n",
    "        n_pos = int(pos_mask.sum())\n",
    "        if n_pos == 0:\n",
    "            out[g] = {\"n_total\": int(mask_g.sum()), \"n_pos\": 0, \"tp\": 0, \"fn\": 0, \"fnr\": float(\"nan\")}\n",
    "            continue\n",
    "\n",
    "        tp = int(((y_pred == 1) & pos_mask).sum())\n",
    "        fn = int(((y_pred == 0) & pos_mask).sum())\n",
    "        fnr = float(fn / max(1, (fn + tp)))\n",
    "        out[g] = {\"n_total\": int(mask_g.sum()), \"n_pos\": int(n_pos), \"tp\": int(tp), \"fn\": int(fn), \"fnr\": float(fnr)}\n",
    "\n",
    "    fnr_m = out.get(\"M\", {}).get(\"fnr\", float(\"nan\"))\n",
    "    fnr_f = out.get(\"F\", {}).get(\"fnr\", float(\"nan\"))\n",
    "    if (not np.isnan(fnr_m)) and (not np.isnan(fnr_f)):\n",
    "        delta_signed = float(fnr_f - fnr_m)\n",
    "        delta_abs = float(abs(delta_signed))\n",
    "    else:\n",
    "        delta_signed = float(\"nan\")\n",
    "        delta_abs = float(\"nan\")\n",
    "\n",
    "    return out, delta_signed, delta_abs\n",
    "\n",
    "def compute_confusion_counts(y_true, y_prob, thr=0.5):\n",
    "    # Convenience wrapper for confusion counts at a threshold.\n",
    "    y_true = np.asarray(y_true, dtype=np.int64)\n",
    "    y_prob = np.asarray(y_prob, dtype=np.float64)\n",
    "    y_pred = (y_prob >= float(thr)).astype(np.int64)\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=[0, 1])\n",
    "    return {\"TN\": int(cm[0, 0]), \"FP\": int(cm[0, 1]), \"FN\": int(cm[1, 0]), \"TP\": int(cm[1, 1])}\n",
    "\n",
    "def compute_confusion_by_group(y_true, y_prob, groups, thr=0.5):\n",
    "    # Builds confusion counts per group label (M/F/UNK).\n",
    "    y_true = np.asarray(y_true, dtype=np.int64)\n",
    "    y_prob = np.asarray(y_prob, dtype=np.float64)\n",
    "    groups = np.asarray(list(groups), dtype=object)\n",
    "    out = {}\n",
    "    for g in sorted(set(groups.tolist())):\n",
    "        mask = (groups == g)\n",
    "        if int(mask.sum()) == 0:\n",
    "            continue\n",
    "        out[g] = {\"n\": int(mask.sum()), \"confusion\": compute_confusion_counts(y_true[mask], y_prob[mask], thr=thr)}\n",
    "    return out\n",
    "\n",
    "# -------------------------\n",
    "# 10) Reproducibility seed setter\n",
    "# -------------------------\n",
    "# Keeps per-seed evaluation deterministic.\n",
    "def set_all_seeds(seed: int):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# -------------------------\n",
    "# 11) Find the latest matching D7 trainval experiment\n",
    "# -------------------------\n",
    "# Searches exp_* folders (newest first) and picks the first that:\n",
    "# - includes REQUIRED_EXP_SUBSTRING in the folder name\n",
    "# - contains all three best_heads.pt files and summary_trainval.json\n",
    "TRAINVAL_ROOT = Path(D7_OUT_ROOT) / \"trainval_runs\"\n",
    "if not TRAINVAL_ROOT.exists():\n",
    "    raise FileNotFoundError(f\"Missing trainval_runs folder under D7_OUT_ROOT: {str(TRAINVAL_ROOT)}\")\n",
    "\n",
    "exp_dirs = sorted([p for p in TRAINVAL_ROOT.glob(\"exp_*\") if p.is_dir()], key=lambda p: p.stat().st_mtime, reverse=True)\n",
    "if not exp_dirs:\n",
    "    raise FileNotFoundError(f\"No exp_* folders found under: {str(TRAINVAL_ROOT)}\")\n",
    "\n",
    "train_dataset_id = \"D7\"  # expected naming from trainval code (run_D7_seedXXXX)\n",
    "\n",
    "def _is_enhanced_exp_dir(exp_path: Path, required_substring: str) -> bool:\n",
    "    return (required_substring.lower() in exp_path.name.lower())\n",
    "\n",
    "def _has_all_seeds_and_summary(exp_path: Path, dataset_id: str, seeds: list) -> bool:\n",
    "    summary_path = exp_path / \"summary_trainval.json\"\n",
    "    if not summary_path.exists():\n",
    "        return False\n",
    "    for s in seeds:\n",
    "        p = exp_path / f\"run_{dataset_id}_seed{s}\" / \"best_heads.pt\"\n",
    "        if not p.exists():\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "chosen_exp = None\n",
    "for ed in exp_dirs:\n",
    "    if not _is_enhanced_exp_dir(ed, REQUIRED_EXP_SUBSTRING):\n",
    "        continue\n",
    "    if _has_all_seeds_and_summary(ed, train_dataset_id, SEEDS):\n",
    "        chosen_exp = ed\n",
    "        break\n",
    "\n",
    "if chosen_exp is None:\n",
    "    sample = exp_dirs[0]\n",
    "    raise FileNotFoundError(\n",
    "        \"Could not find a recent D7 *ENHANCED* trainval experiment folder that:\\n\"\n",
    "        f\"  (1) contains substring '{REQUIRED_EXP_SUBSTRING}' (case-insensitive) in the exp folder name, and\\n\"\n",
    "        \"  (2) contains all 3 best_heads.pt files + summary_trainval.json.\\n\\n\"\n",
    "        f\"Most recent exp checked (for reference): {str(sample)}\"\n",
    "    )\n",
    "\n",
    "# Full experiment folder name is used as the run tag.\n",
    "FULL_TRAINVAL_EXP_TAG = chosen_exp.name\n",
    "TAG_SAFE = _sanitize_tag(FULL_TRAINVAL_EXP_TAG)\n",
    "RUN_PARENT_DIRNAME = f\"run_{TAG_SAFE}__{RUN_STAMP}\"\n",
    "\n",
    "# Output layout:\n",
    "# - RUN_ROOT is the unique folder for this run (tag + timestamp)\n",
    "# - TAG_ROOT is the stable folder used to store a tag-specific pointer file\n",
    "TEST_ROOT = Path(D7_OUT_ROOT) / \"monolingual_test_runs\"\n",
    "RUN_ROOT  = TEST_ROOT / RUN_PARENT_DIRNAME\n",
    "RUN_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "TAG_ROOT = TEST_ROOT / f\"run_{TAG_SAFE}\"\n",
    "TAG_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Additional config/log locations that align with the builder naming style.\n",
    "cfg_dir  = Path(D7_OUT_ROOT) / \"config\" / f\"D7_{TAG_SAFE}_on_D2_Test\"\n",
    "logs_dir = Path(D7_OUT_ROOT) / \"logs\"   / f\"D7_{TAG_SAFE}_on_D2_Test\"\n",
    "cfg_dir.mkdir(parents=True, exist_ok=True)\n",
    "logs_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "RUN_CONFIG_PATH       = cfg_dir / \"run_config.json\"\n",
    "WARNINGS_CSV_PATH     = logs_dir / \"preprocess_warnings.csv\"\n",
    "DATASET_SUMMARY_PATH  = logs_dir / \"dataset_summary.json\"\n",
    "\n",
    "print(\"\\nUsing D7 ENHANCED Train+Val experiment folder:\")\n",
    "print(\" \", str(chosen_exp))\n",
    "print(\"FULL_TRAINVAL_EXP_TAG:\", FULL_TRAINVAL_EXP_TAG)\n",
    "print(\"RUN_ROOT:\", str(RUN_ROOT))\n",
    "print(\"cfg_dir:\", str(cfg_dir))\n",
    "print(\"logs_dir:\", str(logs_dir))\n",
    "\n",
    "# --------- GUARD B ----------\n",
    "# Confirms expected head files exist after experiment selection.\n",
    "for s in SEEDS:\n",
    "    p = chosen_exp / f\"run_{train_dataset_id}_seed{s}\" / \"best_heads.pt\"\n",
    "    if not p.exists():\n",
    "        raise RuntimeError(f\"Trainval artifact missing after choosing exp. Missing: {str(p)}\")\n",
    "\n",
    "# Read trainval summary to fetch the global (mean) threshold.\n",
    "summary_trainval_path = chosen_exp / \"summary_trainval.json\"\n",
    "with open(summary_trainval_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    d7_trainval_summary = json.load(f)\n",
    "\n",
    "# -------------------------\n",
    "# 11.5) Select one global threshold for all test seeds\n",
    "# -------------------------\n",
    "# Uses mean val-opt threshold from trainval summary; falls back to 0.5 if missing.\n",
    "val_opt_obj = (d7_trainval_summary or {}).get(\"val_optimal_threshold\", {}) or {}\n",
    "thr_mean_sd = (val_opt_obj.get(\"mean_sd\", {}) or {})\n",
    "\n",
    "def _get_mean_val_opt_threshold() -> float:\n",
    "    try:\n",
    "        return float(thr_mean_sd.get(\"mean\", float(\"nan\")))\n",
    "    except Exception:\n",
    "        return float(\"nan\")\n",
    "\n",
    "THR_MEAN_FROM_TRAINVAL = _get_mean_val_opt_threshold()\n",
    "\n",
    "if np.isnan(THR_MEAN_FROM_TRAINVAL):\n",
    "    THR_USED_GLOBAL = 0.5\n",
    "    THR_GLOBAL_NOTE = (\n",
    "        \"Mean val-opt threshold was missing/NaN in D7 enhanced summary_trainval.json. \"\n",
    "        \"Fallback: THR_USED_GLOBAL=0.5 for ALL seeds.\"\n",
    "    )\n",
    "else:\n",
    "    THR_USED_GLOBAL = float(THR_MEAN_FROM_TRAINVAL)\n",
    "    THR_GLOBAL_NOTE = None\n",
    "\n",
    "print(\"\\nVAL-opt threshold selection for TEST (GLOBAL):\")\n",
    "print(\"  Source: summary_trainval.json -> val_optimal_threshold.mean_sd.mean\")\n",
    "print(f\"  THR_USED_GLOBAL: {THR_USED_GLOBAL:.6f}\")\n",
    "if THR_GLOBAL_NOTE is not None:\n",
    "    print(\"  NOTE:\", THR_GLOBAL_NOTE)\n",
    "\n",
    "# -------------------------\n",
    "# 13) Build the D2 test DataLoader\n",
    "# -------------------------\n",
    "# Feeds audio batches to the model for inference.\n",
    "test_ds = AudioManifestDataset(test_df)\n",
    "test_loader = DataLoader(\n",
    "    test_ds,\n",
    "    batch_size=PER_DEVICE_BS,\n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=PIN_MEMORY,\n",
    "    collate_fn=collate_fn,\n",
    ")\n",
    "\n",
    "# -------------------------\n",
    "# 14) Warm-up loader to catch issues early\n",
    "# -------------------------\n",
    "# Loads a few batches to surface shape or file problems before full inference.\n",
    "print(\"\\nWarm-up: loading up to 3 D2 TEST batches...\")\n",
    "t0 = time.time()\n",
    "\n",
    "def _warmup(loader, name):\n",
    "    nb = len(loader)\n",
    "    wb = min(3, nb)\n",
    "    if wb == 0:\n",
    "        raise RuntimeError(f\"{name} DataLoader has 0 batches. Check df length and PER_DEVICE_BS.\")\n",
    "    it = iter(loader)\n",
    "    for i in range(wb):\n",
    "        _ = next(it)\n",
    "        print(f\"  loaded warmup {name} batch {i+1}/{wb}\")\n",
    "\n",
    "_warmup(test_loader, \"D2 TEST\")\n",
    "print(f\"Warm-up done in {time.time()-t0:.2f}s\")\n",
    "\n",
    "# -------------------------\n",
    "# 15) Load trained heads into a fresh model instance\n",
    "# -------------------------\n",
    "# Loads only the head parameters saved by trainval (backbone stays frozen).\n",
    "def load_heads_into_model(model: Wav2Vec2TwoHeadClassifier, best_heads_path: Path):\n",
    "    if not best_heads_path.exists():\n",
    "        raise FileNotFoundError(f\"Missing best_heads.pt: {str(best_heads_path)}\")\n",
    "    state = torch.load(str(best_heads_path), map_location=\"cpu\")\n",
    "    model.pre_vowel.load_state_dict(state[\"pre_vowel\"], strict=True)\n",
    "    model.pre_other.load_state_dict(state[\"pre_other\"], strict=True)\n",
    "    model.head_vowel.load_state_dict(state[\"head_vowel\"], strict=True)\n",
    "    model.head_other.load_state_dict(state[\"head_other\"], strict=True)\n",
    "    return model\n",
    "\n",
    "# -------------------------\n",
    "# 16) Inference helper that also returns metadata\n",
    "# -------------------------\n",
    "# Returns:\n",
    "# - y_true and y_score for metrics\n",
    "# - sex_norm, task_group, clip_path, speaker_id for predictions.csv and subgroup reports\n",
    "def _infer_probs_with_meta(loader, model, desc):\n",
    "    use_amp = bool(USE_AMP and DEVICE.type == \"cuda\")\n",
    "    all_probs, all_true, all_sex = [], [], []\n",
    "    all_clip, all_spk, all_task = [], [], []\n",
    "\n",
    "    pbar = tqdm(loader, desc=desc, dynamic_ncols=True)\n",
    "    with torch.inference_mode():\n",
    "        for batch in pbar:\n",
    "            input_values = batch[\"input_values\"].to(DEVICE, non_blocking=False)\n",
    "            attention_mask = batch[\"attention_mask\"].to(DEVICE, non_blocking=False)\n",
    "            labels = batch[\"labels\"].to(DEVICE, non_blocking=False)\n",
    "            task_group = batch[\"task_group\"]\n",
    "            sex_norm = batch[\"sex_norm\"]\n",
    "            clip_paths = batch[\"clip_path\"]\n",
    "            speaker_ids = batch[\"speaker_id\"]\n",
    "\n",
    "            with torch.amp.autocast(device_type=\"cuda\", enabled=use_amp):\n",
    "                logits = model.forward_logits(input_values, attention_mask, task_group)\n",
    "\n",
    "            probs = torch.softmax(logits.float(), dim=-1)[:, 1].detach().cpu().numpy().astype(np.float64)\n",
    "            all_probs.extend(probs.tolist())\n",
    "            all_true.extend(labels.detach().cpu().numpy().astype(np.int64).tolist())\n",
    "            all_sex.extend(list(sex_norm))\n",
    "            all_task.extend(list(task_group))\n",
    "            all_clip.extend(list(clip_paths))\n",
    "            all_spk.extend([(\"\" if (x is None or (isinstance(x, float) and np.isnan(x))) else str(x)) for x in speaker_ids])\n",
    "\n",
    "    return (\n",
    "        np.asarray(all_true, dtype=np.int64),\n",
    "        np.asarray(all_probs, dtype=np.float64),\n",
    "        np.asarray(all_sex, dtype=object),\n",
    "        np.asarray(all_clip, dtype=object),\n",
    "        np.asarray(all_spk, dtype=object),\n",
    "        np.asarray(all_task, dtype=object),\n",
    "    )\n",
    "\n",
    "# -------------------------\n",
    "# 17) Run evaluation for one seed\n",
    "# -------------------------\n",
    "# Produces:\n",
    "# - predictions.csv for that seed\n",
    "# - metrics.json for that seed\n",
    "# - ROC and confusion plots (overall + by sex when available)\n",
    "def run_test_once(seed: int):\n",
    "    set_all_seeds(seed)\n",
    "\n",
    "    run_dir = RUN_ROOT / f\"run_{train_dataset_id}_on_{d2_dataset_id}test_seed{seed}\"\n",
    "    run_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    best_heads_path = chosen_exp / f\"run_{train_dataset_id}_seed{seed}\" / \"best_heads.pt\"\n",
    "\n",
    "    print(f\"\\n[seed={seed}] Loading model + heads from:\")\n",
    "    print(\" \", str(best_heads_path))\n",
    "\n",
    "    model = Wav2Vec2TwoHeadClassifier(BACKBONE_CKPT, dropout_p=DROPOUT_P).to(DEVICE)\n",
    "    model = load_heads_into_model(model, best_heads_path)\n",
    "    model.eval()\n",
    "\n",
    "    # Single global threshold applied to all seeds.\n",
    "    thr_used = float(THR_USED_GLOBAL)\n",
    "    thr_note = THR_GLOBAL_NOTE\n",
    "\n",
    "    # Inference (includes metadata for predictions.csv).\n",
    "    yt_true, yt_prob, yt_sex, yt_clip, yt_spk, yt_task = _infer_probs_with_meta(\n",
    "        test_loader, model, desc=f\"[seed={seed}] Test (D2 TEST)\"\n",
    "    )\n",
    "    test_auc = compute_auc(yt_true, yt_prob)\n",
    "\n",
    "    # Metrics and subgroup summaries at the global threshold.\n",
    "    thr_metrics_test = compute_threshold_metrics(yt_true, yt_prob, thr=thr_used)\n",
    "    fnr_by_sex, delta_f_minus_m, delta_abs = compute_fnr_by_group_signed(yt_true, yt_prob, yt_sex, thr=thr_used)\n",
    "    confusion_by_sex = compute_confusion_by_group(yt_true, yt_prob, yt_sex, thr=thr_used)\n",
    "\n",
    "    # Plots (overall).\n",
    "    roc_png = run_dir / \"roc_curve.png\"\n",
    "    cm_png  = run_dir / \"confusion_matrix.png\"\n",
    "    save_roc_curve_png(yt_true, yt_prob, str(roc_png), title_suffix=f\"D2 TEST (seed={seed})\")\n",
    "    save_confusion_png(yt_true, yt_prob, str(cm_png), thr=thr_used, title_suffix=f\"D2 TEST (seed={seed})\")\n",
    "\n",
    "    # Plots (by sex: M and F only).\n",
    "    cm_m_png = None\n",
    "    cm_f_png = None\n",
    "    mask_m = (yt_sex == \"M\")\n",
    "    mask_f = (yt_sex == \"F\")\n",
    "\n",
    "    if int(mask_m.sum()) > 0:\n",
    "        cm_m_png = run_dir / \"confusion_matrix_M.png\"\n",
    "        save_confusion_png(yt_true[mask_m], yt_prob[mask_m], str(cm_m_png), thr=thr_used, title_suffix=f\"D2 TEST SEX=M (seed={seed})\")\n",
    "\n",
    "    if int(mask_f.sum()) > 0:\n",
    "        cm_f_png = run_dir / \"confusion_matrix_F.png\"\n",
    "        save_confusion_png(yt_true[mask_f], yt_prob[mask_f], str(cm_f_png), thr=thr_used, title_suffix=f\"D2 TEST SEX=F (seed={seed})\")\n",
    "\n",
    "    # predictions.csv: one row per clip, includes model score and key metadata.\n",
    "    pred_df = pd.DataFrame({\n",
    "        \"clip_path\": yt_clip.astype(str),\n",
    "        \"y_true\": yt_true.astype(int),\n",
    "        \"y_score\": yt_prob.astype(float),\n",
    "        \"sex_norm\": yt_sex.astype(str),\n",
    "        \"speaker_id\": yt_spk.astype(str),\n",
    "        \"task_group\": yt_task.astype(str),\n",
    "        \"seed\": int(seed),\n",
    "        \"trainval_exp_tag\": str(FULL_TRAINVAL_EXP_TAG),\n",
    "        \"run_stamp\": str(RUN_STAMP),\n",
    "        \"threshold_used_global\": float(thr_used),\n",
    "    })\n",
    "    pred_csv_path = run_dir / \"predictions.csv\"\n",
    "    pred_df.to_csv(pred_csv_path, index=False)\n",
    "\n",
    "    # metrics.json: full record of settings, metrics, and artifact paths for this seed.\n",
    "    metrics = {\n",
    "        \"train_dataset\": train_dataset_id,\n",
    "        \"test_dataset\": d2_dataset_id,\n",
    "        \"seed\": int(seed),\n",
    "\n",
    "        \"n_test\": int(len(test_df)),\n",
    "        \"label_counts_test\": test_df[\"label_num\"].value_counts(dropna=False).to_dict(),\n",
    "        \"sex_counts_test_norm\": test_df[\"sex_norm\"].value_counts(dropna=False).to_dict(),\n",
    "\n",
    "        \"test_auroc\": float(test_auc),\n",
    "\n",
    "        \"threshold_source\": \"D7 enhanced trainval summary_trainval.json -> val_optimal_threshold.mean_sd.mean\",\n",
    "        \"trainval_experiment_used\": str(chosen_exp),\n",
    "        \"trainval_exp_tag\": str(FULL_TRAINVAL_EXP_TAG),\n",
    "        \"trainval_summary_path\": str(summary_trainval_path),\n",
    "\n",
    "        \"test_threshold_used_global\": float(thr_used),\n",
    "        \"test_threshold_note_global\": thr_note,\n",
    "\n",
    "        \"threshold_metrics_test_at_thr_used\": thr_metrics_test,\n",
    "\n",
    "        \"fairness_test_at_thr_used\": {\n",
    "            \"definition\": \"H3: ΔFNR = FNR(F) - FNR(M), where FNR(sex)=FN/(FN+TP) computed on PD-only true labels at test_threshold_used_global.\",\n",
    "            \"threshold_used\": float(thr_used),\n",
    "            \"fnr_by_sex_norm\": fnr_by_sex,\n",
    "            \"delta_fnr_F_minus_M\": float(delta_f_minus_m),\n",
    "            \"delta_fnr_abs\": float(delta_abs),\n",
    "            \"note\": \"If n_PD for a sex is 0, its FNR is NaN and ΔFNR is NaN.\",\n",
    "            \"sex_normalization_note\": \"D2 mapping: exact 'male'->M and 'female'->F (case-sensitive); otherwise UNK.\",\n",
    "        },\n",
    "\n",
    "        \"confusion_by_sex_norm_at_thr_used\": confusion_by_sex,\n",
    "\n",
    "        \"artifacts\": {\n",
    "            \"predictions_csv\": str(pred_csv_path),\n",
    "            \"roc_curve_png\": str(roc_png),\n",
    "            \"confusion_matrix_png\": str(cm_png),\n",
    "            \"confusion_matrix_M_png\": str(cm_m_png) if cm_m_png is not None else None,\n",
    "            \"confusion_matrix_F_png\": str(cm_f_png) if cm_f_png is not None else None,\n",
    "        },\n",
    "\n",
    "        \"d7_out_root\": D7_OUT_ROOT,\n",
    "        \"d2_out_root\": D2_OUT_ROOT,\n",
    "        \"d2_manifest_all\": D2_MANIFEST_ALL,\n",
    "\n",
    "        \"best_heads_path\": str(best_heads_path),\n",
    "        \"backbone_ckpt\": BACKBONE_CKPT,\n",
    "        \"dropout_p\": float(DROPOUT_P),\n",
    "        \"timestamp\": time.strftime(\"%Y%m%d_%H%M%S\"),\n",
    "    }\n",
    "\n",
    "    metrics_path = run_dir / \"metrics.json\"\n",
    "    with open(metrics_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(metrics, f, indent=2)\n",
    "\n",
    "    print(f\"[seed={seed}] DONE | test_AUROC={test_auc:.6f}\")\n",
    "    print(f\"[seed={seed}] Threshold used (GLOBAL mean from D7 enhanced trainval): {thr_used:.6f}\")\n",
    "    print(f\"[seed={seed}] WROTE:\")\n",
    "    print(\" \", str(metrics_path))\n",
    "    print(\" \", str(pred_csv_path))\n",
    "    print(\" \", str(roc_png))\n",
    "    print(\" \", str(cm_png))\n",
    "\n",
    "    return {\n",
    "        \"seed\": int(seed),\n",
    "        \"thr_used\": float(thr_used),\n",
    "        \"thr_note\": thr_note,\n",
    "        \"test_auc\": float(test_auc),\n",
    "        \"thr_metrics_test\": thr_metrics_test,\n",
    "        \"fnr_by_sex\": fnr_by_sex,\n",
    "        \"delta_signed\": float(delta_f_minus_m),\n",
    "        \"delta_abs\": float(delta_abs),\n",
    "        \"run_dir\": str(run_dir),\n",
    "        \"predictions_csv\": str(pred_csv_path),\n",
    "    }\n",
    "\n",
    "# -------------------------\n",
    "# 18) Run all seeds and aggregate results\n",
    "# -------------------------\n",
    "# Computes:\n",
    "# - AUROC mean and 95% CI (t, n=3)\n",
    "# - threshold metrics mean ± SD across seeds\n",
    "# - fairness mean ± SD across seeds\n",
    "results = []\n",
    "for seed in SEEDS:\n",
    "    results.append(run_test_once(seed))\n",
    "\n",
    "aurocs = [r[\"test_auc\"] for r in results]\n",
    "t_crit = 4.302652729911275  # df=2, 95% CI\n",
    "n = len(aurocs)\n",
    "mean_auc = float(np.mean(aurocs))\n",
    "std_auc = float(np.std(aurocs, ddof=1)) if n > 1 else 0.0\n",
    "half_width = float(t_crit * (std_auc / math.sqrt(n))) if n > 1 else 0.0\n",
    "ci95 = [float(mean_auc - half_width), float(mean_auc + half_width)]\n",
    "\n",
    "def _mean_sd(vals):\n",
    "    # NaN-aware mean and standard deviation.\n",
    "    vals = np.asarray(vals, dtype=np.float64)\n",
    "    mu = float(np.nanmean(vals)) if np.any(~np.isnan(vals)) else float(\"nan\")\n",
    "    sd = float(np.nanstd(vals, ddof=1)) if np.sum(~np.isnan(vals)) > 1 else 0.0\n",
    "    return mu, sd\n",
    "\n",
    "# Aggregate standard threshold metrics across seeds.\n",
    "thr_list = [r[\"thr_metrics_test\"] for r in results]\n",
    "keys = [\"accuracy\",\"precision\",\"recall\",\"f1\",\"sensitivity\",\"specificity\",\"mcc\",\"p_value_fisher_two_sided\"]\n",
    "agg = {}\n",
    "for k in keys:\n",
    "    v = [float(tm.get(k, float(\"nan\"))) for tm in thr_list]\n",
    "    mu, sd = _mean_sd(v)\n",
    "    agg[k] = {\n",
    "        \"mean\": float(mu),\n",
    "        \"sd\": float(sd),\n",
    "        \"values_by_seed\": {str(s): float(tm.get(k, float(\"nan\"))) for s, tm in zip(SEEDS, thr_list)},\n",
    "    }\n",
    "\n",
    "# Confusion counts per seed (at the global threshold).\n",
    "cm_by_seed = {\n",
    "    str(s): {\"tn\": int(thr_list[i][\"tn\"]), \"fp\": int(thr_list[i][\"fp\"]), \"fn\": int(thr_list[i][\"fn\"]), \"tp\": int(thr_list[i][\"tp\"])}\n",
    "    for i, s in enumerate(SEEDS)\n",
    "}\n",
    "\n",
    "# Fairness summaries per seed (ΔFNR and FNR per sex).\n",
    "fnr_by_seed = {str(r[\"seed\"]): r[\"fnr_by_sex\"] for r in results}\n",
    "delta_signed_by_seed = {str(r[\"seed\"]): float(r[\"delta_signed\"]) for r in results}\n",
    "delta_abs_by_seed = {str(r[\"seed\"]): float(r[\"delta_abs\"]) for r in results}\n",
    "\n",
    "fnr_m_vals, fnr_f_vals, n_pd_m_vals, n_pd_f_vals = [], [], [], []\n",
    "d_signed_vals, d_abs_vals = [], []\n",
    "for r in results:\n",
    "    d = r[\"fnr_by_sex\"] or {}\n",
    "    fnr_m_vals.append(float(d.get(\"M\", {}).get(\"fnr\", float(\"nan\"))))\n",
    "    fnr_f_vals.append(float(d.get(\"F\", {}).get(\"fnr\", float(\"nan\"))))\n",
    "    n_pd_m_vals.append(float(d.get(\"M\", {}).get(\"n_pos\", float(\"nan\"))))\n",
    "    n_pd_f_vals.append(float(d.get(\"F\", {}).get(\"n_pos\", float(\"nan\"))))\n",
    "    d_signed_vals.append(float(r[\"delta_signed\"]))\n",
    "    d_abs_vals.append(float(r[\"delta_abs\"]))\n",
    "\n",
    "fnr_m_mean, fnr_m_sd = _mean_sd(fnr_m_vals)\n",
    "fnr_f_mean, fnr_f_sd = _mean_sd(fnr_f_vals)\n",
    "d_signed_mean, d_signed_sd = _mean_sd(d_signed_vals)\n",
    "d_abs_mean, d_abs_sd = _mean_sd(d_abs_vals)\n",
    "\n",
    "print(\"\\nTest AUROC by seed:\")\n",
    "for r in results:\n",
    "    print(f\"  seed {r['seed']}: {r['test_auc']:.6f}\")\n",
    "print(f\"\\nMean Test AUROC: {mean_auc:.6f}\")\n",
    "print(f\"95% CI (t, n=3): [{ci95[0]:.6f}, {ci95[1]:.6f}]\")\n",
    "\n",
    "print(\"\\nTEST threshold used (GLOBAL mean val-opt from D7 enhanced trainval):\")\n",
    "print(f\"  THR_USED_GLOBAL: {THR_USED_GLOBAL:.6f}\")\n",
    "if THR_GLOBAL_NOTE is not None:\n",
    "    print(\"  NOTE:\", THR_GLOBAL_NOTE)\n",
    "\n",
    "print(\"\\nThreshold metrics on D2 TEST @ THR_USED_GLOBAL (mean ± SD across seeds):\")\n",
    "for k in [\"accuracy\",\"precision\",\"sensitivity\",\"specificity\",\"f1\",\"mcc\"]:\n",
    "    print(f\"  {k}: {agg[k]['mean']:.6f} ± {agg[k]['sd']:.6f}\")\n",
    "print(\"  fisher_p_value_two_sided:\", f\"{agg['p_value_fisher_two_sided']['mean']:.6g} ± {agg['p_value_fisher_two_sided']['sd']:.6g}\")\n",
    "\n",
    "print(\"\\nFAIRNESS (H3) on D2 TEST @ THR_USED_GLOBAL across seeds (mean ± SD):\")\n",
    "print(f\"  FNR_M: {fnr_m_mean:.6f} ± {fnr_m_sd:.6f}\")\n",
    "print(f\"  FNR_F: {fnr_f_mean:.6f} ± {fnr_f_sd:.6f}\")\n",
    "print(f\"  ΔFNR (F-M): {d_signed_mean:.6f} ± {d_signed_sd:.6f}\")\n",
    "print(f\"  |ΔFNR|: {d_abs_mean:.6f} ± {d_abs_sd:.6f}\")\n",
    "\n",
    "# -------------------------\n",
    "# 18.1) Build and write run-level summary + pointer files\n",
    "# -------------------------\n",
    "# Writes:\n",
    "# - summary_test.json inside the run folder\n",
    "# - history_index.jsonl appended (run log)\n",
    "# - summary_latest.json overwritten (quick \"latest run\" reference)\n",
    "# - last_run_pointer.json overwritten (quick pointer)\n",
    "# - tag_run_pointer.json written under a stable tag folder\n",
    "summary = {\n",
    "    \"run_tag_full_trainval_exp\": str(FULL_TRAINVAL_EXP_TAG),\n",
    "    \"run_tag_safe\": str(TAG_SAFE),\n",
    "    \"run_stamp\": str(RUN_STAMP),\n",
    "\n",
    "    \"train_dataset\": train_dataset_id,\n",
    "    \"test_dataset\": d2_dataset_id,\n",
    "\n",
    "    \"d7_out_root\": D7_OUT_ROOT,\n",
    "    \"d2_out_root\": D2_OUT_ROOT,\n",
    "    \"d2_manifest_all\": D2_MANIFEST_ALL,\n",
    "\n",
    "    \"enhanced_exp_required_substring_case_insensitive\": REQUIRED_EXP_SUBSTRING,\n",
    "    \"trainval_experiment_used\": str(chosen_exp),\n",
    "    \"trainval_summary_path\": str(summary_trainval_path),\n",
    "\n",
    "    \"seeds\": SEEDS,\n",
    "\n",
    "    \"n_test\": int(len(test_df)),\n",
    "    \"label_counts_test\": test_df[\"label_num\"].value_counts(dropna=False).to_dict(),\n",
    "    \"sex_counts_test_norm\": test_df[\"sex_norm\"].value_counts(dropna=False).to_dict(),\n",
    "\n",
    "    \"test_threshold_used\": {\n",
    "        \"threshold_used_global\": float(THR_USED_GLOBAL),\n",
    "        \"threshold_source\": \"D7 enhanced trainval summary_trainval.json -> val_optimal_threshold.mean_sd.mean\",\n",
    "        \"note_global\": (THR_GLOBAL_NOTE if THR_GLOBAL_NOTE is not None else \"\"),\n",
    "        \"per_seed_repetition_for_audit\": {str(r[\"seed\"]): float(r[\"thr_used\"]) for r in results},\n",
    "    },\n",
    "\n",
    "    \"test_aurocs_by_seed\": {str(r[\"seed\"]): float(r[\"test_auc\"]) for r in results},\n",
    "    \"mean_test_auroc\": float(mean_auc),\n",
    "    \"t_crit_df2_95\": float(t_crit),\n",
    "    \"half_width_95_ci\": float(half_width),\n",
    "    \"ci95_test_auroc\": ci95,\n",
    "\n",
    "    \"threshold_metrics_mean_sd_test_at_thr_used\": agg,\n",
    "    \"confusion_matrix_by_seed_test_at_thr_used\": cm_by_seed,\n",
    "\n",
    "    \"fairness_test_at_thr_used\": {\n",
    "        \"definition\": \"H3: ΔFNR = FNR(F) - FNR(M), where FNR(sex)=FN/(FN+TP) computed on PD-only true labels at test_threshold_used_global.\",\n",
    "        \"fnr_by_sex_norm_by_seed\": fnr_by_seed,\n",
    "        \"delta_fnr_F_minus_M_by_seed\": delta_signed_by_seed,\n",
    "        \"delta_fnr_abs_by_seed\": delta_abs_by_seed,\n",
    "        \"fnr_M_mean_sd\": {\"mean\": float(fnr_m_mean), \"sd\": float(fnr_m_sd)},\n",
    "        \"fnr_F_mean_sd\": {\"mean\": float(fnr_f_mean), \"sd\": float(fnr_f_sd)},\n",
    "        \"delta_fnr_F_minus_M_mean_sd\": {\"mean\": float(d_signed_mean), \"sd\": float(d_signed_sd)},\n",
    "        \"delta_fnr_abs_mean_sd\": {\"mean\": float(d_abs_mean), \"sd\": float(d_abs_sd)},\n",
    "    },\n",
    "\n",
    "    \"run_dirs\": [r[\"run_dir\"] for r in results],\n",
    "    \"predictions_csv_by_seed\": {str(r[\"seed\"]): str(r[\"predictions_csv\"]) for r in results},\n",
    "}\n",
    "\n",
    "summary_path = RUN_ROOT / \"summary_test.json\"\n",
    "with open(summary_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "history_path = TEST_ROOT / \"history_index.jsonl\"\n",
    "with open(history_path, \"a\", encoding=\"utf-8\") as f:\n",
    "    f.write(json.dumps(summary) + \"\\n\")\n",
    "\n",
    "summary_latest_path = TEST_ROOT / \"summary_latest.json\"\n",
    "latest_summary_obj = {\n",
    "    \"run_tag_full_trainval_exp\": str(FULL_TRAINVAL_EXP_TAG),\n",
    "    \"run_tag_safe\": str(TAG_SAFE),\n",
    "    \"run_stamp\": str(RUN_STAMP),\n",
    "    \"run_root\": str(RUN_ROOT),\n",
    "    \"summary_test_json\": str(summary_path),\n",
    "    \"seed_run_dirs\": [str(RUN_ROOT / f\"run_{train_dataset_id}_on_{d2_dataset_id}test_seed{s}\") for s in SEEDS],\n",
    "}\n",
    "with open(summary_latest_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(latest_summary_obj, f, indent=2)\n",
    "\n",
    "global_pointer_path = TEST_ROOT / \"last_run_pointer.json\"\n",
    "with open(global_pointer_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(latest_summary_obj, f, indent=2)\n",
    "\n",
    "tag_pointer_path = TAG_ROOT / \"tag_run_pointer.json\"\n",
    "tag_pointer_obj = dict(latest_summary_obj)\n",
    "tag_pointer_obj[\"tag_root\"] = str(TAG_ROOT)\n",
    "with open(tag_pointer_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(tag_pointer_obj, f, indent=2)\n",
    "\n",
    "print(\"\\nWROTE run summary:\", str(summary_path))\n",
    "print(\"APPENDED history index:\", str(history_path))\n",
    "print(\"WROTE latest summary:\", str(summary_latest_path))\n",
    "print(\"WROTE global pointer:\", str(global_pointer_path))\n",
    "print(\"WROTE tag pointer:\", str(tag_pointer_path))\n",
    "print(\"Open this folder to access artifacts:\", str(RUN_ROOT))\n",
    "\n",
    "# -------------------------\n",
    "# 18.5) Write builder-style config/log placeholders\n",
    "# -------------------------\n",
    "# Writes:\n",
    "# - run_config.json (run settings and pointers)\n",
    "# - dataset_summary.json (key dataset stats for this test run)\n",
    "# - preprocess_warnings.csv placeholder (kept for consistent folder structure)\n",
    "_backup_if_exists(RUN_CONFIG_PATH)\n",
    "_backup_if_exists(WARNINGS_CSV_PATH)\n",
    "_backup_if_exists(DATASET_SUMMARY_PATH)\n",
    "\n",
    "run_config = {\n",
    "    \"mode\": f\"D7_{TAG_SAFE}_on_D2_Test\",\n",
    "    \"created_utc\": datetime.utcnow().isoformat(),\n",
    "    \"run_stamp\": RUN_STAMP,\n",
    "\n",
    "    \"d7_out_root\": D7_OUT_ROOT,\n",
    "    \"d2_out_root\": D2_OUT_ROOT,\n",
    "    \"d2_manifest_all\": D2_MANIFEST_ALL,\n",
    "\n",
    "    \"enhanced_exp_required_substring_case_insensitive\": REQUIRED_EXP_SUBSTRING,\n",
    "    \"trainval_experiment_used\": str(chosen_exp),\n",
    "    \"trainval_exp_tag\": str(FULL_TRAINVAL_EXP_TAG),\n",
    "    \"trainval_summary_path\": str(summary_trainval_path),\n",
    "\n",
    "    \"threshold_source\": \"summary_trainval.json -> val_optimal_threshold.mean_sd.mean\",\n",
    "    \"threshold_used_global\": float(THR_USED_GLOBAL),\n",
    "    \"threshold_note_global\": (THR_GLOBAL_NOTE if THR_GLOBAL_NOTE is not None else \"\"),\n",
    "\n",
    "    \"seeds\": SEEDS,\n",
    "    \"use_amp\": bool(USE_AMP and DEVICE.type == \"cuda\"),\n",
    "    \"per_device_bs\": int(PER_DEVICE_BS),\n",
    "    \"effective_bs\": int(PER_DEVICE_BS * GRAD_ACCUM),\n",
    "    \"num_workers\": int(NUM_WORKERS),\n",
    "    \"pin_memory\": bool(PIN_MEMORY),\n",
    "    \"backbone_ckpt\": BACKBONE_CKPT,\n",
    "    \"dropout_p\": float(DROPOUT_P),\n",
    "\n",
    "    \"monolingual_test_runs_root\": str(TEST_ROOT),\n",
    "    \"run_root\": str(RUN_ROOT),\n",
    "    \"summary_test_json\": str(summary_path),\n",
    "    \"last_run_pointer_json\": str(global_pointer_path),\n",
    "    \"tag_run_pointer_json\": str(tag_pointer_path),\n",
    "}\n",
    "with open(RUN_CONFIG_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(run_config, f, indent=2)\n",
    "\n",
    "dataset_summary = {\n",
    "    \"mode\": f\"D7_{TAG_SAFE}_on_D2_Test\",\n",
    "    \"created_utc\": datetime.utcnow().isoformat(),\n",
    "    \"status\": \"SUCCESS\",\n",
    "    \"run_stamp\": RUN_STAMP,\n",
    "\n",
    "    \"d2_dataset_id\": d2_dataset_id,\n",
    "    \"n_test\": int(len(test_df)),\n",
    "    \"label_counts_test\": test_df[\"label_num\"].value_counts(dropna=False).to_dict(),\n",
    "    \"sex_counts_test_raw\": test_df[\"sex\"].value_counts(dropna=False).to_dict(),\n",
    "    \"sex_counts_test_norm\": test_df[\"sex_norm\"].value_counts(dropna=False).to_dict(),\n",
    "\n",
    "    \"trainval_experiment_used\": str(chosen_exp),\n",
    "    \"trainval_exp_tag\": str(FULL_TRAINVAL_EXP_TAG),\n",
    "    \"trainval_summary_path\": str(summary_trainval_path),\n",
    "\n",
    "    \"threshold_used_global\": float(THR_USED_GLOBAL),\n",
    "\n",
    "    \"monolingual_test_runs_root\": str(TEST_ROOT),\n",
    "    \"run_root\": str(RUN_ROOT),\n",
    "    \"summary_test_json\": str(summary_path),\n",
    "    \"last_run_pointer_json\": str(global_pointer_path),\n",
    "}\n",
    "with open(DATASET_SUMMARY_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(dataset_summary, f, indent=2)\n",
    "\n",
    "with open(WARNINGS_CSV_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"ts,level,message\\n\")\n",
    "\n",
    "print(\"\\nWROTE (builder-aligned):\")\n",
    "print(\" \", str(RUN_CONFIG_PATH))\n",
    "print(\" \", str(WARNINGS_CSV_PATH))\n",
    "print(\" \", str(DATASET_SUMMARY_PATH))\n",
    "\n",
    "# -------------------------\n",
    "# 19) Stop runtime (release GPU)\n",
    "# -------------------------\n",
    "# Frees Colab GPU resources after the run finishes.\n",
    "print(\"\\nAll done. Unassigning the runtime...\")\n",
    "try:\n",
    "    from google.colab import runtime  # type: ignore\n",
    "    runtime.unassign()\n",
    "except Exception as e:\n",
    "    print(\"Could not unassign runtime automatically. Stop runtime manually if needed.\")\n",
    "    print(\"Reason:\", repr(e))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "0a3a3c1bee0d42f29aed8e9f2e0efe57",
      "7e99b571cf004ab1b8b9b62711593167",
      "7a47657703084e978dc6e3f812680b77",
      "137ca6ad0d1e4f7997f4554a278d499c",
      "34650d08c555443b84891d40d1c3e2bf",
      "3a229cceb6de480589743a79262aee64",
      "c970a9c262424425b1e4fe99d2bd2d40",
      "deacf6676ee94b6ba707474e536aba9e",
      "ba6f923131ff4cde9f747bfb85560d05",
      "695a97b4d01c43d5a31cf86f1ae2059a",
      "79935ce9542f4bb09f3947b1592983c4",
      "331db4dabac549338d697056e0bfb2a1",
      "9881be128f3b43769b2b9bb733254e63",
      "5d2a3c657c6245f3b25fbe15e6aa0728",
      "93d6269ee11c4a81a977bb64b19cabcf",
      "1dcce1f5e3444d9e9a214d386f42d117",
      "ade328a76e7a497bac2165ff81844bf4",
      "a7123133cf5a4fe8acec54bc595b17ba",
      "200711835e4c4f7382ae235dc58bdf55",
      "9efe5587077241a08c1f1e8aae34dc7a",
      "6b2706247c054066b3ae79ad85dd7708",
      "6c8e3a9705db4b408e7bebac40276bd3",
      "cfdbefaa64354cd2a06c40ed2d5d07b3",
      "7b6b230064d745c9ab99bdb535551700",
      "a170447edda4470f82d908236de0eb26",
      "e7c979b2e6d0449d9b153ea75ba15c9e",
      "1472f6feeba841d9b3a30d6892f7e0dd",
      "dea049f66ab646c389c03898acbc2fb1",
      "d5c7dfa7042349a7b9174212879b8bb7",
      "6dc39f6f30c148e18369636be3270a1e",
      "2caf33cf469547f7a69902b78d866bb9",
      "70e3a6761bd542a99820185f4540eede",
      "bdbf1a95d0c6419e809aea05b06cf5a2",
      "9ac91836e1c64eac9342e5bdd452d477",
      "fa28dc7cec304ae18426cee1c2636642",
      "91e9fad2f7fd47a2b9529cb7344ef515",
      "e6b88a8f8b8f4f7ea8c3eea15e098e11",
      "a638edb90b9e41e185787e0e8b183dba",
      "2a0264d26d6d4c4e8aa7d15b5e99cb02",
      "9fe477ebb93d455fa71a153e766eee2e",
      "149e02dd1ee641bdb2c0be7f1cf1da38",
      "f78b3711b47a4635a1ca9f953b1eb9cf",
      "9cfc70f21dc54ae780c347fd730fd719",
      "c0370e569cfe4f8e941bef93851f4732",
      "9bb43f47fd8c47d0a3254e9b7200016d",
      "d1092e36ab554494bc59655c53177440",
      "3c202950f9ca4944855b80bbe1784bc0",
      "1c898ba80f4e417da94ab8a11faa05c2",
      "33c7f9d7449b44cb97ae1b6d4405b484",
      "25fa33efd8874dd79f5a7c88c07b25da",
      "186341d9b9e64712bde474af9f7e029b",
      "893e6f2ba1864ca4820c3e302a0e90ff",
      "3b60a42620eb4b039bf9ab76e8599441",
      "0eddd40a8d5648bea2b6b36ad88d1b3c",
      "2582b8793c4748079fcbd200e3569806",
      "0a741901f99b41d3bdbdf42732c9fade",
      "827b6992c83e4b3ca79d007bf358527b",
      "f8d561fc27b9422aa5ad418b1daeffc9",
      "b1f39e26c83b42eca9c72e24da9c4421",
      "ef2156aee2d34013851136021f8250d7",
      "82b6e8a5b20743b5a5539dea4287f9db",
      "1e6e79080ed14672ab9f56d8e969bdfa",
      "d3a0d4e57d1d4aecba1ea1de9533236f",
      "e5beb25579d84f64bb720663fcbcdb66",
      "3c42993708454b6bbf96ba820044388e",
      "7963628eb3164ae0ba4336e967ae5417"
     ]
    },
    "id": "3NmXzHNXgHjI"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "The following cell builds the enhanced D7 training split **train_enh2** by starting with the full original D7 training set and then adding a new, carefully chosen subset from the D2 training data. The added D2 data comes from about **10 percent of all D2 training speakers**, based on the total number of D2 training speakers before any filtering. Speakers that were already included in the earlier **train_enh1** build are explicitly excluded, so there is no overlap between the two enhanced training sets. The output is a new clips folder and a matching manifest that can be used directly in later D7 training and validation steps.\n",
    "\n",
    "The cell begins by mounting Google Drive if needed and defining the preprocessed data roots for D7 and D2. It creates the required output folders under D7 for clips, manifests, logs, and configuration files. It then loads `manifest_all.csv` for both datasets, along with `manifest_train_enh1.csv`, which is used to identify D2 speakers that must be excluded. At this stage, several basic checks are applied: the column layout must match the expected schema, dataset identifiers must be correct, and class labels must be limited to Healthy (0) or Parkinson’s (1). Any literal `\"NaN\"` strings are converted to real missing values, and sex values are standardized to a simple **M/F** format.\n",
    "\n",
    "Next, the cell gathers the source rows for the build. This includes all D7 rows with `split = \"train\"` and all D2 rows with `split = \"train\"`. For D2, it checks that each speaker has a single, consistent diagnosis across all clips, which is required for speaker-level sampling. Using the full D2 training speaker count, it computes the target number of speakers as **10 percent**, rounds this up, and then adjusts it to be an even number so Healthy and Parkinson’s speakers can be selected in equal amounts. The exclusion list from **train_enh1** is applied, and the code confirms that enough eligible speakers remain. From the remaining pool, the required number of Healthy and Parkinson’s speakers is selected at random using a fixed seed, and all training clips from those speakers are included.\n",
    "\n",
    "Before copying any audio, the cell checks that every referenced source file exists for both the D7 training data and the selected D2 subset. It then creates a copy plan with strict rules to avoid overwriting files. D7 training clips are copied into `clips/train_enh2/` using their original filenames. D2 clips are copied into the same folder using new, deterministic filenames that encode the split, class, speaker ID, task type, and a stable index. A preflight check looks for existing destination files: files with the correct size are skipped, while any size mismatch or file error causes the run to stop. At this point, the cell writes initial run metadata, warnings, and a preliminary dataset summary so the process is traceable even if it fails.\n",
    "\n",
    "During the copy step, only missing files are copied. Any copy error is recorded and treated as a failure. After copying completes successfully, the cell creates `manifest_train_enh2.csv` by combining the rewritten D7 training rows with the newly added D2 rows. All rows are marked with `split = \"train_enh2\"`. D7 rows keep their identity but point to the new clip locations. D2 rows are rewritten so they appear as D7 entries for training, with updated file paths and sample IDs, and each one includes a `source_dataset` field set to “D2” to preserve where it came from. The final manifest is checked to ensure there are no literal `\"NaN\"` strings and that every listed audio file exists, and then it is written safely to disk.\n",
    "\n",
    "The cell finishes by writing a final dataset summary and updating the run configuration with details about speaker selection, exclusions, file checks, and copy results. It prints the paths to the new **train_enh2** clips folder, the corresponding manifest, and the related log and configuration files for reference."
   ],
   "metadata": {
    "id": "qk6_jq86wAqx"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# =========================\n",
    "# D7 train_enh2 Builder (Add fixed 10% of D2 train speakers)\n",
    "# Inputs: D7 full manifest, D2 full manifest, prior train_enh1 manifest\n",
    "# Outputs: train_enh2 clips folder, train_enh2 manifest, run config, warnings log, summary JSON\n",
    "# =========================\n",
    "# =========================\n",
    "# D7 TRAIN_ENH2 BUILDER (CRASH-PROOF, THIRD-PARTY AUDITABLE) — ADD ceil(10% of D2 TRAIN speakers)\n",
    "# FIXED DENOMINATOR POLICY:\n",
    "# - The 10% target is computed from the FULL D2 TRAIN speaker count (before exclusions)\n",
    "# - Exclusions only affect ELIGIBLE pool, not the draw size\n",
    "#\n",
    "# - Creates: <D7_OUT_ROOT>/clips/train_enh2/\n",
    "# - Writes:\n",
    "#     * manifests/manifest_train_enh2.csv\n",
    "#     * config/D7_Enh2_on_D2_Test/run_config.json\n",
    "#     * logs/D7_Enh2_on_D2_Test/preprocess_warnings.csv\n",
    "#     * logs/D7_Enh2_on_D2_Test/dataset_summary.json\n",
    "# - COPY only, no overwrite\n",
    "# =========================\n",
    "\n",
    "import os, json, re, shutil, math\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# -------------------------\n",
    "# 0) Drive access (Colab-friendly)\n",
    "# -------------------------\n",
    "# Makes sure files can be read and written when running in Google Colab.\n",
    "try:\n",
    "    from google.colab import drive  # type: ignore\n",
    "    if not os.path.exists(\"/content/drive\"):\n",
    "        drive.mount(\"/content/drive\")\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# -------------------------\n",
    "# 1) Key inputs and output locations\n",
    "# -------------------------\n",
    "# Inputs: D7 manifest_all, D2 manifest_all, and the prior train_enh1 manifest (for exclusions).\n",
    "# Outputs: a new train_enh2 clips folder and manifest, plus logs and a run config record.\n",
    "D7_OUT_ROOT = Path(\"/content/drive/MyDrive/AI_PD_Project/Datasets/D7-Multilingual (D1_D4_D5v2_D6)/preprocessed_v1\")\n",
    "D2_OUT_ROOT = Path(\"/content/drive/MyDrive/AI_PD_Project/Datasets/D2-Slovak (EWA-DB)/EWA-DB/preprocessed_v1\")\n",
    "\n",
    "D7_MANIFEST_ALL = D7_OUT_ROOT / \"manifests\" / \"manifest_all.csv\"\n",
    "D2_MANIFEST_ALL = D2_OUT_ROOT / \"manifests\" / \"manifest_all.csv\"\n",
    "\n",
    "# Prior selection source (used to enforce zero overlap with draw 1)\n",
    "MANIFEST_TRAIN_ENH1 = Path(\n",
    "    \"/content/drive/MyDrive/AI_PD_Project/Datasets/D7-Multilingual (D1_D4_D5v2_D6)/preprocessed_v1/manifests/manifest_train_enh1.csv\"\n",
    ")\n",
    "\n",
    "# New outputs\n",
    "TRAIN_ENH2_DIR = D7_OUT_ROOT / \"clips\" / \"train_enh2\"\n",
    "MANIFEST_TRAIN_ENH2 = D7_OUT_ROOT / \"manifests\" / \"manifest_train_enh2.csv\"\n",
    "\n",
    "# Logs/config for this builder run\n",
    "RUN_FOLDER = \"D7_Enh2_on_D2_Test\"\n",
    "LOGS_SUBDIR = D7_OUT_ROOT / \"logs\" / RUN_FOLDER\n",
    "CFG_SUBDIR  = D7_OUT_ROOT / \"config\" / RUN_FOLDER\n",
    "\n",
    "WARNINGS_CSV = LOGS_SUBDIR / \"preprocess_warnings.csv\"\n",
    "SUMMARY_JSON = LOGS_SUBDIR / \"dataset_summary.json\"\n",
    "RUN_CONFIG_JSON = CFG_SUBDIR / \"run_config.json\"\n",
    "\n",
    "# Standard folders (created if missing)\n",
    "clips_dir = D7_OUT_ROOT / \"clips\"\n",
    "manif_dir = D7_OUT_ROOT / \"manifests\"\n",
    "logs_dir  = D7_OUT_ROOT / \"logs\"\n",
    "cfg_dir   = D7_OUT_ROOT / \"config\"\n",
    "\n",
    "for d in [clips_dir, manif_dir, logs_dir, cfg_dir, LOGS_SUBDIR, CFG_SUBDIR]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "TRAIN_ENH2_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# -------------------------\n",
    "# 2) Manifest schema (fixed column list)\n",
    "# -------------------------\n",
    "# Locks column names and ordering so downstream training code stays stable.\n",
    "CANON_COLS = [\n",
    "    \"split\",\n",
    "    \"dataset\",\n",
    "    \"task\",\n",
    "    \"speaker_id\",\n",
    "    \"sample_id\",\n",
    "    \"label_str\",\n",
    "    \"label_num\",\n",
    "    \"age\",\n",
    "    \"sex\",\n",
    "    \"speaker_key_rel\",\n",
    "    \"clip_path\",\n",
    "    \"duration_sec\",\n",
    "    \"source_path\",\n",
    "    \"clip_start_sec\",\n",
    "    \"clip_end_sec\",\n",
    "    \"sr_hz\",\n",
    "    \"channels\",\n",
    "    \"clip_is_contiguous\",\n",
    "]\n",
    "FINAL_COLS = CANON_COLS + [\"source_dataset\"]\n",
    "\n",
    "# -------------------------\n",
    "# 3) Sampling policy settings (kept explicit for repeatability)\n",
    "# -------------------------\n",
    "# - target size uses FULL D2 train speaker count (before exclusions)\n",
    "# - speaker-balanced draw (same number of HC and PD speakers)\n",
    "TEN_PCT         = 0.10\n",
    "ROUNDING_POLICY = \"ceil_then_make_even_by_rounding_down_if_needed\"\n",
    "BALANCE_POLICY  = \"speaker-balanced\"\n",
    "SPEAKER_ID_COL  = \"speaker_id\"\n",
    "\n",
    "# Random seed used for this draw (draw 2)\n",
    "RNG_SEED        = 2024\n",
    "\n",
    "LABEL_MAP_NOTE  = \"label_num mapping: 0=Healthy, 1=Parkinson\"\n",
    "\n",
    "# -------------------------\n",
    "# 4) Small utilities and logging\n",
    "# -------------------------\n",
    "# warnings_rows is written to CSV and used to stop on fatal issues.\n",
    "warnings_rows = []\n",
    "\n",
    "def require(cond: bool, msg: str):\n",
    "    # Simple guard helper used throughout the script.\n",
    "    if not cond:\n",
    "        raise RuntimeError(msg)\n",
    "\n",
    "def add_warn(src: str, level: str, code: str, message: str, **extra):\n",
    "    # Records issues in a structured way (later written to preprocess_warnings.csv).\n",
    "    row = {\n",
    "        \"ts\": datetime.utcnow().isoformat(),\n",
    "        \"src\": src,\n",
    "        \"level\": str(level).upper(),\n",
    "        \"code\": code,\n",
    "        \"message\": message,\n",
    "    }\n",
    "    row.update(extra)\n",
    "    warnings_rows.append(row)\n",
    "\n",
    "def count_by_level(rows):\n",
    "    # Quick summary for console prints and run summaries.\n",
    "    out = {\"ERROR\": 0, \"WARN\": 0, \"INFO\": 0}\n",
    "    for r in rows:\n",
    "        lvl = str(r.get(\"level\", \"INFO\")).upper()\n",
    "        out[lvl] = out.get(lvl, 0) + 1\n",
    "    return out\n",
    "\n",
    "def atomic_write_text(dst: Path, text: str):\n",
    "    # Writes via a temp file then renames, to avoid partial files on crashes.\n",
    "    tmp = dst.with_suffix(dst.suffix + \".tmp\")\n",
    "    with open(tmp, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(text)\n",
    "    os.replace(tmp, dst)\n",
    "\n",
    "def atomic_write_csv(dst: Path, df: pd.DataFrame):\n",
    "    # Same atomic pattern for CSV (also enforces blank for NaN).\n",
    "    tmp = dst.with_suffix(dst.suffix + \".tmp\")\n",
    "    df.to_csv(tmp, index=False, na_rep=\"\")\n",
    "    os.replace(tmp, dst)\n",
    "\n",
    "def safe_token(s, max_len=32, default=\"NA\"):\n",
    "    # Makes a filesystem-safe token for deterministic file naming.\n",
    "    if pd.isna(s):\n",
    "        return default\n",
    "    s = str(s).strip()\n",
    "    s = re.sub(r\"\\s+\", \"_\", s)\n",
    "    s = re.sub(r\"[^A-Za-z0-9_]+\", \"\", s)\n",
    "    return (s[:max_len] if s else default)\n",
    "\n",
    "def label_str_from_num(v):\n",
    "    # Human-readable label string derived from label_num.\n",
    "    if pd.isna(v):\n",
    "        return np.nan\n",
    "    iv = int(v)\n",
    "    if iv == 0:\n",
    "        return \"Healthy\"\n",
    "    if iv == 1:\n",
    "        return \"Parkinson\"\n",
    "    return np.nan\n",
    "\n",
    "def hc_pd_from_label_num(v):\n",
    "    # Compact label used in generated filenames.\n",
    "    return \"PD\" if int(v) == 1 else \"HC\"\n",
    "\n",
    "def normalize_sex_to_MF_D7(x):\n",
    "    # Standardize D7 sex values to M/F/NaN.\n",
    "    if pd.isna(x):\n",
    "        return np.nan\n",
    "    s = str(x).strip().lower()\n",
    "    if s in [\"m\", \"male\"]:\n",
    "        return \"M\"\n",
    "    if s in [\"f\", \"female\"]:\n",
    "        return \"F\"\n",
    "    if s in [\"\", \"nan\", \"none\", \"unknown\", \"u\"]:\n",
    "        return np.nan\n",
    "    return np.nan\n",
    "\n",
    "def normalize_sex_to_MF_D2(x):\n",
    "    # Standardize D2 sex values to M/F/NaN (D2 uses male/female).\n",
    "    if pd.isna(x):\n",
    "        return np.nan\n",
    "    s = str(x).strip().lower()\n",
    "    if s in [\"male\", \"m\"]:\n",
    "        return \"M\"\n",
    "    if s in [\"female\", \"f\"]:\n",
    "        return \"F\"\n",
    "    if s in [\"\", \"nan\", \"none\", \"unknown\", \"u\"]:\n",
    "        return np.nan\n",
    "    return np.nan\n",
    "\n",
    "def summarize_counts(df: pd.DataFrame, split_name: str):\n",
    "    # Final dataset summary used by the logs JSON.\n",
    "    out = {}\n",
    "    out[\"total_rows\"] = int(len(df))\n",
    "    out[\"split\"] = split_name\n",
    "    out[\"label_counts_total\"] = {str(k): int(v) for k, v in df[\"label_num\"].value_counts(dropna=False).to_dict().items()}\n",
    "    out[\"by_source_dataset\"] = {sd: int((df[\"source_dataset\"] == sd).sum()) for sd in sorted(df[\"source_dataset\"].dropna().unique())}\n",
    "    out[\"sex_counts\"] = {str(k): int(v) for k, v in df[\"sex\"].value_counts(dropna=False).to_dict().items()}\n",
    "    out[\"n_unique_speakers\"] = int(df[\"speaker_id\"].astype(str).nunique()) if \"speaker_id\" in df.columns else int(0)\n",
    "    return out\n",
    "\n",
    "# -------------------------\n",
    "# 5) Load manifests and validate basics\n",
    "# -------------------------\n",
    "# Loads D7 and D2 manifests, checks required columns, normalizes key fields.\n",
    "# Also loads train_enh1 to build the exclusion list (zero overlap with draw 1).\n",
    "print(\"D7_OUT_ROOT:\", str(D7_OUT_ROOT))\n",
    "print(\"D2_OUT_ROOT:\", str(D2_OUT_ROOT))\n",
    "print(\"D7_MANIFEST_ALL:\", str(D7_MANIFEST_ALL))\n",
    "print(\"D2_MANIFEST_ALL:\", str(D2_MANIFEST_ALL))\n",
    "print(\"MANIFEST_TRAIN_ENH1:\", str(MANIFEST_TRAIN_ENH1))\n",
    "print(\"TRAIN_ENH2_DIR:\", str(TRAIN_ENH2_DIR))\n",
    "print(\"MANIFEST_TRAIN_ENH2:\", str(MANIFEST_TRAIN_ENH2))\n",
    "print(\"LOGS_SUBDIR:\", str(LOGS_SUBDIR))\n",
    "print(\"CFG_SUBDIR:\", str(CFG_SUBDIR))\n",
    "print(\"RNG_SEED:\", int(RNG_SEED))\n",
    "\n",
    "require(D7_MANIFEST_ALL.exists(), f\"Missing D7 manifest_all.csv: {str(D7_MANIFEST_ALL)}\")\n",
    "require(D2_MANIFEST_ALL.exists(), f\"Missing D2 manifest_all.csv: {str(D2_MANIFEST_ALL)}\")\n",
    "require(MANIFEST_TRAIN_ENH1.exists(), f\"Missing prior manifest_train_enh1.csv: {str(MANIFEST_TRAIN_ENH1)}\")\n",
    "\n",
    "d7 = pd.read_csv(D7_MANIFEST_ALL)\n",
    "d2 = pd.read_csv(D2_MANIFEST_ALL)\n",
    "enh1 = pd.read_csv(MANIFEST_TRAIN_ENH1)\n",
    "\n",
    "# Schema checks for D7/D2; enh1 must include speaker_id and source_dataset.\n",
    "missing_d7 = [c for c in CANON_COLS if c not in d7.columns]\n",
    "missing_d2 = [c for c in CANON_COLS if c not in d2.columns]\n",
    "require(len(missing_d7) == 0, f\"D7 manifest missing required columns: {missing_d7}\")\n",
    "require(len(missing_d2) == 0, f\"D2 manifest missing required columns: {missing_d2}\")\n",
    "\n",
    "require(\"speaker_id\" in enh1.columns, f\"manifest_train_enh1.csv missing 'speaker_id'. Found: {list(enh1.columns)}\")\n",
    "require(\"source_dataset\" in enh1.columns, f\"manifest_train_enh1.csv missing 'source_dataset'. Found: {list(enh1.columns)}\")\n",
    "\n",
    "# Ensure source_dataset exists on both base manifests for later summaries.\n",
    "if \"source_dataset\" not in d7.columns:\n",
    "    d7[\"source_dataset\"] = \"D7\"\n",
    "if \"source_dataset\" not in d2.columns:\n",
    "    d2[\"source_dataset\"] = \"D2\"\n",
    "\n",
    "# Convert literal \"NaN\" strings into real missing values.\n",
    "for df in [d7, d2, enh1]:\n",
    "    for col in [\"sex\", \"age\", \"duration_sec\", \"clip_start_sec\", \"clip_end_sec\", \"speaker_key_rel\", \"speaker_id\", \"task\", \"sample_id\"]:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].replace(\"NaN\", np.nan)\n",
    "\n",
    "def infer_dataset_id(df: pd.DataFrame, fallback: str) -> str:\n",
    "    # Infers the dataset id from the most common non-null value in the dataset column.\n",
    "    if \"dataset\" in df.columns and df[\"dataset\"].notna().any():\n",
    "        return str(df[\"dataset\"].astype(str).value_counts(dropna=True).idxmax())\n",
    "    return fallback\n",
    "\n",
    "# Hard guards: these manifests should identify themselves as D7 and D2.\n",
    "require(infer_dataset_id(d7, \"DX\") == \"D7\", \"Expected D7 manifest dataset=='D7'.\")\n",
    "require(infer_dataset_id(d2, \"DX\") == \"D2\", \"Expected D2 manifest dataset=='D2'.\")\n",
    "\n",
    "# Keep only the intended dataset rows.\n",
    "d7 = d7[d7[\"dataset\"].astype(str) == \"D7\"].copy()\n",
    "d2 = d2[d2[\"dataset\"].astype(str) == \"D2\"].copy()\n",
    "\n",
    "# Validate labels and create label_str.\n",
    "for name, df in [(\"D7\", d7), (\"D2\", d2)]:\n",
    "    bad = sorted(set(df[\"label_num\"].dropna().unique()) - {0, 1})\n",
    "    require(len(bad) == 0, f\"{name} label_num contains values outside {{0,1}}: {bad}\")\n",
    "    df[\"label_str\"] = df[\"label_num\"].map(label_str_from_num)\n",
    "\n",
    "# Standardize sex encoding to M/F/NaN.\n",
    "d7[\"sex\"] = d7[\"sex\"].map(normalize_sex_to_MF_D7)\n",
    "d2[\"sex\"] = d2[\"sex\"].map(normalize_sex_to_MF_D2)\n",
    "\n",
    "# -------------------------\n",
    "# 6) Select D7 train split (base content for train_enh2)\n",
    "# -------------------------\n",
    "# All D7 train clips are included in train_enh2 (copied into a new folder).\n",
    "d7_train = d7[d7[\"split\"].astype(str) == \"train\"].copy()\n",
    "require(len(d7_train) > 0, \"D7 train split has 0 rows.\")\n",
    "print(\"\\nD7 train rows:\", int(len(d7_train)))\n",
    "print(\"D7 train label counts:\", d7_train[\"label_num\"].value_counts(dropna=False).to_dict())\n",
    "\n",
    "# -------------------------\n",
    "# 7) Build D2 exclusion list from train_enh1\n",
    "# -------------------------\n",
    "# Excludes any D2 speakers already used in train_enh1 to enforce zero overlap.\n",
    "enh1_d2 = enh1[enh1[\"source_dataset\"].astype(str) == \"D2\"].copy()\n",
    "prior_d2_speakers = sorted(enh1_d2[\"speaker_id\"].dropna().astype(str).unique().tolist())\n",
    "\n",
    "print(\"\\nZero-overlap exclusion from manifest_train_enh1:\")\n",
    "print(\"  Prior D2 rows in enh1:\", int(len(enh1_d2)))\n",
    "print(\"  Prior D2 unique speakers to exclude:\", int(len(prior_d2_speakers)))\n",
    "print(\"  Example excluded speakers (up to 20):\", prior_d2_speakers[:20])\n",
    "\n",
    "# -------------------------\n",
    "# 8) Compute fixed draw size from FULL D2 train speaker universe\n",
    "# -------------------------\n",
    "# Key rule: the 10% target uses the full D2 train speaker count (before exclusions).\n",
    "# Additional rule: the target is forced to be even, then split evenly across HC and PD.\n",
    "d2_train = d2[d2[\"split\"].astype(str) == \"train\"].copy()\n",
    "require(len(d2_train) > 0, \"D2 train split has 0 rows.\")\n",
    "require(SPEAKER_ID_COL in d2_train.columns, f\"D2 missing speaker id column: {SPEAKER_ID_COL}\")\n",
    "\n",
    "print(\"\\nD2 train rows:\", int(len(d2_train)))\n",
    "print(\"D2 train label counts:\", d2_train[\"label_num\"].value_counts(dropna=False).to_dict())\n",
    "\n",
    "# Guard: each speaker must have a single consistent label across all clips.\n",
    "speaker_labels = (\n",
    "    d2_train.groupby(SPEAKER_ID_COL)[\"label_num\"]\n",
    "    .apply(lambda s: sorted(set(s.dropna().astype(int).tolist())))\n",
    ")\n",
    "mixed = speaker_labels[speaker_labels.apply(lambda x: len(x) != 1)]\n",
    "if len(mixed) > 0:\n",
    "    raise RuntimeError(\n",
    "        \"D2 train has speakers with mixed label_num values across clips; Definition B sampling cannot proceed.\\n\"\n",
    "        f\"Examples: {mixed.head(10).to_dict()}\"\n",
    "    )\n",
    "\n",
    "# Build the full speaker list and full class lists (HC vs PD).\n",
    "speaker_to_label_full = speaker_labels.apply(lambda x: int(x[0])).to_dict()\n",
    "all_speakers_full = sorted([str(x) for x in speaker_to_label_full.keys()])\n",
    "total_speakers_full = len(all_speakers_full)\n",
    "\n",
    "hc_full = sorted([str(spk) for spk, y in speaker_to_label_full.items() if int(y) == 0])\n",
    "pd_full = sorted([str(spk) for spk, y in speaker_to_label_full.items() if int(y) == 1])\n",
    "require(len(hc_full) > 0 and len(pd_full) > 0, \"D2 train does not contain both HC and PD speakers.\")\n",
    "\n",
    "# Fixed-denominator target computation (draw size does not change after exclusions).\n",
    "target_total = int(math.ceil(TEN_PCT * total_speakers_full))\n",
    "if target_total % 2 != 0:\n",
    "    target_total -= 1\n",
    "if target_total < 2:\n",
    "    target_total = 2\n",
    "target_per_class = target_total // 2\n",
    "\n",
    "print(\"\\nFixed-denominator target computation:\")\n",
    "print(\"  total D2 train speakers (FULL):\", total_speakers_full)\n",
    "print(f\"  target_total = ceil(0.10 * {total_speakers_full}) then even:\", target_total, f\"=> {target_per_class} HC + {target_per_class} PD\")\n",
    "\n",
    "# -------------------------\n",
    "# 9) Apply exclusions, then sample from the eligible pool\n",
    "# -------------------------\n",
    "# Exclusions shrink the eligible pool only; the target draw size stays the same.\n",
    "exclude_set = set(prior_d2_speakers)\n",
    "speaker_to_label_eligible = {str(spk): int(lbl) for spk, lbl in speaker_to_label_full.items() if str(spk) not in exclude_set}\n",
    "\n",
    "hc_eligible = sorted([spk for spk, y in speaker_to_label_eligible.items() if y == 0])\n",
    "pd_eligible = sorted([spk for spk, y in speaker_to_label_eligible.items() if y == 1])\n",
    "\n",
    "print(\"\\nEligible pool after exclusion (does not affect target size):\")\n",
    "print(\"  eligible speakers total:\", len(speaker_to_label_eligible))\n",
    "print(\"  eligible HC speakers:\", len(hc_eligible))\n",
    "print(\"  eligible PD speakers:\", len(pd_eligible))\n",
    "\n",
    "require(len(hc_eligible) >= target_per_class, f\"Not enough eligible HC speakers to draw {target_per_class}. Eligible HC={len(hc_eligible)}\")\n",
    "require(len(pd_eligible) >= target_per_class, f\"Not enough eligible PD speakers to draw {target_per_class}. Eligible PD={len(pd_eligible)}\")\n",
    "\n",
    "# Random but repeatable sampling (seeded RNG).\n",
    "rng = np.random.default_rng(int(RNG_SEED))\n",
    "sel_hc = sorted(rng.choice(hc_eligible, size=target_per_class, replace=False).tolist())\n",
    "sel_pd = sorted(rng.choice(pd_eligible, size=target_per_class, replace=False).tolist())\n",
    "selected_speakers = sorted(sel_hc + sel_pd)\n",
    "\n",
    "# Keep all D2 train clips for the selected speakers.\n",
    "d2_sel = d2_train[d2_train[SPEAKER_ID_COL].astype(str).isin([str(x) for x in selected_speakers])].copy()\n",
    "\n",
    "print(\"\\nD2 speaker sampling (DRAW 2, fixed denominator + zero overlap):\")\n",
    "print(\"  target_total (fixed):\", target_total, \"=>\", target_per_class, \"HC +\", target_per_class, \"PD\")\n",
    "print(\"  selected D2 rows (all clips for selected speakers):\", int(len(d2_sel)))\n",
    "print(\"  selected label counts:\", d2_sel[\"label_num\"].value_counts(dropna=False).to_dict())\n",
    "\n",
    "# -------------------------\n",
    "# 10) Confirm all source clip paths exist (fail early)\n",
    "# -------------------------\n",
    "# Prevents partial builds caused by missing audio files.\n",
    "def fail_fast_missing_paths(df: pd.DataFrame, name: str):\n",
    "    missing_paths = []\n",
    "    for p in tqdm(df[\"clip_path\"].astype(str).tolist(), desc=f\"Check {name} clip_path exists\", dynamic_ncols=True):\n",
    "        if not os.path.exists(p):\n",
    "            missing_paths.append(p)\n",
    "            if len(missing_paths) >= 25:\n",
    "                break\n",
    "    if missing_paths:\n",
    "        raise FileNotFoundError(f\"{name}: missing clip_path(s). Examples (up to 25): {missing_paths}\")\n",
    "\n",
    "fail_fast_missing_paths(d7_train, \"D7 TRAIN\")\n",
    "fail_fast_missing_paths(d2_sel, \"D2 TRAIN (selected speakers)\")\n",
    "\n",
    "# -------------------------\n",
    "# 11) Build the copy plan and enforce no-overwrite rules\n",
    "# -------------------------\n",
    "# Two sources are copied into one folder:\n",
    "# - D7 train clips keep their original filenames\n",
    "# - D2 clips get deterministic new filenames to avoid collisions\n",
    "copy_plan = []\n",
    "\n",
    "# A) D7 → train_enh2 (same filenames)\n",
    "for i, row in d7_train.reset_index(drop=True).iterrows():\n",
    "    src_path = Path(str(row[\"clip_path\"]))\n",
    "    dst_path = TRAIN_ENH2_DIR / src_path.name\n",
    "    copy_plan.append({\n",
    "        \"src_path\": str(src_path),\n",
    "        \"dst_path\": str(dst_path),\n",
    "        \"origin\": \"D7_train_existing\",\n",
    "        \"source_dataset\": str(row.get(\"source_dataset\", \"D7\")),\n",
    "        \"row_index\": int(i),\n",
    "    })\n",
    "\n",
    "# B) D2 selected → train_enh2 (deterministic names)\n",
    "d2_sel_reset = d2_sel.reset_index(drop=True).copy()\n",
    "d2_sel_reset[\"_speaker_tok\"] = d2_sel_reset[\"speaker_id\"].map(lambda x: safe_token(x, 32, \"NA\"))\n",
    "d2_sel_reset[\"_task_tok\"] = d2_sel_reset[\"task\"].map(lambda x: safe_token(x, 12, \"0\"))\n",
    "d2_sel_reset[\"_clip_path_str\"] = d2_sel_reset[\"clip_path\"].astype(str)\n",
    "d2_sel_reset = d2_sel_reset.sort_values(by=[\"_speaker_tok\", \"_task_tok\", \"_clip_path_str\"]).reset_index(drop=True)\n",
    "\n",
    "for j, row in d2_sel_reset.iterrows():\n",
    "    src_path = Path(str(row[\"clip_path\"]))\n",
    "    hc_pd = hc_pd_from_label_num(row[\"label_num\"])\n",
    "    spk_tok = safe_token(row[\"speaker_id\"], 32, \"NA\")\n",
    "    task_tok = safe_token(row[\"task\"], 12, \"0\")\n",
    "    out_name = f\"D7_D2add_train_enh2_{hc_pd}_{spk_tok}_{task_tok}_{j+1:06d}.wav\"\n",
    "    dst_path = TRAIN_ENH2_DIR / out_name\n",
    "    copy_plan.append({\n",
    "        \"src_path\": str(src_path),\n",
    "        \"dst_path\": str(dst_path),\n",
    "        \"origin\": \"D2_train_selected\",\n",
    "        \"source_dataset\": \"D2\",\n",
    "        \"row_index\": int(j),\n",
    "    })\n",
    "\n",
    "# Preflight: decide copy vs skip, and detect dangerous conflicts.\n",
    "n_dest_exists_ok = 0\n",
    "n_dest_exists_mismatch = 0\n",
    "n_will_copy = 0\n",
    "\n",
    "print(\"\\nPreflight: destination existence and size checks (no overwrite)...\")\n",
    "for item in tqdm(copy_plan, desc=\"Preflight (dest checks)\", dynamic_ncols=True):\n",
    "    sp = Path(item[\"src_path\"])\n",
    "    dp = Path(item[\"dst_path\"])\n",
    "    require(sp.exists(), f\"Source clip missing at preflight: {str(sp)}\")\n",
    "    if dp.exists():\n",
    "        try:\n",
    "            # If sizes match, treat as already-copied and skip later.\n",
    "            if dp.stat().st_size == sp.stat().st_size:\n",
    "                n_dest_exists_ok += 1\n",
    "            else:\n",
    "                # Same name but different content is treated as a fatal error.\n",
    "                n_dest_exists_mismatch += 1\n",
    "                add_warn(\n",
    "                    \"D7_TRAIN_ENH2\", \"ERROR\", \"DEST_EXISTS_SIZE_MISMATCH\",\n",
    "                    \"Destination exists but file size differs from source\",\n",
    "                    src_path=str(sp), dest_path=str(dp),\n",
    "                    src_size=int(sp.stat().st_size), dest_size=int(dp.stat().st_size),\n",
    "                )\n",
    "        except Exception as e:\n",
    "            n_dest_exists_mismatch += 1\n",
    "            add_warn(\n",
    "                \"D7_TRAIN_ENH2\", \"ERROR\", \"DEST_EXISTS_STAT_ERROR\",\n",
    "                \"Failed to stat source/destination during preflight\",\n",
    "                src_path=str(sp), dest_path=str(dp), error=repr(e),\n",
    "            )\n",
    "    else:\n",
    "        n_will_copy += 1\n",
    "\n",
    "preflight_stats = {\n",
    "    \"total_planned_files\": int(len(copy_plan)),\n",
    "    \"n_dest_exists_ok\": int(n_dest_exists_ok),\n",
    "    \"n_dest_exists_mismatch\": int(n_dest_exists_mismatch),\n",
    "    \"n_will_copy\": int(n_will_copy),\n",
    "    \"warnings_by_level\": count_by_level(warnings_rows),\n",
    "}\n",
    "\n",
    "print(\"\\nPreflight summary:\")\n",
    "print(\"  Planned files:\", int(len(copy_plan)))\n",
    "print(\"  Destination exists (size OK):\", n_dest_exists_ok)\n",
    "print(\"  Destination exists (size mismatch/stat error):\", n_dest_exists_mismatch)\n",
    "print(\"  Will copy:\", n_will_copy)\n",
    "print(\"  Warnings by level:\", preflight_stats[\"warnings_by_level\"])\n",
    "\n",
    "# -------------------------\n",
    "# 12) Write run config and early summary (before copying)\n",
    "# -------------------------\n",
    "# These files capture the selection and policies even if copying fails later.\n",
    "run_config = {\n",
    "    \"dataset\": \"D7\",\n",
    "    \"mode\": \"train_enh2_builder\",\n",
    "    \"created_utc\": datetime.utcnow().isoformat(),\n",
    "    \"run_folder\": RUN_FOLDER,\n",
    "    \"d7_out_root\": str(D7_OUT_ROOT),\n",
    "    \"d2_out_root\": str(D2_OUT_ROOT),\n",
    "    \"d7_manifest_all\": str(D7_MANIFEST_ALL),\n",
    "    \"d2_manifest_all\": str(D2_MANIFEST_ALL),\n",
    "    \"manifest_train_enh1\": str(MANIFEST_TRAIN_ENH1),\n",
    "    \"train_enh2_dir\": str(TRAIN_ENH2_DIR),\n",
    "    \"manifest_train_enh2\": str(MANIFEST_TRAIN_ENH2),\n",
    "    \"policy\": {\n",
    "        \"definition\": \"Add ceil(10%) of D2 TRAIN by speaker (Definition B) into D7 training clips folder\",\n",
    "        \"pct_speakers\": float(TEN_PCT),\n",
    "        \"denominator_policy\": \"FIXED_FULL_D2_TRAIN_SPEAKER_COUNT\",\n",
    "        \"rounding_policy\": ROUNDING_POLICY,\n",
    "        \"balance_policy\": BALANCE_POLICY,\n",
    "        \"speaker_id_col_used\": SPEAKER_ID_COL,\n",
    "        \"rng_seed\": int(RNG_SEED),\n",
    "        \"label_note\": LABEL_MAP_NOTE,\n",
    "        \"file_operation\": \"copy\",\n",
    "        \"no_overwrite_rule\": \"skip if dest exists with matching size; error if size differs\",\n",
    "        \"zero_overlap\": {\n",
    "            \"enabled\": True,\n",
    "            \"exclude_speakers_source\": \"manifest_train_enh1 where source_dataset == 'D2'\",\n",
    "            \"n_excluded_d2_speakers\": int(len(prior_d2_speakers)),\n",
    "            \"excluded_d2_speakers\": prior_d2_speakers,\n",
    "        },\n",
    "    },\n",
    "    \"inputs\": {\n",
    "        \"d7_train_rows\": int(len(d7_train)),\n",
    "        \"d2_train_rows\": int(len(d2_train)),\n",
    "        \"d2_train_total_speakers_full\": int(total_speakers_full),\n",
    "        \"d2_train_total_speakers_eligible\": int(len(speaker_to_label_eligible)),\n",
    "        \"d2_train_speakers_hc_full\": int(len(hc_full)),\n",
    "        \"d2_train_speakers_pd_full\": int(len(pd_full)),\n",
    "        \"d2_train_speakers_hc_eligible\": int(len(hc_eligible)),\n",
    "        \"d2_train_speakers_pd_eligible\": int(len(pd_eligible)),\n",
    "    },\n",
    "    \"selection\": {\n",
    "        \"target_total_speakers_fixed\": int(target_total),\n",
    "        \"target_per_class_fixed\": int(target_per_class),\n",
    "        \"selected_speakers_hc\": sel_hc,\n",
    "        \"selected_speakers_pd\": sel_pd,\n",
    "        \"selected_speakers_all\": selected_speakers,\n",
    "        \"selected_d2_rows\": int(len(d2_sel)),\n",
    "        \"selected_d2_label_counts\": d2_sel[\"label_num\"].value_counts(dropna=False).to_dict(),\n",
    "    },\n",
    "    \"preflight\": preflight_stats,\n",
    "}\n",
    "\n",
    "early_summary = {\n",
    "    \"dataset\": \"D7\",\n",
    "    \"created_utc\": datetime.utcnow().isoformat(),\n",
    "    \"status\": \"PRECHECK_COMPLETE\",\n",
    "    \"d7_out_root\": str(D7_OUT_ROOT),\n",
    "    \"train_enh2_dir\": str(TRAIN_ENH2_DIR),\n",
    "    \"preflight\": preflight_stats,\n",
    "    \"selection\": run_config[\"selection\"],\n",
    "}\n",
    "\n",
    "atomic_write_csv(WARNINGS_CSV, pd.DataFrame(warnings_rows))\n",
    "atomic_write_text(RUN_CONFIG_JSON, json.dumps(run_config, indent=2))\n",
    "atomic_write_text(SUMMARY_JSON, json.dumps(early_summary, indent=2))\n",
    "\n",
    "# Stop immediately if preflight found fatal conflicts.\n",
    "fatal_pre = [w for w in warnings_rows if str(w.get(\"level\", \"\")).upper() == \"ERROR\"]\n",
    "if fatal_pre:\n",
    "    raise RuntimeError(f\"Preflight failed with {len(fatal_pre)} ERROR(s). See {str(WARNINGS_CSV)}\")\n",
    "\n",
    "# -------------------------\n",
    "# 13) Copy clips into the train_enh2 folder (copy-only, no overwrite)\n",
    "# -------------------------\n",
    "# Copies only missing destination files; existing files are left untouched.\n",
    "print(\"\\nCopy stage: copying into train_enh2 (no overwrite)...\")\n",
    "\n",
    "copied = 0\n",
    "skipped_exists = 0\n",
    "copy_errors = 0\n",
    "\n",
    "for item in tqdm(copy_plan, desc=\"Copying clips\", dynamic_ncols=True):\n",
    "    sp = Path(item[\"src_path\"])\n",
    "    dp = Path(item[\"dst_path\"])\n",
    "\n",
    "    if not sp.exists():\n",
    "        # Protects against sources disappearing mid-run.\n",
    "        copy_errors += 1\n",
    "        add_warn(\"D7_TRAIN_ENH2\", \"ERROR\", \"SOURCE_CLIP_DISAPPEARED\", \"Source clip missing during copy stage\", clip_path=str(sp))\n",
    "        continue\n",
    "\n",
    "    if dp.exists():\n",
    "        skipped_exists += 1\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        shutil.copy2(sp, dp)\n",
    "        copied += 1\n",
    "    except Exception as e:\n",
    "        copy_errors += 1\n",
    "        add_warn(\"D7_TRAIN_ENH2\", \"ERROR\", \"COPY_FAILED\", \"Failed to copy file\", src_path=str(sp), dest_path=str(dp), error=repr(e))\n",
    "\n",
    "copy_stats = {\n",
    "    \"copied\": int(copied),\n",
    "    \"skipped_exists\": int(skipped_exists),\n",
    "    \"copy_errors\": int(copy_errors),\n",
    "    \"total_planned_files\": int(len(copy_plan)),\n",
    "    \"warnings_by_level\": count_by_level(warnings_rows),\n",
    "}\n",
    "\n",
    "# Persist warnings after the copy stage.\n",
    "atomic_write_csv(WARNINGS_CSV, pd.DataFrame(warnings_rows))\n",
    "\n",
    "# Stop if any fatal copy errors occurred.\n",
    "fatal_copy = [w for w in warnings_rows if str(w.get(\"level\", \"\")).upper() == \"ERROR\"]\n",
    "if fatal_copy:\n",
    "    run_config[\"copy_stats\"] = copy_stats\n",
    "    atomic_write_text(RUN_CONFIG_JSON, json.dumps(run_config, indent=2))\n",
    "    fail_summary = dict(early_summary)\n",
    "    fail_summary[\"status\"] = \"FAILED_DURING_COPY\"\n",
    "    fail_summary[\"copy_stats\"] = copy_stats\n",
    "    atomic_write_text(SUMMARY_JSON, json.dumps(fail_summary, indent=2))\n",
    "    raise RuntimeError(f\"Copy failed with {len(fatal_copy)} ERROR(s). See {str(WARNINGS_CSV)}\")\n",
    "\n",
    "# -------------------------\n",
    "# 14) Build the train_enh2 manifest\n",
    "# -------------------------\n",
    "# Creates a single manifest that points to the new train_enh2 clip paths.\n",
    "# - D7 rows: same clips renamed only by folder move\n",
    "# - D2 rows: new deterministic filenames, dataset set to D7, source_dataset set to D2\n",
    "d7_enh2 = d7_train[CANON_COLS].copy()\n",
    "d7_enh2[\"split\"] = \"train_enh2\"\n",
    "d7_enh2[\"dataset\"] = \"D7\"\n",
    "d7_enh2[\"clip_path\"] = d7_train[\"clip_path\"].astype(str).map(lambda p: str(TRAIN_ENH2_DIR / Path(p).name))\n",
    "d7_enh2[\"sample_id\"] = d7_enh2[\"clip_path\"].map(lambda p: Path(p).stem)\n",
    "d7_enh2[\"source_dataset\"] = d7_train.get(\"source_dataset\", pd.Series([\"D7\"] * len(d7_train))).astype(str).tolist()\n",
    "\n",
    "d2_enh2 = d2_sel_reset[CANON_COLS].copy()\n",
    "d2_enh2[\"split\"] = \"train_enh2\"\n",
    "d2_enh2[\"dataset\"] = \"D7\"\n",
    "\n",
    "# Recreate the deterministic output names so the manifest matches the copied files.\n",
    "new_paths, new_ids = [], []\n",
    "for j, row in d2_sel_reset.iterrows():\n",
    "    hc_pd = hc_pd_from_label_num(row[\"label_num\"])\n",
    "    spk_tok = safe_token(row[\"speaker_id\"], 32, \"NA\")\n",
    "    task_tok = safe_token(row[\"task\"], 12, \"0\")\n",
    "    out_name = f\"D7_D2add_train_enh2_{hc_pd}_{spk_tok}_{task_tok}_{j+1:06d}.wav\"\n",
    "    out_path = TRAIN_ENH2_DIR / out_name\n",
    "    new_paths.append(str(out_path))\n",
    "    new_ids.append(out_path.stem)\n",
    "\n",
    "d2_enh2[\"clip_path\"] = new_paths\n",
    "d2_enh2[\"sample_id\"] = new_ids\n",
    "d2_enh2[\"source_dataset\"] = \"D2\"\n",
    "\n",
    "# Combine D7 base train + selected D2 additions into one training manifest.\n",
    "train_enh2 = pd.concat([d7_enh2, d2_enh2], axis=0, ignore_index=True)\n",
    "train_enh2 = train_enh2[FINAL_COLS].copy()\n",
    "\n",
    "# Guard: do not allow the literal string \"NaN\" in output columns.\n",
    "for c in FINAL_COLS:\n",
    "    if train_enh2[c].dtype == object and (train_enh2[c] == \"NaN\").any():\n",
    "        raise RuntimeError(f\"Found literal string 'NaN' in column '{c}'.\")\n",
    "\n",
    "# Final check: every manifest clip_path must exist on disk.\n",
    "missing_enh2 = []\n",
    "for p in tqdm(train_enh2[\"clip_path\"].astype(str).tolist(), desc=\"Check TRAIN_ENH2 clip_path exists\", dynamic_ncols=True):\n",
    "    if not os.path.exists(p):\n",
    "        missing_enh2.append(p)\n",
    "        if len(missing_enh2) >= 25:\n",
    "            break\n",
    "require(len(missing_enh2) == 0, f\"train_enh2 manifest points to missing files. Examples: {missing_enh2[:25]}\")\n",
    "\n",
    "# -------------------------\n",
    "# 15) Write the manifest and final summaries\n",
    "# -------------------------\n",
    "# Outputs:\n",
    "# - manifest_train_enh2.csv (training input for the train_enh2 train+val cell)\n",
    "# - dataset_summary.json and preprocess_warnings.csv (run record)\n",
    "# - run_config.json updated with outputs and copy stats\n",
    "atomic_write_csv(MANIFEST_TRAIN_ENH2, train_enh2)\n",
    "\n",
    "final_summary = {\n",
    "    \"dataset\": \"D7\",\n",
    "    \"created_utc\": datetime.utcnow().isoformat(),\n",
    "    \"status\": \"SUCCESS\",\n",
    "    \"d7_out_root\": str(D7_OUT_ROOT),\n",
    "    \"train_enh2_dir\": str(TRAIN_ENH2_DIR),\n",
    "    \"manifest_train_enh2\": str(MANIFEST_TRAIN_ENH2),\n",
    "    \"selection\": run_config[\"selection\"],\n",
    "    \"copy_stats\": copy_stats,\n",
    "    \"counts_train_enh2\": summarize_counts(train_enh2, split_name=\"train_enh2\"),\n",
    "}\n",
    "\n",
    "atomic_write_text(SUMMARY_JSON, json.dumps(final_summary, indent=2))\n",
    "\n",
    "run_config[\"copy_stats\"] = copy_stats\n",
    "run_config[\"outputs\"] = {\n",
    "    \"train_enh2_dir\": str(TRAIN_ENH2_DIR),\n",
    "    \"manifest_train_enh2\": str(MANIFEST_TRAIN_ENH2),\n",
    "    \"dataset_summary_json\": str(SUMMARY_JSON),\n",
    "    \"preprocess_warnings_csv\": str(WARNINGS_CSV),\n",
    "    \"run_config_json\": str(RUN_CONFIG_JSON),\n",
    "}\n",
    "atomic_write_text(RUN_CONFIG_JSON, json.dumps(run_config, indent=2))\n",
    "\n",
    "print(\"\\n✅ D7 train_enh2 build complete (FIXED DENOMINATOR).\")\n",
    "print(\"- Train_enh2 folder:\", str(TRAIN_ENH2_DIR))\n",
    "print(\"- Manifest:\", str(MANIFEST_TRAIN_ENH2))\n",
    "print(\"- Summary:\", str(SUMMARY_JSON))\n",
    "print(\"- Warnings:\", str(WARNINGS_CSV))\n",
    "print(\"- Config:\", str(RUN_CONFIG_JSON))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "d6773f10456e4530acd0488331dda9e8",
      "edfa4c29d99e435f8bb1c150e093b214",
      "c5a8bfabd54544fc9eae761e6b01933d",
      "ab8ce21503d34417858086238a0223b8",
      "acbfbfc8d5ef437b85678c68c191d747",
      "3defe1274f4842b09711d1ee81db21d3",
      "72f525903a72431298787420436811c7",
      "c1879cc1ea2848188fe9be8eb3dbfab7",
      "b4e478ee313a4e0ca326e091cccbb64e",
      "8d573199471e4d719a0450b2c1853463",
      "d2c394ec350e446fade643d11862ba80",
      "b81b1c7c947e47e495a5ebcfc974fc7a",
      "22591734c9c648dcb87b55f05eac47cb",
      "9c7a7a5934fa434fbc539829c9d4138f",
      "d9a8b03d91024e97b9801dc37c696dc8",
      "492e0146e8154230a04b098774db7f66",
      "8619c2b3046f4634b69dbf30a7fea91c",
      "367a390cb5434806b082f22502fd3e34",
      "32d843473fd8405ba40c22da3b300cdb",
      "07ad5953cddf450f957c607fd0ae87cc",
      "276c3ef86e26496181c80a7d4fe9aed9",
      "5788301a7bce4fdcab3937a39d2a8a34",
      "3a018edffc9d4762be923fd049c4077b",
      "cd7d24e4b5e84594ac976a888d8e020c",
      "c34c4430fe1d4931a9207b2840bc881c",
      "115b7e3cdef44da284ab0eaafff8e4b7",
      "1d29bed30244443b84d4b9520fd7e5b3",
      "5c41f156545c446881b98b89159967b8",
      "da66ce4a2b814aeca8522b1c2ffea701",
      "e9a17d3382974b5c90833aeaa8372315",
      "1289d5f2101b48128c1b1e8e3316a12d",
      "2aac54d4697b405f82e1d9d378b21093",
      "54937beef0b442d1b6b9e755a61e79e4",
      "176ab1d6676441fa8a9834f0632638c7",
      "7ad80287a16b4ccbb7a6c31e81c65652",
      "07ebd99d9e204369898b18c6f0d021e6",
      "d0184f51cb8643148734f3821ed60eed",
      "d506b7d1ad444d5fbe774424ddd7db84",
      "4319b1c9695848348dba9b5592fb6b79",
      "9183bb468b8340c8bf133ce630bdf329",
      "2aeebedf01064adf94256a0b77b45395",
      "b846c973dbed44238fb880b23c2170ef",
      "8743911663a9407789f9b1fda5d4b776",
      "6cc8e9999e0f40638558e7aab306045b",
      "c3e3780fba2d4368acadffe55a81c758",
      "efd14c8fd8a64c409c3ef4e5576c90ec",
      "077d7816911a48e9b0d63fce9ff38af9",
      "6179791e943948e582a25814bc29bf31",
      "8992d7c1096d46b89990e1be5c661d9c",
      "c83fc829cb694d77a01e78305b42c790",
      "ec4b2e6dcdb24708be49137ac301b5f7",
      "f03f9630608f44ef96fb2fd8153a3565",
      "2869078a7c93488e90ccb676498edec9",
      "36bfaf5de57f4095ab3553bef3519dd2",
      "6b0aa7d169cc47e48b752d685d981d09"
     ]
    },
    "id": "widYhcWv-uMz"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "The following cell trains and validates the **D7 enhanced model (train_enh2)** using only two existing inputs: (1) `manifest_train_enh2.csv` filtered to rows where `split == \"train_enh2\"` for training, and (2) the standard D7 `manifest_all.csv` filtered to rows where `split == \"val\"` for validation. No new data splits are created. The cell simply reads the prepared manifests, checks that required columns are present, confirms that the validation data truly belongs to **D7**, and stops immediately if any listed audio file paths are missing.\n",
    "\n",
    "The model uses a **frozen wav2vec2 backbone**, so the main speech encoder is not updated during training. Only small trainable components are updated: **two separate classification heads** (one for vowel clips and one for all other speech) and a small **LayerNorm plus Dropout block** in front of each head. Each clip is assigned to a task group using a strict rule: **`task == \"vowl\"` is treated as “vowel”, everything else as “other”**. During data loading, an attention mask is created so that for vowel clips the model **ignores trailing padded silence** (based on a small amplitude threshold), while other clips use full attention. This reduces the chance of learning from padding instead of real speech.\n",
    "\n",
    "Before training starts, the cell automatically finds the **most recent baseline D7 train and validation experiment** and uses it to **initialize the trainable heads**. A guard ensures that an enhanced run is not used as the baseline by excluding any prior experiment whose `summary_trainval.json` shows that it was trained on a `train_enh` split. Training is run for **three fixed random seeds** (1337, 2024, 7777). For each seed, the model trains for up to 10 epochs with early stopping (patience of 2 epochs), using gradient accumulation to reach an effective batch size of 64. After every epoch, the model is evaluated on the D7 validation set and the **best epoch is selected based on validation AUROC**.\n",
    "\n",
    "At the best validation AUROC epoch for each seed, the cell computes a **validation-optimal probability threshold** using **Youden’s J statistic** (the ROC point that maximizes TPR minus FPR). It also computes threshold-based validation metrics at both **0.5** and the **optimal threshold**, including accuracy, precision, recall or sensitivity, specificity, F1, MCC, and Fisher exact test p-value. For each seed, the cell saves the best head weights (`best_heads.pt`), a validation ROC curve, confusion matrix plots, and a detailed `metrics.json` describing the settings, the baseline initialization used, and the results.\n",
    "\n",
    "After all three seeds complete, the cell writes a single experiment-level `summary_trainval.json` inside a new timestamped folder under `trainval_runs/exp_<tag>_<timestamp>/`. This summary includes per-seed AUROC values, the mean AUROC with a 95% confidence interval across seeds, and the **canonical validation threshold summary**, stored as:\n",
    "\n",
    "* `val_optimal_threshold.by_seed`\n",
    "* `val_optimal_threshold.mean_sd.mean`\n",
    "* `val_optimal_threshold.mean_sd.sd`\n",
    "\n",
    "The full experiment summary is also appended as a new line to `trainval_runs/history_index.jsonl`, which keeps a running record of all experiments. The cell finishes by **unassigning the Colab runtime** to shut down the GPU instance."
   ],
   "metadata": {
    "id": "8PQq_Oatvz_V"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# =========================\n",
    "# D7 train_enh2 Train + Val (Frozen Backbone, Two Heads)\n",
    "# Inputs: D7 train_enh2 train manifest + D7 val split from the full manifest\n",
    "# Outputs: Per-seed best heads, per-seed metrics and plots, experiment summary, history log\n",
    "# =========================\n",
    "# Train + Val ONLY (CRASH-PROOF, WITH PROGRESS + HISTORY) — D7 ENHANCED (train_enh2)\n",
    "# - Frozen Wav2Vec2 backbone\n",
    "# - Two task heads + tiny LayerNorm + Dropout pre-head (trainable heads only)\n",
    "# - Uses ONLY: <DX_OUT_ROOT>/manifests/manifest_train_enh2.csv  (split == \"train_enh2\")\n",
    "#            + <DX_OUT_ROOT>/manifests/manifest_all.csv         (split == \"val\")\n",
    "# - Initializes heads from MOST RECENT BASELINE D7 trainval experiment under:\n",
    "#     <DX_OUT_ROOT>/trainval_runs/exp_*/run_D7_seed{seed}/best_heads.pt\n",
    "#   Baseline guard: excludes any exp whose summary shows train_manifest_used contains \"train_enh\"\n",
    "# - Writes ONLY under: <DX_OUT_ROOT>/trainval_runs/exp_<tag>_<timestamp>/\n",
    "# - Saves best-epoch plots + metrics per seed, plus per-experiment summary + history_index.jsonl\n",
    "# - Adds additional metrics: accuracy, precision, recall/sensitivity, specificity, F1, MCC, Fisher p-value\n",
    "# - Determines VAL-opt threshold via Youden J at the BEST-AUROC epoch (per seed)\n",
    "# - Stores thresholds ONLY as the canonical aggregate in summary_trainval.json:\n",
    "#     val_optimal_threshold.by_seed\n",
    "#     val_optimal_threshold.mean_sd.mean\n",
    "#     val_optimal_threshold.mean_sd.sd\n",
    "# - Ends by unassigning Colab runtime (L4) with messages\n",
    "#\n",
    "# NOTES\n",
    "# - Train rows come from the prebuilt train_enh2 manifest (no resplitting here)\n",
    "# - Val rows come from the standard D7 val split in the full manifest\n",
    "# - dataset_id is inferred from the val manifest (expected \"D7\") and used for folder naming\n",
    "# =========================\n",
    "\n",
    "import os, json, math, random, time, warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import soundfile as sf\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from transformers import Wav2Vec2Model\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, roc_curve,\n",
    "    confusion_matrix, accuracy_score,\n",
    "    precision_recall_fscore_support,\n",
    "    matthews_corrcoef\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# -------------------------\n",
    "# 0) Safety check: prevent importing the wrong torch/transformers\n",
    "# -------------------------\n",
    "# Colab can accidentally import a local file named torch.py or transformers.py.\n",
    "# Fail early so results are not silently corrupted.\n",
    "if os.path.exists(\"/content/torch.py\") or os.path.exists(\"/content/torch/__init__.py\"):\n",
    "    raise RuntimeError(\"Found local /content/torch.py or /content/torch/ that shadows PyTorch. Rename/remove it and restart runtime.\")\n",
    "if os.path.exists(\"/content/transformers.py\") or os.path.exists(\"/content/transformers/__init__.py\"):\n",
    "    raise RuntimeError(\"Found local /content/transformers.py or /content/transformers/ that shadows Hugging Face Transformers. Rename/remove it and restart runtime.\")\n",
    "\n",
    "# -------------------------\n",
    "# 1) Mount Drive (safe if already mounted)\n",
    "# -------------------------\n",
    "# Needed to read manifests and write training outputs.\n",
    "try:\n",
    "    from google.colab import drive  # type: ignore\n",
    "    if not os.path.isdir(\"/content/drive/MyDrive\"):\n",
    "        drive.mount(\"/content/drive\")\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# -------------------------\n",
    "# 2) Set run root and input manifest files\n",
    "# -------------------------\n",
    "# Inputs: train_enh2 manifest (train split) + full manifest (val split).\n",
    "D7_OUT_ROOT_FALLBACK = \"/content/drive/MyDrive/AI_PD_Project/Datasets/D7-Multilingual (D1_D4_D5v2_D6)/preprocessed_v1\"\n",
    "DX_OUT_ROOT = str(globals().get(\"DX_OUT_ROOT\", D7_OUT_ROOT_FALLBACK))\n",
    "globals()[\"DX_OUT_ROOT\"] = DX_OUT_ROOT\n",
    "\n",
    "MANIFEST_ALL = f\"{DX_OUT_ROOT}/manifests/manifest_all.csv\"\n",
    "\n",
    "# train_enh2 manifest + split label used inside the file\n",
    "MANIFEST_TRAIN_ENH2 = f\"{DX_OUT_ROOT}/manifests/manifest_train_enh2.csv\"\n",
    "TRAIN_SPLIT_NAME = \"train_enh2\"\n",
    "\n",
    "# -------------------------\n",
    "# 3) Experiment naming and output folder\n",
    "# -------------------------\n",
    "# Outputs: a new exp_* folder containing 3 seed runs + an experiment summary.\n",
    "EXPERIMENT_TAG = \"frozen_LNDO_trainEnh2_initBaseline\"\n",
    "RUN_STAMP = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "TRAINVAL_ROOT = Path(DX_OUT_ROOT) / \"trainval_runs\"\n",
    "EXP_ROOT = TRAINVAL_ROOT / f\"exp_{EXPERIMENT_TAG}_{RUN_STAMP}\"\n",
    "EXP_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# -------------------------\n",
    "# 4) Fixed settings (kept stable for repeatable comparisons)\n",
    "# -------------------------\n",
    "# These settings define training length, batch behavior, and model configuration.\n",
    "MAX_EPOCHS     = 10\n",
    "EFFECTIVE_BS   = 64\n",
    "PER_DEVICE_BS  = 16\n",
    "GRAD_ACCUM     = max(1, EFFECTIVE_BS // PER_DEVICE_BS)\n",
    "\n",
    "LR             = 1e-3\n",
    "PATIENCE       = 2\n",
    "SEEDS          = [1337, 2024, 7777]\n",
    "\n",
    "BACKBONE_CKPT  = \"facebook/wav2vec2-base\"\n",
    "SR_EXPECTED    = 16000\n",
    "TINY_THRESH    = 1e-4\n",
    "\n",
    "DROPOUT_P      = 0.2\n",
    "\n",
    "NUM_WORKERS    = 0\n",
    "PIN_MEMORY     = False\n",
    "\n",
    "VOWEL_TASK_VALUE = \"vowl\"\n",
    "\n",
    "# Pick GPU if available, otherwise run on CPU.\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "try:\n",
    "    torch.set_float32_matmul_precision(\"high\")\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# Reduce noisy warnings that do not affect results.\n",
    "warnings.filterwarnings(\"ignore\", message=\"None of the inputs have requires_grad=True. Gradients will be None\")\n",
    "warnings.filterwarnings(\"ignore\", message=\"Passing `gradient_checkpointing` to a config initialization is deprecated\")\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"huggingface_hub\")\n",
    "\n",
    "# Print key run info (kept as-is).\n",
    "print(\"DX_OUT_ROOT:\", DX_OUT_ROOT)\n",
    "print(\"MANIFEST_TRAIN_ENH2:\", MANIFEST_TRAIN_ENH2)\n",
    "print(\"TRAIN_SPLIT_NAME:\", TRAIN_SPLIT_NAME)\n",
    "print(\"MANIFEST_ALL (val source):\", MANIFEST_ALL)\n",
    "print(\"DEVICE:\", DEVICE)\n",
    "if DEVICE.type == \"cuda\":\n",
    "    print(\"GPU:\", torch.cuda.get_device_name(0))\n",
    "print(\"PER_DEVICE_BS:\", PER_DEVICE_BS, \"| GRAD_ACCUM:\", GRAD_ACCUM, \"| EFFECTIVE_BS:\", PER_DEVICE_BS * GRAD_ACCUM)\n",
    "print(\"NUM_WORKERS:\", NUM_WORKERS, \"| PIN_MEMORY:\", PIN_MEMORY)\n",
    "print(\"EXPERIMENT_TAG:\", EXPERIMENT_TAG, \"| RUN_STAMP:\", RUN_STAMP)\n",
    "print(\"EXP_ROOT:\", str(EXP_ROOT))\n",
    "\n",
    "# -------------------------\n",
    "# 5) Read manifests and build train/val tables\n",
    "# -------------------------\n",
    "# Inputs:\n",
    "# - Train: manifest_train_enh2.csv filtered to split == train_enh2\n",
    "# - Val:   manifest_all.csv filtered to split == val\n",
    "# Output: train_df and val_df with a consistent set of columns.\n",
    "if not os.path.exists(MANIFEST_TRAIN_ENH2):\n",
    "    raise FileNotFoundError(\n",
    "        \"Missing manifest_train_enh2.csv at:\\n\"\n",
    "        f\"  {MANIFEST_TRAIN_ENH2}\\n\"\n",
    "        \"Run the train_enh2 builder first.\"\n",
    "    )\n",
    "if not os.path.exists(MANIFEST_ALL):\n",
    "    raise FileNotFoundError(\n",
    "        \"Missing manifest_all.csv at:\\n\"\n",
    "        f\"  {MANIFEST_ALL}\\n\"\n",
    "        \"Confirm D7 merge-builder wrote manifests/manifest_all.csv under DX_OUT_ROOT.\"\n",
    "    )\n",
    "\n",
    "m_train = pd.read_csv(MANIFEST_TRAIN_ENH2)\n",
    "m_all   = pd.read_csv(MANIFEST_ALL)\n",
    "\n",
    "# Basic schema check (only the columns needed later).\n",
    "req_cols = {\"split\", \"clip_path\", \"label_num\", \"task\"}\n",
    "for name, df in [(\"manifest_train_enh2\", m_train), (\"manifest_all\", m_all)]:\n",
    "    missing = [c for c in sorted(req_cols) if c not in df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"{name} missing required columns: {missing}. Found: {list(df.columns)}\")\n",
    "\n",
    "# Split selection (train_enh2 vs val).\n",
    "m_train = m_train[m_train[\"split\"].astype(str) == TRAIN_SPLIT_NAME].copy()\n",
    "m_val   = m_all[m_all[\"split\"].astype(str) == \"val\"].copy()\n",
    "\n",
    "if len(m_train) == 0:\n",
    "    raise RuntimeError(f\"After filtering manifest_train_enh2.csv to split=={TRAIN_SPLIT_NAME!r}, 0 rows remain.\")\n",
    "if len(m_val) == 0:\n",
    "    raise RuntimeError(\"After filtering manifest_all.csv to split=='val', 0 rows remain.\")\n",
    "\n",
    "# Infer dataset_id from val (expected D7); used for naming run folders.\n",
    "if \"dataset\" in m_val.columns and m_val[\"dataset\"].notna().any():\n",
    "    dataset_id = str(m_val[\"dataset\"].astype(str).value_counts(dropna=True).idxmax())\n",
    "    m_val = m_val[m_val[\"dataset\"].astype(str) == dataset_id].copy()\n",
    "else:\n",
    "    dataset_id = \"DX\"\n",
    "\n",
    "# Hard guard: this cell is only for D7.\n",
    "if dataset_id != \"D7\":\n",
    "    raise RuntimeError(f\"Dataset inferred from VAL manifest is {dataset_id!r}. Expected 'D7'. Check DX_OUT_ROOT/manifests/manifest_all.csv.\")\n",
    "\n",
    "# Keep a stable minimal column set across train and val.\n",
    "keep_cols = [\"clip_path\", \"label_num\", \"task\", \"speaker_id\", \"duration_sec\", \"split\"]\n",
    "for df in [m_train, m_val]:\n",
    "    for c in keep_cols:\n",
    "        if c not in df.columns:\n",
    "            df[c] = np.nan\n",
    "\n",
    "m_train = m_train[keep_cols].copy()\n",
    "m_val   = m_val[keep_cols].copy()\n",
    "\n",
    "train_df = m_train.copy().reset_index(drop=True)\n",
    "val_df   = m_val.copy().reset_index(drop=True)\n",
    "\n",
    "# Quick split stats (kept as-is).\n",
    "print(f\"\\nDataset inferred (from VAL): {dataset_id}\")\n",
    "print(f\"Train rows ({TRAIN_SPLIT_NAME}): {len(train_df)} | Val rows: {len(val_df)}\")\n",
    "print(\"Train label counts:\", train_df[\"label_num\"].value_counts(dropna=False).to_dict())\n",
    "print(\"Val label counts:\",   val_df[\"label_num\"].value_counts(dropna=False).to_dict())\n",
    "\n",
    "# -------------------------\n",
    "# 6) Fail fast if audio files are missing\n",
    "# -------------------------\n",
    "# This prevents long training runs from failing halfway through.\n",
    "def _fail_fast_missing_paths(df: pd.DataFrame, name: str):\n",
    "    missing_paths = []\n",
    "    for p in tqdm(df[\"clip_path\"].astype(str).tolist(), desc=f\"Check {name} clip_path exists\", dynamic_ncols=True):\n",
    "        if not os.path.exists(p):\n",
    "            missing_paths.append(p)\n",
    "            if len(missing_paths) >= 10:\n",
    "                break\n",
    "    if missing_paths:\n",
    "        raise FileNotFoundError(f\"{name}: missing clip_path(s). Examples: {missing_paths}\")\n",
    "\n",
    "_fail_fast_missing_paths(train_df, \"TRAIN_ENH2\")\n",
    "_fail_fast_missing_paths(val_df, \"VAL\")\n",
    "\n",
    "# -------------------------\n",
    "# 7) Map each clip to a task group\n",
    "# -------------------------\n",
    "# Rule: task == \"vowl\" is treated as vowel, everything else as other.\n",
    "def _task_group(task_val) -> str:\n",
    "    return \"vowel\" if str(task_val) == VOWEL_TASK_VALUE else \"other\"\n",
    "\n",
    "train_df[\"task_group\"] = train_df[\"task\"].apply(_task_group)\n",
    "val_df[\"task_group\"]   = val_df[\"task\"].apply(_task_group)\n",
    "\n",
    "# -------------------------\n",
    "# 8) Dataset and batch collation\n",
    "# -------------------------\n",
    "# Inputs: the manifest tables (train_df, val_df).\n",
    "# Outputs: padded batches of waveforms, attention masks, labels, and task_group.\n",
    "class AudioManifestDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Reads one audio clip and builds a sample-level attention mask.\n",
    "\n",
    "    Mask rule:\n",
    "    - vowel: mask trailing near-silence so padding does not drive learning\n",
    "    - other: keep full attention\n",
    "    \"\"\"\n",
    "    def __init__(self, df: pd.DataFrame):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        row = self.df.iloc[idx]\n",
    "        clip_path = str(row[\"clip_path\"])\n",
    "        label = int(row[\"label_num\"])\n",
    "        task_group = str(row[\"task_group\"])\n",
    "\n",
    "        if not os.path.exists(clip_path):\n",
    "            raise FileNotFoundError(f\"Missing clip_path: {clip_path}\")\n",
    "\n",
    "        # Load audio, convert to mono if needed, enforce dtype.\n",
    "        y, sr = sf.read(clip_path, always_2d=False)\n",
    "        if isinstance(y, np.ndarray) and y.ndim > 1:\n",
    "            y = np.mean(y, axis=1)\n",
    "        y = np.asarray(y, dtype=np.float32)\n",
    "\n",
    "        # Guard: model assumes a single sample rate.\n",
    "        if int(sr) != SR_EXPECTED:\n",
    "            raise ValueError(f\"Sample rate mismatch (expected {SR_EXPECTED}): {clip_path} has sr={sr}\")\n",
    "\n",
    "        # Build attention mask in sample space.\n",
    "        attn = np.ones((len(y),), dtype=np.int64)\n",
    "\n",
    "        if task_group == \"vowel\":\n",
    "            # Find the last sample that is not near-zero, then mask the rest.\n",
    "            k = -1\n",
    "            for j in range(len(y) - 1, -1, -1):\n",
    "                if abs(float(y[j])) > TINY_THRESH:\n",
    "                    k = j\n",
    "                    break\n",
    "            if k >= 0:\n",
    "                attn[:k+1] = 1\n",
    "                attn[k+1:] = 0\n",
    "            else:\n",
    "                attn[:] = 1\n",
    "        else:\n",
    "            attn[:] = 1\n",
    "\n",
    "        return {\n",
    "            \"input_values\": torch.from_numpy(y),                 # float32 [T]\n",
    "            \"attention_mask\": torch.from_numpy(attn),            # int64   [T]\n",
    "            \"labels\": torch.tensor(label, dtype=torch.long),     # int64   []\n",
    "            \"task_group\": task_group,                            # str\n",
    "        }\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"Pads waveforms and masks to the longest clip in the batch.\"\"\"\n",
    "    max_len = int(max(b[\"input_values\"].numel() for b in batch))\n",
    "    input_vals, attn_masks, labels, task_groups = [], [], [], []\n",
    "    for b in batch:\n",
    "        x = b[\"input_values\"]\n",
    "        a = b[\"attention_mask\"]\n",
    "        pad = max_len - x.numel()\n",
    "        if pad > 0:\n",
    "            x = torch.cat([x, torch.zeros(pad, dtype=x.dtype)], dim=0)\n",
    "            a = torch.cat([a, torch.zeros(pad, dtype=a.dtype)], dim=0)\n",
    "        input_vals.append(x)\n",
    "        attn_masks.append(a)\n",
    "        labels.append(b[\"labels\"])\n",
    "        task_groups.append(b[\"task_group\"])\n",
    "    return {\n",
    "        \"input_values\": torch.stack(input_vals, dim=0),      # [B,T]\n",
    "        \"attention_mask\": torch.stack(attn_masks, dim=0),    # [B,T]\n",
    "        \"labels\": torch.stack(labels, dim=0),                # [B]\n",
    "        \"task_group\": task_groups,                           # list[str]\n",
    "    }\n",
    "\n",
    "# -------------------------\n",
    "# 9) Model: frozen backbone with two task-specific heads\n",
    "# -------------------------\n",
    "# Backbone: wav2vec2-base (frozen)\n",
    "# Trainable: LayerNorm+Dropout blocks + two linear heads (vowel/other)\n",
    "class Wav2Vec2TwoHeadClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    Frozen backbone with two heads.\n",
    "    Only head-related layers are updated during training.\n",
    "    \"\"\"\n",
    "    def __init__(self, ckpt: str, dropout_p: float):\n",
    "        super().__init__()\n",
    "        self.backbone = Wav2Vec2Model.from_pretrained(\n",
    "            ckpt,\n",
    "            use_safetensors=True,\n",
    "            local_files_only=False\n",
    "        )\n",
    "        for p in self.backbone.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "        H = int(self.backbone.config.hidden_size)\n",
    "        self.pre_vowel = nn.Sequential(nn.LayerNorm(H), nn.Dropout(float(dropout_p)))\n",
    "        self.pre_other = nn.Sequential(nn.LayerNorm(H), nn.Dropout(float(dropout_p)))\n",
    "\n",
    "        self.head_vowel = nn.Linear(H, 2)\n",
    "        self.head_other = nn.Linear(H, 2)\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    def masked_mean_pool(self, last_hidden, attn_mask_samples):\n",
    "        # Convert sample-level mask to feature-frame mask, then do masked mean pooling.\n",
    "        feat_mask = self.backbone._get_feature_vector_attention_mask(last_hidden.shape[1], attn_mask_samples)\n",
    "        feat_mask = feat_mask.to(last_hidden.device).unsqueeze(-1).type_as(last_hidden)\n",
    "        masked = last_hidden * feat_mask\n",
    "        denom = feat_mask.sum(dim=1).clamp(min=1.0)\n",
    "        return masked.sum(dim=1) / denom\n",
    "\n",
    "    def forward(self, input_values, attention_mask, labels, task_group):\n",
    "        # Feature extraction is frozen; gradients flow only through the heads.\n",
    "        with torch.no_grad():\n",
    "            out = self.backbone(input_values=input_values, attention_mask=attention_mask)\n",
    "            last_hidden = out.last_hidden_state  # [B,T',H]\n",
    "\n",
    "        pooled = self.masked_mean_pool(last_hidden, attention_mask).float()  # [B,H]\n",
    "\n",
    "        z_v = self.pre_vowel(pooled)\n",
    "        z_o = self.pre_other(pooled)\n",
    "\n",
    "        logits_v = self.head_vowel(z_v)\n",
    "        logits_o = self.head_other(z_o)\n",
    "\n",
    "        # Select the head based on task_group for each sample in the batch.\n",
    "        is_vowel = torch.tensor([tg == \"vowel\" for tg in task_group], device=pooled.device, dtype=torch.bool)\n",
    "        logits = logits_o.clone()\n",
    "        if is_vowel.any():\n",
    "            logits[is_vowel] = logits_v[is_vowel]\n",
    "\n",
    "        loss = self.loss_fn(logits, labels)\n",
    "        return loss, logits\n",
    "\n",
    "# -------------------------\n",
    "# 9.5) Head initialization from baseline run\n",
    "# -------------------------\n",
    "# Starts train_enh2 heads from the most recent baseline heads for the same seed.\n",
    "def load_heads_into_model(model: Wav2Vec2TwoHeadClassifier, best_heads_path: Path):\n",
    "    if not best_heads_path.exists():\n",
    "        raise FileNotFoundError(f\"Missing best_heads.pt: {str(best_heads_path)}\")\n",
    "    state = torch.load(str(best_heads_path), map_location=\"cpu\")\n",
    "    model.pre_vowel.load_state_dict(state[\"pre_vowel\"], strict=True)\n",
    "    model.pre_other.load_state_dict(state[\"pre_other\"], strict=True)\n",
    "    model.head_vowel.load_state_dict(state[\"head_vowel\"], strict=True)\n",
    "    model.head_other.load_state_dict(state[\"head_other\"], strict=True)\n",
    "    return model\n",
    "\n",
    "# -------------------------\n",
    "# 10) Metrics and plotting helpers\n",
    "# -------------------------\n",
    "# Outputs: AUROC, threshold-based metrics, optional Youden-J threshold, and PNG plots.\n",
    "def compute_auc(y_true, y_prob):\n",
    "    y_true = np.asarray(y_true, dtype=np.int64)\n",
    "    y_prob = np.asarray(y_prob, dtype=np.float64)\n",
    "    if len(np.unique(y_true)) < 2:\n",
    "        return float(\"nan\")\n",
    "    return float(roc_auc_score(y_true, y_prob))\n",
    "\n",
    "def compute_threshold_metrics(y_true, y_prob, thr=0.5):\n",
    "    # Converts probabilities into predicted labels, then reports common classification metrics.\n",
    "    y_true = np.asarray(y_true, dtype=np.int64)\n",
    "    y_prob = np.asarray(y_prob, dtype=np.float64)\n",
    "    y_pred = (y_prob >= float(thr)).astype(np.int64)\n",
    "\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=[0, 1])\n",
    "    tn, fp, fn, tp = (cm.ravel().tolist() if cm.size == 4 else [0, 0, 0, 0])\n",
    "\n",
    "    acc = float(accuracy_score(y_true, y_pred))\n",
    "    prec, rec, f1, _ = precision_recall_fscore_support(y_true, y_pred, average=\"binary\", zero_division=0)\n",
    "    mcc = float(matthews_corrcoef(y_true, y_pred)) if len(np.unique(y_true)) > 1 else float(\"nan\")\n",
    "\n",
    "    sensitivity = float(rec)\n",
    "    specificity = float(tn / (tn + fp)) if (tn + fp) > 0 else float(\"nan\")\n",
    "\n",
    "    # Fisher exact test on the 2x2 confusion matrix (if available).\n",
    "    p_value = float(\"nan\")\n",
    "    try:\n",
    "        from scipy.stats import fisher_exact  # type: ignore\n",
    "        _, p_value = fisher_exact([[tn, fp], [fn, tp]], alternative=\"two-sided\")\n",
    "        p_value = float(p_value)\n",
    "    except Exception:\n",
    "        p_value = float(\"nan\")\n",
    "\n",
    "    return {\n",
    "        \"threshold\": float(thr),\n",
    "        \"tn\": int(tn), \"fp\": int(fp), \"fn\": int(fn), \"tp\": int(tp),\n",
    "        \"accuracy\": float(acc),\n",
    "        \"precision\": float(prec),\n",
    "        \"recall\": float(rec),\n",
    "        \"f1\": float(f1),\n",
    "        \"sensitivity\": float(sensitivity),\n",
    "        \"specificity\": float(specificity),\n",
    "        \"mcc\": float(mcc),\n",
    "        \"p_value_fisher\": float(p_value),\n",
    "    }\n",
    "\n",
    "def compute_youden_j_threshold(y_true, y_prob):\n",
    "    # Picks the threshold that maximizes (TPR - FPR) on the ROC curve.\n",
    "    y_true = np.asarray(y_true, dtype=np.int64)\n",
    "    y_prob = np.asarray(y_prob, dtype=np.float64)\n",
    "    if len(np.unique(y_true)) < 2:\n",
    "        return float(\"nan\"), {\"youden_j\": float(\"nan\"), \"tpr\": float(\"nan\"), \"fpr\": float(\"nan\")}\n",
    "    fpr, tpr, thr = roc_curve(y_true, y_prob)\n",
    "    j = tpr - fpr\n",
    "    idx = int(np.argmax(j))\n",
    "    return float(thr[idx]), {\"youden_j\": float(j[idx]), \"tpr\": float(tpr[idx]), \"fpr\": float(fpr[idx])}\n",
    "\n",
    "def save_roc_curve_png(y_true, y_prob, out_png):\n",
    "    # Saves a simple ROC curve image for the best epoch (val).\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_prob)\n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr)\n",
    "    plt.plot([0, 1], [0, 1], linestyle=\"--\")\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(\"ROC Curve (Val)\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_png, dpi=150)\n",
    "    plt.close()\n",
    "\n",
    "def save_confusion_png(y_true, y_prob, out_png, thr=0.5):\n",
    "    # Saves a confusion matrix image for a chosen threshold.\n",
    "    y_true = np.asarray(y_true, dtype=np.int64)\n",
    "    y_prob = np.asarray(y_prob, dtype=np.float64)\n",
    "    y_pred = (y_prob >= float(thr)).astype(int)\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=[0, 1])\n",
    "\n",
    "    plt.figure()\n",
    "    plt.imshow(cm)\n",
    "    plt.xticks([0, 1], [\"Healthy(0)\", \"PD(1)\"])\n",
    "    plt.yticks([0, 1], [\"Healthy(0)\", \"PD(1)\"])\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"True\")\n",
    "    plt.title(f\"Confusion Matrix (Val, thr={thr:.4f})\")\n",
    "    for (i, j), v in np.ndenumerate(cm):\n",
    "        plt.text(j, i, str(v), ha=\"center\", va=\"center\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_png, dpi=150)\n",
    "    plt.close()\n",
    "\n",
    "def mean_sd(vals):\n",
    "    # Small helper for mean and sample SD (used for 3-seed summaries).\n",
    "    vals = np.asarray(vals, dtype=np.float64)\n",
    "    mu = float(np.nanmean(vals)) if np.any(~np.isnan(vals)) else float(\"nan\")\n",
    "    sd = float(np.nanstd(vals, ddof=1)) if np.sum(~np.isnan(vals)) > 1 else 0.0\n",
    "    return mu, sd\n",
    "\n",
    "# -------------------------\n",
    "# 11) Seed control for repeatable runs\n",
    "# -------------------------\n",
    "def set_all_seeds(seed: int):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# -------------------------\n",
    "# 11.5) Find the most recent BASELINE experiment for head init\n",
    "# -------------------------\n",
    "# Baseline is detected by reading summary_trainval.json and rejecting any run that used train_enh manifests.\n",
    "BASELINE_TRAINVAL_ROOT = Path(DX_OUT_ROOT) / \"trainval_runs\"\n",
    "if not BASELINE_TRAINVAL_ROOT.exists():\n",
    "    raise FileNotFoundError(f\"Missing trainval_runs folder under DX_OUT_ROOT: {str(BASELINE_TRAINVAL_ROOT)}\")\n",
    "\n",
    "exp_dirs = sorted([p for p in BASELINE_TRAINVAL_ROOT.glob(\"exp_*\") if p.is_dir()],\n",
    "                  key=lambda p: p.stat().st_mtime, reverse=True)\n",
    "if not exp_dirs:\n",
    "    raise FileNotFoundError(f\"No exp_* folders found under: {str(BASELINE_TRAINVAL_ROOT)}\")\n",
    "\n",
    "train_dataset_id = \"D7\"\n",
    "\n",
    "def _is_baseline_exp(exp_path: Path) -> bool:\n",
    "    # Uses summary_trainval.json as the record of which train manifest was used.\n",
    "    summary_path = exp_path / \"summary_trainval.json\"\n",
    "    if not summary_path.exists():\n",
    "        return False\n",
    "    try:\n",
    "        with open(summary_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            s = json.load(f)\n",
    "        train_manifest_used = str(s.get(\"train_manifest_used\", \"\")).lower()\n",
    "        if \"train_enh\" in train_manifest_used or \"manifest_train_enh\" in train_manifest_used:\n",
    "            return False\n",
    "        return True\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "def _has_all_seeds(exp_path: Path, dataset_id: str, seeds: list) -> bool:\n",
    "    # Ensures each seed has a best_heads.pt file.\n",
    "    for s in seeds:\n",
    "        p = exp_path / f\"run_{dataset_id}_seed{s}\" / \"best_heads.pt\"\n",
    "        if not p.exists():\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "baseline_exp = None\n",
    "for ed in exp_dirs:\n",
    "    if ed.resolve() == EXP_ROOT.resolve():\n",
    "        continue\n",
    "    if _is_baseline_exp(ed) and _has_all_seeds(ed, train_dataset_id, SEEDS):\n",
    "        baseline_exp = ed\n",
    "        break\n",
    "\n",
    "if baseline_exp is None:\n",
    "    raise FileNotFoundError(\n",
    "        \"Could not find a BASELINE D7 trainval experiment with all 3 best_heads.pt files.\\n\"\n",
    "        \"Baseline guard excludes experiments whose summary_trainval.json shows train_manifest_used contains 'train_enh'.\\n\"\n",
    "        f\"Searched under: {str(BASELINE_TRAINVAL_ROOT)}/exp_*/run_D7_seedXXXX/best_heads.pt\"\n",
    "    )\n",
    "\n",
    "baseline_summary_path = baseline_exp / \"summary_trainval.json\"\n",
    "with open(baseline_summary_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    baseline_summary = json.load(f)\n",
    "\n",
    "# Print baseline selection (kept as-is).\n",
    "print(\"\\nBaseline initialization experiment selected:\")\n",
    "print(\" \", str(baseline_exp))\n",
    "print(\" \", \"summary:\", str(baseline_summary_path))\n",
    "print(\" \", \"train_manifest_used (baseline):\", baseline_summary.get(\"train_manifest_used\", \"NA\"))\n",
    "\n",
    "# -------------------------\n",
    "# 12) One seed run: train until no improvement, keep best epoch\n",
    "# -------------------------\n",
    "# Outputs per seed: best_heads.pt, metrics.json, ROC and confusion plots.\n",
    "def run_trainval_once(seed: int):\n",
    "    set_all_seeds(seed)\n",
    "\n",
    "    run_dir = EXP_ROOT / f\"run_{dataset_id}_seed{seed}\"\n",
    "    run_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Build datasets and loaders for this seed.\n",
    "    train_ds = AudioManifestDataset(train_df)\n",
    "    val_ds   = AudioManifestDataset(val_df)\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_ds,\n",
    "        batch_size=PER_DEVICE_BS,\n",
    "        shuffle=True,\n",
    "        num_workers=NUM_WORKERS,\n",
    "        pin_memory=PIN_MEMORY,\n",
    "        collate_fn=collate_fn\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_ds,\n",
    "        batch_size=PER_DEVICE_BS,\n",
    "        shuffle=False,\n",
    "        num_workers=NUM_WORKERS,\n",
    "        pin_memory=PIN_MEMORY,\n",
    "        collate_fn=collate_fn\n",
    "    )\n",
    "\n",
    "    # Quick loader check so failures happen immediately.\n",
    "    print(f\"\\n[seed={seed}] Warm-up: loading 3 train batches...\")\n",
    "    t0 = time.time()\n",
    "    it = iter(train_loader)\n",
    "    for i in range(3):\n",
    "        _ = next(it)\n",
    "        print(f\"  loaded warmup batch {i+1}/3\")\n",
    "    print(f\"[seed={seed}] Warm-up done in {time.time()-t0:.2f}s\")\n",
    "\n",
    "    # Create model and initialize heads from baseline (same seed).\n",
    "    model = Wav2Vec2TwoHeadClassifier(BACKBONE_CKPT, dropout_p=DROPOUT_P).to(DEVICE)\n",
    "\n",
    "    baseline_heads_path = baseline_exp / f\"run_{train_dataset_id}_seed{seed}\" / \"best_heads.pt\"\n",
    "    print(f\"[seed={seed}] Initializing heads from baseline:\")\n",
    "    print(\" \", str(baseline_heads_path))\n",
    "    model = load_heads_into_model(model, baseline_heads_path)\n",
    "    model.train()\n",
    "\n",
    "    # Only head-related layers are optimized.\n",
    "    trainable_params = (\n",
    "        list(model.pre_vowel.parameters()) + list(model.pre_other.parameters()) +\n",
    "        list(model.head_vowel.parameters()) + list(model.head_other.parameters())\n",
    "    )\n",
    "    opt = torch.optim.Adam(trainable_params, lr=LR)\n",
    "\n",
    "    # Track best epoch by val AUROC.\n",
    "    best_auc = -1.0\n",
    "    best_epoch = -1\n",
    "    no_improve = 0\n",
    "\n",
    "    best_state = None\n",
    "    best_val_probs = None\n",
    "    best_val_true = None\n",
    "\n",
    "    # Store threshold details for the best epoch only.\n",
    "    best_thr_youden = float(\"nan\")\n",
    "    best_thr_youden_details = None\n",
    "    best_val_metrics_thr05 = None\n",
    "    best_val_metrics_thr_opt = None\n",
    "\n",
    "    for epoch in range(1, MAX_EPOCHS + 1):\n",
    "        model.train()\n",
    "        train_losses = []\n",
    "        opt.zero_grad(set_to_none=True)\n",
    "\n",
    "        # Train loop (with grad accumulation to reach the effective batch size).\n",
    "        pbar = tqdm(train_loader, desc=f\"[seed={seed}] Train epoch {epoch}\", dynamic_ncols=True)\n",
    "        step = 0\n",
    "        for batch in pbar:\n",
    "            step += 1\n",
    "            input_values = batch[\"input_values\"].to(DEVICE, non_blocking=False)\n",
    "            attention_mask = batch[\"attention_mask\"].to(DEVICE, non_blocking=False)\n",
    "            labels = batch[\"labels\"].to(DEVICE, non_blocking=False)\n",
    "            task_group = batch[\"task_group\"]\n",
    "\n",
    "            loss, _ = model(input_values, attention_mask, labels, task_group)\n",
    "            loss = loss / GRAD_ACCUM\n",
    "            loss.backward()\n",
    "\n",
    "            train_losses.append(float(loss.detach().cpu().item()) * GRAD_ACCUM)\n",
    "\n",
    "            if (step % GRAD_ACCUM) == 0:\n",
    "                opt.step()\n",
    "                opt.zero_grad(set_to_none=True)\n",
    "\n",
    "        # Flush any partial accumulation at the end of the epoch.\n",
    "        if (step % GRAD_ACCUM) != 0:\n",
    "            opt.step()\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "\n",
    "        avg_train_loss = float(np.mean(train_losses)) if train_losses else float(\"nan\")\n",
    "\n",
    "        # Val loop: collect probabilities for AUROC and threshold selection.\n",
    "        model.eval()\n",
    "        all_probs, all_true = [], []\n",
    "        vpbar = tqdm(val_loader, desc=f\"[seed={seed}] Val epoch {epoch}\", dynamic_ncols=True)\n",
    "        with torch.inference_mode():\n",
    "            for batch in vpbar:\n",
    "                input_values = batch[\"input_values\"].to(DEVICE, non_blocking=False)\n",
    "                attention_mask = batch[\"attention_mask\"].to(DEVICE, non_blocking=False)\n",
    "                labels = batch[\"labels\"].to(DEVICE, non_blocking=False)\n",
    "                task_group = batch[\"task_group\"]\n",
    "\n",
    "                _, logits = model(input_values, attention_mask, labels, task_group)\n",
    "                probs = torch.softmax(logits.float(), dim=-1)[:, 1].detach().cpu().numpy().astype(np.float64)\n",
    "                all_probs.extend(probs.tolist())\n",
    "                all_true.extend(labels.detach().cpu().numpy().astype(np.int64).tolist())\n",
    "\n",
    "        val_auc = compute_auc(all_true, all_probs)\n",
    "        print(f\"seed={seed} | epoch {epoch:02d}/{MAX_EPOCHS} | train_loss={avg_train_loss:.5f} | val_AUROC={val_auc:.5f}\")\n",
    "\n",
    "        # Update \"best\" only when AUROC improves.\n",
    "        improved = (not math.isnan(val_auc)) and (val_auc > best_auc + 1e-12)\n",
    "        if improved:\n",
    "            best_auc = float(val_auc)\n",
    "            best_epoch = int(epoch)\n",
    "            no_improve = 0\n",
    "\n",
    "            # Save only head-related weights (backbone is frozen and unchanged).\n",
    "            best_state = {\n",
    "                \"pre_vowel\": {k: v.detach().cpu().clone() for k, v in model.pre_vowel.state_dict().items()},\n",
    "                \"pre_other\": {k: v.detach().cpu().clone() for k, v in model.pre_other.state_dict().items()},\n",
    "                \"head_vowel\": {k: v.detach().cpu().clone() for k, v in model.head_vowel.state_dict().items()},\n",
    "                \"head_other\": {k: v.detach().cpu().clone() for k, v in model.head_other.state_dict().items()},\n",
    "            }\n",
    "\n",
    "            best_val_probs = list(all_probs)\n",
    "            best_val_true  = list(all_true)\n",
    "\n",
    "            # Save metrics at a fixed threshold (0.5) and at the val-opt threshold.\n",
    "            best_val_metrics_thr05 = compute_threshold_metrics(best_val_true, best_val_probs, thr=0.5)\n",
    "\n",
    "            thr_opt, details = compute_youden_j_threshold(best_val_true, best_val_probs)\n",
    "            best_thr_youden = float(thr_opt)\n",
    "            best_thr_youden_details = details\n",
    "            best_val_metrics_thr_opt = compute_threshold_metrics(best_val_true, best_val_probs, thr=best_thr_youden)\n",
    "        else:\n",
    "            no_improve += 1\n",
    "\n",
    "        # Early stop after PATIENCE epochs without improvement.\n",
    "        if no_improve >= PATIENCE:\n",
    "            break\n",
    "\n",
    "    if best_state is None or best_val_probs is None or best_val_true is None:\n",
    "        raise RuntimeError(\n",
    "            \"No best epoch captured. Validation AUROC may be NaN due to single-class validation split \"\n",
    "            \"or earlier failures.\"\n",
    "        )\n",
    "\n",
    "    # Save best heads for this seed.\n",
    "    best_heads_path = run_dir / \"best_heads.pt\"\n",
    "    torch.save(best_state, str(best_heads_path))\n",
    "\n",
    "    # Save plots for the best epoch only.\n",
    "    roc_png = run_dir / \"roc_curve.png\"\n",
    "    cm_png_05 = run_dir / \"confusion_matrix_thr0p5.png\"\n",
    "    cm_png_opt = run_dir / \"confusion_matrix_thr_opt.png\"\n",
    "\n",
    "    save_roc_curve_png(np.asarray(best_val_true, dtype=np.int64), np.asarray(best_val_probs, dtype=np.float64), str(roc_png))\n",
    "    save_confusion_png(np.asarray(best_val_true, dtype=np.int64), np.asarray(best_val_probs, dtype=np.float64), str(cm_png_05), thr=0.5)\n",
    "    if not np.isnan(best_thr_youden):\n",
    "        save_confusion_png(np.asarray(best_val_true, dtype=np.int64), np.asarray(best_val_probs, dtype=np.float64), str(cm_png_opt), thr=float(best_thr_youden))\n",
    "\n",
    "    # Write per-seed metrics (includes val-opt threshold for later test-time reuse).\n",
    "    metrics = {\n",
    "        \"dataset\": dataset_id,\n",
    "        \"seed\": int(seed),\n",
    "        \"best_val_auroc\": float(best_auc),\n",
    "        \"best_epoch\": int(best_epoch),\n",
    "\n",
    "        \"train_manifest_used\": MANIFEST_TRAIN_ENH2,\n",
    "        \"val_manifest_used\": MANIFEST_ALL,\n",
    "\n",
    "        \"init_heads\": {\n",
    "            \"mode\": \"baseline_best_heads\",\n",
    "            \"baseline_exp_used\": str(baseline_exp),\n",
    "            \"baseline_summary_path\": str(baseline_summary_path),\n",
    "            \"baseline_best_heads_path\": str(baseline_heads_path),\n",
    "        },\n",
    "\n",
    "        \"n_train\": int(len(train_df)),\n",
    "        \"n_val\": int(len(val_df)),\n",
    "        \"label_counts_train\": train_df[\"label_num\"].value_counts(dropna=False).to_dict(),\n",
    "        \"label_counts_val\": val_df[\"label_num\"].value_counts(dropna=False).to_dict(),\n",
    "\n",
    "        \"experiment_tag\": EXPERIMENT_TAG,\n",
    "        \"run_stamp\": RUN_STAMP,\n",
    "\n",
    "        \"dropout_p\": float(DROPOUT_P),\n",
    "        \"lr\": float(LR),\n",
    "        \"effective_batch_size\": int(PER_DEVICE_BS * GRAD_ACCUM),\n",
    "        \"per_device_batch_size\": int(PER_DEVICE_BS),\n",
    "        \"grad_accum_steps\": int(GRAD_ACCUM),\n",
    "\n",
    "        \"backbone_ckpt\": BACKBONE_CKPT,\n",
    "\n",
    "        \"val_opt_threshold_method\": \"Youden J (maximize TPR - FPR on VAL ROC curve)\",\n",
    "        \"val_opt_threshold\": float(best_thr_youden),\n",
    "        \"val_opt_details\": best_thr_youden_details,\n",
    "\n",
    "        \"thr_metrics_val_thr0p5\": best_val_metrics_thr05,\n",
    "        \"thr_metrics_val_thr_opt\": best_val_metrics_thr_opt,\n",
    "\n",
    "        \"artifacts\": {\n",
    "            \"roc_curve_png\": str(roc_png),\n",
    "            \"confusion_thr0p5_png\": str(cm_png_05),\n",
    "            \"confusion_thr_opt_png\": str(cm_png_opt),\n",
    "            \"best_heads_pt\": str(best_heads_path),\n",
    "        },\n",
    "    }\n",
    "\n",
    "    metrics_path = run_dir / \"metrics.json\"\n",
    "    with open(metrics_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(metrics, f, indent=2)\n",
    "\n",
    "    # Progress prints (kept as-is).\n",
    "    print(f\"[seed={seed}] VAL-opt threshold (Youden J): {float(best_thr_youden):.6f}\")\n",
    "    print(f\"[seed={seed}] WROTE:\")\n",
    "    print(\" \", str(metrics_path))\n",
    "    print(\" \", str(roc_png))\n",
    "    print(\" \", str(cm_png_05))\n",
    "    print(\" \", str(cm_png_opt))\n",
    "    print(\" \", str(best_heads_path))\n",
    "\n",
    "    return {\n",
    "        \"seed\": int(seed),\n",
    "        \"best_val_auroc\": float(best_auc),\n",
    "        \"best_epoch\": int(best_epoch),\n",
    "        \"val_opt_thr\": float(best_thr_youden),\n",
    "        \"run_dir\": str(run_dir),\n",
    "        \"seed_metrics\": metrics,\n",
    "    }\n",
    "\n",
    "# -------------------------\n",
    "# 13) Run all seeds and write the experiment summary\n",
    "# -------------------------\n",
    "# Outputs:\n",
    "# - Per-seed runs under the experiment folder\n",
    "# - summary_trainval.json for the experiment\n",
    "# - history_index.jsonl appended at the trainval_runs root\n",
    "results = []\n",
    "for seed in SEEDS:\n",
    "    results.append(run_trainval_once(seed))\n",
    "\n",
    "aucs = [r[\"best_val_auroc\"] for r in results]\n",
    "thr_vals = [r[\"val_opt_thr\"] for r in results]\n",
    "\n",
    "# 95% CI across 3 seeds (t distribution, df=2).\n",
    "t_crit = 4.302652729911275  # df=2, 95% CI\n",
    "n = len(aucs)\n",
    "mean_auc = float(np.mean(aucs))\n",
    "std_auc = float(np.std(aucs, ddof=1)) if n > 1 else 0.0\n",
    "half_width = float(t_crit * (std_auc / math.sqrt(n))) if n > 1 else 0.0\n",
    "ci95 = [float(mean_auc - half_width), float(mean_auc + half_width)]\n",
    "\n",
    "# Aggregate threshold across seeds (mean ± SD).\n",
    "thr_mean, thr_sd = mean_sd(thr_vals)\n",
    "\n",
    "# Print summary stats (kept as-is).\n",
    "print(\"\\nAUROC by seed:\")\n",
    "for r in results:\n",
    "    print(f\"  seed {r['seed']}: {r['best_val_auroc']:.6f}\")\n",
    "print(f\"\\nMean AUROC: {mean_auc:.6f}\")\n",
    "print(f\"95% CI (t, n=3): [{ci95[0]:.6f}, {ci95[1]:.6f}]\")\n",
    "\n",
    "print(\"\\nVAL-opt thresholds (Youden J) by seed:\")\n",
    "for r in results:\n",
    "    print(f\"  seed {r['seed']}: {r['val_opt_thr']:.6f}\")\n",
    "print(f\"  mean ± SD: {thr_mean:.6f} ± {thr_sd:.6f}\")\n",
    "\n",
    "# Canonical threshold record saved only in the experiment summary.\n",
    "val_optimal_threshold_obj = {\n",
    "    \"method\": \"Youden J (maximize TPR - FPR on VAL ROC curve)\",\n",
    "    \"by_seed\": {str(r[\"seed\"]): float(r[\"val_opt_thr\"]) for r in results},\n",
    "    \"mean_sd\": {\"mean\": float(thr_mean), \"sd\": float(thr_sd)},\n",
    "}\n",
    "\n",
    "# Experiment-level summary used later by test-only cells.\n",
    "exp_summary = {\n",
    "    \"dataset\": dataset_id,\n",
    "    \"dx_out_root\": DX_OUT_ROOT,\n",
    "\n",
    "    \"train_manifest_used\": MANIFEST_TRAIN_ENH2,\n",
    "    \"val_manifest_used\": MANIFEST_ALL,\n",
    "\n",
    "    \"init_heads\": {\n",
    "        \"mode\": \"baseline_best_heads\",\n",
    "        \"baseline_exp_used\": str(baseline_exp),\n",
    "        \"baseline_summary_path\": str(baseline_summary_path),\n",
    "        \"baseline_best_heads_by_seed\": {\n",
    "            str(s): str(baseline_exp / f\"run_{train_dataset_id}_seed{s}\" / \"best_heads.pt\") for s in SEEDS\n",
    "        },\n",
    "    },\n",
    "\n",
    "    \"experiment_tag\": EXPERIMENT_TAG,\n",
    "    \"run_stamp\": RUN_STAMP,\n",
    "    \"exp_root\": str(EXP_ROOT),\n",
    "    \"run_dirs\": [r[\"run_dir\"] for r in results],\n",
    "    \"seeds\": SEEDS,\n",
    "\n",
    "    \"aurocs\": [float(x) for x in aucs],\n",
    "    \"mean_auroc\": float(mean_auc),\n",
    "    \"t_crit_df2_95\": float(t_crit),\n",
    "    \"half_width_95_ci\": float(half_width),\n",
    "    \"ci95\": ci95,\n",
    "\n",
    "    \"n_train\": int(len(train_df)),\n",
    "    \"n_val\": int(len(val_df)),\n",
    "    \"label_counts_train\": train_df[\"label_num\"].value_counts(dropna=False).to_dict(),\n",
    "    \"label_counts_val\": val_df[\"label_num\"].value_counts(dropna=False).to_dict(),\n",
    "\n",
    "    \"effective_batch_size\": int(PER_DEVICE_BS * GRAD_ACCUM),\n",
    "    \"per_device_batch_size\": int(PER_DEVICE_BS),\n",
    "    \"grad_accum_steps\": int(GRAD_ACCUM),\n",
    "\n",
    "    \"backbone_ckpt\": BACKBONE_CKPT,\n",
    "    \"dropout_p\": float(DROPOUT_P),\n",
    "    \"lr\": float(LR),\n",
    "\n",
    "    \"val_optimal_threshold\": val_optimal_threshold_obj,\n",
    "    \"per_seed_metrics\": [r[\"seed_metrics\"] for r in results],\n",
    "}\n",
    "\n",
    "# Write experiment summary and append global history log.\n",
    "summary_path = EXP_ROOT / \"summary_trainval.json\"\n",
    "with open(summary_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(exp_summary, f, indent=2)\n",
    "\n",
    "history_path = TRAINVAL_ROOT / \"history_index.jsonl\"\n",
    "TRAINVAL_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "with open(history_path, \"a\", encoding=\"utf-8\") as f:\n",
    "    f.write(json.dumps(exp_summary) + \"\\n\")\n",
    "\n",
    "print(\"\\nWROTE per-experiment summary:\", str(summary_path))\n",
    "print(\"APPENDED global history index:\", str(history_path))\n",
    "print(\"\\nOpen this folder to access artifacts:\", str(EXP_ROOT))\n",
    "\n",
    "# -------------------------\n",
    "# 14) Unassign runtime (stop L4)\n",
    "# -------------------------\n",
    "# Frees the GPU at the end so the session does not keep running.\n",
    "print(\"\\nAll done. Unassigning the runtime to stop the L4 instance...\")\n",
    "try:\n",
    "    from google.colab import runtime  # type: ignore\n",
    "    print(\"Calling runtime.unassign() now.\")\n",
    "    runtime.unassign()\n",
    "except Exception as e:\n",
    "    print(\"Could not unassign runtime automatically. Error:\", repr(e))\n",
    "    print(\"Manual stop: Runtime -> Disconnect and delete runtime.\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "caf7f350db0442a5bccfa0c76cbd3118",
      "536dbea9cd464a3698e5fb22b1035923",
      "794f565327bb411380ffdbaca9419359",
      "555a6a0cc4ae41bbaaec60e6332bbe41",
      "78b55dd7710149d1aa572cd231e34990",
      "9846c20591ea41bcb6f66156ff16222e",
      "acb663122391492dbc1b0cc2e2e75772",
      "e7a7af00065a42b49eb43844c7a52af2",
      "1360accc8f084886b5332e34f46257c5",
      "480d6e5e2cc74547bf6ca4a17541ccbc",
      "da61d8c427b94737a1c4b011985b80d6",
      "3bd7d33da42f4949b8e59ffdfb043b1b",
      "225efd68af5441fe8de4bc1019512766",
      "4e6834cb70a44d489bb51c02f6b1ac44",
      "6ff9c5950ecf4ca5b83faae097b997ff",
      "d377f1e9623d4ed986b7b25971d702e6",
      "efd42749e2b34b0eb389ec9c57fcc1fd",
      "affe9a074d574e17b8b1368a42173c32",
      "dec5c7c2dc6e470580a513dde8215416",
      "ff710ffc530646c19fa29e598e8503f0",
      "53c77d5a083047319bb487262d7933a8",
      "5919f60a32a84f06975b53b95aee704a",
      "094b0343c0a34e0a91f5f70576314215",
      "272fbb421b5e47b1a8734a2091842f87",
      "08f483fb40f641c8a750bbc38949896e",
      "9611e2ddb865422192537b67d3db5c51",
      "e56068c60a874513b5e68b719177a306",
      "14bbc3e4af7e4c8eb1e11923b4f375f3",
      "dc4ed1f55cfc413d948076cb570294ef",
      "71986d823ef046d5b0bdf675cc8f12c1",
      "09c48e7040554d03838c8553de7f6e0d",
      "a604ca46a1c14be190ac89ebf6c802f0",
      "e4bd5df4164e46b9accda8f760c547c0",
      "8ed2165b5e824e4b9923d5da039323ae",
      "d5f39b2f58454b5a8017a64b21a66d1d",
      "4e1ab87582294ea1ab502ba2382d0810",
      "f2da577ddacb4858917064cdf3ef8304",
      "f1bc5f7ade894d329b19d8fc436ce162",
      "40382e906ce24edebc4576d4f1652686",
      "9ee8f1b3ad084dcfbb56da9529582137",
      "9659e311fd2f453f9e6869ff359ca882",
      "931347dd48c64489bc23cc8af13cd66b",
      "25fac3cc7c484cb7b93491f7c82886b9",
      "6fda4eab071c43b29cfc1cc88fac0181",
      "2cb2a77b76c94bcab341712389bc2262",
      "ec10871f91074dd7ac6fb0965acb9cfc",
      "08075efbc9e943948565cb49e3d97d56",
      "08137ae08c38416d8f8e9e7637951bf8",
      "61c9c10096ea4eddae2a4430ac2e346f",
      "5577500665ee4d01b3aaa47a09dbd5a2",
      "3ac38059f3094f8187fc0215ce5aa6c4",
      "c33bfc5b41594cbc92a9e48cc6699814",
      "57d6c8c5fc2146058a9714fdbe7762e4",
      "b065805bf38147fda7923a59661485b7",
      "7c9059e7600f45de8d56948ff1cd0921",
      "b7fdbb58d4874d8eb79d0a39b89f797c",
      "f3ade1a7f2c74db7bdacdfd9283f4511",
      "f41b0812b41146aa8941747c5afa4c96",
      "43e46891c1204ecb859f12605e6052b7",
      "7c6868a5eb804e2c9f96396be45e31b8",
      "6dd5b1fbc12e4dffbd84c3bb06d5e733",
      "bab6eb5fbf1b4981a1a96e5e80bdba59",
      "dd8cf83c9f9247b991c7e17ce0619cdb",
      "c2bc27815aa64479848132e31c13f793",
      "ffc9b94266f443d48776211c98531bf6",
      "0f053f6b453f427f9668d4bdb9963080",
      "d88649c292a34563a728cf27cc2dd8df",
      "a4df30264cbb46cd98de4f8d4835d40d",
      "7c7ebe1a2df84782bdcc7dad01b0b1d2",
      "a6541cd5f7d94b9e9798abf2f0d833eb",
      "256e625abde4481d814657d88729f046",
      "eabdb26dd0144f8eb449109dfa40bc1d",
      "84b86bb3d96d4fa59a55b0d4127ff4ba",
      "471e7eb50b8c450ea2580d0edc22690f",
      "9a430d30170343c7abf9ea545cfd74a6",
      "9f84d282625c4917a3ba1bb2fd9ad90f",
      "720c337eac844c748b1ec0be2b138e0c",
      "fbed7eb32651495b81cd7742bbd20082",
      "cb1ee602b3234fef942c68ab571a8a1f",
      "a597c6eef4864a18baf078a1ae46e043",
      "41c76e3e191b48c59c32fff6803a15be",
      "db98472e650743d6b607d6b85e12092c",
      "c52262ee3e074aba8e3e6d79c24356d2",
      "3403c8006da541e680ecf4137df467bc",
      "58491853428248c5a63a11973d4044c0",
      "5e144c2d849b4b1286fed02479d7bdd1",
      "888c328aed224b7cb7ac5e456830c117",
      "44dc02fa222141b3a33a418cacbbec29",
      "98dc39f17a444952b804d43b4737604f",
      "a5f2e58d9255413aa0721523c11391ec",
      "28f92fef621f417ab248856b2e92fca4",
      "42b8aca7a9644145a76190043eb8e781",
      "2ca9806360cb43bdbc24d11148a9c777",
      "3082eaee0a6d44f78c6eda4aba2fe964",
      "81fe094c54ef4325a560551c24395eb9",
      "ff52d3631d0d4092b2ca499e09a64430",
      "47d480ddc05c4b71b05538f81cdec63d",
      "07f5faf4af0d4e589883e4a383a4bc8d",
      "c349592b9df3423da92169c2b8410beb",
      "c7a8f51b09fa44519e13d8c20dbee823",
      "7d4d654f55f34b5a9dae2ef6227739ae",
      "4f10a2421ce14af19f8efbdc23c11004",
      "2f95814f39ac4170bfe89a3dc1bf605a",
      "b8c1b3c442354a8f9cf64c6c981162c8",
      "0fefc05eefc74e8f8e7717298dc321f6",
      "fe9df80722b241e590c2169ed74e6b8c",
      "ac59bb6fd80f4329b7c6d7493ee07d27",
      "c880dd4804fa4b27a0591ca1429fb3f3",
      "3e236ad0ae0c4e5a9579949623bcbb79",
      "0c702f42869547238b8f1928992f9890",
      "e2a3c65432544c7cb8fd7f01617036d5",
      "af1e9869f8614bf396c59d3881d0f447",
      "f8600c709ebe4c20b6e93e9ba8c80ece",
      "87e72af2153c4c4096553e5abac3034d",
      "97d9f70f043a48ef893b84b48ddf0c26",
      "46051dac8ae24680a7538d2e02005276",
      "55941b7322424fe1ace2be2331fe5634",
      "6e2f8099c29e461cb97776a4a0cf401c",
      "502b76f78c6f4921984a921bb57f649b",
      "fc1d226c09aa4751a5f5ad077f706b32",
      "dfabd5f6620e46398a6e1462380c3a72",
      "7bedda4d7dda4ff486ed38506c0941d1",
      "a6454789f86547688d5fb839679ed6cf",
      "4130d92c6ead474bb465d23b8743d6ec",
      "8db5a1a4e3394663b84313e1ede11cb5",
      "3d7dd760e825424f9d6dc90cd36534c8",
      "c6b12b0a0a004f8c9cf81c53ccd77954",
      "1e8584ea73834d85b5d74f477552b4da",
      "38d5ef769d3e468aa9581a0dec127492",
      "30d513d80804498386c60f3cfe9a39af",
      "b1baeb94b84941e9aba7614b94b4715c",
      "cf320d887a464244b979d68efdf084b4",
      "b6fecc4896aa47f0b51f188e3e1c8768",
      "e785f1616eb6407292aecfbfd7123e2c",
      "8a2d45d0af2f477e80f71c98dfc0cbb4",
      "83cdb8bbada440e9b3bd968c480c1573",
      "7f6698b538774b95a2b1a7f81a77673f",
      "d4cdd0a38b634d908f54700cd0d24be9",
      "568298042a784f78b60f42ead45a9fdb",
      "60f6fcaed7d34e44af636747ee24938a",
      "850c947fd8bb440f9ff9e0204f7d03c1",
      "f3b8f8c8bc2849d3b4613804fa121cb8",
      "28b9e53bf70a4b61b2ebb6f673f1e431",
      "d389c129f8d54fe58fa99aa45e9af626",
      "dc53c794f64245cfa026b8c10501d7a5",
      "77533bc1155942d2aa4e05d673dcbb66",
      "c13e650c99694deb96d8801820240564",
      "621fcebea0bb4507ba314eb888692880",
      "9993ecdd78e14019a1644fd8cbc8f7a0",
      "72053a4fd83e4910aaefe6050b706656",
      "76e4c23ec9ff4674930219dd14f0bf35",
      "76e1db63121c4479817cf7291f6a5ef5",
      "994198aa552b47e1959b33c85829c22c",
      "cf0ed88953844c03ac62cf208ae86d62",
      "cb738dd835d546e4864946837af8dde2",
      "2968910bb8c64903bb8ca6464a61b32d",
      "715fe826e87d47e198db96c2a65d91f3",
      "73bad825b3a34d02b5872ed151f2c232",
      "a889755ae47441208867a4c763c8edcb",
      "ce67e67023b64194bce3a1a535e75ec6",
      "6d8ca577823b4fc490cb48cfbbed3f49",
      "e7a90ea3c5ab48049debd65bf5f92b62",
      "f287e9162cfc4e9cb70b5bec4f598f1b",
      "8a4adb4a18df4034a0fc52ccaf721533",
      "bcc43789438f4f3d8c6b286bc3ba131a",
      "f01c06822c5340328a7bb24aed3edbdf",
      "dbcecd8367994e07b83674a91880f412",
      "440b9a43a1a342fa94f30a5711f5f718",
      "e5f522412151420c8708bf6cc2bae1ae",
      "808634da8bf7421792ed5c418a026f9e",
      "1239a7e66c67493db332b5be5c567f1d",
      "7c06bce91f6f4c18b65b77ff6c82b759",
      "1118a8116e0044179d2c12c900a90326",
      "07a58ffbfc6a474eb623942a5800f06e",
      "f531c699fafd41c69bd0a820485a27ac",
      "30c529f1e9dd4d7e87ceea1995b84a7a",
      "0d974046f8874b12b61d73931bbeea1a",
      "8e9e838e80e2485db09bebbb4fe8a371",
      "c5ef9e0fea0b46509d3ed484b0f6e84e",
      "5568516d25174f1ab3459b3994b2fadc",
      "d62ffa842384448a85fc84d1cfe56c07",
      "cd66314edf704c7b9ff7b8cf17bbc536",
      "efdad83aee9f4c3c858f5016cd9f40ca",
      "58596337832e49c79468f6b5bd17c6fb",
      "9c26ef5706a246e486d784d2ae353287",
      "b57872dd99b64418be6dfe942b9cc3f2",
      "d336e170a9784242960b4ed1e282503b",
      "2b38496d34a8451fb2398cad2ad1bd41",
      "2180610ab9954ddb8beb9395e73029cd",
      "c9606c6aba3a4cc680a3b92cfc183590",
      "9d6682128e4243809fdf2c604d174319",
      "f865b90255554654bd16268aef184d0c",
      "d5f1cbb82853469d927daf6c0c76f5d4",
      "b4191c3346024d5db4d44700cc7aebcc",
      "921046a5f30b4b64b65f3961594ee250",
      "a880cfd0bf67456ead000ec97ddea8c5",
      "03183dee4313428e8907cc9166115a9d",
      "8ea8c3e777eb4363be788b02e437f4df",
      "e3e1e05f5a4541508835b717ef12c643",
      "02ddaf0ffaf14746835b6b0ff57fcd7e",
      "dd798553dc8341edabfa68ed4a7b78d2",
      "ae9a6dcd25434511bf5dc5353997ec72",
      "45d53d13930b45888fd92549bd211621",
      "00d4eef3a9894a57be346cc816637044",
      "32930236bfb243e0ae9ace5b152aa8f4",
      "433a47bbf846458398428876997f12b5",
      "0a03e1118b4046b2892b8d0c22b1acc6",
      "fd09a7d70bce40659810819f70b4521b",
      "427f489abc3241b9845fedfee6a1254f",
      "252860dc7fff4d078a0b53e030f5a29a",
      "3d29d0dda42c4e1d9d4427a124320fa0",
      "d1c9323dea224229a1966492da13d330",
      "d90ad0034d8d48a5ab4c53bc3269eee1",
      "3cecf028fa98435292629648e10e3005",
      "01606363574642bdbf0974f6be721846",
      "09162180b19848a4bbf6e29d919cba89",
      "d2ecf54cc4294ef49b0f73e945b9ac93",
      "141f5f8289ca41caa1a7e470ab5e5a7d",
      "e97da34e799948d499aaa799cfc9f2b2",
      "fe3a347a4eba4d829df80dbde449adf2",
      "f590f384f4914d55853d92ab9df076da",
      "84315d656e5246d287ecea86ccdfc61e",
      "1532331f53e5453da4b0f31ccc34b6c1",
      "3b119a4df72c40a9821332ffd74e5e29",
      "39370109ac084391a83ef2c2d8bd6ef4",
      "13d6dd5741bb4d29882f6698e60ad739",
      "787c87ee09054e7c8dae06812fa84296",
      "cc76f6197d7a4cac93a66d833f663a68",
      "abb532bb719d44f2b374772c7d22014b",
      "0a4526c5e6ac453dba2626e4ee79e7a2",
      "bb64d060aeb543d6af452d27177790a5",
      "b55cd623cb54411684f7468d61592c52",
      "b68b5baa83d04d5c80e7d68922d64a24",
      "252f31d9bfaa4fe39f05e6d8365e45fb",
      "9476d2e0c83244fc96fad46ee07da3fe",
      "7a0d14439afa4ef1a84b1098f87ef534",
      "4e559a39f12a448cac75b788ce58b295",
      "4ce26a8bdc5f4f9089d96bc52f5d0158",
      "086c4821e93541ef88383b1a1efc0455",
      "201da02893bc40c3967136a6f366e01b",
      "94ccd8558e4844fdbe2816508f7f46be",
      "372648d973d4485d9fe6a3b1cf5a42c0",
      "807a65a742b74e0199adbb9c543d6643",
      "09e9b3ea6880400ea14de0e4aeb75a38",
      "8891151e2be94737a5c72bb33db59c30",
      "137e9be6eb4f47f6a453d474a2376c52",
      "7a28231b69e546f18ed320709de1b66a",
      "c04e144e975e4247bc326903717ec6cc",
      "b28b9ddc1658404d81e3caa12bd7a3f8",
      "7d1c47c915fe4dd1b814fee31972cd6e",
      "d642879444ae4bcf900c2af6015b8e11",
      "6a23d59b6f744fe3ae457367bc49145f",
      "c75a3b27714b444285b65fe92cc521ca",
      "fcd2b7e15af646b9b2238051c7492baa",
      "621af9d9b52c44e9a76997705d58a079",
      "a8459bb8ae0542f192958ec3c4b901ac",
      "51462e9126904eee89b93f4864884d34",
      "5f8a9bc95f7243349917578025e0cb3d",
      "fa2a4deff2934bd7b7624b0a7a3f03aa",
      "4d4b84b799704c82bca6bdf7f6dc8226",
      "7584c4a4bb3345898db57a6171e3ce7d",
      "344647d8aab8490aa0fe988fba9cdd69",
      "5ead64d288bb419f97f3034bb4e6a1c9",
      "b0c05faedbdd43ba89302043e2197885",
      "0f7b9a25e79a4914b91c29d02f8435c3",
      "7bc3a941c0f742d88653ef85f5e4b00a",
      "1fd349e4b6c74a3395c384aa59f16f37",
      "c63cbff2191144bca8b126aab567f95b",
      "e6e2e8043e7d4a57b961fd88afa19fd8",
      "1ee7b80ca27a4e36a83dc37cda2e84a2",
      "e4aeebc6f6ef4eb98632bcf439059061",
      "399c173df29d4dcfa524627ef329d24d",
      "05f548c3c9e648f2884c8b6857db65b8",
      "1a4af29f481e42ddbe3d849bb5687572",
      "335273490e674e26aa25d86763e6a617",
      "2d713d75181c4a2097de1978f2aaa844",
      "9649a5a12ec04d44947a327c0416e9d3",
      "0428f4368c914228b1a8ff7f3c84ccd4",
      "c3ec5ddfd9ed49fe960680b6daa83249",
      "b064db844e7147bd9cf565d8a5658094",
      "5476346df0a242e5ae07f887b858c8f4",
      "7267289178d04cabaf1bf705180255b5",
      "6eeb79a34cc440c79fc4725c2a1362ae",
      "2a1c1f91f7f44a148fb6e0009419b0a3",
      "4e3adc9da6d145618d805ab5232cb575",
      "b8b065ade430492197f13b4d99873092",
      "383742b2c2144a2c8c26d024b8a00d8f",
      "81931a65e44345fda97228da2c428250",
      "cee6632f62734e3d9a4d8d9ca7888fb7",
      "101feb48b3b04e45bae5ff85810247cc",
      "3d119a8e8b144a6c8a49382487dc906a",
      "c6313e066ada4b8199d3b50acfc711f1",
      "382b6b99dce54dabbec8e5ba0cae234a",
      "608de16008ec4869a54ad84a252dd66b",
      "17a08080f96a421c96f55ce4a945927b",
      "60fb7affe7a9427fb555edf6445cc25a",
      "1fce11ad43704cd9a537604c31b7d267",
      "52a3a4366c384bfeb06891e1166bc34b",
      "9f146e6b1a3d40d6be8e2e803a111d8b",
      "0e6cd084f1534790a559bf6a42940eff",
      "1f9f749a000b483780b71c28de2006e6",
      "e4323c56261d4a39970f427cd2121f72",
      "75c9c053aac94120a735e95f77f0af73",
      "11412659979c47cfb80bf9eafc33a4dc",
      "0b8e33a6c9564dc0abc5e9d7338c3bcb",
      "958f0d1949094696bdc6073e7d7097dd",
      "2bb01c6fb5ea400b859943ef53dfc7e1",
      "65275d5546834590b40de96358e6a497",
      "8521ab67c5b44bdbbda10be2e3c7f2e3",
      "0c186635d7774ca096ab911eb6641537",
      "c47e443aef6f49ff8d33946050bb1146",
      "3402a2a36e1642c9a836efc911f47ea7",
      "2db0ce32501a4e678139ae0802b8132a",
      "700ff2ec7dc94607b24e72304c6bbee1",
      "34e1cba5559843429114e901b824883d",
      "e8303e2679214bb9a4c9aff8f6f128dc",
      "8897ad4ac9ca4372b235af1d0ac39161",
      "15ae0dcdf5384301b70948e5bb989e01",
      "5af2026da5e542e6853d7758e930c630",
      "56b7d3fca6d84ac2a90a446d13f13543",
      "45a7b1c4aec04f4284b796629be5e299",
      "413a8cb9477d45faa7a1084f3b5f93ba",
      "069a6a195d6644a58b07b11ad077e687",
      "f8af48df522a4ae09f3439ba374b1b13",
      "4a53fdcbbdf04e8698ef6674612d7c86",
      "43a4af99620d491ab2613eedd6f650a2",
      "a158c13be34d402c96f0b0d9773cd8ae",
      "6626447e56da4501ac975a35e6523879",
      "88d3da932b1047fd897c64b136b12dc2",
      "7d1343b53ed2428d9f1c1ad7a95dce23"
     ]
    },
    "id": "a4jTqq6UHVG2"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "The following cell evaluates the **enhanced D7 model trained with the trainEnh2 dataset** on the **D2 test split only**, using the original D2 `manifest_all.csv` as the only source of test clips. It automatically finds the most recent D7 train and validation experiment whose folder name includes **“trainEnh2”**, and checks that all required files are present. This includes the three saved model heads for seeds 1337, 2024, and 7777, as well as the matching training summary file. As a safety check, the cell confirms that the test manifest truly belongs to **dataset D2** and rechecks that all model files exist after the experiment folder is selected.\n",
    "\n",
    "A single global decision threshold is loaded from the train and validation summary and is applied to all three seeds during testing. This threshold is not tuned using D2 data. If the threshold is missing or cannot be read, the cell falls back to **0.5** and records that choice in the outputs. Each D2 test audio file is checked to ensure it exists and that it is sampled at **16 kHz**. Clips are grouped into two task types: **vowel** for sustained vowel tasks, and **other** for all remaining speech tasks. For vowel clips, an attention mask is created to ignore padded silence at the end of the signal so the model does not score non speech regions. Sex values are standardized for later analysis by mapping “male” and “female” to **M** and **F**, with all other values treated as unknown.\n",
    "\n",
    "For each random seed, the cell rebuilds the model, which consists of a frozen speech feature extractor and two task specific classification heads, and loads the trained weights for that seed. It then runs inference on the full D2 test set. Per clip results are written to a `predictions.csv` file that includes the clip path, true label, predicted probability, sex, speaker ID, task group, seed, run tag, and the decision threshold used. Performance is reported using **AUROC**, standard threshold based metrics such as accuracy, precision, recall, specificity, F1 score, Matthews correlation coefficient, and Fisher exact test p value, along with a fairness measure defined as the difference in false negative rates between female and male speakers at the chosen threshold. For each seed, the cell also saves an ROC curve, an overall confusion matrix, and separate confusion matrices for male and female groups when enough data are available.\n",
    "\n",
    "After all three seeds are evaluated, the cell produces an overall summary. This includes the **mean AUROC with a 95 percent confidence interval** across seeds, as well as the **mean and standard deviation** of all threshold based metrics and fairness values. Outputs are stored in a timestamped run folder under `monolingual_test_runs`, together with structured summary files that make it easy to track and compare results with earlier runs. Pointer files are updated to mark the latest run and to maintain a running history tied to the training tag.\n",
    "\n",
    "At the end, the cell writes a small set of configuration and log files that match the dataset and evaluation naming scheme, backing up any existing versions first. It then releases the Colab runtime resources, including the GPU, to signal that the evaluation is complete."
   ],
   "metadata": {
    "id": "X_18tL6xvpD5"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# D7 trainEnh2 heads: test on D2 test split (3 seeds)\n",
    "# Inputs: D2 manifest (test split only), most recent D7 trainEnh2 trainval run (best_heads.pt + summary_trainval.json)\n",
    "# Outputs: per-seed predictions and metrics + plots, plus run-level summaries and pointers\n",
    "\n",
    "# =========================\n",
    "# TEST ONLY (CRASH-PROOF, WITH PROGRESS + STORED METRICS) — D7 ENHANCED → D2 TEST\n",
    "# - Evaluates the D7 ENHANCED trained heads (trained on train_enh2) on the D2 TEST split only\n",
    "# - Uses ONLY D2: <D2_OUT_ROOT>/manifests/manifest_all.csv  (TEST split)\n",
    "# - Loads finished heads from MOST RECENT D7 *ENHANCED* trainval experiment under:\n",
    "#     <D7_OUT_ROOT>/trainval_runs/exp_*/run_D7_seed{seed}/best_heads.pt\n",
    "#   Selection rule: exp folder name must contain substring \"trainEnh2\" (case-insensitive)\n",
    "#   and must contain all three seeds + summary_trainval.json.\n",
    "# - Uses the SINGLE MEAN VAL-optimal threshold stored by that D7 trainval in:\n",
    "#     summary_trainval.json -> val_optimal_threshold.mean_sd.mean\n",
    "#   (No VAL threshold recomputation in this cell)\n",
    "# - Evaluates 3 seeds separately (1337, 2024, 7777)\n",
    "# - Reports:\n",
    "#     * mean Test AUROC ± 95% CI (t, n=3)\n",
    "#     * A single threshold used for ALL seeds (mean val-opt threshold) + note if fallback to 0.5\n",
    "#     * Threshold metrics on D2 TEST @ that single threshold as mean ± SD\n",
    "#     * FAIRNESS (H3) on D2 TEST @ that single threshold as mean ± SD\n",
    "#     * Confusion charts split by sex (M/F) on D2 TEST @ that single threshold\n",
    "# - Writes all artifacts under:\n",
    "#     <D7_OUT_ROOT>/monolingual_test_runs/run_<FULL_TRAINVAL_EXP_TAG>__<RUN_STAMP>/...\n",
    "#   plus:\n",
    "#     * monolingual_test_runs/last_run_pointer.json (intentional overwrite)\n",
    "#     * monolingual_test_runs/summary_latest.json (intentional overwrite)\n",
    "#     * monolingual_test_runs/history_index.jsonl (append-only)\n",
    "#     * monolingual_test_runs/run_<TAG>/tag_run_pointer.json (never overwritten across tags)\n",
    "#\n",
    "# D2-SPECIFIC NOTE (the manifest):\n",
    "# - sex is encoded as the exact strings \"male\" / \"female\" (case-sensitive)\n",
    "#   This code maps: \"male\" -> M, \"female\" -> F, and anything else -> UNK\n",
    "#\n",
    "# GUARDS:\n",
    "# A) Hard-assert D2 dataset_id == \"D2\" after inference from D2 manifest\n",
    "# B) Re-assert all best_heads.pt exist immediately after chosen_exp is selected\n",
    "# =========================\n",
    "\n",
    "import os, json, math, random, time, warnings\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import soundfile as sf\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from transformers import Wav2Vec2Model\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, confusion_matrix, matthews_corrcoef\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# -------------------------\n",
    "# 0) Environment guards\n",
    "#    Prevents importing the wrong library due to a local file/folder name clash.\n",
    "# -------------------------\n",
    "if os.path.exists(\"/content/torch.py\") or os.path.exists(\"/content/torch/__init__.py\"):\n",
    "    raise RuntimeError(\"Found local /content/torch.py or /content/torch/ that shadows PyTorch. Rename/remove it and restart runtime.\")\n",
    "if os.path.exists(\"/content/transformers.py\") or os.path.exists(\"/content/transformers/__init__.py\"):\n",
    "    raise RuntimeError(\"Found local /content/transformers.py or /content/transformers/ that shadows Hugging Face Transformers. Rename/remove it and restart runtime.\")\n",
    "\n",
    "# -------------------------\n",
    "# 1) Drive mount (Colab-safe)\n",
    "#    Makes files available when running in Colab; harmless elsewhere.\n",
    "# -------------------------\n",
    "try:\n",
    "    from google.colab import drive  # type: ignore\n",
    "    if not os.path.isdir(\"/content/drive/MyDrive\"):\n",
    "        drive.mount(\"/content/drive\")\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# -------------------------\n",
    "# 2) Input roots and shared run settings\n",
    "#    - Reads D2 manifest from D2_OUT_ROOT\n",
    "#    - Reads D7 trainval artifacts and writes test outputs under D7_OUT_ROOT\n",
    "# -------------------------\n",
    "D7_OUT_ROOT_FALLBACK = \"/content/drive/MyDrive/AI_PD_Project/Datasets/D7-Multilingual (D1_D4_D5v2_D6)/preprocessed_v1\"\n",
    "D2_OUT_ROOT_FALLBACK = \"/content/drive/MyDrive/AI_PD_Project/Datasets/D2-Slovak (EWA-DB)/EWA-DB/preprocessed_v1\"\n",
    "\n",
    "D7_OUT_ROOT = str(globals().get(\"D7_OUT_ROOT\", D7_OUT_ROOT_FALLBACK))\n",
    "D2_OUT_ROOT = str(globals().get(\"D2_OUT_ROOT\", D2_OUT_ROOT_FALLBACK))\n",
    "\n",
    "D2_MANIFEST_ALL = f\"{D2_OUT_ROOT}/manifests/manifest_all.csv\"\n",
    "\n",
    "# Keep DX_OUT_ROOT aligned to the run root (D7) for consistent naming across cells.\n",
    "DX_OUT_ROOT = D7_OUT_ROOT\n",
    "globals()[\"DX_OUT_ROOT\"] = DX_OUT_ROOT\n",
    "globals()[\"D7_OUT_ROOT\"] = D7_OUT_ROOT\n",
    "globals()[\"D2_OUT_ROOT\"] = D2_OUT_ROOT\n",
    "\n",
    "# Timestamp used to create a unique output folder name for this run.\n",
    "RUN_STAMP = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# Backup helper for files that are intentionally overwritten in the builder-aligned outputs.\n",
    "def _backup_if_exists(p: Path):\n",
    "    if p.exists():\n",
    "        bak = p.with_suffix(p.suffix + f\".bak_{RUN_STAMP}\")\n",
    "        try:\n",
    "            p.rename(bak)\n",
    "            print(f\"BACKUP: {str(p)} -> {str(bak)}\")\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Could not backup existing file before overwrite: {str(p)}. Error: {repr(e)}\")\n",
    "\n",
    "# Stable tag name for folder names and pointer files.\n",
    "def _sanitize_tag(s: str) -> str:\n",
    "    s = str(s).strip()\n",
    "    out = []\n",
    "    for ch in s:\n",
    "        if ch.isalnum() or ch in [\"-\", \"_\"]:\n",
    "            out.append(ch)\n",
    "        else:\n",
    "            out.append(\"_\")\n",
    "    out = \"\".join(out).strip(\"_\")\n",
    "    return out if out else \"tag\"\n",
    "\n",
    "# -------------------------\n",
    "# 3) Fixed evaluation settings\n",
    "#    Mirrors the training setup where needed (seeds, backbone, audio sampling rate).\n",
    "# -------------------------\n",
    "SEEDS          = [1337, 2024, 7777]\n",
    "BACKBONE_CKPT  = \"facebook/wav2vec2-base\"\n",
    "SR_EXPECTED    = 16000\n",
    "TINY_THRESH    = 1e-4\n",
    "\n",
    "# Batch sizing: PER_DEVICE_BS controls runtime memory; GRAD_ACCUM kept for consistent prints.\n",
    "EFFECTIVE_BS   = 64\n",
    "PER_DEVICE_BS  = 16\n",
    "GRAD_ACCUM     = max(1, EFFECTIVE_BS // PER_DEVICE_BS)\n",
    "\n",
    "DROPOUT_P      = 0.2\n",
    "\n",
    "NUM_WORKERS    = 0\n",
    "PIN_MEMORY     = False\n",
    "\n",
    "USE_AMP        = True\n",
    "\n",
    "# Trainval folder selection filter for trainEnh2.\n",
    "REQUIRED_EXP_SUBSTRING = \"trainEnh2\"  # case-insensitive\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "try:\n",
    "    torch.set_float32_matmul_precision(\"high\")\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# Reduce noisy warnings not relevant to test-only inference.\n",
    "warnings.filterwarnings(\"ignore\", message=\"None of the inputs have requires_grad=True. Gradients will be None\")\n",
    "warnings.filterwarnings(\"ignore\", message=\"Passing `gradient_checkpointing` to a config initialization is deprecated\")\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"huggingface_hub\")\n",
    "\n",
    "print(\"D7_OUT_ROOT:\", D7_OUT_ROOT)\n",
    "print(\"D2_OUT_ROOT:\", D2_OUT_ROOT)\n",
    "print(\"D2_MANIFEST_ALL:\", D2_MANIFEST_ALL)\n",
    "print(\"DEVICE:\", DEVICE)\n",
    "if DEVICE.type == \"cuda\":\n",
    "    print(\"GPU:\", torch.cuda.get_device_name(0))\n",
    "print(\"PER_DEVICE_BS:\", PER_DEVICE_BS, \"| EFFECTIVE_BS:\", PER_DEVICE_BS * GRAD_ACCUM)\n",
    "print(\"NUM_WORKERS:\", NUM_WORKERS, \"| PIN_MEMORY:\", PIN_MEMORY)\n",
    "print(\"USE_AMP:\", bool(USE_AMP and DEVICE.type == \"cuda\"))\n",
    "print(\"Enhanced exp required substring (case-insensitive):\", REQUIRED_EXP_SUBSTRING)\n",
    "\n",
    "# -------------------------\n",
    "# 4) Load D2 manifest and build the D2 TEST table\n",
    "#    - Keeps only split == \"test\"\n",
    "#    - Confirms the manifest belongs to dataset \"D2\" (Guard A)\n",
    "# -------------------------\n",
    "if not os.path.exists(D2_MANIFEST_ALL):\n",
    "    raise FileNotFoundError(f\"Missing D2 manifest_all.csv: {D2_MANIFEST_ALL}\")\n",
    "\n",
    "m_all = pd.read_csv(D2_MANIFEST_ALL)\n",
    "\n",
    "# Basic required columns for testing and group analysis.\n",
    "req_cols = {\"split\", \"clip_path\", \"label_num\", \"task\", \"sex\", \"age\"}\n",
    "missing = [c for c in sorted(req_cols) if c not in m_all.columns]\n",
    "if missing:\n",
    "    raise ValueError(f\"D2 manifest missing required columns: {missing}. Found: {list(m_all.columns)}\")\n",
    "\n",
    "# Infer dataset id from the most common value in the manifest (then filter to it).\n",
    "if \"dataset\" in m_all.columns and m_all[\"dataset\"].notna().any():\n",
    "    d2_dataset_id = str(m_all[\"dataset\"].astype(str).value_counts(dropna=True).idxmax())\n",
    "    m_all = m_all[m_all[\"dataset\"].astype(str) == d2_dataset_id].copy()\n",
    "else:\n",
    "    d2_dataset_id = \"DX\"\n",
    "\n",
    "# --------- GUARD A ----------\n",
    "# Ensures the evaluation is truly on D2 and not an accidental path mix-up.\n",
    "if d2_dataset_id != \"D2\":\n",
    "    raise RuntimeError(\n",
    "        f\"Expected D2 dataset_id=='D2' but got {d2_dataset_id!r}. \"\n",
    "        \"This usually means D2_OUT_ROOT is wrong or the manifest is not D2. \"\n",
    "        f\"D2_OUT_ROOT={D2_OUT_ROOT}\"\n",
    "    )\n",
    "\n",
    "# Keep a small, consistent set of columns (fill with NaN if missing).\n",
    "keep_cols = [\"clip_path\", \"label_num\", \"task\", \"speaker_id\", \"sex\", \"age\", \"duration_sec\", \"split\"]\n",
    "for c in keep_cols:\n",
    "    if c not in m_all.columns:\n",
    "        m_all[c] = np.nan\n",
    "m_all = m_all[keep_cols].copy()\n",
    "\n",
    "# IMPORTANT: TEST split only\n",
    "test_df = m_all[m_all[\"split\"].astype(str) == \"test\"].reset_index(drop=True)\n",
    "\n",
    "print(f\"\\nD2 dataset inferred: {d2_dataset_id}\")\n",
    "print(f\"D2 TEST rows: {len(test_df)}\")\n",
    "print(\"D2 TEST label counts:\", test_df[\"label_num\"].value_counts(dropna=False).to_dict())\n",
    "print(\"D2 TEST sex counts (raw):\", test_df[\"sex\"].value_counts(dropna=False).to_dict())\n",
    "\n",
    "if len(test_df) == 0:\n",
    "    raise RuntimeError(\"After filtering to split=='test', D2 manifest has 0 rows.\")\n",
    "\n",
    "# -------------------------\n",
    "# 5) Fail-fast: confirm D2 TEST clips exist on disk\n",
    "#    Stops early with a short list of missing paths.\n",
    "# -------------------------\n",
    "def _fail_fast_missing_paths(df: pd.DataFrame, name: str):\n",
    "    missing_paths = []\n",
    "    for p in tqdm(df[\"clip_path\"].astype(str).tolist(), desc=f\"Check {name} clip_path exists\", dynamic_ncols=True):\n",
    "        if not os.path.exists(p):\n",
    "            missing_paths.append(p)\n",
    "            if len(missing_paths) >= 10:\n",
    "                break\n",
    "    if missing_paths:\n",
    "        raise FileNotFoundError(f\"{name}: missing clip_path(s). Examples: {missing_paths}\")\n",
    "\n",
    "_fail_fast_missing_paths(test_df, \"D2 TEST\")\n",
    "\n",
    "# -------------------------\n",
    "# 6) Add task grouping used by the two-head model\n",
    "#    Exact rule: task == \"vowl\" -> vowel, everything else -> other\n",
    "# -------------------------\n",
    "def _task_group(task_val) -> str:\n",
    "    return \"vowel\" if str(task_val) == \"vowl\" else \"other\"\n",
    "\n",
    "test_df[\"task_group\"] = test_df[\"task\"].apply(_task_group)\n",
    "\n",
    "# -------------------------\n",
    "# 6.5) Normalize sex for fairness and subgroup plots\n",
    "#    D2 encoding is case-sensitive: \"male\"/\"female\", otherwise UNK\n",
    "# -------------------------\n",
    "def normalize_sex_d2_case_sensitive(val) -> str:\n",
    "    if pd.isna(val):\n",
    "        return \"UNK\"\n",
    "    if val == \"male\":\n",
    "        return \"M\"\n",
    "    if val == \"female\":\n",
    "        return \"F\"\n",
    "    return \"UNK\"\n",
    "\n",
    "test_df[\"sex_norm\"] = test_df[\"sex\"].apply(normalize_sex_d2_case_sensitive)\n",
    "print(\"D2 TEST sex counts (normalized):\", test_df[\"sex_norm\"].value_counts(dropna=False).to_dict())\n",
    "if (test_df[\"sex_norm\"] == \"UNK\").any():\n",
    "    print(\"NOTE: Some D2 'sex' values were not exactly 'male'/'female' and were mapped to 'UNK'.\")\n",
    "\n",
    "# -------------------------\n",
    "# 7) Dataset and batching\n",
    "#    - Loads audio from clip_path\n",
    "#    - Builds attention_mask to ignore trailing padding for vowel clips\n",
    "#    - Returns extra fields for predictions.csv\n",
    "# -------------------------\n",
    "class AudioManifestDataset(Dataset):\n",
    "    def __init__(self, df: pd.DataFrame):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        row = self.df.iloc[idx]\n",
    "        clip_path = str(row[\"clip_path\"])\n",
    "        label = int(row[\"label_num\"])\n",
    "        task_group = str(row[\"task_group\"])\n",
    "        sex_norm = str(row[\"sex_norm\"])\n",
    "        speaker_id = row[\"speaker_id\"] if \"speaker_id\" in row.index else np.nan\n",
    "\n",
    "        if not os.path.exists(clip_path):\n",
    "            raise FileNotFoundError(f\"Missing clip_path: {clip_path}\")\n",
    "\n",
    "        y, sr = sf.read(clip_path, always_2d=False)\n",
    "        if isinstance(y, np.ndarray) and y.ndim > 1:\n",
    "            y = np.mean(y, axis=1)\n",
    "        y = np.asarray(y, dtype=np.float32)\n",
    "\n",
    "        # Strict sample rate check to match training assumptions.\n",
    "        if int(sr) != SR_EXPECTED:\n",
    "            raise ValueError(f\"Sample rate mismatch (expected {SR_EXPECTED}): {clip_path} has sr={sr}\")\n",
    "\n",
    "        # Attention mask logic:\n",
    "        # - vowel: mask out trailing near-zero tail (reduces learning from padded silence)\n",
    "        # - other: keep all samples\n",
    "        attn = np.ones((len(y),), dtype=np.int64)\n",
    "        if task_group == \"vowel\":\n",
    "            k = -1\n",
    "            for j in range(len(y) - 1, -1, -1):\n",
    "                if abs(float(y[j])) > TINY_THRESH:\n",
    "                    k = j\n",
    "                    break\n",
    "            if k >= 0:\n",
    "                attn[:k+1] = 1\n",
    "                attn[k+1:] = 0\n",
    "            else:\n",
    "                attn[:] = 1\n",
    "        else:\n",
    "            attn[:] = 1\n",
    "\n",
    "        return {\n",
    "            \"input_values\": torch.from_numpy(y),\n",
    "            \"attention_mask\": torch.from_numpy(attn),\n",
    "            \"labels\": torch.tensor(label, dtype=torch.long),\n",
    "            \"task_group\": task_group,\n",
    "            \"sex_norm\": sex_norm,\n",
    "            \"clip_path\": clip_path,\n",
    "            \"speaker_id\": speaker_id,\n",
    "        }\n",
    "\n",
    "# Pads variable-length audio in a batch and carries over metadata lists.\n",
    "def collate_fn(batch):\n",
    "    max_len = int(max(b[\"input_values\"].numel() for b in batch))\n",
    "    input_vals, attn_masks, labels = [], [], []\n",
    "    task_groups, sex_norms, clip_paths, speaker_ids = [], [], [], []\n",
    "    for b in batch:\n",
    "        x = b[\"input_values\"]\n",
    "        a = b[\"attention_mask\"]\n",
    "        pad = max_len - x.numel()\n",
    "        if pad > 0:\n",
    "            x = torch.cat([x, torch.zeros(pad, dtype=x.dtype)], dim=0)\n",
    "            a = torch.cat([a, torch.zeros(pad, dtype=a.dtype)], dim=0)\n",
    "        input_vals.append(x)\n",
    "        attn_masks.append(a)\n",
    "        labels.append(b[\"labels\"])\n",
    "        task_groups.append(b[\"task_group\"])\n",
    "        sex_norms.append(b[\"sex_norm\"])\n",
    "        clip_paths.append(b[\"clip_path\"])\n",
    "        speaker_ids.append(b[\"speaker_id\"])\n",
    "    return {\n",
    "        \"input_values\": torch.stack(input_vals, dim=0),\n",
    "        \"attention_mask\": torch.stack(attn_masks, dim=0),\n",
    "        \"labels\": torch.stack(labels, dim=0),\n",
    "        \"task_group\": task_groups,\n",
    "        \"sex_norm\": sex_norms,\n",
    "        \"clip_path\": clip_paths,\n",
    "        \"speaker_id\": speaker_ids,\n",
    "    }\n",
    "\n",
    "# -------------------------\n",
    "# 8) Model definition (backbone frozen, heads loaded from trainval)\n",
    "#    - Two heads: one for vowel, one for other\n",
    "#    - Chooses head per sample using task_group\n",
    "# -------------------------\n",
    "class Wav2Vec2TwoHeadClassifier(nn.Module):\n",
    "    def __init__(self, ckpt: str, dropout_p: float):\n",
    "        super().__init__()\n",
    "        self.backbone = Wav2Vec2Model.from_pretrained(ckpt, use_safetensors=True)\n",
    "        for p in self.backbone.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "        H = int(self.backbone.config.hidden_size)\n",
    "        self.pre_vowel = nn.Sequential(nn.LayerNorm(H), nn.Dropout(float(dropout_p)))\n",
    "        self.pre_other = nn.Sequential(nn.LayerNorm(H), nn.Dropout(float(dropout_p)))\n",
    "        self.head_vowel = nn.Linear(H, 2)\n",
    "        self.head_other = nn.Linear(H, 2)\n",
    "\n",
    "    # Mean pooling with masking aligned to wav2vec2 feature frames.\n",
    "    def masked_mean_pool(self, last_hidden, attn_mask_samples):\n",
    "        feat_mask = self.backbone._get_feature_vector_attention_mask(last_hidden.shape[1], attn_mask_samples)\n",
    "        feat_mask = feat_mask.to(last_hidden.device).unsqueeze(-1).type_as(last_hidden)\n",
    "        masked = last_hidden * feat_mask\n",
    "        denom = feat_mask.sum(dim=1).clamp(min=1.0)\n",
    "        return masked.sum(dim=1) / denom\n",
    "\n",
    "    # Forces head computation in fp32 (stable with AMP).\n",
    "    def _heads_fp32(self, x_any: torch.Tensor, head: nn.Module) -> torch.Tensor:\n",
    "        x = x_any.float()\n",
    "        if x.is_cuda:\n",
    "            with torch.amp.autocast(device_type=\"cuda\", enabled=False):\n",
    "                return head(x)\n",
    "        return head(x)\n",
    "\n",
    "    # Produces logits for class 0/1 using the head matching task_group.\n",
    "    def forward_logits(self, input_values, attention_mask, task_group):\n",
    "        with torch.no_grad():\n",
    "            out = self.backbone(input_values=input_values, attention_mask=attention_mask)\n",
    "            last_hidden = out.last_hidden_state\n",
    "\n",
    "        pooled = self.masked_mean_pool(last_hidden, attention_mask)\n",
    "\n",
    "        z_v = self.pre_vowel(pooled.float())\n",
    "        z_o = self.pre_other(pooled.float())\n",
    "\n",
    "        logits_v = self._heads_fp32(z_v, self.head_vowel)\n",
    "        logits_o = self._heads_fp32(z_o, self.head_other)\n",
    "\n",
    "        is_vowel = torch.tensor([tg == \"vowel\" for tg in task_group], device=pooled.device, dtype=torch.bool)\n",
    "        logits = logits_o.clone()\n",
    "        if is_vowel.any():\n",
    "            logits[is_vowel] = logits_v[is_vowel]\n",
    "        return logits\n",
    "\n",
    "# -------------------------\n",
    "# 9) Metrics and plotting helpers\n",
    "#    - AUROC uses probabilities for class 1\n",
    "#    - Threshold metrics use a fixed threshold (global mean from trainval)\n",
    "# -------------------------\n",
    "def compute_auc(y_true, y_prob):\n",
    "    y_true = np.asarray(y_true, dtype=np.int64)\n",
    "    y_prob = np.asarray(y_prob, dtype=np.float64)\n",
    "    if len(np.unique(y_true)) < 2:\n",
    "        return float(\"nan\")\n",
    "    return float(roc_auc_score(y_true, y_prob))\n",
    "\n",
    "def compute_threshold_metrics(y_true, y_prob, thr=0.5):\n",
    "    y_true = np.asarray(y_true, dtype=np.int64)\n",
    "    y_prob = np.asarray(y_prob, dtype=np.float64)\n",
    "    y_pred = (y_prob >= float(thr)).astype(np.int64)\n",
    "\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=[0, 1])\n",
    "    TN, FP, FN, TP = int(cm[0, 0]), int(cm[0, 1]), int(cm[1, 0]), int(cm[1, 1])\n",
    "\n",
    "    eps = 1e-12\n",
    "    acc = (TP + TN) / max(1, (TP + TN + FP + FN))\n",
    "    prec = TP / (TP + FP + eps)\n",
    "    rec = TP / (TP + FN + eps)     # sensitivity\n",
    "    f1 = 2 * prec * rec / (prec + rec + eps)\n",
    "    spec = TN / (TN + FP + eps)\n",
    "\n",
    "    try:\n",
    "        mcc = float(matthews_corrcoef(y_true, y_pred)) if len(np.unique(y_pred)) > 1 else float(\"nan\")\n",
    "    except Exception:\n",
    "        mcc = float(\"nan\")\n",
    "\n",
    "    pval = float(\"nan\")\n",
    "    try:\n",
    "        from scipy.stats import fisher_exact  # type: ignore\n",
    "        _, pval = fisher_exact([[TN, FP], [FN, TP]], alternative=\"two-sided\")\n",
    "        pval = float(pval)\n",
    "    except Exception:\n",
    "        pval = float(\"nan\")\n",
    "\n",
    "    return {\n",
    "        \"threshold\": float(thr),\n",
    "        \"tn\": TN, \"fp\": FP, \"fn\": FN, \"tp\": TP,\n",
    "        \"accuracy\": float(acc),\n",
    "        \"precision\": float(prec),\n",
    "        \"recall\": float(rec),\n",
    "        \"f1\": float(f1),\n",
    "        \"sensitivity\": float(rec),\n",
    "        \"specificity\": float(spec),\n",
    "        \"mcc\": float(mcc),\n",
    "        \"p_value_fisher_two_sided\": float(pval),\n",
    "    }\n",
    "\n",
    "# Saves a basic ROC curve image for quick inspection.\n",
    "def save_roc_curve_png(y_true, y_prob, out_png, title_suffix=\"Test\"):\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_prob)\n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr)\n",
    "    plt.plot([0, 1], [0, 1], linestyle=\"--\")\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(f\"ROC Curve ({title_suffix})\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_png, dpi=150)\n",
    "    plt.close()\n",
    "\n",
    "# Saves a basic confusion matrix image at a chosen threshold.\n",
    "def save_confusion_png(y_true, y_prob, out_png, thr=0.5, title_suffix=\"Test\"):\n",
    "    y_true = np.asarray(y_true, dtype=np.int64)\n",
    "    y_prob = np.asarray(y_prob, dtype=np.float64)\n",
    "    y_pred = (y_prob >= float(thr)).astype(int)\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=[0, 1])\n",
    "\n",
    "    plt.figure()\n",
    "    plt.imshow(cm)\n",
    "    plt.xticks([0, 1], [\"Healthy(0)\", \"PD(1)\"])\n",
    "    plt.yticks([0, 1], [\"Healthy(0)\", \"PD(1)\"])\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"True\")\n",
    "    plt.title(f\"Confusion Matrix ({title_suffix}, thr={thr:.4f})\")\n",
    "    for (i, j), v in np.ndenumerate(cm):\n",
    "        plt.text(j, i, str(v), ha=\"center\", va=\"center\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_png, dpi=150)\n",
    "    plt.close()\n",
    "\n",
    "# -------------------------\n",
    "# 9.5) Fairness helper (H3)\n",
    "#    Computes FNR for M and F, then ΔFNR = FNR(F) - FNR(M)\n",
    "# -------------------------\n",
    "def compute_fnr_by_group_signed(y_true, y_prob, groups, thr=0.5):\n",
    "    y_true = np.asarray(y_true, dtype=np.int64)\n",
    "    y_prob = np.asarray(y_prob, dtype=np.float64)\n",
    "    groups = np.asarray(list(groups), dtype=object)\n",
    "    y_pred = (y_prob >= float(thr)).astype(np.int64)\n",
    "\n",
    "    out = {}\n",
    "    for g in sorted(set(groups.tolist())):\n",
    "        mask_g = (groups == g)\n",
    "        if int(mask_g.sum()) == 0:\n",
    "            continue\n",
    "\n",
    "        pos_mask = mask_g & (y_true == 1)\n",
    "        n_pos = int(pos_mask.sum())\n",
    "        if n_pos == 0:\n",
    "            out[g] = {\"n_total\": int(mask_g.sum()), \"n_pos\": 0, \"tp\": 0, \"fn\": 0, \"fnr\": float(\"nan\")}\n",
    "            continue\n",
    "\n",
    "        tp = int(((y_pred == 1) & pos_mask).sum())\n",
    "        fn = int(((y_pred == 0) & pos_mask).sum())\n",
    "        fnr = float(fn / max(1, (fn + tp)))\n",
    "        out[g] = {\"n_total\": int(mask_g.sum()), \"n_pos\": int(n_pos), \"tp\": int(tp), \"fn\": int(fn), \"fnr\": float(fnr)}\n",
    "\n",
    "    fnr_m = out.get(\"M\", {}).get(\"fnr\", float(\"nan\"))\n",
    "    fnr_f = out.get(\"F\", {}).get(\"fnr\", float(\"nan\"))\n",
    "    if (not np.isnan(fnr_m)) and (not np.isnan(fnr_f)):\n",
    "        delta_signed = float(fnr_f - fnr_m)\n",
    "        delta_abs = float(abs(delta_signed))\n",
    "    else:\n",
    "        delta_signed = float(\"nan\")\n",
    "        delta_abs = float(\"nan\")\n",
    "\n",
    "    return out, delta_signed, delta_abs\n",
    "\n",
    "# Small helpers to report confusion matrices by subgroup.\n",
    "def compute_confusion_counts(y_true, y_prob, thr=0.5):\n",
    "    y_true = np.asarray(y_true, dtype=np.int64)\n",
    "    y_prob = np.asarray(y_prob, dtype=np.float64)\n",
    "    y_pred = (y_prob >= float(thr)).astype(np.int64)\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=[0, 1])\n",
    "    return {\"TN\": int(cm[0, 0]), \"FP\": int(cm[0, 1]), \"FN\": int(cm[1, 0]), \"TP\": int(cm[1, 1])}\n",
    "\n",
    "def compute_confusion_by_group(y_true, y_prob, groups, thr=0.5):\n",
    "    y_true = np.asarray(y_true, dtype=np.int64)\n",
    "    y_prob = np.asarray(y_prob, dtype=np.float64)\n",
    "    groups = np.asarray(list(groups), dtype=object)\n",
    "    out = {}\n",
    "    for g in sorted(set(groups.tolist())):\n",
    "        mask = (groups == g)\n",
    "        if int(mask.sum()) == 0:\n",
    "            continue\n",
    "        out[g] = {\"n\": int(mask.sum()), \"confusion\": compute_confusion_counts(y_true[mask], y_prob[mask], thr=thr)}\n",
    "    return out\n",
    "\n",
    "# -------------------------\n",
    "# 10) Seed control\n",
    "#    Keeps results repeatable for a given seed.\n",
    "# -------------------------\n",
    "def set_all_seeds(seed: int):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# -------------------------\n",
    "# 11) Find the most recent matching D7 trainval experiment (trainEnh2)\n",
    "#    Requires: summary_trainval.json + best_heads.pt for all three seeds.\n",
    "# -------------------------\n",
    "TRAINVAL_ROOT = Path(D7_OUT_ROOT) / \"trainval_runs\"\n",
    "if not TRAINVAL_ROOT.exists():\n",
    "    raise FileNotFoundError(f\"Missing trainval_runs folder under D7_OUT_ROOT: {str(TRAINVAL_ROOT)}\")\n",
    "\n",
    "exp_dirs = sorted([p for p in TRAINVAL_ROOT.glob(\"exp_*\") if p.is_dir()], key=lambda p: p.stat().st_mtime, reverse=True)\n",
    "if not exp_dirs:\n",
    "    raise FileNotFoundError(f\"No exp_* folders found under: {str(TRAINVAL_ROOT)}\")\n",
    "\n",
    "train_dataset_id = \"D7\"  # run folder naming from trainval code (run_D7_seedXXXX)\n",
    "\n",
    "def _is_enhanced_exp_dir(exp_path: Path, required_substring: str) -> bool:\n",
    "    return (required_substring.lower() in exp_path.name.lower())\n",
    "\n",
    "def _has_all_seeds_and_summary(exp_path: Path, dataset_id: str, seeds: list) -> bool:\n",
    "    summary_path = exp_path / \"summary_trainval.json\"\n",
    "    if not summary_path.exists():\n",
    "        return False\n",
    "    for s in seeds:\n",
    "        p = exp_path / f\"run_{dataset_id}_seed{s}\" / \"best_heads.pt\"\n",
    "        if not p.exists():\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "chosen_exp = None\n",
    "for ed in exp_dirs:\n",
    "    if not _is_enhanced_exp_dir(ed, REQUIRED_EXP_SUBSTRING):\n",
    "        continue\n",
    "    if _has_all_seeds_and_summary(ed, train_dataset_id, SEEDS):\n",
    "        chosen_exp = ed\n",
    "        break\n",
    "\n",
    "if chosen_exp is None:\n",
    "    sample = exp_dirs[0]\n",
    "    raise FileNotFoundError(\n",
    "        \"Could not find a recent D7 *ENHANCED* trainval experiment folder that:\\n\"\n",
    "        f\"  (1) contains substring '{REQUIRED_EXP_SUBSTRING}' (case-insensitive) in the exp folder name, and\\n\"\n",
    "        \"  (2) contains all 3 best_heads.pt files + summary_trainval.json.\\n\\n\"\n",
    "        f\"Most recent exp checked (for reference): {str(sample)}\"\n",
    "    )\n",
    "\n",
    "# Run tag uses the full experiment folder name (keeps traceability across runs).\n",
    "FULL_TRAINVAL_EXP_TAG = chosen_exp.name\n",
    "TAG_SAFE = _sanitize_tag(FULL_TRAINVAL_EXP_TAG)\n",
    "RUN_PARENT_DIRNAME = f\"run_{TAG_SAFE}__{RUN_STAMP}\"\n",
    "\n",
    "# Output structure: one stamped folder per run + one stable folder per tag.\n",
    "TEST_ROOT = Path(D7_OUT_ROOT) / \"monolingual_test_runs\"\n",
    "RUN_ROOT  = TEST_ROOT / RUN_PARENT_DIRNAME\n",
    "RUN_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "TAG_ROOT = TEST_ROOT / f\"run_{TAG_SAFE}\"\n",
    "TAG_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Builder-aligned config and logs folders (stable for this enhancement tag).\n",
    "ENH_TAG_SAFE = _sanitize_tag(REQUIRED_EXP_SUBSTRING)\n",
    "cfg_dir  = Path(D7_OUT_ROOT) / \"config\" / f\"D7_{ENH_TAG_SAFE}_on_D2_Test\"\n",
    "logs_dir = Path(D7_OUT_ROOT) / \"logs\"   / f\"D7_{ENH_TAG_SAFE}_on_D2_Test\"\n",
    "cfg_dir.mkdir(parents=True, exist_ok=True)\n",
    "logs_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "RUN_CONFIG_PATH       = cfg_dir / \"run_config.json\"\n",
    "WARNINGS_CSV_PATH     = logs_dir / \"preprocess_warnings.csv\"\n",
    "DATASET_SUMMARY_PATH  = logs_dir / \"dataset_summary.json\"\n",
    "\n",
    "print(\"\\nUsing D7 ENHANCED Train+Val experiment folder:\")\n",
    "print(\" \", str(chosen_exp))\n",
    "print(\"FULL_TRAINVAL_EXP_TAG:\", FULL_TRAINVAL_EXP_TAG)\n",
    "print(\"RUN_ROOT:\", str(RUN_ROOT))\n",
    "print(\"cfg_dir:\", str(cfg_dir))\n",
    "print(\"logs_dir:\", str(logs_dir))\n",
    "\n",
    "# --------- GUARD B ----------\n",
    "# Double-check that the chosen folder still contains the required head files.\n",
    "for s in SEEDS:\n",
    "    p = chosen_exp / f\"run_{train_dataset_id}_seed{s}\" / \"best_heads.pt\"\n",
    "    if not p.exists():\n",
    "        raise RuntimeError(f\"Trainval artifact missing after choosing exp. Missing: {str(p)}\")\n",
    "\n",
    "# Load trainval summary for the global threshold.\n",
    "summary_trainval_path = chosen_exp / \"summary_trainval.json\"\n",
    "with open(summary_trainval_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    d7_trainval_summary = json.load(f)\n",
    "\n",
    "# -------------------------\n",
    "# 11.5) Threshold: use one global mean val-opt threshold for all seeds\n",
    "#    Falls back to 0.5 if the summary value is missing.\n",
    "# -------------------------\n",
    "val_opt_obj = (d7_trainval_summary or {}).get(\"val_optimal_threshold\", {}) or {}\n",
    "thr_mean_sd = (val_opt_obj.get(\"mean_sd\", {}) or {})\n",
    "\n",
    "def _get_mean_val_opt_threshold() -> float:\n",
    "    try:\n",
    "        return float(thr_mean_sd.get(\"mean\", float(\"nan\")))\n",
    "    except Exception:\n",
    "        return float(\"nan\")\n",
    "\n",
    "THR_MEAN_FROM_TRAINVAL = _get_mean_val_opt_threshold()\n",
    "\n",
    "if np.isnan(THR_MEAN_FROM_TRAINVAL):\n",
    "    THR_USED_GLOBAL = 0.5\n",
    "    THR_GLOBAL_NOTE = (\n",
    "        \"Mean val-opt threshold was missing/NaN in D7 enhanced summary_trainval.json. \"\n",
    "        \"Fallback: THR_USED_GLOBAL=0.5 for ALL seeds.\"\n",
    "    )\n",
    "else:\n",
    "    THR_USED_GLOBAL = float(THR_MEAN_FROM_TRAINVAL)\n",
    "    THR_GLOBAL_NOTE = None\n",
    "\n",
    "print(\"\\nVAL-opt threshold selection for TEST (GLOBAL):\")\n",
    "print(\"  Source: summary_trainval.json -> val_optimal_threshold.mean_sd.mean\")\n",
    "print(f\"  THR_USED_GLOBAL: {THR_USED_GLOBAL:.6f}\")\n",
    "if THR_GLOBAL_NOTE is not None:\n",
    "    print(\"  NOTE:\", THR_GLOBAL_NOTE)\n",
    "\n",
    "# -------------------------\n",
    "# 13) Build DataLoader for D2 TEST\n",
    "# -------------------------\n",
    "test_ds = AudioManifestDataset(test_df)\n",
    "test_loader = DataLoader(\n",
    "    test_ds,\n",
    "    batch_size=PER_DEVICE_BS,\n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=PIN_MEMORY,\n",
    "    collate_fn=collate_fn,\n",
    ")\n",
    "\n",
    "# -------------------------\n",
    "# 14) Warm-up: read a few batches to catch data issues early\n",
    "# -------------------------\n",
    "print(\"\\nWarm-up: loading up to 3 D2 TEST batches...\")\n",
    "t0 = time.time()\n",
    "\n",
    "def _warmup(loader, name):\n",
    "    nb = len(loader)\n",
    "    wb = min(3, nb)\n",
    "    if wb == 0:\n",
    "        raise RuntimeError(f\"{name} DataLoader has 0 batches. Check df length and PER_DEVICE_BS.\")\n",
    "    it = iter(loader)\n",
    "    for i in range(wb):\n",
    "        _ = next(it)\n",
    "        print(f\"  loaded warmup {name} batch {i+1}/{wb}\")\n",
    "\n",
    "_warmup(test_loader, \"D2 TEST\")\n",
    "print(f\"Warm-up done in {time.time()-t0:.2f}s\")\n",
    "\n",
    "# -------------------------\n",
    "# 15) Load heads from best_heads.pt into the model skeleton\n",
    "#    Backbone stays frozen; only head weights are loaded.\n",
    "# -------------------------\n",
    "def load_heads_into_model(model: Wav2Vec2TwoHeadClassifier, best_heads_path: Path):\n",
    "    if not best_heads_path.exists():\n",
    "        raise FileNotFoundError(f\"Missing best_heads.pt: {str(best_heads_path)}\")\n",
    "    state = torch.load(str(best_heads_path), map_location=\"cpu\")\n",
    "    model.pre_vowel.load_state_dict(state[\"pre_vowel\"], strict=True)\n",
    "    model.pre_other.load_state_dict(state[\"pre_other\"], strict=True)\n",
    "    model.head_vowel.load_state_dict(state[\"head_vowel\"], strict=True)\n",
    "    model.head_other.load_state_dict(state[\"head_other\"], strict=True)\n",
    "    return model\n",
    "\n",
    "# -------------------------\n",
    "# 16) Inference helper\n",
    "#    Returns probabilities plus metadata needed for predictions.csv.\n",
    "# -------------------------\n",
    "def _infer_probs_with_meta(loader, model, desc):\n",
    "    use_amp = bool(USE_AMP and DEVICE.type == \"cuda\")\n",
    "    all_probs, all_true, all_sex = [], [], []\n",
    "    all_clip, all_spk, all_task = [], [], []\n",
    "\n",
    "    pbar = tqdm(loader, desc=desc, dynamic_ncols=True)\n",
    "    with torch.inference_mode():\n",
    "        for batch in pbar:\n",
    "            input_values = batch[\"input_values\"].to(DEVICE, non_blocking=False)\n",
    "            attention_mask = batch[\"attention_mask\"].to(DEVICE, non_blocking=False)\n",
    "            labels = batch[\"labels\"].to(DEVICE, non_blocking=False)\n",
    "            task_group = batch[\"task_group\"]\n",
    "            sex_norm = batch[\"sex_norm\"]\n",
    "            clip_paths = batch[\"clip_path\"]\n",
    "            speaker_ids = batch[\"speaker_id\"]\n",
    "\n",
    "            with torch.amp.autocast(device_type=\"cuda\", enabled=use_amp):\n",
    "                logits = model.forward_logits(input_values, attention_mask, task_group)\n",
    "\n",
    "            probs = torch.softmax(logits.float(), dim=-1)[:, 1].detach().cpu().numpy().astype(np.float64)\n",
    "            all_probs.extend(probs.tolist())\n",
    "            all_true.extend(labels.detach().cpu().numpy().astype(np.int64).tolist())\n",
    "            all_sex.extend(list(sex_norm))\n",
    "            all_task.extend(list(task_group))\n",
    "            all_clip.extend(list(clip_paths))\n",
    "            all_spk.extend([(\"\" if (x is None or (isinstance(x, float) and np.isnan(x))) else str(x)) for x in speaker_ids])\n",
    "\n",
    "    return (\n",
    "        np.asarray(all_true, dtype=np.int64),\n",
    "        np.asarray(all_probs, dtype=np.float64),\n",
    "        np.asarray(all_sex, dtype=object),\n",
    "        np.asarray(all_clip, dtype=object),\n",
    "        np.asarray(all_spk, dtype=object),\n",
    "        np.asarray(all_task, dtype=object),\n",
    "    )\n",
    "\n",
    "# -------------------------\n",
    "# 17) Single-seed evaluation\n",
    "#    Writes: metrics.json, predictions.csv, roc_curve.png, confusion matrix images.\n",
    "# -------------------------\n",
    "def run_test_once(seed: int):\n",
    "    set_all_seeds(seed)\n",
    "\n",
    "    run_dir = RUN_ROOT / f\"run_{train_dataset_id}_on_{d2_dataset_id}test_seed{seed}\"\n",
    "    run_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    best_heads_path = chosen_exp / f\"run_{train_dataset_id}_seed{seed}\" / \"best_heads.pt\"\n",
    "\n",
    "    print(f\"\\n[seed={seed}] Loading model + heads from:\")\n",
    "    print(\" \", str(best_heads_path))\n",
    "\n",
    "    model = Wav2Vec2TwoHeadClassifier(BACKBONE_CKPT, dropout_p=DROPOUT_P).to(DEVICE)\n",
    "    model = load_heads_into_model(model, best_heads_path)\n",
    "    model.eval()\n",
    "\n",
    "    # One global threshold across all seeds for apples-to-apples comparison.\n",
    "    thr_used = float(THR_USED_GLOBAL)\n",
    "    thr_note = THR_GLOBAL_NOTE\n",
    "\n",
    "    # Inference (also collects fields for predictions.csv)\n",
    "    yt_true, yt_prob, yt_sex, yt_clip, yt_spk, yt_task = _infer_probs_with_meta(\n",
    "        test_loader, model, desc=f\"[seed={seed}] Test (D2 TEST)\"\n",
    "    )\n",
    "    test_auc = compute_auc(yt_true, yt_prob)\n",
    "\n",
    "    # Metrics and subgroup diagnostics at thr_used\n",
    "    thr_metrics_test = compute_threshold_metrics(yt_true, yt_prob, thr=thr_used)\n",
    "    fnr_by_sex, delta_f_minus_m, delta_abs = compute_fnr_by_group_signed(yt_true, yt_prob, yt_sex, thr=thr_used)\n",
    "    confusion_by_sex = compute_confusion_by_group(yt_true, yt_prob, yt_sex, thr=thr_used)\n",
    "\n",
    "    # Plots (overall)\n",
    "    roc_png = run_dir / \"roc_curve.png\"\n",
    "    cm_png  = run_dir / \"confusion_matrix.png\"\n",
    "    save_roc_curve_png(yt_true, yt_prob, str(roc_png), title_suffix=f\"D2 TEST (seed={seed})\")\n",
    "    save_confusion_png(yt_true, yt_prob, str(cm_png), thr=thr_used, title_suffix=f\"D2 TEST (seed={seed})\")\n",
    "\n",
    "    # Plots (by sex: M/F only)\n",
    "    cm_m_png = None\n",
    "    cm_f_png = None\n",
    "    mask_m = (yt_sex == \"M\")\n",
    "    mask_f = (yt_sex == \"F\")\n",
    "\n",
    "    if int(mask_m.sum()) > 0:\n",
    "        cm_m_png = run_dir / \"confusion_matrix_M.png\"\n",
    "        save_confusion_png(yt_true[mask_m], yt_prob[mask_m], str(cm_m_png), thr=thr_used, title_suffix=f\"D2 TEST SEX=M (seed={seed})\")\n",
    "\n",
    "    if int(mask_f.sum()) > 0:\n",
    "        cm_f_png = run_dir / \"confusion_matrix_F.png\"\n",
    "        save_confusion_png(yt_true[mask_f], yt_prob[mask_f], str(cm_f_png), thr=thr_used, title_suffix=f\"D2 TEST SEX=F (seed={seed})\")\n",
    "\n",
    "    # predictions.csv: one row per clip, includes metadata for later analysis.\n",
    "    pred_df = pd.DataFrame({\n",
    "        \"clip_path\": yt_clip.astype(str),\n",
    "        \"y_true\": yt_true.astype(int),\n",
    "        \"y_score\": yt_prob.astype(float),\n",
    "        \"sex_norm\": yt_sex.astype(str),\n",
    "        \"speaker_id\": yt_spk.astype(str),\n",
    "        \"task_group\": yt_task.astype(str),\n",
    "        \"seed\": int(seed),\n",
    "        \"trainval_exp_tag\": str(FULL_TRAINVAL_EXP_TAG),\n",
    "        \"run_stamp\": str(RUN_STAMP),\n",
    "        \"threshold_used_global\": float(thr_used),\n",
    "    })\n",
    "    pred_csv_path = run_dir / \"predictions.csv\"\n",
    "    pred_df.to_csv(pred_csv_path, index=False)\n",
    "\n",
    "    # metrics.json: per-seed results and pointers to artifacts.\n",
    "    metrics = {\n",
    "        \"train_dataset\": train_dataset_id,\n",
    "        \"test_dataset\": d2_dataset_id,\n",
    "        \"seed\": int(seed),\n",
    "\n",
    "        \"n_test\": int(len(test_df)),\n",
    "        \"label_counts_test\": test_df[\"label_num\"].value_counts(dropna=False).to_dict(),\n",
    "        \"sex_counts_test_norm\": test_df[\"sex_norm\"].value_counts(dropna=False).to_dict(),\n",
    "\n",
    "        \"test_auroc\": float(test_auc),\n",
    "\n",
    "        \"threshold_source\": \"D7 enhanced trainval summary_trainval.json -> val_optimal_threshold.mean_sd.mean\",\n",
    "        \"trainval_experiment_used\": str(chosen_exp),\n",
    "        \"trainval_exp_tag\": str(FULL_TRAINVAL_EXP_TAG),\n",
    "        \"trainval_summary_path\": str(summary_trainval_path),\n",
    "\n",
    "        \"test_threshold_used_global\": float(thr_used),\n",
    "        \"test_threshold_note_global\": thr_note,\n",
    "\n",
    "        \"threshold_metrics_test_at_thr_used\": thr_metrics_test,\n",
    "\n",
    "        \"fairness_test_at_thr_used\": {\n",
    "            \"definition\": \"H3: ΔFNR = FNR(F) - FNR(M), where FNR(sex)=FN/(FN+TP) computed on PD-only true labels at test_threshold_used_global.\",\n",
    "            \"threshold_used\": float(thr_used),\n",
    "            \"fnr_by_sex_norm\": fnr_by_sex,\n",
    "            \"delta_fnr_F_minus_M\": float(delta_f_minus_m),\n",
    "            \"delta_fnr_abs\": float(delta_abs),\n",
    "            \"note\": \"If n_PD for a sex is 0, its FNR is NaN and ΔFNR is NaN.\",\n",
    "            \"sex_normalization_note\": \"D2 mapping: exact 'male'->M and 'female'->F (case-sensitive); otherwise UNK.\",\n",
    "        },\n",
    "\n",
    "        \"confusion_by_sex_norm_at_thr_used\": confusion_by_sex,\n",
    "\n",
    "        \"artifacts\": {\n",
    "            \"predictions_csv\": str(pred_csv_path),\n",
    "            \"roc_curve_png\": str(roc_png),\n",
    "            \"confusion_matrix_png\": str(cm_png),\n",
    "            \"confusion_matrix_M_png\": str(cm_m_png) if cm_m_png is not None else None,\n",
    "            \"confusion_matrix_F_png\": str(cm_f_png) if cm_f_png is not None else None,\n",
    "        },\n",
    "\n",
    "        \"d7_out_root\": D7_OUT_ROOT,\n",
    "        \"d2_out_root\": D2_OUT_ROOT,\n",
    "        \"d2_manifest_all\": D2_MANIFEST_ALL,\n",
    "\n",
    "        \"best_heads_path\": str(best_heads_path),\n",
    "        \"backbone_ckpt\": BACKBONE_CKPT,\n",
    "        \"dropout_p\": float(DROPOUT_P),\n",
    "        \"timestamp\": time.strftime(\"%Y%m%d_%H%M%S\"),\n",
    "    }\n",
    "\n",
    "    metrics_path = run_dir / \"metrics.json\"\n",
    "    with open(metrics_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(metrics, f, indent=2)\n",
    "\n",
    "    print(f\"[seed={seed}] DONE | test_AUROC={test_auc:.6f}\")\n",
    "    print(f\"[seed={seed}] Threshold used (GLOBAL mean from D7 enhanced trainval): {thr_used:.6f}\")\n",
    "    print(f\"[seed={seed}] WROTE:\")\n",
    "    print(\" \", str(metrics_path))\n",
    "    print(\" \", str(pred_csv_path))\n",
    "    print(\" \", str(roc_png))\n",
    "    print(\" \", str(cm_png))\n",
    "\n",
    "    return {\n",
    "        \"seed\": int(seed),\n",
    "        \"thr_used\": float(thr_used),\n",
    "        \"thr_note\": thr_note,\n",
    "        \"test_auc\": float(test_auc),\n",
    "        \"thr_metrics_test\": thr_metrics_test,\n",
    "        \"fnr_by_sex\": fnr_by_sex,\n",
    "        \"delta_signed\": float(delta_f_minus_m),\n",
    "        \"delta_abs\": float(delta_abs),\n",
    "        \"run_dir\": str(run_dir),\n",
    "        \"predictions_csv\": str(pred_csv_path),\n",
    "    }\n",
    "\n",
    "# -------------------------\n",
    "# 18) Run all seeds and aggregate results\n",
    "#    - AUROC: mean ± 95% CI (t, n=3)\n",
    "#    - Other metrics and fairness: mean ± SD\n",
    "# -------------------------\n",
    "results = []\n",
    "for seed in SEEDS:\n",
    "    results.append(run_test_once(seed))\n",
    "\n",
    "aurocs = [r[\"test_auc\"] for r in results]\n",
    "t_crit = 4.302652729911275  # df=2, 95% CI\n",
    "n = len(aurocs)\n",
    "mean_auc = float(np.mean(aurocs))\n",
    "std_auc = float(np.std(aurocs, ddof=1)) if n > 1 else 0.0\n",
    "half_width = float(t_crit * (std_auc / math.sqrt(n))) if n > 1 else 0.0\n",
    "ci95 = [float(mean_auc - half_width), float(mean_auc + half_width)]\n",
    "\n",
    "def _mean_sd(vals):\n",
    "    vals = np.asarray(vals, dtype=np.float64)\n",
    "    mu = float(np.nanmean(vals)) if np.any(~np.isnan(vals)) else float(\"nan\")\n",
    "    sd = float(np.nanstd(vals, ddof=1)) if np.sum(~np.isnan(vals)) > 1 else 0.0\n",
    "    return mu, sd\n",
    "\n",
    "# Aggregate threshold-based metrics.\n",
    "thr_list = [r[\"thr_metrics_test\"] for r in results]\n",
    "keys = [\"accuracy\",\"precision\",\"recall\",\"f1\",\"sensitivity\",\"specificity\",\"mcc\",\"p_value_fisher_two_sided\"]\n",
    "agg = {}\n",
    "for k in keys:\n",
    "    v = [float(tm.get(k, float(\"nan\"))) for tm in thr_list]\n",
    "    mu, sd = _mean_sd(v)\n",
    "    agg[k] = {\n",
    "        \"mean\": float(mu),\n",
    "        \"sd\": float(sd),\n",
    "        \"values_by_seed\": {str(s): float(tm.get(k, float(\"nan\"))) for s, tm in zip(SEEDS, thr_list)},\n",
    "    }\n",
    "\n",
    "# Confusion counts by seed (useful for spot checks).\n",
    "cm_by_seed = {\n",
    "    str(s): {\"tn\": int(thr_list[i][\"tn\"]), \"fp\": int(thr_list[i][\"fp\"]), \"fn\": int(thr_list[i][\"fn\"]), \"tp\": int(thr_list[i][\"tp\"])}\n",
    "    for i, s in enumerate(SEEDS)\n",
    "}\n",
    "\n",
    "# Fairness aggregation (signed and absolute ΔFNR).\n",
    "fnr_by_seed = {str(r[\"seed\"]): r[\"fnr_by_sex\"] for r in results}\n",
    "delta_signed_by_seed = {str(r[\"seed\"]): float(r[\"delta_signed\"]) for r in results}\n",
    "delta_abs_by_seed = {str(r[\"seed\"]): float(r[\"delta_abs\"]) for r in results}\n",
    "\n",
    "fnr_m_vals, fnr_f_vals = [], []\n",
    "d_signed_vals, d_abs_vals = [], []\n",
    "for r in results:\n",
    "    d = r[\"fnr_by_sex\"] or {}\n",
    "    fnr_m_vals.append(float(d.get(\"M\", {}).get(\"fnr\", float(\"nan\"))))\n",
    "    fnr_f_vals.append(float(d.get(\"F\", {}).get(\"fnr\", float(\"nan\"))))\n",
    "    d_signed_vals.append(float(r[\"delta_signed\"]))\n",
    "    d_abs_vals.append(float(r[\"delta_abs\"]))\n",
    "\n",
    "fnr_m_mean, fnr_m_sd = _mean_sd(fnr_m_vals)\n",
    "fnr_f_mean, fnr_f_sd = _mean_sd(fnr_f_vals)\n",
    "d_signed_mean, d_signed_sd = _mean_sd(d_signed_vals)\n",
    "d_abs_mean, d_abs_sd = _mean_sd(d_abs_vals)\n",
    "\n",
    "# Console summary (quick read without opening files).\n",
    "print(\"\\nTest AUROC by seed:\")\n",
    "for r in results:\n",
    "    print(f\"  seed {r['seed']}: {r['test_auc']:.6f}\")\n",
    "print(f\"\\nMean Test AUROC: {mean_auc:.6f}\")\n",
    "print(f\"95% CI (t, n=3): [{ci95[0]:.6f}, {ci95[1]:.6f}]\")\n",
    "\n",
    "print(\"\\nTEST threshold used (GLOBAL mean val-opt from D7 enhanced trainval):\")\n",
    "print(f\"  THR_USED_GLOBAL: {THR_USED_GLOBAL:.6f}\")\n",
    "if THR_GLOBAL_NOTE is not None:\n",
    "    print(\"  NOTE:\", THR_GLOBAL_NOTE)\n",
    "\n",
    "print(\"\\nThreshold metrics on D2 TEST @ THR_USED_GLOBAL (mean ± SD across seeds):\")\n",
    "for k in [\"accuracy\",\"precision\",\"sensitivity\",\"specificity\",\"f1\",\"mcc\"]:\n",
    "    print(f\"  {k}: {agg[k]['mean']:.6f} ± {agg[k]['sd']:.6f}\")\n",
    "print(\"  fisher_p_value_two_sided:\", f\"{agg['p_value_fisher_two_sided']['mean']:.6g} ± {agg['p_value_fisher_two_sided']['sd']:.6g}\")\n",
    "\n",
    "print(\"\\nFAIRNESS (H3) on D2 TEST @ THR_USED_GLOBAL across seeds (mean ± SD):\")\n",
    "print(f\"  FNR_M: {fnr_m_mean:.6f} ± {fnr_m_sd:.6f}\")\n",
    "print(f\"  FNR_F: {fnr_f_mean:.6f} ± {fnr_f_sd:.6f}\")\n",
    "print(f\"  ΔFNR (F-M): {d_signed_mean:.6f} ± {d_signed_sd:.6f}\")\n",
    "print(f\"  |ΔFNR|: {d_abs_mean:.6f} ± {d_abs_sd:.6f}\")\n",
    "\n",
    "# -------------------------\n",
    "# 18.1) Build run-level summary object for files and pointers\n",
    "#    Includes per-seed results plus aggregated statistics.\n",
    "# -------------------------\n",
    "summary = {\n",
    "    \"run_tag_full_trainval_exp\": str(FULL_TRAINVAL_EXP_TAG),\n",
    "    \"run_tag_safe\": str(TAG_SAFE),\n",
    "    \"enh_tag\": str(REQUIRED_EXP_SUBSTRING),\n",
    "    \"run_stamp\": str(RUN_STAMP),\n",
    "\n",
    "    \"train_dataset\": train_dataset_id,\n",
    "    \"test_dataset\": d2_dataset_id,\n",
    "\n",
    "    \"d7_out_root\": D7_OUT_ROOT,\n",
    "    \"d2_out_root\": D2_OUT_ROOT,\n",
    "    \"d2_manifest_all\": D2_MANIFEST_ALL,\n",
    "\n",
    "    \"enhanced_exp_required_substring_case_insensitive\": REQUIRED_EXP_SUBSTRING,\n",
    "    \"trainval_experiment_used\": str(chosen_exp),\n",
    "    \"trainval_summary_path\": str(summary_trainval_path),\n",
    "\n",
    "    \"seeds\": SEEDS,\n",
    "\n",
    "    \"n_test\": int(len(test_df)),\n",
    "    \"label_counts_test\": test_df[\"label_num\"].value_counts(dropna=False).to_dict(),\n",
    "    \"sex_counts_test_norm\": test_df[\"sex_norm\"].value_counts(dropna=False).to_dict(),\n",
    "\n",
    "    \"test_threshold_used\": {\n",
    "        \"threshold_used_global\": float(THR_USED_GLOBAL),\n",
    "        \"threshold_source\": \"D7 enhanced trainval summary_trainval.json -> val_optimal_threshold.mean_sd.mean\",\n",
    "        \"note_global\": (THR_GLOBAL_NOTE if THR_GLOBAL_NOTE is not None else \"\"),\n",
    "        \"per_seed_repetition_for_audit\": {str(r[\"seed\"]): float(r[\"thr_used\"]) for r in results},\n",
    "    },\n",
    "\n",
    "    \"test_aurocs_by_seed\": {str(r[\"seed\"]): float(r[\"test_auc\"]) for r in results},\n",
    "    \"mean_test_auroc\": float(mean_auc),\n",
    "    \"t_crit_df2_95\": float(t_crit),\n",
    "    \"half_width_95_ci\": float(half_width),\n",
    "    \"ci95_test_auroc\": ci95,\n",
    "\n",
    "    \"threshold_metrics_mean_sd_test_at_thr_used\": agg,\n",
    "    \"confusion_matrix_by_seed_test_at_thr_used\": cm_by_seed,\n",
    "\n",
    "    \"fairness_test_at_thr_used\": {\n",
    "        \"definition\": \"H3: ΔFNR = FNR(F) - FNR(M), where FNR(sex)=FN/(FN+TP) computed on PD-only true labels at test_threshold_used_global.\",\n",
    "        \"fnr_by_sex_norm_by_seed\": fnr_by_seed,\n",
    "        \"delta_fnr_F_minus_M_by_seed\": delta_signed_by_seed,\n",
    "        \"delta_fnr_abs_by_seed\": delta_abs_by_seed,\n",
    "        \"fnr_M_mean_sd\": {\"mean\": float(fnr_m_mean), \"sd\": float(fnr_m_sd)},\n",
    "        \"fnr_F_mean_sd\": {\"mean\": float(fnr_f_mean), \"sd\": float(fnr_f_sd)},\n",
    "        \"delta_fnr_F_minus_M_mean_sd\": {\"mean\": float(d_signed_mean), \"sd\": float(d_signed_sd)},\n",
    "        \"delta_fnr_abs_mean_sd\": {\"mean\": float(d_abs_mean), \"sd\": float(d_abs_sd)},\n",
    "    },\n",
    "\n",
    "    \"run_dirs\": [r[\"run_dir\"] for r in results],\n",
    "    \"predictions_csv_by_seed\": {str(r[\"seed\"]): str(r[\"predictions_csv\"]) for r in results},\n",
    "}\n",
    "\n",
    "# -------------------------\n",
    "# 18.2) Write summary files and pointer files\n",
    "#    - summary_test.json inside this run folder\n",
    "#    - history_index.jsonl append-only\n",
    "#    - summary_latest.json and last_run_pointer.json overwritten for convenience\n",
    "#    - tag_run_pointer.json stable per experiment tag\n",
    "# -------------------------\n",
    "summary_path = RUN_ROOT / \"summary_test.json\"\n",
    "with open(summary_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "history_path = TEST_ROOT / \"history_index.jsonl\"\n",
    "with open(history_path, \"a\", encoding=\"utf-8\") as f:\n",
    "    f.write(json.dumps(summary) + \"\\n\")\n",
    "\n",
    "summary_latest_path = TEST_ROOT / \"summary_latest.json\"\n",
    "latest_summary_obj = {\n",
    "    \"run_tag_full_trainval_exp\": str(FULL_TRAINVAL_EXP_TAG),\n",
    "    \"enh_tag\": str(REQUIRED_EXP_SUBSTRING),\n",
    "    \"run_stamp\": str(RUN_STAMP),\n",
    "    \"run_root\": str(RUN_ROOT),\n",
    "    \"summary_test_json\": str(summary_path),\n",
    "    \"seed_run_dirs\": [str(RUN_ROOT / f\"run_{train_dataset_id}_on_{d2_dataset_id}test_seed{s}\") for s in SEEDS],\n",
    "}\n",
    "with open(summary_latest_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(latest_summary_obj, f, indent=2)\n",
    "\n",
    "global_pointer_path = TEST_ROOT / \"last_run_pointer.json\"\n",
    "with open(global_pointer_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(latest_summary_obj, f, indent=2)\n",
    "\n",
    "tag_pointer_path = TAG_ROOT / \"tag_run_pointer.json\"\n",
    "tag_pointer_obj = dict(latest_summary_obj)\n",
    "tag_pointer_obj[\"tag_root\"] = str(TAG_ROOT)\n",
    "with open(tag_pointer_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(tag_pointer_obj, f, indent=2)\n",
    "\n",
    "print(\"\\nWROTE run summary:\", str(summary_path))\n",
    "print(\"APPENDED history index:\", str(history_path))\n",
    "print(\"WROTE latest summary:\", str(summary_latest_path))\n",
    "print(\"WROTE global pointer:\", str(global_pointer_path))\n",
    "print(\"WROTE tag pointer:\", str(tag_pointer_path))\n",
    "print(\"Open this folder to access artifacts:\", str(RUN_ROOT))\n",
    "\n",
    "# -------------------------\n",
    "# 18.5) Builder-aligned artifacts (config + simple logs placeholder)\n",
    "#    Creates stable config/log summaries under the enhancement tag folder.\n",
    "# -------------------------\n",
    "_backup_if_exists(RUN_CONFIG_PATH)\n",
    "_backup_if_exists(WARNINGS_CSV_PATH)\n",
    "_backup_if_exists(DATASET_SUMMARY_PATH)\n",
    "\n",
    "run_config = {\n",
    "    \"mode\": f\"D7_{ENH_TAG_SAFE}_on_D2_Test\",\n",
    "    \"created_utc\": datetime.utcnow().isoformat(),\n",
    "    \"run_stamp\": RUN_STAMP,\n",
    "\n",
    "    \"enh_tag\": str(REQUIRED_EXP_SUBSTRING),\n",
    "\n",
    "    \"d7_out_root\": D7_OUT_ROOT,\n",
    "    \"d2_out_root\": D2_OUT_ROOT,\n",
    "    \"d2_manifest_all\": D2_MANIFEST_ALL,\n",
    "\n",
    "    \"enhanced_exp_required_substring_case_insensitive\": REQUIRED_EXP_SUBSTRING,\n",
    "    \"trainval_experiment_used\": str(chosen_exp),\n",
    "    \"trainval_exp_tag\": str(FULL_TRAINVAL_EXP_TAG),\n",
    "    \"trainval_summary_path\": str(summary_trainval_path),\n",
    "\n",
    "    \"threshold_source\": \"summary_trainval.json -> val_optimal_threshold.mean_sd.mean\",\n",
    "    \"threshold_used_global\": float(THR_USED_GLOBAL),\n",
    "    \"threshold_note_global\": (THR_GLOBAL_NOTE if THR_GLOBAL_NOTE is not None else \"\"),\n",
    "\n",
    "    \"seeds\": SEEDS,\n",
    "    \"use_amp\": bool(USE_AMP and DEVICE.type == \"cuda\"),\n",
    "    \"per_device_bs\": int(PER_DEVICE_BS),\n",
    "    \"effective_bs\": int(PER_DEVICE_BS * GRAD_ACCUM),\n",
    "    \"num_workers\": int(NUM_WORKERS),\n",
    "    \"pin_memory\": bool(PIN_MEMORY),\n",
    "    \"backbone_ckpt\": BACKBONE_CKPT,\n",
    "    \"dropout_p\": float(DROPOUT_P),\n",
    "\n",
    "    \"monolingual_test_runs_root\": str(TEST_ROOT),\n",
    "    \"run_root\": str(RUN_ROOT),\n",
    "    \"summary_test_json\": str(summary_path),\n",
    "    \"last_run_pointer_json\": str(global_pointer_path),\n",
    "    \"tag_run_pointer_json\": str(tag_pointer_path),\n",
    "}\n",
    "with open(RUN_CONFIG_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(run_config, f, indent=2)\n",
    "\n",
    "dataset_summary = {\n",
    "    \"mode\": f\"D7_{ENH_TAG_SAFE}_on_D2_Test\",\n",
    "    \"created_utc\": datetime.utcnow().isoformat(),\n",
    "    \"status\": \"SUCCESS\",\n",
    "    \"run_stamp\": RUN_STAMP,\n",
    "\n",
    "    \"enh_tag\": str(REQUIRED_EXP_SUBSTRING),\n",
    "\n",
    "    \"d2_dataset_id\": d2_dataset_id,\n",
    "    \"n_test\": int(len(test_df)),\n",
    "    \"label_counts_test\": test_df[\"label_num\"].value_counts(dropna=False).to_dict(),\n",
    "    \"sex_counts_test_raw\": test_df[\"sex\"].value_counts(dropna=False).to_dict(),\n",
    "    \"sex_counts_test_norm\": test_df[\"sex_norm\"].value_counts(dropna=False).to_dict(),\n",
    "\n",
    "    \"trainval_experiment_used\": str(chosen_exp),\n",
    "    \"trainval_exp_tag\": str(FULL_TRAINVAL_EXP_TAG),\n",
    "    \"trainval_summary_path\": str(summary_trainval_path),\n",
    "\n",
    "    \"threshold_used_global\": float(THR_USED_GLOBAL),\n",
    "\n",
    "    \"monolingual_test_runs_root\": str(TEST_ROOT),\n",
    "    \"run_root\": str(RUN_ROOT),\n",
    "    \"summary_test_json\": str(summary_path),\n",
    "    \"last_run_pointer_json\": str(global_pointer_path),\n",
    "}\n",
    "with open(DATASET_SUMMARY_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(dataset_summary, f, indent=2)\n",
    "\n",
    "# Placeholder for symmetry with preprocessing logs.\n",
    "with open(WARNINGS_CSV_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"ts,level,message\\n\")\n",
    "\n",
    "print(\"\\nWROTE (builder-aligned):\")\n",
    "print(\" \", str(RUN_CONFIG_PATH))\n",
    "print(\" \", str(WARNINGS_CSV_PATH))\n",
    "print(\" \", str(DATASET_SUMMARY_PATH))\n",
    "\n",
    "# -------------------------\n",
    "# 19) Unassign runtime (stop GPU)\n",
    "# -------------------------\n",
    "print(\"\\nAll done. Unassigning the runtime...\")\n",
    "try:\n",
    "    from google.colab import runtime  # type: ignore\n",
    "    runtime.unassign()\n",
    "except Exception as e:\n",
    "    print(\"Could not unassign runtime automatically. Stop runtime manually if needed.\")\n",
    "    print(\"Reason:\", repr(e))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "38cae404eb04457aae4ff86e3f60251a",
      "d7b0cdecc5594f2f883304b6e5d23eea",
      "39f53f104dd74acdbe64e529167742eb",
      "dd6875c24c86434081fccb81be58ebce",
      "a2fc0702c99648979743fd44ca3990d3",
      "371786869ce6488cbaf7e22a4a6168ae",
      "e73d107f4f4c425e933e5d4007f46dba",
      "4dcef26fb411471396f4e512ba87b28c",
      "b0a080968e714d54ad568d287ddeab03",
      "b4e56a86281541afb3cbe8e4aa60ed28",
      "cb57d1ad78d742cab584ce411475c497",
      "33f80735d5ad498f999d8896caa8c083",
      "5bed98bff9be4a109ae63a09c074e3bb",
      "1804ba1f97eb45ecb919536ed541b684",
      "5f985cd3cff047df90f6b486164904f7",
      "a438eccddbce4dccbecf9657b6cf2aa2",
      "81cb4ca6a74d42f99bcba6cabbcd207c",
      "bca602946f2e4d94bcab3ba75b3fcb7f",
      "89f6028846ae41f28a44f3b7d893735f",
      "a98ba152598a498388623e034332b4cd",
      "5129f6ee1b454a1fac017bf99b7e8dd1",
      "935bfc7b0d62457da84542a8763b851b",
      "bbce1c569a6c459988d5c03d72d45057",
      "ee33e43f092c4e3c9df037034238e75d",
      "92100755bfab4ecab44d631312434339",
      "c7c7183afc46499f8cb4c5106775a71b",
      "70cca91f4dac4d12ada3b5f91d7267fb",
      "1b18f0066d524cb59ce1e4895378dd61",
      "faa2a7412b8e41c6a4b08f4f9339acc6",
      "69e4230f93cf4b239d3b3255dc4dcf48",
      "11bf125344754d1c8af3748a2fe13b33",
      "ee22ea40164247e1a3891a5f0cf05da0",
      "58119b34fc4f49d987385fd3463fe8c6",
      "ce1c07cd512c4b7f83b1a2c770c93a22",
      "c103e339031d4358b705f35f053fb1e4",
      "c7bbc4e175034a3588e06f293a8bf4cc",
      "4004a4457a3b4996b18e9caed2e4d845",
      "c34c3aaa1ea94d5cb2cf2a149535d198",
      "b969323d871c4d919417b1daf9d8cdcb",
      "050e88f4fc584550ba6143ac1f68edd4",
      "347edacec2394304b696a3f6527f166a",
      "37d8ca7d4aec4a719780318b8be1a7be",
      "bc27518c1cc041c9ba01732495670c72",
      "7e4d50404e454db0bab563483b7f1a94",
      "aebd63aeaab64108ba09a63c7ccf14af",
      "edaea24ee008461687cd1fbc37aaa725",
      "f18ea202dbde49e09682122ffebdc686",
      "76e115f5e243415986a2daf6bc460187",
      "5abb4d12a42c4db8b27cddbba524c7d8",
      "f0a2c98e082a4b408a9a3cf5489d1da7",
      "0427602585e44a43b83bbc4cd917e360",
      "b9c04e8f067148de9c4cae6f2e73ff5e",
      "9fda11c52600438e81512b4c0fd489aa",
      "40c573b15bc94734be8d416ca83f06f0",
      "4fc5351383a84359b64f898f8aa67082",
      "e50c760fcfd04e2c83bd71c8418d69f5",
      "f674f16c895a4793a7cebe5280699b7f",
      "7763b11acb9b4215b88ad18705c67e34",
      "567c14677b3a4dc3ac45029e501bc40c",
      "c71268f416bc49eaadffdeaf3b40ecc1",
      "e8f0525c7a0b45b096d773086ff5ce9b",
      "fddd589b2f5d46be8d1612b1db7bd890",
      "4e0facf494314c5aa068c8cc33cf91f5",
      "986fb9107bb24d9cb59e57056699b07f",
      "6069f34212a14e39975720dd34ed1709",
      "b6d58d8db1a24fde92d56ff973281b6e"
     ]
    },
    "id": "AOuhImLjL-Wc"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "The following cell builds the enhanced D7 training split **train_enh3** by starting with the full original D7 training set and adding a new, speaker-balanced subset from the D2 training data. The D2 portion is about **10 percent of all D2 training speakers** and is selected at the speaker level, so when a speaker is chosen, all of that speaker’s training clips are included. Any D2 speakers that were already used in **train_enh1** or **train_enh2** are excluded, so **train_enh3 only adds D2 speakers that have not appeared in earlier enhanced sets**.\n",
    "\n",
    "The cell first loads `manifest_all.csv` for D7 and D2, along with the manifests from the earlier enhanced builds. It checks that all required columns exist, confirms dataset names are correct, and enforces basic data rules. Any literal `\"NaN\"` strings are converted to real missing values, sex values are standardized to **M** or **F**, and class labels are limited to Healthy (0) and Parkinson’s (1). Using the earlier enhanced manifests, it collects the full list of D2 speakers that have already been used and removes them from the pool of eligible speakers.\n",
    "\n",
    "Next, the cell calculates how many D2 speakers to add. This is based on **10 percent of the total D2 training speaker count**, rounded up and adjusted to an even number so Healthy and Parkinson’s speakers can be selected in equal numbers. This target is fixed before exclusions are applied. If, after exclusions, there are not enough eligible speakers in one or both classes, the target is reduced evenly for Healthy and Parkinson’s speakers, and this adjustment is recorded. A fixed random seed is used so the same speakers are chosen each time the cell is run.\n",
    "\n",
    "With the speaker list finalized, the cell prepares a copy plan for the new folder `clips/train_enh3/`. All original D7 training clips are copied using their original filenames. Clips from the selected D2 speakers are copied using new, predictable filenames that include class, speaker ID, task type, and a stable index. This avoids name conflicts and keeps files easy to trace. Before copying, the cell checks that every source file exists and applies a strict no-overwrite rule: files that already exist with the same size are skipped, while any mismatch or file error stops the run. Initial configuration files and summaries are written at this point so the setup is recorded even if copying does not finish.\n",
    "\n",
    "During the copy step, files are copied without changing the original datasets. The cell tracks how many files are copied and how many are skipped. Any copy failure causes the run to stop with a clear error. After copying completes, the cell creates `manifest_train_enh3.csv` by combining the original D7 training rows with the newly added D2 rows. All rows are labeled with `split = \"train_enh3\"`. File paths are updated to point to the new clip locations, and D2-derived rows include `source_dataset = \"D2\"` while still being treated as part of the D7 training set. The final manifest is checked to confirm that all files exist and that no literal `\"NaN\"` values remain.\n",
    "\n",
    "The cell finishes by writing the final outputs in a safe, non-destructive way. These include the new `train_enh3` clip folder, the training manifest, a detailed run configuration file, a warnings log, and a dataset summary that reports counts and key details of the build."
   ],
   "metadata": {
    "id": "ybs33UHLvbRn"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# D7 trainEnh3 builder: add a new D2 speaker draw into D7 training\n",
    "# Inputs: D7 manifest (train split), D2 manifest (train split), prior train_enh1 and train_enh2 manifests\n",
    "# Outputs: train_enh3 clip folder (copied files), manifest_train_enh3.csv, run_config.json, dataset_summary.json, preprocess_warnings.csv\n",
    "#\n",
    "# =========================\n",
    "# D7 TRAIN_ENH3 BUILDER (CRASH-PROOF, THIRD-PARTY AUDITABLE)\n",
    "# — Add ~9–10% of D2 TRAIN speakers (Definition B) with:\n",
    "#   * FIXED DENOMINATOR POLICY (same as train_enh2):\n",
    "#       target_total = ceil(10% of FULL D2 TRAIN speaker count), then make even\n",
    "#       exclusions only shrink eligible pool, not the denominator\n",
    "#   * FORCE ZERO OVERLAP with prior draws:\n",
    "#       exclude ANY D2 speakers used in manifest_train_enh1 AND manifest_train_enh2\n",
    "#   * If eligible pool is insufficient to meet target_per_class:\n",
    "#       downsize BOTH classes to keep speaker-balanced sampling\n",
    "#\n",
    "# - Creates: <D7_OUT_ROOT>/clips/train_enh3/\n",
    "# - Writes:\n",
    "#     * manifests/manifest_train_enh3.csv\n",
    "#     * config/D7_Enh3_on_D2_Test/run_config.json\n",
    "#     * logs/D7_Enh3_on_D2_Test/preprocess_warnings.csv\n",
    "#     * logs/D7_Enh3_on_D2_Test/dataset_summary.json\n",
    "# - COPY only, no overwrite\n",
    "# =========================\n",
    "\n",
    "import os, json, re, shutil, math\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# -------------------------\n",
    "# 0) Drive mount (only needed in Colab)\n",
    "# -------------------------\n",
    "try:\n",
    "    from google.colab import drive  # type: ignore\n",
    "    if not os.path.exists(\"/content/drive\"):\n",
    "        drive.mount(\"/content/drive\")\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# -------------------------\n",
    "# 1) Key inputs and output pointers\n",
    "#    - Inputs: D7/D2 manifests and prior enh1/enh2 manifests (to enforce zero overlap)\n",
    "#    - Outputs: train_enh3 clips folder + manifest + logs + config\n",
    "# -------------------------\n",
    "D7_OUT_ROOT = Path(\"/content/drive/MyDrive/AI_PD_Project/Datasets/D7-Multilingual (D1_D4_D5v2_D6)/preprocessed_v1\")\n",
    "D2_OUT_ROOT = Path(\"/content/drive/MyDrive/AI_PD_Project/Datasets/D2-Slovak (EWA-DB)/EWA-DB/preprocessed_v1\")\n",
    "\n",
    "D7_MANIFEST_ALL = D7_OUT_ROOT / \"manifests\" / \"manifest_all.csv\"\n",
    "D2_MANIFEST_ALL = D2_OUT_ROOT / \"manifests\" / \"manifest_all.csv\"\n",
    "\n",
    "# Prior selection sources (used only to extract excluded D2 speakers)\n",
    "MANIFEST_TRAIN_ENH1 = Path(\"/content/drive/MyDrive/AI_PD_Project/Datasets/D7-Multilingual (D1_D4_D5v2_D6)/preprocessed_v1/manifests/manifest_train_enh1.csv\")\n",
    "MANIFEST_TRAIN_ENH2 = Path(\"/content/drive/MyDrive/AI_PD_Project/Datasets/D7-Multilingual (D1_D4_D5v2_D6)/preprocessed_v1/manifests/manifest_train_enh2.csv\")\n",
    "\n",
    "# New outputs\n",
    "TRAIN_ENH3_DIR = D7_OUT_ROOT / \"clips\" / \"train_enh3\"\n",
    "MANIFEST_TRAIN_ENH3 = D7_OUT_ROOT / \"manifests\" / \"manifest_train_enh3.csv\"\n",
    "\n",
    "# -------------------------\n",
    "# 1.5) Run subfolders for logs and config\n",
    "# -------------------------\n",
    "RUN_FOLDER_LOGS = \"D7_Enh3_on_D2_Test\"\n",
    "RUN_FOLDER_CFG  = \"D7_Enh3_on_D2_Test\"\n",
    "\n",
    "LOGS_SUBDIR = D7_OUT_ROOT / \"logs\" / RUN_FOLDER_LOGS\n",
    "CFG_SUBDIR  = D7_OUT_ROOT / \"config\" / RUN_FOLDER_CFG\n",
    "\n",
    "WARNINGS_CSV    = LOGS_SUBDIR / \"preprocess_warnings.csv\"\n",
    "SUMMARY_JSON    = LOGS_SUBDIR / \"dataset_summary.json\"\n",
    "RUN_CONFIG_JSON = CFG_SUBDIR  / \"run_config.json\"\n",
    "\n",
    "# -------------------------\n",
    "# 1.6) Sampling knobs and file copy rules\n",
    "# -------------------------\n",
    "TEN_PCT         = 0.10\n",
    "ROUNDING_POLICY = \"ceil_then_make_even_by_rounding_down_if_needed\"\n",
    "BALANCE_POLICY  = \"speaker-balanced\"\n",
    "SPEAKER_ID_COL  = \"speaker_id\"\n",
    "\n",
    "RNG_SEED        = 7777\n",
    "COPY_ALL_D7_TRAIN_CLIPS = True\n",
    "\n",
    "LABEL_MAP_NOTE  = \"label_num mapping: 0=Healthy, 1=Parkinson\"\n",
    "\n",
    "# -------------------------\n",
    "# 2) Create required folders (safe if already present)\n",
    "# -------------------------\n",
    "clips_dir = D7_OUT_ROOT / \"clips\"\n",
    "manif_dir = D7_OUT_ROOT / \"manifests\"\n",
    "logs_dir  = D7_OUT_ROOT / \"logs\"\n",
    "cfg_dir   = D7_OUT_ROOT / \"config\"\n",
    "\n",
    "for d in [clips_dir, manif_dir, logs_dir, cfg_dir, LOGS_SUBDIR, CFG_SUBDIR]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "TRAIN_ENH3_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# -------------------------\n",
    "# 3) Locked manifest schema (kept consistent across datasets)\n",
    "# -------------------------\n",
    "CANON_COLS = [\n",
    "    \"split\",\n",
    "    \"dataset\",\n",
    "    \"task\",\n",
    "    \"speaker_id\",\n",
    "    \"sample_id\",\n",
    "    \"label_str\",\n",
    "    \"label_num\",\n",
    "    \"age\",\n",
    "    \"sex\",\n",
    "    \"speaker_key_rel\",\n",
    "    \"clip_path\",\n",
    "    \"duration_sec\",\n",
    "    \"source_path\",\n",
    "    \"clip_start_sec\",\n",
    "    \"clip_end_sec\",\n",
    "    \"sr_hz\",\n",
    "    \"channels\",\n",
    "    \"clip_is_contiguous\",\n",
    "]\n",
    "FINAL_COLS = CANON_COLS + [\"source_dataset\"]\n",
    "\n",
    "# -------------------------\n",
    "# 4) Small helpers\n",
    "#    - require(): stop early with a clear error\n",
    "#    - add_warn(): collect warnings and errors for the log file\n",
    "#    - atomic writes: avoid partial files if the runtime stops mid-write\n",
    "# -------------------------\n",
    "warnings_rows = []\n",
    "\n",
    "def require(cond: bool, msg: str):\n",
    "    if not cond:\n",
    "        raise RuntimeError(msg)\n",
    "\n",
    "def add_warn(src: str, level: str, code: str, message: str, **extra):\n",
    "    row = {\n",
    "        \"ts\": datetime.utcnow().isoformat(),\n",
    "        \"src\": src,\n",
    "        \"level\": str(level).upper(),\n",
    "        \"code\": code,\n",
    "        \"message\": message,\n",
    "    }\n",
    "    row.update(extra)\n",
    "    warnings_rows.append(row)\n",
    "\n",
    "def count_by_level(rows):\n",
    "    out = {\"ERROR\": 0, \"WARN\": 0, \"INFO\": 0}\n",
    "    for r in rows:\n",
    "        lvl = str(r.get(\"level\", \"INFO\")).upper()\n",
    "        out[lvl] = out.get(lvl, 0) + 1\n",
    "    return out\n",
    "\n",
    "def atomic_write_text(dst: Path, text: str):\n",
    "    tmp = dst.with_suffix(dst.suffix + \".tmp\")\n",
    "    with open(tmp, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(text)\n",
    "    os.replace(tmp, dst)\n",
    "\n",
    "def atomic_write_csv(dst: Path, df: pd.DataFrame):\n",
    "    tmp = dst.with_suffix(dst.suffix + \".tmp\")\n",
    "    df.to_csv(tmp, index=False, na_rep=\"\")\n",
    "    os.replace(tmp, dst)\n",
    "\n",
    "# Safe tokens for deterministic file names (avoid spaces and symbols)\n",
    "def safe_token(s, max_len=32, default=\"NA\"):\n",
    "    if pd.isna(s):\n",
    "        return default\n",
    "    s = str(s).strip()\n",
    "    s = re.sub(r\"\\s+\", \"_\", s)\n",
    "    s = re.sub(r\"[^A-Za-z0-9_]+\", \"\", s)\n",
    "    return (s[:max_len] if s else default)\n",
    "\n",
    "# Convert numeric label to text label (used for readability in manifests)\n",
    "def label_str_from_num(v):\n",
    "    if pd.isna(v):\n",
    "        return np.nan\n",
    "    iv = int(v)\n",
    "    if iv == 0:\n",
    "        return \"Healthy\"\n",
    "    if iv == 1:\n",
    "        return \"Parkinson\"\n",
    "    return np.nan\n",
    "\n",
    "def hc_pd_from_label_num(v):\n",
    "    return \"PD\" if int(v) == 1 else \"HC\"\n",
    "\n",
    "# Normalize sex values to M/F/NaN (separate rules for each dataset)\n",
    "def normalize_sex_to_MF_D7(x):\n",
    "    if pd.isna(x):\n",
    "        return np.nan\n",
    "    s = str(x).strip().lower()\n",
    "    if s in [\"m\", \"male\"]:\n",
    "        return \"M\"\n",
    "    if s in [\"f\", \"female\"]:\n",
    "        return \"F\"\n",
    "    if s in [\"\", \"nan\", \"none\", \"unknown\", \"u\"]:\n",
    "        return np.nan\n",
    "    return np.nan\n",
    "\n",
    "def normalize_sex_to_MF_D2(x):\n",
    "    if pd.isna(x):\n",
    "        return np.nan\n",
    "    s = str(x).strip().lower()\n",
    "    if s in [\"male\", \"m\"]:\n",
    "        return \"M\"\n",
    "    if s in [\"female\", \"f\"]:\n",
    "        return \"F\"\n",
    "    if s in [\"\", \"nan\", \"none\", \"unknown\", \"u\"]:\n",
    "        return np.nan\n",
    "    return np.nan\n",
    "\n",
    "# Quick summary used in the final dataset_summary.json\n",
    "def summarize_counts(df: pd.DataFrame, split_name: str):\n",
    "    out = {}\n",
    "    out[\"total_rows\"] = int(len(df))\n",
    "    out[\"split\"] = split_name\n",
    "    out[\"label_counts_total\"] = {str(k): int(v) for k, v in df[\"label_num\"].value_counts(dropna=False).to_dict().items()}\n",
    "    out[\"by_source_dataset\"] = {sd: int((df[\"source_dataset\"] == sd).sum()) for sd in sorted(df[\"source_dataset\"].dropna().unique())}\n",
    "    out[\"sex_counts\"] = {str(k): int(v) for k, v in df[\"sex\"].value_counts(dropna=False).to_dict().items()}\n",
    "    out[\"n_unique_speakers\"] = int(df[\"speaker_id\"].astype(str).nunique()) if \"speaker_id\" in df.columns else int(0)\n",
    "    return out\n",
    "\n",
    "# -------------------------\n",
    "# 5) Load manifests and validate schema\n",
    "#    - Stops early if a required input is missing or malformed\n",
    "# -------------------------\n",
    "print(\"D7_OUT_ROOT:\", str(D7_OUT_ROOT))\n",
    "print(\"D2_OUT_ROOT:\", str(D2_OUT_ROOT))\n",
    "print(\"D7_MANIFEST_ALL:\", str(D7_MANIFEST_ALL))\n",
    "print(\"D2_MANIFEST_ALL:\", str(D2_MANIFEST_ALL))\n",
    "print(\"MANIFEST_TRAIN_ENH1:\", str(MANIFEST_TRAIN_ENH1))\n",
    "print(\"MANIFEST_TRAIN_ENH2:\", str(MANIFEST_TRAIN_ENH2))\n",
    "print(\"TRAIN_ENH3_DIR:\", str(TRAIN_ENH3_DIR))\n",
    "print(\"MANIFEST_TRAIN_ENH3:\", str(MANIFEST_TRAIN_ENH3))\n",
    "print(\"LOGS_SUBDIR:\", str(LOGS_SUBDIR))\n",
    "print(\"CFG_SUBDIR:\", str(CFG_SUBDIR))\n",
    "print(\"RNG_SEED:\", int(RNG_SEED))\n",
    "print(\"COPY_ALL_D7_TRAIN_CLIPS:\", bool(COPY_ALL_D7_TRAIN_CLIPS))\n",
    "\n",
    "require(D7_MANIFEST_ALL.exists(), f\"Missing D7 manifest_all.csv: {str(D7_MANIFEST_ALL)}\")\n",
    "require(D2_MANIFEST_ALL.exists(), f\"Missing D2 manifest_all.csv: {str(D2_MANIFEST_ALL)}\")\n",
    "require(MANIFEST_TRAIN_ENH1.exists(), f\"Missing prior manifest_train_enh1.csv: {str(MANIFEST_TRAIN_ENH1)}\")\n",
    "require(MANIFEST_TRAIN_ENH2.exists(), f\"Missing prior manifest_train_enh2.csv: {str(MANIFEST_TRAIN_ENH2)}\")\n",
    "\n",
    "d7 = pd.read_csv(D7_MANIFEST_ALL)\n",
    "d2 = pd.read_csv(D2_MANIFEST_ALL)\n",
    "enh1 = pd.read_csv(MANIFEST_TRAIN_ENH1)\n",
    "enh2 = pd.read_csv(MANIFEST_TRAIN_ENH2)\n",
    "\n",
    "# Verify the core columns exist in both base manifests\n",
    "missing_d7 = [c for c in CANON_COLS if c not in d7.columns]\n",
    "missing_d2 = [c for c in CANON_COLS if c not in d2.columns]\n",
    "require(len(missing_d7) == 0, f\"D7 manifest missing required columns: {missing_d7}\")\n",
    "require(len(missing_d2) == 0, f\"D2 manifest missing required columns: {missing_d2}\")\n",
    "\n",
    "# Prior enh manifests must contain speaker_id and source_dataset to support exclusions\n",
    "for name, df in [(\"enh1\", enh1), (\"enh2\", enh2)]:\n",
    "    require(\"speaker_id\" in df.columns, f\"{name} missing 'speaker_id'. Found: {list(df.columns)}\")\n",
    "    require(\"source_dataset\" in df.columns, f\"{name} missing 'source_dataset'. Found: {list(df.columns)}\")\n",
    "\n",
    "# Ensure source_dataset exists everywhere (used for reporting and exclusions)\n",
    "if \"source_dataset\" not in d7.columns:\n",
    "    d7[\"source_dataset\"] = \"D7\"\n",
    "if \"source_dataset\" not in d2.columns:\n",
    "    d2[\"source_dataset\"] = \"D2\"\n",
    "\n",
    "# Convert literal \"NaN\" strings into real missing values\n",
    "for df in [d7, d2, enh1, enh2]:\n",
    "    for col in [\"sex\", \"age\", \"duration_sec\", \"clip_start_sec\", \"clip_end_sec\", \"speaker_key_rel\", \"speaker_id\", \"task\", \"sample_id\"]:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].replace(\"NaN\", np.nan)\n",
    "\n",
    "# Basic dataset identity checks (avoid mixing wrong manifests)\n",
    "def infer_dataset_id(df: pd.DataFrame, fallback: str) -> str:\n",
    "    if \"dataset\" in df.columns and df[\"dataset\"].notna().any():\n",
    "        return str(df[\"dataset\"].astype(str).value_counts(dropna=True).idxmax())\n",
    "    return fallback\n",
    "\n",
    "require(infer_dataset_id(d7, \"DX\") == \"D7\", \"Expected D7 manifest dataset=='D7'.\")\n",
    "require(infer_dataset_id(d2, \"DX\") == \"D2\", \"Expected D2 manifest dataset=='D2'.\")\n",
    "\n",
    "# Keep only the intended dataset rows (defensive filter)\n",
    "d7 = d7[d7[\"dataset\"].astype(str) == \"D7\"].copy()\n",
    "d2 = d2[d2[\"dataset\"].astype(str) == \"D2\"].copy()\n",
    "\n",
    "# Validate labels and rebuild label_str for consistency\n",
    "for name, df in [(\"D7\", d7), (\"D2\", d2)]:\n",
    "    bad = sorted(set(df[\"label_num\"].dropna().unique()) - {0, 1})\n",
    "    require(len(bad) == 0, f\"{name} label_num contains values outside {{0,1}}: {bad}\")\n",
    "    df[\"label_str\"] = df[\"label_num\"].map(label_str_from_num)\n",
    "\n",
    "# Standardize sex encoding\n",
    "d7[\"sex\"] = d7[\"sex\"].map(normalize_sex_to_MF_D7)\n",
    "d2[\"sex\"] = d2[\"sex\"].map(normalize_sex_to_MF_D2)\n",
    "\n",
    "# -------------------------\n",
    "# 6) Extract D7 training rows (base training content)\n",
    "# -------------------------\n",
    "d7_train = d7[d7[\"split\"].astype(str) == \"train\"].copy()\n",
    "require(len(d7_train) > 0, \"D7 train split has 0 rows.\")\n",
    "print(\"\\nD7 train rows:\", int(len(d7_train)))\n",
    "print(\"D7 train label counts:\", d7_train[\"label_num\"].value_counts(dropna=False).to_dict())\n",
    "\n",
    "# -------------------------\n",
    "# 7) Build the D2 speaker exclusion list from enh1 and enh2 (union)\n",
    "#    - Ensures the new draw shares zero D2 speakers with previous draws\n",
    "# -------------------------\n",
    "enh1_d2 = enh1[enh1[\"source_dataset\"].astype(str) == \"D2\"].copy()\n",
    "enh2_d2 = enh2[enh2[\"source_dataset\"].astype(str) == \"D2\"].copy()\n",
    "\n",
    "prior_d2_speakers_enh1 = set(enh1_d2[\"speaker_id\"].dropna().astype(str).unique().tolist())\n",
    "prior_d2_speakers_enh2 = set(enh2_d2[\"speaker_id\"].dropna().astype(str).unique().tolist())\n",
    "prior_d2_speakers_union = sorted(list(prior_d2_speakers_enh1.union(prior_d2_speakers_enh2)))\n",
    "\n",
    "print(\"\\nZero-overlap exclusion from manifest_train_enh1 + manifest_train_enh2:\")\n",
    "print(\"  Prior D2 unique speakers in enh1:\", int(len(prior_d2_speakers_enh1)))\n",
    "print(\"  Prior D2 unique speakers in enh2:\", int(len(prior_d2_speakers_enh2)))\n",
    "print(\"  UNION excluded D2 unique speakers:\", int(len(prior_d2_speakers_union)))\n",
    "print(\"  Example excluded speakers (up to 20):\", prior_d2_speakers_union[:20])\n",
    "\n",
    "# -------------------------\n",
    "# 8) Build the full D2 train speaker set and compute the fixed target size\n",
    "#    - Target is based on ALL D2 train speakers (before exclusions)\n",
    "#    - Definition B requires each speaker to have a single consistent label\n",
    "# -------------------------\n",
    "d2_train = d2[d2[\"split\"].astype(str) == \"train\"].copy()\n",
    "require(len(d2_train) > 0, \"D2 train split has 0 rows.\")\n",
    "require(SPEAKER_ID_COL in d2_train.columns, f\"D2 missing speaker id column: {SPEAKER_ID_COL}\")\n",
    "\n",
    "print(\"\\nD2 train rows:\", int(len(d2_train)))\n",
    "print(\"D2 train label counts:\", d2_train[\"label_num\"].value_counts(dropna=False).to_dict())\n",
    "\n",
    "# Speaker label consistency check (one label per speaker)\n",
    "speaker_labels = (\n",
    "    d2_train.groupby(SPEAKER_ID_COL)[\"label_num\"]\n",
    "    .apply(lambda s: sorted(set(s.dropna().astype(int).tolist())))\n",
    ")\n",
    "mixed = speaker_labels[speaker_labels.apply(lambda x: len(x) != 1)]\n",
    "if len(mixed) > 0:\n",
    "    raise RuntimeError(\n",
    "        \"D2 train has speakers with mixed label_num values across clips; Definition B sampling cannot proceed.\\n\"\n",
    "        f\"Examples: {mixed.head(10).to_dict()}\"\n",
    "    )\n",
    "\n",
    "speaker_to_label_full = speaker_labels.apply(lambda x: int(x[0])).to_dict()\n",
    "all_speakers_full = sorted([str(x) for x in speaker_to_label_full.keys()])\n",
    "total_speakers_full = len(all_speakers_full)\n",
    "\n",
    "hc_full = sorted([str(spk) for spk, y in speaker_to_label_full.items() if int(y) == 0])\n",
    "pd_full = sorted([str(spk) for spk, y in speaker_to_label_full.items() if int(y) == 1])\n",
    "require(len(hc_full) > 0 and len(pd_full) > 0, \"D2 train does not contain both HC and PD speakers.\")\n",
    "\n",
    "# Fixed-denominator target (computed before applying exclusions)\n",
    "target_total = int(math.ceil(TEN_PCT * total_speakers_full))\n",
    "if target_total % 2 != 0:\n",
    "    target_total -= 1\n",
    "if target_total < 2:\n",
    "    target_total = 2\n",
    "target_per_class = target_total // 2\n",
    "\n",
    "print(\"\\nFixed-denominator target computation (same policy as train_enh2):\")\n",
    "print(\"  total D2 train speakers (FULL):\", total_speakers_full)\n",
    "print(f\"  target_total = ceil(0.10 * {total_speakers_full}) then even:\", target_total, f\"=> {target_per_class} HC + {target_per_class} PD\")\n",
    "\n",
    "# -------------------------\n",
    "# 9) Apply exclusions to form the eligible pool, then sample speakers\n",
    "#    - If needed, downsize both classes to keep HC/PD speaker counts matched\n",
    "# -------------------------\n",
    "exclude_set = set(prior_d2_speakers_union)\n",
    "speaker_to_label_eligible = {str(spk): int(lbl) for spk, lbl in speaker_to_label_full.items() if str(spk) not in exclude_set}\n",
    "\n",
    "hc_eligible = sorted([spk for spk, y in speaker_to_label_eligible.items() if y == 0])\n",
    "pd_eligible = sorted([spk for spk, y in speaker_to_label_eligible.items() if y == 1])\n",
    "\n",
    "print(\"\\nEligible pool after exclusion (does not affect denominator target):\")\n",
    "print(\"  eligible speakers total:\", len(speaker_to_label_eligible))\n",
    "print(\"  eligible HC speakers:\", len(hc_eligible))\n",
    "print(\"  eligible PD speakers:\", len(pd_eligible))\n",
    "\n",
    "# Downsize if eligible pool is too small (keeps the draw speaker-balanced)\n",
    "max_per_class = min(len(hc_eligible), len(pd_eligible), int(target_per_class))\n",
    "require(max_per_class >= 1, \"After exclusions, eligible pool has <1 HC or <1 PD speaker. Cannot build a balanced train_enh3 draw.\")\n",
    "\n",
    "downsized = (max_per_class != int(target_per_class))\n",
    "effective_per_class = int(max_per_class)\n",
    "effective_total = int(2 * effective_per_class)\n",
    "\n",
    "if downsized:\n",
    "    add_warn(\n",
    "        \"D7_TRAIN_ENH3\", \"WARN\", \"DOWNSIZED_DUE_TO_ELIGIBLE_POOL\",\n",
    "        \"Eligible pool insufficient to meet fixed-denominator target_per_class; downsized to keep balanced sampling.\",\n",
    "        target_per_class=int(target_per_class),\n",
    "        effective_per_class=int(effective_per_class),\n",
    "        eligible_hc=int(len(hc_eligible)),\n",
    "        eligible_pd=int(len(pd_eligible)),\n",
    "    )\n",
    "\n",
    "print(\"\\nEffective draw size (speaker-balanced):\")\n",
    "print(\"  requested target_per_class:\", int(target_per_class))\n",
    "print(\"  effective_per_class:\", int(effective_per_class))\n",
    "print(\"  effective_total speakers:\", int(effective_total), f\"(~{100.0*effective_total/max(1,total_speakers_full):.2f}% of FULL D2 train speaker denom)\")\n",
    "\n",
    "# Random draw (deterministic via RNG_SEED)\n",
    "rng = np.random.default_rng(int(RNG_SEED))\n",
    "sel_hc = sorted(rng.choice(hc_eligible, size=effective_per_class, replace=False).tolist())\n",
    "sel_pd = sorted(rng.choice(pd_eligible, size=effective_per_class, replace=False).tolist())\n",
    "selected_speakers = sorted(sel_hc + sel_pd)\n",
    "\n",
    "# Pull all D2 train clips for the selected speakers\n",
    "d2_sel = d2_train[d2_train[SPEAKER_ID_COL].astype(str).isin([str(x) for x in selected_speakers])].copy()\n",
    "\n",
    "print(\"\\nD2 speaker sampling (DRAW 3, zero overlap with enh1+enh2):\")\n",
    "print(\"  selected HC speakers:\", int(len(sel_hc)))\n",
    "print(\"  selected PD speakers:\", int(len(sel_pd)))\n",
    "print(\"  selected D2 rows (all clips for selected speakers):\", int(len(d2_sel)))\n",
    "print(\"  selected label counts:\", d2_sel[\"label_num\"].value_counts(dropna=False).to_dict())\n",
    "\n",
    "# -------------------------\n",
    "# 10) Fail-fast: confirm all source clips exist before planning copies\n",
    "# -------------------------\n",
    "def fail_fast_missing_paths(df: pd.DataFrame, name: str):\n",
    "    missing_paths = []\n",
    "    for p in tqdm(df[\"clip_path\"].astype(str).tolist(), desc=f\"Check {name} clip_path exists\", dynamic_ncols=True):\n",
    "        if not os.path.exists(p):\n",
    "            missing_paths.append(p)\n",
    "            if len(missing_paths) >= 25:\n",
    "                break\n",
    "    if missing_paths:\n",
    "        raise FileNotFoundError(f\"{name}: missing clip_path(s). Examples (up to 25): {missing_paths}\")\n",
    "\n",
    "if COPY_ALL_D7_TRAIN_CLIPS:\n",
    "    fail_fast_missing_paths(d7_train, \"D7 TRAIN\")\n",
    "fail_fast_missing_paths(d2_sel, \"D2 TRAIN (selected speakers)\")\n",
    "\n",
    "# -------------------------\n",
    "# 11) Build a copy plan and enforce no-overwrite rules\n",
    "#    - If destination exists with same file size: skip\n",
    "#    - If destination exists with different size: log ERROR and stop\n",
    "# -------------------------\n",
    "copy_plan = []\n",
    "\n",
    "# A) D7 train clips copied into train_enh3 with the same file names\n",
    "if COPY_ALL_D7_TRAIN_CLIPS:\n",
    "    for i, row in d7_train.reset_index(drop=True).iterrows():\n",
    "        src_path = Path(str(row[\"clip_path\"]))\n",
    "        dst_path = TRAIN_ENH3_DIR / src_path.name\n",
    "        copy_plan.append({\n",
    "            \"src_path\": str(src_path),\n",
    "            \"dst_path\": str(dst_path),\n",
    "            \"origin\": \"D7_train_existing\",\n",
    "            \"source_dataset\": str(row.get(\"source_dataset\", \"D7\")),\n",
    "            \"row_index\": int(i),\n",
    "        })\n",
    "\n",
    "# B) D2 selected clips copied into train_enh3 with deterministic new names\n",
    "d2_sel_reset = d2_sel.reset_index(drop=True).copy()\n",
    "d2_sel_reset[\"_speaker_tok\"] = d2_sel_reset[\"speaker_id\"].map(lambda x: safe_token(x, 32, \"NA\"))\n",
    "d2_sel_reset[\"_task_tok\"] = d2_sel_reset[\"task\"].map(lambda x: safe_token(x, 12, \"0\"))\n",
    "d2_sel_reset[\"_clip_path_str\"] = d2_sel_reset[\"clip_path\"].astype(str)\n",
    "d2_sel_reset = d2_sel_reset.sort_values(by=[\"_speaker_tok\", \"_task_tok\", \"_clip_path_str\"]).reset_index(drop=True)\n",
    "\n",
    "for j, row in d2_sel_reset.iterrows():\n",
    "    src_path = Path(str(row[\"clip_path\"]))\n",
    "    hc_pd = hc_pd_from_label_num(row[\"label_num\"])\n",
    "    spk_tok = safe_token(row[\"speaker_id\"], 32, \"NA\")\n",
    "    task_tok = safe_token(row[\"task\"], 12, \"0\")\n",
    "    out_name = f\"D7_D2add_train_enh3_{hc_pd}_{spk_tok}_{task_tok}_{j+1:06d}.wav\"\n",
    "    dst_path = TRAIN_ENH3_DIR / out_name\n",
    "    copy_plan.append({\n",
    "        \"src_path\": str(src_path),\n",
    "        \"dst_path\": str(dst_path),\n",
    "        \"origin\": \"D2_train_selected\",\n",
    "        \"source_dataset\": \"D2\",\n",
    "        \"row_index\": int(j),\n",
    "    })\n",
    "\n",
    "n_dest_exists_ok = 0\n",
    "n_dest_exists_mismatch = 0\n",
    "n_will_copy = 0\n",
    "\n",
    "print(\"\\nPreflight: destination existence and size checks (no overwrite)...\")\n",
    "for item in tqdm(copy_plan, desc=\"Preflight (dest checks)\", dynamic_ncols=True):\n",
    "    sp = Path(item[\"src_path\"])\n",
    "    dp = Path(item[\"dst_path\"])\n",
    "    require(sp.exists(), f\"Source clip missing at preflight: {str(sp)}\")\n",
    "    if dp.exists():\n",
    "        try:\n",
    "            if dp.stat().st_size == sp.stat().st_size:\n",
    "                n_dest_exists_ok += 1\n",
    "            else:\n",
    "                n_dest_exists_mismatch += 1\n",
    "                add_warn(\n",
    "                    \"D7_TRAIN_ENH3\", \"ERROR\", \"DEST_EXISTS_SIZE_MISMATCH\",\n",
    "                    \"Destination exists but file size differs from source\",\n",
    "                    src_path=str(sp), dest_path=str(dp),\n",
    "                    src_size=int(sp.stat().st_size), dest_size=int(dp.stat().st_size),\n",
    "                )\n",
    "        except Exception as e:\n",
    "            n_dest_exists_mismatch += 1\n",
    "            add_warn(\n",
    "                \"D7_TRAIN_ENH3\", \"ERROR\", \"DEST_EXISTS_STAT_ERROR\",\n",
    "                \"Failed to stat source/destination during preflight\",\n",
    "                src_path=str(sp), dest_path=str(dp), error=repr(e),\n",
    "            )\n",
    "    else:\n",
    "        n_will_copy += 1\n",
    "\n",
    "preflight_stats = {\n",
    "    \"total_planned_files\": int(len(copy_plan)),\n",
    "    \"n_dest_exists_ok\": int(n_dest_exists_ok),\n",
    "    \"n_dest_exists_mismatch\": int(n_dest_exists_mismatch),\n",
    "    \"n_will_copy\": int(n_will_copy),\n",
    "    \"warnings_by_level\": count_by_level(warnings_rows),\n",
    "}\n",
    "\n",
    "print(\"\\nPreflight summary:\")\n",
    "print(\"  Planned files:\", int(len(copy_plan)))\n",
    "print(\"  Destination exists (size OK):\", n_dest_exists_ok)\n",
    "print(\"  Destination exists (size mismatch/stat error):\", n_dest_exists_mismatch)\n",
    "print(\"  Will copy:\", n_will_copy)\n",
    "print(\"  Warnings by level:\", preflight_stats[\"warnings_by_level\"])\n",
    "\n",
    "# -------------------------\n",
    "# 12) Write config and early summary before copying\n",
    "#    - Captures the draw, exclusions, and preflight checks even if the session stops later\n",
    "# -------------------------\n",
    "run_config = {\n",
    "    \"dataset\": \"D7\",\n",
    "    \"mode\": \"train_enh3_builder\",\n",
    "    \"created_utc\": datetime.utcnow().isoformat(),\n",
    "    \"run_folder_logs\": RUN_FOLDER_LOGS,\n",
    "    \"run_folder_cfg\": RUN_FOLDER_CFG,\n",
    "    \"d7_out_root\": str(D7_OUT_ROOT),\n",
    "    \"d2_out_root\": str(D2_OUT_ROOT),\n",
    "    \"d7_manifest_all\": str(D7_MANIFEST_ALL),\n",
    "    \"d2_manifest_all\": str(D2_MANIFEST_ALL),\n",
    "    \"manifest_train_enh1\": str(MANIFEST_TRAIN_ENH1),\n",
    "    \"manifest_train_enh2\": str(MANIFEST_TRAIN_ENH2),\n",
    "    \"train_enh3_dir\": str(TRAIN_ENH3_DIR),\n",
    "    \"manifest_train_enh3\": str(MANIFEST_TRAIN_ENH3),\n",
    "    \"policy\": {\n",
    "        \"definition\": \"Add ~10% of D2 TRAIN by speaker (Definition B) into D7 training clips folder\",\n",
    "        \"pct_speakers\": float(TEN_PCT),\n",
    "        \"denominator_policy\": \"FIXED_FULL_D2_TRAIN_SPEAKER_COUNT\",\n",
    "        \"rounding_policy\": ROUNDING_POLICY,\n",
    "        \"balance_policy\": BALANCE_POLICY,\n",
    "        \"speaker_id_col_used\": SPEAKER_ID_COL,\n",
    "        \"rng_seed\": int(RNG_SEED),\n",
    "        \"copy_all_d7_train_clips\": bool(COPY_ALL_D7_TRAIN_CLIPS),\n",
    "        \"label_note\": LABEL_MAP_NOTE,\n",
    "        \"file_operation\": \"copy\",\n",
    "        \"no_overwrite_rule\": \"skip if dest exists with matching size; error if size differs\",\n",
    "        \"zero_overlap\": {\n",
    "            \"enabled\": True,\n",
    "            \"exclude_speakers_source\": \"manifest_train_enh1 + manifest_train_enh2 where source_dataset == 'D2'\",\n",
    "            \"n_excluded_d2_speakers_union\": int(len(prior_d2_speakers_union)),\n",
    "            \"excluded_d2_speakers_union\": prior_d2_speakers_union,\n",
    "        },\n",
    "        \"downsizing_rule_if_insufficient_eligible_pool\": \"downsize BOTH classes to min(eligible_hc, eligible_pd, target_per_class) to keep speaker-balanced\",\n",
    "    },\n",
    "    \"inputs\": {\n",
    "        \"d7_train_rows\": int(len(d7_train)),\n",
    "        \"d2_train_rows\": int(len(d2_train)),\n",
    "        \"d2_train_total_speakers_full\": int(total_speakers_full),\n",
    "        \"d2_train_speakers_hc_full\": int(len(hc_full)),\n",
    "        \"d2_train_speakers_pd_full\": int(len(pd_full)),\n",
    "        \"d2_train_speakers_hc_eligible\": int(len(hc_eligible)),\n",
    "        \"d2_train_speakers_pd_eligible\": int(len(pd_eligible)),\n",
    "    },\n",
    "    \"selection\": {\n",
    "        \"target_total_speakers_fixed\": int(target_total),\n",
    "        \"target_per_class_fixed\": int(target_per_class),\n",
    "        \"effective_total_speakers\": int(effective_total),\n",
    "        \"effective_per_class\": int(effective_per_class),\n",
    "        \"downsized\": bool(downsized),\n",
    "        \"selected_speakers_hc\": sel_hc,\n",
    "        \"selected_speakers_pd\": sel_pd,\n",
    "        \"selected_speakers_all\": selected_speakers,\n",
    "        \"selected_d2_rows\": int(len(d2_sel)),\n",
    "        \"selected_d2_label_counts\": d2_sel[\"label_num\"].value_counts(dropna=False).to_dict(),\n",
    "    },\n",
    "    \"preflight\": preflight_stats,\n",
    "}\n",
    "\n",
    "early_summary = {\n",
    "    \"dataset\": \"D7\",\n",
    "    \"created_utc\": datetime.utcnow().isoformat(),\n",
    "    \"status\": \"PRECHECK_COMPLETE\",\n",
    "    \"d7_out_root\": str(D7_OUT_ROOT),\n",
    "    \"train_enh3_dir\": str(TRAIN_ENH3_DIR),\n",
    "    \"preflight\": preflight_stats,\n",
    "    \"selection\": run_config[\"selection\"],\n",
    "}\n",
    "\n",
    "atomic_write_csv(WARNINGS_CSV, pd.DataFrame(warnings_rows))\n",
    "atomic_write_text(RUN_CONFIG_JSON, json.dumps(run_config, indent=2))\n",
    "atomic_write_text(SUMMARY_JSON, json.dumps(early_summary, indent=2))\n",
    "\n",
    "# Stop before copying if any ERROR was logged during preflight\n",
    "fatal_pre = [w for w in warnings_rows if str(w.get(\"level\", \"\")).upper() == \"ERROR\"]\n",
    "if fatal_pre:\n",
    "    raise RuntimeError(f\"Preflight failed with {len(fatal_pre)} ERROR(s). See {str(WARNINGS_CSV)}\")\n",
    "\n",
    "# -------------------------\n",
    "# 13) Copy files into train_enh3 (copy only, never overwrite)\n",
    "# -------------------------\n",
    "print(\"\\nCopy stage: copying into train_enh3 (no overwrite)...\")\n",
    "\n",
    "copied = 0\n",
    "skipped_exists = 0\n",
    "copy_errors = 0\n",
    "\n",
    "for item in tqdm(copy_plan, desc=\"Copying clips\", dynamic_ncols=True):\n",
    "    sp = Path(item[\"src_path\"])\n",
    "    dp = Path(item[\"dst_path\"])\n",
    "\n",
    "    if not sp.exists():\n",
    "        copy_errors += 1\n",
    "        add_warn(\"D7_TRAIN_ENH3\", \"ERROR\", \"SOURCE_CLIP_DISAPPEARED\", \"Source clip missing during copy stage\", clip_path=str(sp))\n",
    "        continue\n",
    "\n",
    "    if dp.exists():\n",
    "        skipped_exists += 1\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        shutil.copy2(sp, dp)\n",
    "        copied += 1\n",
    "    except Exception as e:\n",
    "        copy_errors += 1\n",
    "        add_warn(\"D7_TRAIN_ENH3\", \"ERROR\", \"COPY_FAILED\", \"Failed to copy file\", src_path=str(sp), dest_path=str(dp), error=repr(e))\n",
    "\n",
    "copy_stats = {\n",
    "    \"copied\": int(copied),\n",
    "    \"skipped_exists\": int(skipped_exists),\n",
    "    \"copy_errors\": int(copy_errors),\n",
    "    \"total_planned_files\": int(len(copy_plan)),\n",
    "    \"warnings_by_level\": count_by_level(warnings_rows),\n",
    "}\n",
    "\n",
    "atomic_write_csv(WARNINGS_CSV, pd.DataFrame(warnings_rows))\n",
    "\n",
    "# Stop if any copy ERROR happened (keeps outputs consistent)\n",
    "fatal_copy = [w for w in warnings_rows if str(w.get(\"level\", \"\")).upper() == \"ERROR\"]\n",
    "if fatal_copy:\n",
    "    run_config[\"copy_stats\"] = copy_stats\n",
    "    atomic_write_text(RUN_CONFIG_JSON, json.dumps(run_config, indent=2))\n",
    "    fail_summary = dict(early_summary)\n",
    "    fail_summary[\"status\"] = \"FAILED_DURING_COPY\"\n",
    "    fail_summary[\"copy_stats\"] = copy_stats\n",
    "    atomic_write_text(SUMMARY_JSON, json.dumps(fail_summary, indent=2))\n",
    "    raise RuntimeError(f\"Copy failed with {len(fatal_copy)} ERROR(s). See {str(WARNINGS_CSV)}\")\n",
    "\n",
    "# -------------------------\n",
    "# 14) Create manifest_train_enh3.csv\n",
    "#    - Combines: D7 train clips + selected D2 clips (renamed)\n",
    "#    - Updates clip_path to point to files inside train_enh3\n",
    "# -------------------------\n",
    "parts = []\n",
    "\n",
    "# D7 portion (same clips, new split name)\n",
    "d7_enh3 = d7_train[CANON_COLS].copy()\n",
    "d7_enh3[\"split\"] = \"train_enh3\"\n",
    "d7_enh3[\"dataset\"] = \"D7\"\n",
    "d7_enh3[\"clip_path\"] = d7_train[\"clip_path\"].astype(str).map(lambda p: str(TRAIN_ENH3_DIR / Path(p).name))\n",
    "d7_enh3[\"sample_id\"] = d7_enh3[\"clip_path\"].map(lambda p: Path(p).stem)\n",
    "d7_enh3[\"source_dataset\"] = d7_train.get(\"source_dataset\", pd.Series([\"D7\"] * len(d7_train))).astype(str).tolist()\n",
    "parts.append(d7_enh3)\n",
    "\n",
    "# D2 add-on portion (new file names already planned above)\n",
    "d2_enh3 = d2_sel_reset[CANON_COLS].copy()\n",
    "d2_enh3[\"split\"] = \"train_enh3\"\n",
    "d2_enh3[\"dataset\"] = \"D7\"\n",
    "\n",
    "new_paths, new_ids = [], []\n",
    "for j, row in d2_sel_reset.iterrows():\n",
    "    hc_pd = hc_pd_from_label_num(row[\"label_num\"])\n",
    "    spk_tok = safe_token(row[\"speaker_id\"], 32, \"NA\")\n",
    "    task_tok = safe_token(row[\"task\"], 12, \"0\")\n",
    "    out_name = f\"D7_D2add_train_enh3_{hc_pd}_{spk_tok}_{task_tok}_{j+1:06d}.wav\"\n",
    "    out_path = TRAIN_ENH3_DIR / out_name\n",
    "    new_paths.append(str(out_path))\n",
    "    new_ids.append(out_path.stem)\n",
    "\n",
    "d2_enh3[\"clip_path\"] = new_paths\n",
    "d2_enh3[\"sample_id\"] = new_ids\n",
    "d2_enh3[\"source_dataset\"] = \"D2\"\n",
    "parts.append(d2_enh3)\n",
    "\n",
    "train_enh3 = pd.concat(parts, axis=0, ignore_index=True)\n",
    "train_enh3 = train_enh3[FINAL_COLS].copy()\n",
    "\n",
    "# Ensure missing values are real NaN, not the literal string \"NaN\"\n",
    "for c in FINAL_COLS:\n",
    "    if train_enh3[c].dtype == object and (train_enh3[c] == \"NaN\").any():\n",
    "        raise RuntimeError(f\"Found literal string 'NaN' in column '{c}'.\")\n",
    "\n",
    "# Final file existence check for the new manifest\n",
    "missing_enh3 = []\n",
    "for p in tqdm(train_enh3[\"clip_path\"].astype(str).tolist(), desc=\"Check TRAIN_ENH3 clip_path exists\", dynamic_ncols=True):\n",
    "    if not os.path.exists(p):\n",
    "        missing_enh3.append(p)\n",
    "        if len(missing_enh3) >= 25:\n",
    "            break\n",
    "require(len(missing_enh3) == 0, f\"train_enh3 manifest points to missing files. Examples: {missing_enh3[:25]}\")\n",
    "\n",
    "# -------------------------\n",
    "# 15) Write outputs and a final summary\n",
    "# -------------------------\n",
    "atomic_write_csv(MANIFEST_TRAIN_ENH3, train_enh3)\n",
    "\n",
    "final_summary = {\n",
    "    \"dataset\": \"D7\",\n",
    "    \"created_utc\": datetime.utcnow().isoformat(),\n",
    "    \"status\": \"SUCCESS\",\n",
    "    \"d7_out_root\": str(D7_OUT_ROOT),\n",
    "    \"train_enh3_dir\": str(TRAIN_ENH3_DIR),\n",
    "    \"manifest_train_enh3\": str(MANIFEST_TRAIN_ENH3),\n",
    "    \"selection\": run_config[\"selection\"],\n",
    "    \"copy_stats\": copy_stats,\n",
    "    \"counts_train_enh3\": summarize_counts(train_enh3, split_name=\"train_enh3\"),\n",
    "}\n",
    "\n",
    "atomic_write_text(SUMMARY_JSON, json.dumps(final_summary, indent=2))\n",
    "\n",
    "# Update run_config with final outputs and copy stats\n",
    "run_config[\"copy_stats\"] = copy_stats\n",
    "run_config[\"outputs\"] = {\n",
    "    \"train_enh3_dir\": str(TRAIN_ENH3_DIR),\n",
    "    \"manifest_train_enh3\": str(MANIFEST_TRAIN_ENH3),\n",
    "    \"dataset_summary_json\": str(SUMMARY_JSON),\n",
    "    \"preprocess_warnings_csv\": str(WARNINGS_CSV),\n",
    "    \"run_config_json\": str(RUN_CONFIG_JSON),\n",
    "}\n",
    "atomic_write_text(RUN_CONFIG_JSON, json.dumps(run_config, indent=2))\n",
    "\n",
    "print(\"\\n✅ D7 train_enh3 build complete (FIXED DENOMINATOR + ZERO OVERLAP enh1+enh2).\")\n",
    "print(\"- Train_enh3 folder:\", str(TRAIN_ENH3_DIR))\n",
    "print(\"- Manifest:\", str(MANIFEST_TRAIN_ENH3))\n",
    "print(\"- Summary:\", str(SUMMARY_JSON))\n",
    "print(\"- Warnings:\", str(WARNINGS_CSV))\n",
    "print(\"- Config:\", str(RUN_CONFIG_JSON))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "58d649f89ed34edaba31cf6eafa8efa4",
      "0eceeab028e3448b9ea811b08237180f",
      "e4a6b268b5c347349a0f607f32d967b5",
      "a92c6b0d570b455d93a4821d66998309",
      "d9a8208cfe7447e7aa3284ee0301e651",
      "c14bb4c46cf34e4597f78d99d8b801fb",
      "01e24785b5974c6ea8c75325de6cfbf8",
      "b5429995de2140249fcdb0d02ec33556",
      "39091f5ddfa047f1ba4a07b198161088",
      "a14bc20282584d148f5a823d08ca27d7",
      "0384e09d4c4244a887e3351dcf3a0382",
      "99d3afb09c214928802d09ff92026950",
      "8787f83e701240eb8f90c46def39295e",
      "1bd6520f536e452ca7ee9a1a0ccee7df",
      "25ee957abe25467599c739d167e43fce",
      "ac5816e031a04b68a1ad3066c2412140",
      "4d092c90e780415e80a312c1cfec8d36",
      "05315fb37873418a888a77aec092f703",
      "c3aa3e9a82c34d6db9c80d96d4857667",
      "010fc119b1e141debaf4177a2b0061a4",
      "57c5327bf4a34c4f9338d312dacddecd",
      "44d3cddc1be84ceca649d70a16cbeb81",
      "6e60656909a24c58b4189eb0bb9d1929",
      "0e003ef38c834809a7ff7dbe796e6c2e",
      "c19588aa3d6d4d01901561f35df51de8",
      "87499f617a0747e19520577e33be6157",
      "8170e4a69e5940ccb78f525661d48a1d",
      "987ce35d59e040b4a05c668b7935e5c6",
      "22a10a2d3f0c4473ac702de2b0993133",
      "af3e5a40d9aa49fcbae38571ae8def02",
      "85e36a028d3745ebb470ad0306f578c1",
      "78c54e9c7a7c4b9e9766601bd49eafa3",
      "6804b6d5784542999361cc1b1e407745",
      "5ddc2a26408d405184c9f62aeaa52c9f",
      "5feb39882cee4ffeb03fddcf3c3eef4c",
      "fdb396b3317b451e8bdb769389bbb14f",
      "64977fa927f64ae4871ccefc37df5e37",
      "4df70ded25af438aba891553e9a177c2",
      "954a9ec6633e4cb690440567e32dea07",
      "b72bca6952ab4ddead4e3b790e80594f",
      "66dd20d3309e48bd818526cb88b494e8",
      "304c19e528584a46ade430995e061800",
      "dc75f11f384a45faa7a02580ff5a2ec9",
      "2bf8856fceb74f2182e297e2cc73e4b7",
      "1e0a9ed97840482e92b18f2dc4553b3a",
      "997548af59cf4dbc8feba35319c0037c",
      "80bee7dcc9d04fe094e27845df9cc76d",
      "c1d6af4247044ab483a181140e99e483",
      "c1feb72252f045e6b337af1b608e8ee7",
      "97ecbb1266ec49fe8fce7c28c6760cb7",
      "e0181f04248c4a98ac8bde608d1ac019",
      "d3910ef6ac1c45cf891d3edfb8d08069",
      "9fcebc2c259d4ca4a7c8b0be29fa2306",
      "29af196bbe2d4f339a4e3703091d2bec",
      "d22a109a3f3146c7b1cf7c74b775725a"
     ]
    },
    "id": "vFBZzUAia5yg"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "The following cell runs **training and validation for the D7 enhanced model using the train_enh3 split**, without changing any data splits or reprocessing audio. Training data is read from `manifests/manifest_train_enh3.csv` (rows where `split == \"train_enh3\"`), and validation data is read from `manifests/manifest_all.csv` (rows where `split == \"val\"`). The cell checks that all required columns are present, confirms the dataset identifier is **D7**, and stops early if any referenced audio files are missing.\n",
    "\n",
    "The model uses a **frozen Wav2Vec2 backbone** with **two small classification heads**, one for vowel clips and one for other speech clips. Each head includes a small LayerNorm and dropout block. Audio is loaded from `clip_path`, verified to be **16 kHz**, and padded for batching. For vowel clips, an attention mask removes trailing near-silence so padded quiet regions do not affect learning. Other clips use the full attention mask.\n",
    "\n",
    "Before training starts, the cell locates the **most recent baseline D7 train+validation experiment** that was not trained on any `train_enh*` data and that contains `best_heads.pt` for all three seeds. For each seed (1337, 2024, 7777), the enhanced model heads are initialized from this baseline run. Training then updates only the head parameters using Adam and gradient accumulation. After each epoch, the model is evaluated on the validation set, **validation AUROC** is recorded, and early stopping is applied if AUROC does not improve for several epochs.\n",
    "\n",
    "For each seed, the cell saves the **best head weights** (`best_heads.pt`), a `metrics.json` file, and validation plots, including a ROC curve and confusion matrices at threshold 0.5 and at an optimal threshold. The validation-optimal threshold is selected using **Youden’s J statistic**, based on the best-AUROC epoch.\n",
    "\n",
    "After all three seeds complete, the cell writes an experiment-level `summary_trainval.json` that includes AUROC per seed, mean AUROC with a 95% t-based confidence interval (n=3), and the stored validation-optimal thresholds under `val_optimal_threshold.by_seed` and `val_optimal_threshold.mean_sd`. The experiment summary is also appended to the global `trainval_runs/history_index.jsonl`. The cell finishes by attempting to **unassign the Colab runtime** to release the GPU."
   ],
   "metadata": {
    "id": "3jTNFaCTupfY"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# D7 trainEnh3: Train and validate task heads (baseline-initialized)\n",
    "# Inputs: D7 train_enh3 manifest (train rows) and D7 full manifest (val rows)\n",
    "# Outputs: Per-seed best heads, per-seed metrics and plots, experiment summary, history entry\n",
    "#\n",
    "# =========================\n",
    "# Train + Val ONLY (CRASH-PROOF, WITH PROGRESS + HISTORY) — D7 ENHANCED (train_enh3)\n",
    "# - Frozen Wav2Vec2 backbone\n",
    "# - Two task heads with small LayerNorm + Dropout blocks (trainable heads only)\n",
    "# - Uses: one manifest for train_enh3 split + one manifest for val split\n",
    "# - Initializes heads from the most recent BASELINE D7 trainval run (not train_enh*)\n",
    "# - Writes: a new exp_*/ folder with per-seed runs, plus summary_trainval.json and history_index.jsonl entry\n",
    "# - Adds threshold metrics (thr=0.5 and val-opt thr) and stores val-opt thresholds in summary_trainval.json\n",
    "# - Ends by unassigning the Colab runtime (L4) with messages\n",
    "# =========================\n",
    "\n",
    "import os, json, math, random, time, warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import soundfile as sf\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from transformers import Wav2Vec2Model\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, roc_curve,\n",
    "    confusion_matrix, accuracy_score,\n",
    "    precision_recall_fscore_support,\n",
    "    matthews_corrcoef\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# -------------------------\n",
    "# 0) Safety check: avoid importing a local file named torch.py or transformers.py\n",
    "# -------------------------\n",
    "if os.path.exists(\"/content/torch.py\") or os.path.exists(\"/content/torch/__init__.py\"):\n",
    "    raise RuntimeError(\"Found local /content/torch.py or /content/torch/ that shadows PyTorch. Rename/remove it and restart runtime.\")\n",
    "if os.path.exists(\"/content/transformers.py\") or os.path.exists(\"/content/transformers/__init__.py\"):\n",
    "    raise RuntimeError(\"Found local /content/transformers.py or /content/transformers/ that shadows Hugging Face Transformers. Rename/remove it and restart runtime.\")\n",
    "\n",
    "# -------------------------\n",
    "# 1) Drive mount (safe if already mounted)\n",
    "# -------------------------\n",
    "try:\n",
    "    from google.colab import drive  # type: ignore\n",
    "    if not os.path.isdir(\"/content/drive/MyDrive\"):\n",
    "        drive.mount(\"/content/drive\")\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# -------------------------\n",
    "# 2) Project root and manifest pointers\n",
    "#    - DX_OUT_ROOT is the D7 preprocessed root (read manifests, write trainval runs)\n",
    "# -------------------------\n",
    "D7_OUT_ROOT_FALLBACK = \"/content/drive/MyDrive/AI_PD_Project/Datasets/D7-Multilingual (D1_D4_D5v2_D6)/preprocessed_v1\"\n",
    "DX_OUT_ROOT = str(globals().get(\"DX_OUT_ROOT\", D7_OUT_ROOT_FALLBACK))\n",
    "globals()[\"DX_OUT_ROOT\"] = DX_OUT_ROOT\n",
    "\n",
    "MANIFEST_ALL = f\"{DX_OUT_ROOT}/manifests/manifest_all.csv\"\n",
    "\n",
    "# Train_enh3 manifest and split label used for filtering rows\n",
    "MANIFEST_TRAIN_ENH3 = f\"{DX_OUT_ROOT}/manifests/manifest_train_enh3.csv\"\n",
    "TRAIN_SPLIT_NAME = \"train_enh3\"\n",
    "\n",
    "# -------------------------\n",
    "# 3) Run identity and output folder for this experiment\n",
    "# -------------------------\n",
    "EXPERIMENT_TAG = \"frozen_LNDO_trainEnh3_initBaseline\"\n",
    "RUN_STAMP = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "TRAINVAL_ROOT = Path(DX_OUT_ROOT) / \"trainval_runs\"\n",
    "EXP_ROOT = TRAINVAL_ROOT / f\"exp_{EXPERIMENT_TAG}_{RUN_STAMP}\"\n",
    "EXP_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# -------------------------\n",
    "# 4) Training settings (kept aligned with the reference style)\n",
    "# -------------------------\n",
    "MAX_EPOCHS     = 10\n",
    "EFFECTIVE_BS   = 64\n",
    "PER_DEVICE_BS  = 16\n",
    "GRAD_ACCUM     = max(1, EFFECTIVE_BS // PER_DEVICE_BS)\n",
    "\n",
    "LR             = 1e-3\n",
    "PATIENCE       = 2\n",
    "SEEDS          = [1337, 2024, 7777]\n",
    "\n",
    "BACKBONE_CKPT  = \"facebook/wav2vec2-base\"\n",
    "SR_EXPECTED    = 16000\n",
    "TINY_THRESH    = 1e-4\n",
    "\n",
    "DROPOUT_P      = 0.2\n",
    "\n",
    "NUM_WORKERS    = 0\n",
    "PIN_MEMORY     = False\n",
    "\n",
    "VOWEL_TASK_VALUE = \"vowl\"\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "try:\n",
    "    torch.set_float32_matmul_precision(\"high\")\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# Reduce common noisy warnings (keeps the notebook output readable)\n",
    "warnings.filterwarnings(\"ignore\", message=\"None of the inputs have requires_grad=True. Gradients will be None\")\n",
    "warnings.filterwarnings(\"ignore\", message=\"Passing `gradient_checkpointing` to a config initialization is deprecated\")\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"huggingface_hub\")\n",
    "\n",
    "print(\"DX_OUT_ROOT:\", DX_OUT_ROOT)\n",
    "print(\"MANIFEST_TRAIN_ENH3:\", MANIFEST_TRAIN_ENH3)\n",
    "print(\"TRAIN_SPLIT_NAME:\", TRAIN_SPLIT_NAME)\n",
    "print(\"MANIFEST_ALL (val source):\", MANIFEST_ALL)\n",
    "print(\"DEVICE:\", DEVICE)\n",
    "if DEVICE.type == \"cuda\":\n",
    "    print(\"GPU:\", torch.cuda.get_device_name(0))\n",
    "print(\"PER_DEVICE_BS:\", PER_DEVICE_BS, \"| GRAD_ACCUM:\", GRAD_ACCUM, \"| EFFECTIVE_BS:\", PER_DEVICE_BS * GRAD_ACCUM)\n",
    "print(\"NUM_WORKERS:\", NUM_WORKERS, \"| PIN_MEMORY:\", PIN_MEMORY)\n",
    "print(\"EXPERIMENT_TAG:\", EXPERIMENT_TAG, \"| RUN_STAMP:\", RUN_STAMP)\n",
    "print(\"EXP_ROOT:\", str(EXP_ROOT))\n",
    "\n",
    "# -------------------------\n",
    "# 5) Load train and val tables from manifests\n",
    "#    - Train: manifest_train_enh3.csv filtered to split == train_enh3\n",
    "#    - Val:   manifest_all.csv filtered to split == val\n",
    "# -------------------------\n",
    "if not os.path.exists(MANIFEST_TRAIN_ENH3):\n",
    "    raise FileNotFoundError(\n",
    "        \"Missing manifest_train_enh3.csv at:\\n\"\n",
    "        f\"  {MANIFEST_TRAIN_ENH3}\\n\"\n",
    "        \"Run the train_enh3 builder first.\"\n",
    "    )\n",
    "if not os.path.exists(MANIFEST_ALL):\n",
    "    raise FileNotFoundError(\n",
    "        \"Missing manifest_all.csv at:\\n\"\n",
    "        f\"  {MANIFEST_ALL}\\n\"\n",
    "        \"Confirm the D7 merge-builder wrote manifests/manifest_all.csv under DX_OUT_ROOT.\"\n",
    "    )\n",
    "\n",
    "m_train = pd.read_csv(MANIFEST_TRAIN_ENH3)\n",
    "m_all   = pd.read_csv(MANIFEST_ALL)\n",
    "\n",
    "# Minimum columns needed for training and evaluation\n",
    "req_cols = {\"split\", \"clip_path\", \"label_num\", \"task\"}\n",
    "for name, df in [(\"manifest_train_enh3\", m_train), (\"manifest_all\", m_all)]:\n",
    "    missing = [c for c in sorted(req_cols) if c not in df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"{name} missing required columns: {missing}. Found: {list(df.columns)}\")\n",
    "\n",
    "# Split filtering (no resplitting here)\n",
    "m_train = m_train[m_train[\"split\"].astype(str) == TRAIN_SPLIT_NAME].copy()\n",
    "m_val   = m_all[m_all[\"split\"].astype(str) == \"val\"].copy()\n",
    "\n",
    "if len(m_train) == 0:\n",
    "    raise RuntimeError(f\"After filtering manifest_train_enh3.csv to split=={TRAIN_SPLIT_NAME!r}, 0 rows remain.\")\n",
    "if len(m_val) == 0:\n",
    "    raise RuntimeError(\"After filtering manifest_all.csv to split=='val', 0 rows remain.\")\n",
    "\n",
    "# Infer dataset_id from the val manifest (used for run folder naming; expected \"D7\")\n",
    "if \"dataset\" in m_val.columns and m_val[\"dataset\"].notna().any():\n",
    "    dataset_id = str(m_val[\"dataset\"].astype(str).value_counts(dropna=True).idxmax())\n",
    "    m_val = m_val[m_val[\"dataset\"].astype(str) == dataset_id].copy()\n",
    "else:\n",
    "    dataset_id = \"DX\"\n",
    "\n",
    "# Guard: this cell is for D7 training\n",
    "if dataset_id != \"D7\":\n",
    "    raise RuntimeError(f\"Dataset inferred from VAL manifest is {dataset_id!r}. Expected 'D7'. Check DX_OUT_ROOT/manifests/manifest_all.csv.\")\n",
    "\n",
    "# Keep a small, consistent set of columns (missing columns become NaN)\n",
    "keep_cols = [\"clip_path\", \"label_num\", \"task\", \"speaker_id\", \"duration_sec\", \"split\"]\n",
    "for df in [m_train, m_val]:\n",
    "    for c in keep_cols:\n",
    "        if c not in df.columns:\n",
    "            df[c] = np.nan\n",
    "\n",
    "m_train = m_train[keep_cols].copy()\n",
    "m_val   = m_val[keep_cols].copy()\n",
    "\n",
    "train_df = m_train.copy().reset_index(drop=True)\n",
    "val_df   = m_val.copy().reset_index(drop=True)\n",
    "\n",
    "print(f\"\\nDataset inferred (from VAL): {dataset_id}\")\n",
    "print(f\"Train rows ({TRAIN_SPLIT_NAME}): {len(train_df)} | Val rows: {len(val_df)}\")\n",
    "print(\"Train label counts:\", train_df[\"label_num\"].value_counts(dropna=False).to_dict())\n",
    "print(\"Val label counts:\",   val_df[\"label_num\"].value_counts(dropna=False).to_dict())\n",
    "\n",
    "# -------------------------\n",
    "# 6) Fail-fast file check: stop early if clip files are missing\n",
    "# -------------------------\n",
    "def _fail_fast_missing_paths(df: pd.DataFrame, name: str):\n",
    "    missing_paths = []\n",
    "    for p in tqdm(df[\"clip_path\"].astype(str).tolist(), desc=f\"Check {name} clip_path exists\", dynamic_ncols=True):\n",
    "        if not os.path.exists(p):\n",
    "            missing_paths.append(p)\n",
    "            if len(missing_paths) >= 10:\n",
    "                break\n",
    "    if missing_paths:\n",
    "        raise FileNotFoundError(f\"{name}: missing clip_path(s). Examples: {missing_paths}\")\n",
    "\n",
    "_fail_fast_missing_paths(train_df, \"TRAIN_ENH3\")\n",
    "_fail_fast_missing_paths(val_df, \"VAL\")\n",
    "\n",
    "# -------------------------\n",
    "# 7) Task grouping used by the two-head model\n",
    "# -------------------------\n",
    "def _task_group(task_val) -> str:\n",
    "    return \"vowel\" if str(task_val) == VOWEL_TASK_VALUE else \"other\"\n",
    "\n",
    "train_df[\"task_group\"] = train_df[\"task\"].apply(_task_group)\n",
    "val_df[\"task_group\"]   = val_df[\"task\"].apply(_task_group)\n",
    "\n",
    "# -------------------------\n",
    "# 8) Dataset and batch padding\n",
    "#    - Loads waveforms from clip_path\n",
    "#    - Builds attention masks so vowel padding is ignored\n",
    "# -------------------------\n",
    "class AudioManifestDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Reads one audio clip and creates an attention mask.\n",
    "\n",
    "    Mask rule:\n",
    "    - vowel clips: mask trailing near-silence so padded zeros do not affect training\n",
    "    - other clips: keep all samples unmasked\n",
    "    \"\"\"\n",
    "    def __init__(self, df: pd.DataFrame):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        row = self.df.iloc[idx]\n",
    "        clip_path = str(row[\"clip_path\"])\n",
    "        label = int(row[\"label_num\"])\n",
    "        task_group = str(row[\"task_group\"])\n",
    "\n",
    "        if not os.path.exists(clip_path):\n",
    "            raise FileNotFoundError(f\"Missing clip_path: {clip_path}\")\n",
    "\n",
    "        y, sr = sf.read(clip_path, always_2d=False)\n",
    "        if isinstance(y, np.ndarray) and y.ndim > 1:\n",
    "            y = np.mean(y, axis=1)\n",
    "        y = np.asarray(y, dtype=np.float32)\n",
    "\n",
    "        if int(sr) != SR_EXPECTED:\n",
    "            raise ValueError(f\"Sample rate mismatch (expected {SR_EXPECTED}): {clip_path} has sr={sr}\")\n",
    "\n",
    "        attn = np.ones((len(y),), dtype=np.int64)\n",
    "\n",
    "        # Vowel clips often include zero padding; mask the padded tail\n",
    "        if task_group == \"vowel\":\n",
    "            k = -1\n",
    "            for j in range(len(y) - 1, -1, -1):\n",
    "                if abs(float(y[j])) > TINY_THRESH:\n",
    "                    k = j\n",
    "                    break\n",
    "            if k >= 0:\n",
    "                attn[:k+1] = 1\n",
    "                attn[k+1:] = 0\n",
    "            else:\n",
    "                attn[:] = 1\n",
    "        else:\n",
    "            attn[:] = 1\n",
    "\n",
    "        return {\n",
    "            \"input_values\": torch.from_numpy(y),                 # float32 [T]\n",
    "            \"attention_mask\": torch.from_numpy(attn),            # int64   [T]\n",
    "            \"labels\": torch.tensor(label, dtype=torch.long),     # int64   []\n",
    "            \"task_group\": task_group,                            # str\n",
    "        }\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"Pads all waveforms and masks to the longest clip in the batch.\"\"\"\n",
    "    max_len = int(max(b[\"input_values\"].numel() for b in batch))\n",
    "    input_vals, attn_masks, labels, task_groups = [], [], [], []\n",
    "    for b in batch:\n",
    "        x = b[\"input_values\"]\n",
    "        a = b[\"attention_mask\"]\n",
    "        pad = max_len - x.numel()\n",
    "        if pad > 0:\n",
    "            x = torch.cat([x, torch.zeros(pad, dtype=x.dtype)], dim=0)\n",
    "            a = torch.cat([a, torch.zeros(pad, dtype=a.dtype)], dim=0)\n",
    "        input_vals.append(x)\n",
    "        attn_masks.append(a)\n",
    "        labels.append(b[\"labels\"])\n",
    "        task_groups.append(b[\"task_group\"])\n",
    "    return {\n",
    "        \"input_values\": torch.stack(input_vals, dim=0),      # [B,T]\n",
    "        \"attention_mask\": torch.stack(attn_masks, dim=0),    # [B,T]\n",
    "        \"labels\": torch.stack(labels, dim=0),                # [B]\n",
    "        \"task_group\": task_groups,                           # list[str]\n",
    "    }\n",
    "\n",
    "# -------------------------\n",
    "# 9) Model: frozen backbone + two task-specific heads\n",
    "# -------------------------\n",
    "class Wav2Vec2TwoHeadClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    Frozen Wav2Vec2 backbone with two small classification heads.\n",
    "    Trainable parts:\n",
    "    - pre_vowel, pre_other (LayerNorm + Dropout)\n",
    "    - head_vowel, head_other (Linear -> 2 logits)\n",
    "    \"\"\"\n",
    "    def __init__(self, ckpt: str, dropout_p: float):\n",
    "        super().__init__()\n",
    "        self.backbone = Wav2Vec2Model.from_pretrained(\n",
    "            ckpt,\n",
    "            use_safetensors=True,\n",
    "            local_files_only=False\n",
    "        )\n",
    "        for p in self.backbone.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "        H = int(self.backbone.config.hidden_size)\n",
    "        self.pre_vowel = nn.Sequential(nn.LayerNorm(H), nn.Dropout(float(dropout_p)))\n",
    "        self.pre_other = nn.Sequential(nn.LayerNorm(H), nn.Dropout(float(dropout_p)))\n",
    "\n",
    "        self.head_vowel = nn.Linear(H, 2)\n",
    "        self.head_other = nn.Linear(H, 2)\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    def masked_mean_pool(self, last_hidden, attn_mask_samples):\n",
    "        # Convert sample-level mask to feature-level mask and pool only valid frames\n",
    "        feat_mask = self.backbone._get_feature_vector_attention_mask(last_hidden.shape[1], attn_mask_samples)\n",
    "        feat_mask = feat_mask.to(last_hidden.device).unsqueeze(-1).type_as(last_hidden)\n",
    "        masked = last_hidden * feat_mask\n",
    "        denom = feat_mask.sum(dim=1).clamp(min=1.0)\n",
    "        return masked.sum(dim=1) / denom\n",
    "\n",
    "    def forward(self, input_values, attention_mask, labels, task_group):\n",
    "        # Backbone forward is kept in no_grad to train heads only\n",
    "        with torch.no_grad():\n",
    "            out = self.backbone(input_values=input_values, attention_mask=attention_mask)\n",
    "            last_hidden = out.last_hidden_state  # [B,T',H]\n",
    "\n",
    "        pooled = self.masked_mean_pool(last_hidden, attention_mask).float()  # [B,H]\n",
    "\n",
    "        z_v = self.pre_vowel(pooled)\n",
    "        z_o = self.pre_other(pooled)\n",
    "\n",
    "        logits_v = self.head_vowel(z_v)\n",
    "        logits_o = self.head_other(z_o)\n",
    "\n",
    "        # Choose which head to use per sample based on task_group\n",
    "        is_vowel = torch.tensor([tg == \"vowel\" for tg in task_group], device=pooled.device, dtype=torch.bool)\n",
    "        logits = logits_o.clone()\n",
    "        if is_vowel.any():\n",
    "            logits[is_vowel] = logits_v[is_vowel]\n",
    "\n",
    "        loss = self.loss_fn(logits, labels)\n",
    "        return loss, logits\n",
    "\n",
    "# -------------------------\n",
    "# 9.5) Baseline head initialization\n",
    "# -------------------------\n",
    "def load_heads_into_model(model: Wav2Vec2TwoHeadClassifier, best_heads_path: Path):\n",
    "    if not best_heads_path.exists():\n",
    "        raise FileNotFoundError(f\"Missing best_heads.pt: {str(best_heads_path)}\")\n",
    "    state = torch.load(str(best_heads_path), map_location=\"cpu\")\n",
    "    model.pre_vowel.load_state_dict(state[\"pre_vowel\"], strict=True)\n",
    "    model.pre_other.load_state_dict(state[\"pre_other\"], strict=True)\n",
    "    model.head_vowel.load_state_dict(state[\"head_vowel\"], strict=True)\n",
    "    model.head_other.load_state_dict(state[\"head_other\"], strict=True)\n",
    "    return model\n",
    "\n",
    "# -------------------------\n",
    "# 10) Metrics and plotting helpers\n",
    "# -------------------------\n",
    "def compute_auc(y_true, y_prob):\n",
    "    y_true = np.asarray(y_true, dtype=np.int64)\n",
    "    y_prob = np.asarray(y_prob, dtype=np.float64)\n",
    "    if len(np.unique(y_true)) < 2:\n",
    "        return float(\"nan\")\n",
    "    return float(roc_auc_score(y_true, y_prob))\n",
    "\n",
    "def compute_threshold_metrics(y_true, y_prob, thr=0.5):\n",
    "    y_true = np.asarray(y_true, dtype=np.int64)\n",
    "    y_prob = np.asarray(y_prob, dtype=np.float64)\n",
    "    y_pred = (y_prob >= float(thr)).astype(np.int64)\n",
    "\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=[0, 1])\n",
    "    tn, fp, fn, tp = (cm.ravel().tolist() if cm.size == 4 else [0, 0, 0, 0])\n",
    "\n",
    "    # Common threshold metrics (kept together for easy reporting)\n",
    "    acc = float(accuracy_score(y_true, y_pred))\n",
    "    prec, rec, f1, _ = precision_recall_fscore_support(y_true, y_pred, average=\"binary\", zero_division=0)\n",
    "    mcc = float(matthews_corrcoef(y_true, y_pred)) if len(np.unique(y_true)) > 1 else float(\"nan\")\n",
    "\n",
    "    sensitivity = float(rec)\n",
    "    specificity = float(tn / (tn + fp)) if (tn + fp) > 0 else float(\"nan\")\n",
    "\n",
    "    # Fisher exact test on the 2x2 confusion table (if available)\n",
    "    p_value = float(\"nan\")\n",
    "    try:\n",
    "        from scipy.stats import fisher_exact  # type: ignore\n",
    "        _, p_value = fisher_exact([[tn, fp], [fn, tp]], alternative=\"two-sided\")\n",
    "        p_value = float(p_value)\n",
    "    except Exception:\n",
    "        p_value = float(\"nan\")\n",
    "\n",
    "    return {\n",
    "        \"threshold\": float(thr),\n",
    "        \"tn\": int(tn), \"fp\": int(fp), \"fn\": int(fn), \"tp\": int(tp),\n",
    "        \"accuracy\": float(acc),\n",
    "        \"precision\": float(prec),\n",
    "        \"recall\": float(rec),\n",
    "        \"f1\": float(f1),\n",
    "        \"sensitivity\": float(sensitivity),\n",
    "        \"specificity\": float(specificity),\n",
    "        \"mcc\": float(mcc),\n",
    "        \"p_value_fisher\": float(p_value),\n",
    "    }\n",
    "\n",
    "def compute_youden_j_threshold(y_true, y_prob):\n",
    "    # Picks the ROC threshold that maximizes (TPR - FPR) on the val set\n",
    "    y_true = np.asarray(y_true, dtype=np.int64)\n",
    "    y_prob = np.asarray(y_prob, dtype=np.float64)\n",
    "    if len(np.unique(y_true)) < 2:\n",
    "        return float(\"nan\"), {\"youden_j\": float(\"nan\"), \"tpr\": float(\"nan\"), \"fpr\": float(\"nan\")}\n",
    "    fpr, tpr, thr = roc_curve(y_true, y_prob)\n",
    "    j = tpr - fpr\n",
    "    idx = int(np.argmax(j))\n",
    "    return float(thr[idx]), {\"youden_j\": float(j[idx]), \"tpr\": float(tpr[idx]), \"fpr\": float(fpr[idx])}\n",
    "\n",
    "def save_roc_curve_png(y_true, y_prob, out_png):\n",
    "    # Simple ROC plot for the best epoch (val)\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_prob)\n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr)\n",
    "    plt.plot([0, 1], [0, 1], linestyle=\"--\")\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(\"ROC Curve (Val)\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_png, dpi=150)\n",
    "    plt.close()\n",
    "\n",
    "def save_confusion_png(y_true, y_prob, out_png, thr=0.5):\n",
    "    # Confusion matrix at a chosen threshold\n",
    "    y_true = np.asarray(y_true, dtype=np.int64)\n",
    "    y_prob = np.asarray(y_prob, dtype=np.float64)\n",
    "    y_pred = (y_prob >= float(thr)).astype(int)\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=[0, 1])\n",
    "\n",
    "    plt.figure()\n",
    "    plt.imshow(cm)\n",
    "    plt.xticks([0, 1], [\"Healthy(0)\", \"PD(1)\"])\n",
    "    plt.yticks([0, 1], [\"Healthy(0)\", \"PD(1)\"])\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"True\")\n",
    "    plt.title(f\"Confusion Matrix (Val, thr={thr:.4f})\")\n",
    "    for (i, j), v in np.ndenumerate(cm):\n",
    "        plt.text(j, i, str(v), ha=\"center\", va=\"center\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_png, dpi=150)\n",
    "    plt.close()\n",
    "\n",
    "def mean_sd(vals):\n",
    "    # Small helper for reporting across seeds\n",
    "    vals = np.asarray(vals, dtype=np.float64)\n",
    "    mu = float(np.nanmean(vals)) if np.any(~np.isnan(vals)) else float(\"nan\")\n",
    "    sd = float(np.nanstd(vals, ddof=1)) if np.sum(~np.isnan(vals)) > 1 else 0.0\n",
    "    return mu, sd\n",
    "\n",
    "# -------------------------\n",
    "# 11) Reproducibility: set all RNG seeds\n",
    "# -------------------------\n",
    "def set_all_seeds(seed: int):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# -------------------------\n",
    "# 11.5) Find the most recent BASELINE D7 trainval run for head initialization\n",
    "#      - Uses summary_trainval.json to exclude any train_enh runs\n",
    "#      - Requires all three best_heads.pt files\n",
    "# -------------------------\n",
    "BASELINE_TRAINVAL_ROOT = Path(DX_OUT_ROOT) / \"trainval_runs\"\n",
    "if not BASELINE_TRAINVAL_ROOT.exists():\n",
    "    raise FileNotFoundError(f\"Missing trainval_runs folder under DX_OUT_ROOT: {str(BASELINE_TRAINVAL_ROOT)}\")\n",
    "\n",
    "exp_dirs = sorted([p for p in BASELINE_TRAINVAL_ROOT.glob(\"exp_*\") if p.is_dir()],\n",
    "                  key=lambda p: p.stat().st_mtime, reverse=True)\n",
    "if not exp_dirs:\n",
    "    raise FileNotFoundError(f\"No exp_* folders found under: {str(BASELINE_TRAINVAL_ROOT)}\")\n",
    "\n",
    "train_dataset_id = \"D7\"\n",
    "\n",
    "def _is_baseline_exp(exp_path: Path) -> bool:\n",
    "    # Baseline = train_manifest_used does not contain train_enh markers\n",
    "    summary_path = exp_path / \"summary_trainval.json\"\n",
    "    if not summary_path.exists():\n",
    "        return False\n",
    "    try:\n",
    "        with open(summary_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            s = json.load(f)\n",
    "        train_manifest_used = str(s.get(\"train_manifest_used\", \"\")).lower()\n",
    "        if \"train_enh\" in train_manifest_used or \"manifest_train_enh\" in train_manifest_used:\n",
    "            return False\n",
    "        return True\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "def _has_all_seeds(exp_path: Path, dataset_id: str, seeds: list) -> bool:\n",
    "    # Need all per-seed head checkpoints to initialize consistently\n",
    "    for s in seeds:\n",
    "        p = exp_path / f\"run_{dataset_id}_seed{s}\" / \"best_heads.pt\"\n",
    "        if not p.exists():\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "baseline_exp = None\n",
    "for ed in exp_dirs:\n",
    "    if ed.resolve() == EXP_ROOT.resolve():\n",
    "        continue\n",
    "    if _is_baseline_exp(ed) and _has_all_seeds(ed, train_dataset_id, SEEDS):\n",
    "        baseline_exp = ed\n",
    "        break\n",
    "\n",
    "if baseline_exp is None:\n",
    "    raise FileNotFoundError(\n",
    "        \"Could not find a BASELINE D7 trainval experiment with all 3 best_heads.pt files.\\n\"\n",
    "        \"Baseline guard excludes experiments whose summary_trainval.json shows train_manifest_used contains 'train_enh'.\\n\"\n",
    "        f\"Searched under: {str(BASELINE_TRAINVAL_ROOT)}/exp_*/run_D7_seedXXXX/best_heads.pt\"\n",
    "    )\n",
    "\n",
    "baseline_summary_path = baseline_exp / \"summary_trainval.json\"\n",
    "with open(baseline_summary_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    baseline_summary = json.load(f)\n",
    "\n",
    "print(\"\\nBaseline initialization experiment selected:\")\n",
    "print(\" \", str(baseline_exp))\n",
    "print(\" \", \"summary:\", str(baseline_summary_path))\n",
    "print(\" \", \"train_manifest_used (baseline):\", baseline_summary.get(\"train_manifest_used\", \"NA\"))\n",
    "\n",
    "# -------------------------\n",
    "# 12) Single-seed train+val loop with early stopping\n",
    "#      - Tracks best val AUROC\n",
    "#      - Saves best heads and best-epoch val plots\n",
    "#      - Computes val-opt threshold at the best AUROC epoch\n",
    "# -------------------------\n",
    "def run_trainval_once(seed: int):\n",
    "    set_all_seeds(seed)\n",
    "\n",
    "    run_dir = EXP_ROOT / f\"run_{dataset_id}_seed{seed}\"\n",
    "    run_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    train_ds = AudioManifestDataset(train_df)\n",
    "    val_ds   = AudioManifestDataset(val_df)\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_ds,\n",
    "        batch_size=PER_DEVICE_BS,\n",
    "        shuffle=True,\n",
    "        num_workers=NUM_WORKERS,\n",
    "        pin_memory=PIN_MEMORY,\n",
    "        collate_fn=collate_fn\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_ds,\n",
    "        batch_size=PER_DEVICE_BS,\n",
    "        shuffle=False,\n",
    "        num_workers=NUM_WORKERS,\n",
    "        pin_memory=PIN_MEMORY,\n",
    "        collate_fn=collate_fn\n",
    "    )\n",
    "\n",
    "    # Quick read of a few batches to catch loader issues early\n",
    "    print(f\"\\n[seed={seed}] Warm-up: loading 3 train batches...\")\n",
    "    t0 = time.time()\n",
    "    it = iter(train_loader)\n",
    "    for i in range(3):\n",
    "        _ = next(it)\n",
    "        print(f\"  loaded warmup batch {i+1}/3\")\n",
    "    print(f\"[seed={seed}] Warm-up done in {time.time()-t0:.2f}s\")\n",
    "\n",
    "    model = Wav2Vec2TwoHeadClassifier(BACKBONE_CKPT, dropout_p=DROPOUT_P).to(DEVICE)\n",
    "\n",
    "    # Head initialization from the selected baseline experiment (same seed)\n",
    "    baseline_heads_path = baseline_exp / f\"run_{train_dataset_id}_seed{seed}\" / \"best_heads.pt\"\n",
    "    print(f\"[seed={seed}] Initializing heads from baseline:\")\n",
    "    print(\" \", str(baseline_heads_path))\n",
    "    model = load_heads_into_model(model, baseline_heads_path)\n",
    "    model.train()\n",
    "\n",
    "    # Only train the small head blocks (backbone stays frozen)\n",
    "    trainable_params = (\n",
    "        list(model.pre_vowel.parameters()) + list(model.pre_other.parameters()) +\n",
    "        list(model.head_vowel.parameters()) + list(model.head_other.parameters())\n",
    "    )\n",
    "    opt = torch.optim.Adam(trainable_params, lr=LR)\n",
    "\n",
    "    best_auc = -1.0\n",
    "    best_epoch = -1\n",
    "    no_improve = 0\n",
    "\n",
    "    best_state = None\n",
    "    best_val_probs = None\n",
    "    best_val_true = None\n",
    "\n",
    "    best_thr_youden = float(\"nan\")\n",
    "    best_thr_youden_details = None\n",
    "    best_val_metrics_thr05 = None\n",
    "    best_val_metrics_thr_opt = None\n",
    "\n",
    "    for epoch in range(1, MAX_EPOCHS + 1):\n",
    "        model.train()\n",
    "        train_losses = []\n",
    "        opt.zero_grad(set_to_none=True)\n",
    "\n",
    "        pbar = tqdm(train_loader, desc=f\"[seed={seed}] Train epoch {epoch}\", dynamic_ncols=True)\n",
    "        step = 0\n",
    "        for batch in pbar:\n",
    "            step += 1\n",
    "            input_values = batch[\"input_values\"].to(DEVICE, non_blocking=False)\n",
    "            attention_mask = batch[\"attention_mask\"].to(DEVICE, non_blocking=False)\n",
    "            labels = batch[\"labels\"].to(DEVICE, non_blocking=False)\n",
    "            task_group = batch[\"task_group\"]\n",
    "\n",
    "            loss, _ = model(input_values, attention_mask, labels, task_group)\n",
    "            loss = loss / GRAD_ACCUM\n",
    "            loss.backward()\n",
    "\n",
    "            train_losses.append(float(loss.detach().cpu().item()) * GRAD_ACCUM)\n",
    "\n",
    "            if (step % GRAD_ACCUM) == 0:\n",
    "                opt.step()\n",
    "                opt.zero_grad(set_to_none=True)\n",
    "\n",
    "        # Final step if the epoch ends mid accumulation\n",
    "        if (step % GRAD_ACCUM) != 0:\n",
    "            opt.step()\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "\n",
    "        avg_train_loss = float(np.mean(train_losses)) if train_losses else float(\"nan\")\n",
    "\n",
    "        # Validation pass (collect probabilities for AUROC and thresholds)\n",
    "        model.eval()\n",
    "        all_probs, all_true = [], []\n",
    "        vpbar = tqdm(val_loader, desc=f\"[seed={seed}] Val epoch {epoch}\", dynamic_ncols=True)\n",
    "        with torch.inference_mode():\n",
    "            for batch in vpbar:\n",
    "                input_values = batch[\"input_values\"].to(DEVICE, non_blocking=False)\n",
    "                attention_mask = batch[\"attention_mask\"].to(DEVICE, non_blocking=False)\n",
    "                labels = batch[\"labels\"].to(DEVICE, non_blocking=False)\n",
    "                task_group = batch[\"task_group\"]\n",
    "\n",
    "                _, logits = model(input_values, attention_mask, labels, task_group)\n",
    "                probs = torch.softmax(logits.float(), dim=-1)[:, 1].detach().cpu().numpy().astype(np.float64)\n",
    "                all_probs.extend(probs.tolist())\n",
    "                all_true.extend(labels.detach().cpu().numpy().astype(np.int64).tolist())\n",
    "\n",
    "        val_auc = compute_auc(all_true, all_probs)\n",
    "        print(f\"seed={seed} | epoch {epoch:02d}/{MAX_EPOCHS} | train_loss={avg_train_loss:.5f} | val_AUROC={val_auc:.5f}\")\n",
    "\n",
    "        # Track the best val AUROC and snapshot head weights at that epoch\n",
    "        improved = (not math.isnan(val_auc)) and (val_auc > best_auc + 1e-12)\n",
    "        if improved:\n",
    "            best_auc = float(val_auc)\n",
    "            best_epoch = int(epoch)\n",
    "            no_improve = 0\n",
    "\n",
    "            best_state = {\n",
    "                \"pre_vowel\": {k: v.detach().cpu().clone() for k, v in model.pre_vowel.state_dict().items()},\n",
    "                \"pre_other\": {k: v.detach().cpu().clone() for k, v in model.pre_other.state_dict().items()},\n",
    "                \"head_vowel\": {k: v.detach().cpu().clone() for k, v in model.head_vowel.state_dict().items()},\n",
    "                \"head_other\": {k: v.detach().cpu().clone() for k, v in model.head_other.state_dict().items()},\n",
    "            }\n",
    "\n",
    "            best_val_probs = list(all_probs)\n",
    "            best_val_true  = list(all_true)\n",
    "\n",
    "            # Metrics at a fixed threshold and at the val-opt threshold\n",
    "            best_val_metrics_thr05 = compute_threshold_metrics(best_val_true, best_val_probs, thr=0.5)\n",
    "\n",
    "            thr_opt, details = compute_youden_j_threshold(best_val_true, best_val_probs)\n",
    "            best_thr_youden = float(thr_opt)\n",
    "            best_thr_youden_details = details\n",
    "            best_val_metrics_thr_opt = compute_threshold_metrics(best_val_true, best_val_probs, thr=best_thr_youden)\n",
    "        else:\n",
    "            no_improve += 1\n",
    "\n",
    "        # Early stop if AUROC does not improve for PATIENCE epochs\n",
    "        if no_improve >= PATIENCE:\n",
    "            break\n",
    "\n",
    "    if best_state is None or best_val_probs is None or best_val_true is None:\n",
    "        raise RuntimeError(\n",
    "            \"No best epoch captured. Validation AUROC may be NaN due to single-class validation split \"\n",
    "            \"or earlier failures.\"\n",
    "        )\n",
    "\n",
    "    # Save best heads for this seed (used later by test-only cells)\n",
    "    best_heads_path = run_dir / \"best_heads.pt\"\n",
    "    torch.save(best_state, str(best_heads_path))\n",
    "\n",
    "    # Save best-epoch plots for quick inspection\n",
    "    roc_png = run_dir / \"roc_curve.png\"\n",
    "    cm_png_05 = run_dir / \"confusion_matrix_thr0p5.png\"\n",
    "    cm_png_opt = run_dir / \"confusion_matrix_thr_opt.png\"\n",
    "\n",
    "    save_roc_curve_png(np.asarray(best_val_true, dtype=np.int64), np.asarray(best_val_probs, dtype=np.float64), str(roc_png))\n",
    "    save_confusion_png(np.asarray(best_val_true, dtype=np.int64), np.asarray(best_val_probs, dtype=np.float64), str(cm_png_05), thr=0.5)\n",
    "    if not np.isnan(best_thr_youden):\n",
    "        save_confusion_png(np.asarray(best_val_true, dtype=np.int64), np.asarray(best_val_probs, dtype=np.float64), str(cm_png_opt), thr=float(best_thr_youden))\n",
    "\n",
    "    # Per-seed metrics payload saved next to the model heads\n",
    "    metrics = {\n",
    "        \"dataset\": dataset_id,\n",
    "        \"seed\": int(seed),\n",
    "        \"best_val_auroc\": float(best_auc),\n",
    "        \"best_epoch\": int(best_epoch),\n",
    "\n",
    "        \"train_manifest_used\": MANIFEST_TRAIN_ENH3,\n",
    "        \"val_manifest_used\": MANIFEST_ALL,\n",
    "\n",
    "        \"init_heads\": {\n",
    "            \"mode\": \"baseline_best_heads\",\n",
    "            \"baseline_exp_used\": str(baseline_exp),\n",
    "            \"baseline_summary_path\": str(baseline_summary_path),\n",
    "            \"baseline_best_heads_path\": str(baseline_heads_path),\n",
    "        },\n",
    "\n",
    "        \"n_train\": int(len(train_df)),\n",
    "        \"n_val\": int(len(val_df)),\n",
    "        \"label_counts_train\": train_df[\"label_num\"].value_counts(dropna=False).to_dict(),\n",
    "        \"label_counts_val\": val_df[\"label_num\"].value_counts(dropna=False).to_dict(),\n",
    "\n",
    "        \"experiment_tag\": EXPERIMENT_TAG,\n",
    "        \"run_stamp\": RUN_STAMP,\n",
    "\n",
    "        \"dropout_p\": float(DROPOUT_P),\n",
    "        \"lr\": float(LR),\n",
    "        \"effective_batch_size\": int(PER_DEVICE_BS * GRAD_ACCUM),\n",
    "        \"per_device_batch_size\": int(PER_DEVICE_BS),\n",
    "        \"grad_accum_steps\": int(GRAD_ACCUM),\n",
    "\n",
    "        \"backbone_ckpt\": BACKBONE_CKPT,\n",
    "\n",
    "        \"val_opt_threshold_method\": \"Youden J (maximize TPR - FPR on VAL ROC curve)\",\n",
    "        \"val_opt_threshold\": float(best_thr_youden),\n",
    "        \"val_opt_details\": best_thr_youden_details,\n",
    "\n",
    "        \"thr_metrics_val_thr0p5\": best_val_metrics_thr05,\n",
    "        \"thr_metrics_val_thr_opt\": best_val_metrics_thr_opt,\n",
    "\n",
    "        \"artifacts\": {\n",
    "            \"roc_curve_png\": str(roc_png),\n",
    "            \"confusion_thr0p5_png\": str(cm_png_05),\n",
    "            \"confusion_thr_opt_png\": str(cm_png_opt),\n",
    "            \"best_heads_pt\": str(best_heads_path),\n",
    "        },\n",
    "    }\n",
    "\n",
    "    metrics_path = run_dir / \"metrics.json\"\n",
    "    with open(metrics_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(metrics, f, indent=2)\n",
    "\n",
    "    print(f\"[seed={seed}] VAL-opt threshold (Youden J): {float(best_thr_youden):.6f}\")\n",
    "    print(f\"[seed={seed}] WROTE:\")\n",
    "    print(\" \", str(metrics_path))\n",
    "    print(\" \", str(roc_png))\n",
    "    print(\" \", str(cm_png_05))\n",
    "    print(\" \", str(cm_png_opt))\n",
    "    print(\" \", str(best_heads_path))\n",
    "\n",
    "    return {\n",
    "        \"seed\": int(seed),\n",
    "        \"best_val_auroc\": float(best_auc),\n",
    "        \"best_epoch\": int(best_epoch),\n",
    "        \"val_opt_thr\": float(best_thr_youden),\n",
    "        \"run_dir\": str(run_dir),\n",
    "        \"seed_metrics\": metrics,\n",
    "    }\n",
    "\n",
    "# -------------------------\n",
    "# 13) Run all seeds and write the experiment summary\n",
    "#      - AUROC summary: mean and 95% CI across seeds\n",
    "#      - Threshold summary: mean and SD across seeds\n",
    "# -------------------------\n",
    "results = []\n",
    "for seed in SEEDS:\n",
    "    results.append(run_trainval_once(seed))\n",
    "\n",
    "aucs = [r[\"best_val_auroc\"] for r in results]\n",
    "thr_vals = [r[\"val_opt_thr\"] for r in results]\n",
    "\n",
    "t_crit = 4.302652729911275  # df=2, 95% CI\n",
    "n = len(aucs)\n",
    "mean_auc = float(np.mean(aucs))\n",
    "std_auc = float(np.std(aucs, ddof=1)) if n > 1 else 0.0\n",
    "half_width = float(t_crit * (std_auc / math.sqrt(n))) if n > 1 else 0.0\n",
    "ci95 = [float(mean_auc - half_width), float(mean_auc + half_width)]\n",
    "\n",
    "thr_mean, thr_sd = mean_sd(thr_vals)\n",
    "\n",
    "print(\"\\nAUROC by seed:\")\n",
    "for r in results:\n",
    "    print(f\"  seed {r['seed']}: {r['best_val_auroc']:.6f}\")\n",
    "print(f\"\\nMean AUROC: {mean_auc:.6f}\")\n",
    "print(f\"95% CI (t, n=3): [{ci95[0]:.6f}, {ci95[1]:.6f}]\")\n",
    "\n",
    "print(\"\\nVAL-opt thresholds (Youden J) by seed:\")\n",
    "for r in results:\n",
    "    print(f\"  seed {r['seed']}: {r['val_opt_thr']:.6f}\")\n",
    "print(f\"  mean ± SD: {thr_mean:.6f} ± {thr_sd:.6f}\")\n",
    "\n",
    "val_optimal_threshold_obj = {\n",
    "    \"method\": \"Youden J (maximize TPR - FPR on VAL ROC curve)\",\n",
    "    \"by_seed\": {str(r[\"seed\"]): float(r[\"val_opt_thr\"]) for r in results},\n",
    "    \"mean_sd\": {\"mean\": float(thr_mean), \"sd\": float(thr_sd)},\n",
    "}\n",
    "\n",
    "# Compact experiment summary used by downstream test-only code\n",
    "exp_summary = {\n",
    "    \"dataset\": dataset_id,\n",
    "    \"dx_out_root\": DX_OUT_ROOT,\n",
    "\n",
    "    \"train_manifest_used\": MANIFEST_TRAIN_ENH3,\n",
    "    \"val_manifest_used\": MANIFEST_ALL,\n",
    "\n",
    "    \"init_heads\": {\n",
    "        \"mode\": \"baseline_best_heads\",\n",
    "        \"baseline_exp_used\": str(baseline_exp),\n",
    "        \"baseline_summary_path\": str(baseline_summary_path),\n",
    "        \"baseline_best_heads_by_seed\": {\n",
    "            str(s): str(baseline_exp / f\"run_{train_dataset_id}_seed{s}\" / \"best_heads.pt\") for s in SEEDS\n",
    "        },\n",
    "    },\n",
    "\n",
    "    \"experiment_tag\": EXPERIMENT_TAG,\n",
    "    \"run_stamp\": RUN_STAMP,\n",
    "    \"exp_root\": str(EXP_ROOT),\n",
    "    \"run_dirs\": [r[\"run_dir\"] for r in results],\n",
    "    \"seeds\": SEEDS,\n",
    "\n",
    "    \"aurocs\": [float(x) for x in aucs],\n",
    "    \"mean_auroc\": float(mean_auc),\n",
    "    \"t_crit_df2_95\": float(t_crit),\n",
    "    \"half_width_95_ci\": float(half_width),\n",
    "    \"ci95\": ci95,\n",
    "\n",
    "    \"n_train\": int(len(train_df)),\n",
    "    \"n_val\": int(len(val_df)),\n",
    "    \"label_counts_train\": train_df[\"label_num\"].value_counts(dropna=False).to_dict(),\n",
    "    \"label_counts_val\": val_df[\"label_num\"].value_counts(dropna=False).to_dict(),\n",
    "\n",
    "    \"effective_batch_size\": int(PER_DEVICE_BS * GRAD_ACCUM),\n",
    "    \"per_device_batch_size\": int(PER_DEVICE_BS),\n",
    "    \"grad_accum_steps\": int(GRAD_ACCUM),\n",
    "\n",
    "    \"backbone_ckpt\": BACKBONE_CKPT,\n",
    "    \"dropout_p\": float(DROPOUT_P),\n",
    "    \"lr\": float(LR),\n",
    "\n",
    "    \"val_optimal_threshold\": val_optimal_threshold_obj,\n",
    "    \"per_seed_metrics\": [r[\"seed_metrics\"] for r in results],\n",
    "}\n",
    "\n",
    "summary_path = EXP_ROOT / \"summary_trainval.json\"\n",
    "with open(summary_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(exp_summary, f, indent=2)\n",
    "\n",
    "# Append-only history log for quick run tracking\n",
    "history_path = TRAINVAL_ROOT / \"history_index.jsonl\"\n",
    "TRAINVAL_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "with open(history_path, \"a\", encoding=\"utf-8\") as f:\n",
    "    f.write(json.dumps(exp_summary) + \"\\n\")\n",
    "\n",
    "print(\"\\nWROTE per-experiment summary:\", str(summary_path))\n",
    "print(\"APPENDED global history index:\", str(history_path))\n",
    "print(\"\\nOpen this folder to access artifacts:\", str(EXP_ROOT))\n",
    "\n",
    "# -------------------------\n",
    "# 14) Runtime shutdown (stop L4)\n",
    "# -------------------------\n",
    "print(\"\\nAll done. Unassigning the runtime to stop the L4 instance...\")\n",
    "try:\n",
    "    from google.colab import runtime  # type: ignore\n",
    "    print(\"Calling runtime.unassign() now.\")\n",
    "    runtime.unassign()\n",
    "except Exception as e:\n",
    "    print(\"Could not unassign runtime automatically. Error:\", repr(e))\n",
    "    print(\"Manual stop: Runtime -> Disconnect and delete runtime.\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "e314d709347542e8a7706c7b2a5216f4",
      "d1270da5cedb48f5aebffbc2c3f11131",
      "d9359eb3ba3e491b8b415ff9b50ea910",
      "ba56a56b2a06428ba9a0c6e0fc0f4608",
      "24fd2f319614434991d31aab5bbfe236",
      "f2b28019af6c4862911468103fb127a4",
      "20edd1c6a82747a38c0f35cfdd1517ae",
      "a907ffcc275e4d099638f5b7266d9f12",
      "6d76350d659e4e5e91287f8d4456bb6f",
      "c2fd1c338db24a659730c086c278a553",
      "5565b91db2c24a0280f9977067dc5b44",
      "2165dd4c653f461b8d3387325d42db62",
      "04c5c8e1c25542b49d413700108e579a",
      "7df002e44f9e47e0b00a31fbe8ad4faa",
      "062c77d5e8034223b608f6674074466f",
      "846a0932f8fc4fa395840a7656d8cb93",
      "4d0069339a4046f2b7a9310d05929146",
      "abed0b7af18a46f3bd62a246f91ba9cb",
      "d95114fb3ce84f5094d355c2efa83595",
      "fb102d109442460abade35455684c1f3",
      "1e9ad0cd12074c078c7d8160f2ef2212",
      "df5b3e9d95b9470f9cc833560e05bfd2",
      "d379790c9a8e40d1b4843583a40aeefd",
      "64badebc1a41411b8b83c45f7dd1371f",
      "57ad0e1bf0594b43bbbb53fe1e244519",
      "03ade48eda9c4f958b89652dd67d99c7",
      "c10724827e2e480fbda74f46ae1de0a7",
      "5bc92dfffe5b4d2fa136dd4ec17b2605",
      "1d775c936d1042cc9ca376aac011648e",
      "07591535d2d1445999d6165d1c96f2de",
      "29d5c06e6af841e1af14071c7b6fecbb",
      "b17d54dcaa0f4739a57f2ca5c33143b6",
      "5df08cdeb9e8498daead7d72f82948a4",
      "6d71c90a0795419c931905199c8c6a1e",
      "8df05ddcb437499381adb6a09667be75",
      "e36598e39baa454f8692c7a94cbd28fb",
      "81956c410765472d8b133a055ee54a52",
      "8a65a1c373c14ba499139bb84874cfd3",
      "1713312cdb434dd0b373ed96f2ad14c5",
      "6b3749282a2a4c8fba681d84fd48c4e8",
      "2174c5750ace4cc6b06fcd2f7bbef119",
      "7f26b909facc4b02a2a08d8a97a26f7d",
      "50c7ad02b50b4d7ba3dca92a8771908c",
      "d86a4f56e633429e817f25477bebdcfb",
      "2810e14eb92d4caba21a3e0d7771d7f6",
      "176a516d2f4f4c2aa382107c0fb7cf13",
      "272b1f06b5e04b6ea711623cb7f9cf32",
      "9921dda0d8864f238030c5a7f38a3817",
      "9a89212eb9f94be8880097cfd1f42018",
      "63b4134583fd457792ebdaa778cf57bb",
      "943c4f005f374da7957af58541fea588",
      "3d8b2b71d08d490687484b8062717642",
      "fdd80f9af3a8430e8dfc5db29530f68f",
      "9a9f1e2b161a49b0bd049ed7dec72a89",
      "9c7935e1eb6d4c77808417cef9d8b5cd",
      "023d279445b74a8f9a2800362defc1db",
      "db6634b3699e4cff880c5c8dc5f3141a",
      "20ac6ac977834edca19a0749954c43a4",
      "24dd76e9ec994b61af3e5f40f46def56",
      "116aa09a62b743daa90bd126c75c328b",
      "a9a3826bf89842ec8a3d9ba5287b399f",
      "b15aeb69f386486fbe1e3c2f09645cf6",
      "0cef99c5249f4fd9a4a590efbfe80cc9",
      "23cdbacb26ca4810bba07c3eb0b68257",
      "be082ed9fe2e431aaf066e03f8a9f75f",
      "8edc40156d7c4a3bb27ecd5854d7f063",
      "b055d098750543beb6e820402292078c",
      "5462aede59d24262a324c4706ff57e11",
      "7e7408612d504e3eba3b597289643963",
      "cfac4036ec5b438ab8256ae16b0a4d33",
      "de31e431300e4b08b2ed70a312d86ebd",
      "4f2185e91b354e5088161f2a8833a0de",
      "bf19d0efeec547f9a425a7d9f3bed3f5",
      "e43b8c0ac5ff4601b9644432aceeac56",
      "bc15eee6a1ca4cde85343faafe309802",
      "e1d8f58a94aa4020be3ea3bc3a7844ec",
      "7e5959db51ab49fa98aa9a47db1c884e",
      "035b2ffe4ec444baa90b7e24e115162b",
      "25c4910f3ea14692bd478ae80e161e91",
      "42d153b7bb8d45cab5f2136e97546713",
      "6972050daaa94c0fa207bce4d01e9cb4",
      "2de9b0d68bc148708562e01a1d1a009f",
      "e787881e50da4f25b10f523f60d4b4ea",
      "9248f9fefc1f4ee9a6e1792fe858439c",
      "43623a005d3c4932b38e5d66f7c4a47f",
      "9d171648a50d41f0b8a66098e661539d",
      "2a70c3fa713f414c89f48d5df7bfd266",
      "1aa87825ff3647f293e59488a196d4f3",
      "40f69b1796fa462aa4e937550d988922",
      "9395f0c04e8c42ba9b270c9cf8fac444",
      "4ce502db47a14757ba947dd8d83348f9",
      "4d39ce77fd6c43b99d2abdf7aaa2aa38",
      "6d8fb81bf92e45f3b8f9470fd1bb876e",
      "1f9bcb83f9db42689d9de71e6cac9fa1",
      "454cb0ad22904b75a96cd23c79111553",
      "928ac6b20d1c40f4b8a047a4abd0e338",
      "8bb9940fbad845ac8b2dfe177d0f493a",
      "822b29f7eac74a26bd9daf160791fe46",
      "5771fdf2fb6148ab84718d47e1d0d51f",
      "f4d830d9c13449f4b23ce24add78cfcf",
      "3512ef3001fd4f36a91a6d1e1312bfe7",
      "23864250d9f948dbb64cb7a551c3fd4a",
      "b9f23ccb0a304acd90bd4ad04b816d9d",
      "99223456836343a7b8218e834982590d",
      "d38e7bcf8125418691a87b6629de6a5c",
      "060f82d3a2c64152a294a51d4505b881",
      "94b5a4d6e275404cbb8171c2accc4a5a",
      "029971c48ef043b6b4261f4b0cfd11ff",
      "21885a5cb6df445aaf79e577dd9dd398",
      "85f31a1a4adb46a9a839ab42b96e26ef",
      "5e300e5dc0184eb7990c778844835ef4",
      "fdd1c021e8764f3ab4aafb4f1292e659",
      "b3150405a335432682613348a0e34839",
      "246e104f20a248a68249cdec2023f6b6",
      "6b3032972b84408f86c7263212f0e0b5",
      "c3a0a434eb0543b9b820c9717c00bdca",
      "78e11c4542654d689471a2616a22e453",
      "fbab424b4e744011a894905261bfcec9",
      "154ccddb47e14afa96736cd9be9d2569",
      "065ddbd2a6b747e38d47733bfefefc95",
      "f9de7a0a4be44554a0e1f480f211e87a",
      "2f806f4985444d94ac3c31c37c80cda7",
      "411d831d5b3b4bd397461f6c48e089c8",
      "4cdfaf82f05a416da4d42f1096a7eafa",
      "fd4d4483db804752bb0ae84a567b4563",
      "9289e6e7545f442c822bf9f980f17c17",
      "ca604f7d75764f3cb62f87574c9860a0",
      "6d958295a607423ab8df9a6434cc39b6",
      "411b16a0aef144369f557c94e464299b",
      "b192464d499949f69285508de840d09e",
      "687d9328980c4fe98178ab7df6fdf45a",
      "fa8075fc3b8d4e829b41b096a76ca5e7",
      "2a9dbb2c451742c095dcfb6ffa924c0a",
      "973f56b85d7e46debbbf66ca4d08f096",
      "90e4fa883b9b4d4b8bb3cd9606c78cf6",
      "54c0670a515b49da94e0121dedffc243",
      "23ba9c588abd407586f59f5fa28afee6",
      "97dc0ced2dfc4b8db39dfe313e8f19f1",
      "626b78898dea47068a96a00191b26d03",
      "aa85a872a64d44b481a753620a061cee",
      "e001f72a53924f8193947ce2c020183c",
      "18dc652bd4e34989ba5cccc8b409fef8",
      "f539c287e2624b6db0755efeab3dbfcb",
      "8ea1bce184804d8195b0e7dd5779d9c2",
      "cc3ea7ea94cb4456917f66ed776ab470",
      "3a4c933fcb9c459ebb4d91adaf7001c3",
      "57b1ed809fdf47cc8046b0951c3fdf25",
      "a2fcf53006714ba583524152b76d8595",
      "10e2635d15374e2cba70fd63611452cb",
      "2486a57d5f004f8f91dbc3b46f521b54",
      "64ed017fe9d84841a6d8647fe4c27cae",
      "00a6baf0abda45dba412465d62ab80f6",
      "d2f14a6fc7994b17a67be17a8445a88f",
      "8b0ef2ea761b43fa802dd11780dcabce",
      "4f4506bda05c4b08bcd5e046a33fb79f",
      "e93ac178198e4e8abfd2d71f55330b45",
      "44d29487f5924aceb69261c963f2cedd",
      "b9a5a25d6c11477f911b848b6853cf00",
      "946c91cad4184e38beaa264aaa071626",
      "e047f6bc1c3f4a4497b35d6e7270a8e9",
      "c4c65ae865e6432e8e07bd81b2d6bad1",
      "f2663140824e4bd0a81cac074a935c8d",
      "670da3b013c347c78b52acc195e45c11",
      "8eac145f0ece49e2a787a5e6906b24f1",
      "e8c9b4688dec4bc99b6ecacd4e631260",
      "590ed40abf8b4d539520f0948159ab61",
      "679bf2dd8bfb483e9bcfa82fd4e0e1fd",
      "dfa90929724b46b5b1176f819a7d49c7",
      "29c156bc8ceb434481de30e0648ce0c2",
      "d92ef1aeb3f047e3a3e98daa14a0faa3",
      "a9af38241fa04ba9bb16d6cc73036a3f",
      "32843a012a9140ad845bf7ca7f1009e6",
      "309f994060fd4c75a07ef26de0ce98db",
      "f803b2c5df2043d78ae1f4560ba6a72c",
      "608cba9200ff4ec2aec8b1d853827e3a",
      "54821554abef4448a6d5ddfa1e5725d5",
      "7a2a6ed6552b44a291f9f065cea8d3c7",
      "d00146f983dc4d01a03fe073cfea5cf2",
      "c82ebe80d62a42bdab3f680793c39244",
      "72f607fe8b95436d8334e7c96a6a0dd9",
      "63ce011a6be94da3adb9d1b9485af682",
      "52a1db4c1d1f4b848662220c8700e677",
      "aa579567f6434b1ea73e74e79f42db31",
      "2df865516c7a48169a292693ef039ab6",
      "7860c1e7a02f454a97142d391eea19a2",
      "84b4149183f949f8b58f001fe0eed723",
      "9bed9bf74e474575bb5c1acd671f8dd7",
      "4c0e2f239ef44721b3a936347427d440",
      "32dec5ea4fd94f00977b0036109b62ba",
      "084a6bc59e60445ebd61926d0cbf63d7",
      "70a9994337c34cce8a171764393d953f",
      "3254a5a340084a81bf0dcc18663d4d4a",
      "e40de39d557f48bdb2d21ddeb196074f",
      "faff1a193d26490d92d250b7d3e19360",
      "61344df6dd97438482aa868f698e08e0",
      "dbd3d3b3f484495ebcfecad64fe753f0",
      "8272673cec7a42fca01ffabbfebda34b",
      "aae4a60c8ce24bb2a2dfae3039d87cda",
      "fd460741da7943a98cd1f174f6e4e943",
      "16b330973e634073a540fe22dbdc0c1d",
      "edc831255a3b4b2db084111d9b117bc6",
      "d84cb8734ab14d68b980585fc3c7fc84",
      "30c5243a256f4d3d831953e46ee05d94",
      "e8c0538d200747278050a5e18843ac87",
      "9aaaacaa5e8f43f1ad068b75ddb42e55",
      "421327cc8ed1466a96f9eba2b13c8321",
      "79a0574ce61946e8b0a85bd04c291941",
      "8ce04d455f464e51a208a1366b2e3af5",
      "d9237e029153499ebee53c68ae2f540c",
      "b6ffbe3690854f7da8d943eda6fe2cfd",
      "1dd1cbd7223d44da97a317f1191f879b",
      "0f18bb796f7141ac9806ebb4b8de4c9b",
      "37ea2ba3fe664c108105d7c91ddf2e7f",
      "e361aea54ee34396a3d81864570312d8",
      "8ef71c9ad6084b58a7751724aa876bb4",
      "96c49436c0cb4ae78c40a515c1a8810b",
      "dc09c8d5fb96470a843e7500821b3715",
      "08d01d1d64684e56a2e3e3f75a5ce88b",
      "33e711bc4f674679b7d68ce6d2d6924d",
      "8fe8a7895451473d95a873962ed99daa",
      "0fa57e7a4ddf4324917c68d0694c5d72",
      "1e6899a3b7244567aac61e9c1b5f540e",
      "ed454fc034334f2aa4d023d8ec81b7d0",
      "24dd8f1886e649b19bc0d787d32f4089",
      "ff93f7d60d0a4d60811fe07bd429fca4",
      "b6f49732913e421b914bef7523b2374c",
      "c41dc4bd16554dba945e2fa60829ef8b",
      "e3aa1fa2b8fd4971b062b8c84af4b33f",
      "d131f5bd030b4a299a81efad531b273e",
      "dfcb4d366cc043bcb29c55c27e036001",
      "d791c5b884104725bcf0afe03f9f6114",
      "04ef85d446a544f0ad930ad999daa9c4",
      "4285724197dd4d5c91dd98cdd09c336a",
      "b8619eb651d344009aefec62f61561b9",
      "ea3722cd63784e2eb9be43563506a3ce",
      "c93b874168864aba8fc8e58d48316d1e",
      "c14d951980c44a65861ab5eed3bacce7",
      "88d197483a544c3fb78380202d86a64b",
      "7aefed17b85b41b1a2485001443b0b1a",
      "d7e13a302a6c4b48b9b205acbe96a547",
      "99781ea380c74749bc14b66671a39dbe",
      "895943c5a2474808915d699b634bd5f9"
     ]
    },
    "id": "RkvKfrgAeH7R"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "The following cell runs a **test-only evaluation** of the **D7 enhanced model trained with train_enh3** on the **D2 test split**. No training is done in this cell. It reads only one input file, `D2/manifests/manifest_all.csv`, keeps rows where `split == \"test\"`, and confirms that the manifest truly belongs to **dataset D2** (the cell stops immediately if it does not). It also checks that every file listed in `clip_path` exists before continuing, so problems are caught early.\n",
    "\n",
    "The cell then prepares the test data in a consistent way. Each clip is assigned to a simple **task group**: `\"vowel\"` when `task == \"vowl\"`, otherwise `\"other\"`. Sex values are standardized for fairness analysis using D2’s exact labels: `\"male\"` is mapped to `\"M\"`, `\"female\"` to `\"F\"`, and anything else to `\"UNK\"`. The resulting sex counts are printed. During audio loading, each clip is verified to be sampled at **16 kHz**. For vowel clips, the attention mask is trimmed so trailing near-silent padding does not affect the model. Other clips use the full attention mask.\n",
    "\n",
    "Next, the cell locates the **most recent D7 train+validation experiment** under `D7/trainval_runs/` whose folder name contains `\"trainEnh3\"` (case-insensitive) and that includes `summary_trainval.json` and `best_heads.pt` for **all three seeds** (1337, 2024, 7777). After selecting this experiment, it re-checks that all expected `best_heads.pt` files exist as a second safety step. From the selected experiment’s `summary_trainval.json`, it reads a **single global decision threshold** from `val_optimal_threshold.mean_sd.mean`. This same threshold is used for **all three seeds**. If the value is missing or invalid, the cell falls back to **0.5** and records this fallback.\n",
    "\n",
    "For each seed, the cell rebuilds the model using a frozen Wav2Vec2 backbone and the same two-head structure used during training. It loads the trained head weights for that seed and runs inference on the full D2 test set. For each seed, it computes **test AUROC** and saves a ROC curve, an overall confusion matrix at the global threshold, and separate confusion matrices for sex `\"M\"` and `\"F\"` when those groups are present. It also writes a per-seed `predictions.csv` containing the clip path, true label, predicted PD probability, normalized sex, speaker ID, task group, seed, and run metadata including the threshold used.\n",
    "\n",
    "After all three seeds are evaluated, the cell computes and saves overall results. It reports **mean test AUROC with a 95% t-based confidence interval (n=3)** and **threshold-based metrics** (accuracy, precision, sensitivity/recall, specificity, F1, MCC, Fisher p-value) as mean ± SD across seeds. It also computes the main fairness metric **H3** at the same global threshold: **ΔFNR = FNR(F) − FNR(M)**, where FNR is calculated only on true Parkinson’s cases (FN divided by FN + TP). It reports FNR for M and F, the signed ΔFNR, and |ΔFNR| as mean ± SD across seeds, with NaN used when a sex group has no PD cases.\n",
    "\n",
    "All outputs are saved under a new run folder in `D7/monolingual_test_runs/`, named using the selected train+val experiment tag plus a timestamp. For traceability, the cell also updates lightweight pointers and history files: it writes `summary_latest.json` and `last_run_pointer.json` (backing up any existing versions), appends a one-line entry to `history_index.jsonl`, and writes a stable per-tag pointer in `monolingual_test_runs/run_<TAG_SAFE>/tag_run_pointer.json`. Finally, for consistency with the preprocessing layout, it writes a small `run_config.json` and summary log files under `D7/config/...` and `D7/logs/...`, and clears the GPU cache at the end."
   ],
   "metadata": {
    "id": "pNQQBqbQuOud"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# =========================\n",
    "# Enhanced D7 Heads on D2 Test (trainEnh3, fixed threshold)\n",
    "# =========================\n",
    "# Purpose: evaluate saved D7 trainEnh3 heads on the D2 test split using one shared threshold for all seeds.\n",
    "# Inputs: D2 manifest (test split) and the most recent matching D7 trainval experiment (heads + summary).\n",
    "# Outputs: per-seed metrics and predictions, plots, and a run summary plus pointer files for quick lookup.\n",
    "# =========================\n",
    "\n",
    "import os, json, math, random, time, warnings\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import soundfile as sf\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from transformers import Wav2Vec2Model\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, confusion_matrix, matthews_corrcoef\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# -------------------------\n",
    "# 0) Import safety check\n",
    "# -------------------------\n",
    "# Stops early if local files would override PyTorch or Transformers imports.\n",
    "if os.path.exists(\"/content/torch.py\") or os.path.exists(\"/content/torch/__init__.py\"):\n",
    "    raise RuntimeError(\"Found local /content/torch.py or /content/torch/ that shadows PyTorch. Rename/remove it and restart runtime.\")\n",
    "if os.path.exists(\"/content/transformers.py\") or os.path.exists(\"/content/transformers/__init__.py\"):\n",
    "    raise RuntimeError(\"Found local /content/transformers.py or /content/transformers/ that shadows Hugging Face Transformers. Rename/remove it and restart runtime.\")\n",
    "\n",
    "# -------------------------\n",
    "# 1) Drive mount (best-effort)\n",
    "# -------------------------\n",
    "# Mounts Google Drive if running in Colab and not already mounted.\n",
    "try:\n",
    "    from google.colab import drive  # type: ignore\n",
    "    if not os.path.isdir(\"/content/drive/MyDrive\"):\n",
    "        drive.mount(\"/content/drive\")\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# -------------------------\n",
    "# 2) Resolve run roots and manifest path\n",
    "# -------------------------\n",
    "# Uses notebook globals if present; otherwise falls back to the defaults below.\n",
    "# DX_OUT_ROOT follows the D7 root because outputs are written next to D7 trainval runs.\n",
    "D7_OUT_ROOT_FALLBACK = \"/content/drive/MyDrive/AI_PD_Project/Datasets/D7-Multilingual (D1_D4_D5v2_D6)/preprocessed_v1\"\n",
    "D2_OUT_ROOT_FALLBACK = \"/content/drive/MyDrive/AI_PD_Project/Datasets/D2-Slovak (EWA-DB)/EWA-DB/preprocessed_v1\"\n",
    "\n",
    "D7_OUT_ROOT = str(globals().get(\"D7_OUT_ROOT\", D7_OUT_ROOT_FALLBACK))\n",
    "D2_OUT_ROOT = str(globals().get(\"D2_OUT_ROOT\", D2_OUT_ROOT_FALLBACK))\n",
    "\n",
    "D2_MANIFEST_ALL = f\"{D2_OUT_ROOT}/manifests/manifest_all.csv\"\n",
    "\n",
    "DX_OUT_ROOT = D7_OUT_ROOT\n",
    "globals()[\"DX_OUT_ROOT\"] = DX_OUT_ROOT\n",
    "globals()[\"D7_OUT_ROOT\"] = D7_OUT_ROOT\n",
    "globals()[\"D2_OUT_ROOT\"] = D2_OUT_ROOT\n",
    "\n",
    "# Run stamp used only for naming backups and this test run folder.\n",
    "RUN_STAMP = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# Creates a timestamped backup before overwriting files intended to be replaced.\n",
    "def _backup_if_exists(p: Path):\n",
    "    if p.exists():\n",
    "        bak = p.with_suffix(p.suffix + f\".bak_{RUN_STAMP}\")\n",
    "        try:\n",
    "            p.rename(bak)\n",
    "            print(f\"BACKUP: {str(p)} -> {str(bak)}\")\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Could not backup existing file before overwrite: {str(p)}. Error: {repr(e)}\")\n",
    "\n",
    "# Converts an arbitrary string into a filesystem-safe tag used in folder names.\n",
    "def _sanitize_tag(s: str) -> str:\n",
    "    s = str(s).strip()\n",
    "    out = []\n",
    "    for ch in s:\n",
    "        if ch.isalnum() or ch in [\"-\", \"_\"]:\n",
    "            out.append(ch)\n",
    "        else:\n",
    "            out.append(\"_\")\n",
    "    out = \"\".join(out).strip(\"_\")\n",
    "    return out if out else \"tag\"\n",
    "\n",
    "# -------------------------\n",
    "# 3) Fixed evaluation settings\n",
    "# -------------------------\n",
    "# Matches the training backbone and common inference settings.\n",
    "SEEDS          = [1337, 2024, 7777]\n",
    "BACKBONE_CKPT  = \"facebook/wav2vec2-base\"\n",
    "SR_EXPECTED    = 16000\n",
    "TINY_THRESH    = 1e-4\n",
    "\n",
    "# Batch sizing values are printed for consistency tracking (inference uses PER_DEVICE_BS).\n",
    "EFFECTIVE_BS   = 64\n",
    "PER_DEVICE_BS  = 16\n",
    "GRAD_ACCUM     = max(1, EFFECTIVE_BS // PER_DEVICE_BS)\n",
    "\n",
    "DROPOUT_P      = 0.2\n",
    "\n",
    "NUM_WORKERS    = 0\n",
    "PIN_MEMORY     = False\n",
    "\n",
    "USE_AMP        = True\n",
    "\n",
    "# Trainval experiment tag used to pick the most recent enhanced run.\n",
    "ENH_TAG = \"trainEnh3\"\n",
    "REQUIRED_EXP_SUBSTRING = ENH_TAG  # case-insensitive\n",
    "ENH_TAG_SAFE = _sanitize_tag(ENH_TAG)  # kept for traceability; not used for folder naming in this cell\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "try:\n",
    "    torch.set_float32_matmul_precision(\"high\")\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# Reduces common non-critical warnings in notebook output.\n",
    "warnings.filterwarnings(\"ignore\", message=\"None of the inputs have requires_grad=True. Gradients will be None\")\n",
    "warnings.filterwarnings(\"ignore\", message=\"Passing `gradient_checkpointing` to a config initialization is deprecated\")\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"huggingface_hub\")\n",
    "\n",
    "# Quick run context printout.\n",
    "print(\"D7_OUT_ROOT:\", D7_OUT_ROOT)\n",
    "print(\"D2_OUT_ROOT:\", D2_OUT_ROOT)\n",
    "print(\"D2_MANIFEST_ALL:\", D2_MANIFEST_ALL)\n",
    "print(\"DEVICE:\", DEVICE)\n",
    "if DEVICE.type == \"cuda\":\n",
    "    print(\"GPU:\", torch.cuda.get_device_name(0))\n",
    "print(\"PER_DEVICE_BS:\", PER_DEVICE_BS, \"| EFFECTIVE_BS:\", PER_DEVICE_BS * GRAD_ACCUM)\n",
    "print(\"NUM_WORKERS:\", NUM_WORKERS, \"| PIN_MEMORY:\", PIN_MEMORY)\n",
    "print(\"USE_AMP:\", bool(USE_AMP and DEVICE.type == \"cuda\"))\n",
    "print(\"Enhanced exp required substring (case-insensitive):\", REQUIRED_EXP_SUBSTRING)\n",
    "\n",
    "# -------------------------\n",
    "# 4) Load D2 manifest and build the test table\n",
    "# -------------------------\n",
    "# Reads the manifest, checks required columns, confirms it is D2, then filters to split == \"test\".\n",
    "if not os.path.exists(D2_MANIFEST_ALL):\n",
    "    raise FileNotFoundError(f\"Missing D2 manifest_all.csv: {D2_MANIFEST_ALL}\")\n",
    "\n",
    "m_all = pd.read_csv(D2_MANIFEST_ALL)\n",
    "\n",
    "# Required columns for evaluation and fairness reporting.\n",
    "req_cols = {\"split\", \"clip_path\", \"label_num\", \"task\", \"sex\", \"age\"}\n",
    "missing = [c for c in sorted(req_cols) if c not in m_all.columns]\n",
    "if missing:\n",
    "    raise ValueError(f\"D2 manifest missing required columns: {missing}. Found: {list(m_all.columns)}\")\n",
    "\n",
    "# Uses the most common dataset id (when present) and filters to it.\n",
    "if \"dataset\" in m_all.columns and m_all[\"dataset\"].notna().any():\n",
    "    d2_dataset_id = str(m_all[\"dataset\"].astype(str).value_counts(dropna=True).idxmax())\n",
    "    m_all = m_all[m_all[\"dataset\"].astype(str) == d2_dataset_id].copy()\n",
    "else:\n",
    "    d2_dataset_id = \"DX\"\n",
    "\n",
    "# --------- GUARD A ----------\n",
    "# Hard stop if the manifest does not identify as D2.\n",
    "if d2_dataset_id != \"D2\":\n",
    "    raise RuntimeError(\n",
    "        f\"Expected D2 dataset_id=='D2' but got {d2_dataset_id!r}. \"\n",
    "        \"This usually means D2_OUT_ROOT is wrong or the manifest is not D2. \"\n",
    "        f\"D2_OUT_ROOT={D2_OUT_ROOT}\"\n",
    "    )\n",
    "\n",
    "# Keeps a stable set of columns used later (fills missing ones with NaN).\n",
    "keep_cols = [\"clip_path\", \"label_num\", \"task\", \"speaker_id\", \"sex\", \"age\", \"duration_sec\", \"split\"]\n",
    "for c in keep_cols:\n",
    "    if c not in m_all.columns:\n",
    "        m_all[c] = np.nan\n",
    "m_all = m_all[keep_cols].copy()\n",
    "\n",
    "# IMPORTANT: test split only.\n",
    "test_df = m_all[m_all[\"split\"].astype(str) == \"test\"].reset_index(drop=True)\n",
    "\n",
    "print(f\"\\nD2 dataset inferred: {d2_dataset_id}\")\n",
    "print(f\"D2 TEST rows: {len(test_df)}\")\n",
    "print(\"D2 TEST label counts:\", test_df[\"label_num\"].value_counts(dropna=False).to_dict())\n",
    "print(\"D2 TEST sex counts (raw):\", test_df[\"sex\"].value_counts(dropna=False).to_dict())\n",
    "\n",
    "if len(test_df) == 0:\n",
    "    raise RuntimeError(\"After filtering to split=='test', D2 manifest has 0 rows.\")\n",
    "\n",
    "# -------------------------\n",
    "# 5) Fail fast: confirm audio files exist\n",
    "# -------------------------\n",
    "# Checks file existence early to avoid long runs that fail late.\n",
    "def _fail_fast_missing_paths(df: pd.DataFrame, name: str):\n",
    "    missing_paths = []\n",
    "    for p in tqdm(df[\"clip_path\"].astype(str).tolist(), desc=f\"Check {name} clip_path exists\", dynamic_ncols=True):\n",
    "        if not os.path.exists(p):\n",
    "            missing_paths.append(p)\n",
    "            if len(missing_paths) >= 10:\n",
    "                break\n",
    "    if missing_paths:\n",
    "        raise FileNotFoundError(f\"{name}: missing clip_path(s). Examples: {missing_paths}\")\n",
    "\n",
    "_fail_fast_missing_paths(test_df, \"D2 TEST\")\n",
    "\n",
    "# -------------------------\n",
    "# 6) Task grouping for the two-head model\n",
    "# -------------------------\n",
    "# Converts raw task into the head selector used during inference.\n",
    "def _task_group(task_val) -> str:\n",
    "    return \"vowel\" if str(task_val) == \"vowl\" else \"other\"\n",
    "\n",
    "test_df[\"task_group\"] = test_df[\"task\"].apply(_task_group)\n",
    "\n",
    "# -------------------------\n",
    "# 6.5) Sex normalization for fairness reporting\n",
    "# -------------------------\n",
    "# D2 uses exact strings \"male\"/\"female\"; anything else becomes \"UNK\".\n",
    "def normalize_sex_d2_case_sensitive(val) -> str:\n",
    "    if pd.isna(val):\n",
    "        return \"UNK\"\n",
    "    if val == \"male\":\n",
    "        return \"M\"\n",
    "    if val == \"female\":\n",
    "        return \"F\"\n",
    "    return \"UNK\"\n",
    "\n",
    "test_df[\"sex_norm\"] = test_df[\"sex\"].apply(normalize_sex_d2_case_sensitive)\n",
    "print(\"D2 TEST sex counts (normalized):\", test_df[\"sex_norm\"].value_counts(dropna=False).to_dict())\n",
    "if (test_df[\"sex_norm\"] == \"UNK\").any():\n",
    "    print(\"NOTE: Some D2 'sex' values were not exactly 'male'/'female' and were mapped to 'UNK'.\")\n",
    "\n",
    "# -------------------------\n",
    "# 7) Dataset and batch padding\n",
    "# -------------------------\n",
    "# Loads audio from clip_path and builds an attention mask for pooling.\n",
    "# For vowel clips, the mask ignores trailing near-silence so padding does not affect pooling.\n",
    "class AudioManifestDataset(Dataset):\n",
    "    def __init__(self, df: pd.DataFrame):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        row = self.df.iloc[idx]\n",
    "        clip_path = str(row[\"clip_path\"])\n",
    "        label = int(row[\"label_num\"])\n",
    "        task_group = str(row[\"task_group\"])\n",
    "        sex_norm = str(row[\"sex_norm\"])\n",
    "        speaker_id = row[\"speaker_id\"] if \"speaker_id\" in row.index else np.nan\n",
    "\n",
    "        if not os.path.exists(clip_path):\n",
    "            raise FileNotFoundError(f\"Missing clip_path: {clip_path}\")\n",
    "\n",
    "        # Reads audio and converts to mono float32.\n",
    "        y, sr = sf.read(clip_path, always_2d=False)\n",
    "        if isinstance(y, np.ndarray) and y.ndim > 1:\n",
    "            y = np.mean(y, axis=1)\n",
    "        y = np.asarray(y, dtype=np.float32)\n",
    "\n",
    "        # Enforces a single sample rate for consistent model input.\n",
    "        if int(sr) != SR_EXPECTED:\n",
    "            raise ValueError(f\"Sample rate mismatch (expected {SR_EXPECTED}): {clip_path} has sr={sr}\")\n",
    "\n",
    "        # Attention mask: 1 = keep, 0 = ignore during pooling.\n",
    "        attn = np.ones((len(y),), dtype=np.int64)\n",
    "        if task_group == \"vowel\":\n",
    "            # Finds last non-tiny sample; masks the tail after it.\n",
    "            k = -1\n",
    "            for j in range(len(y) - 1, -1, -1):\n",
    "                if abs(float(y[j])) > TINY_THRESH:\n",
    "                    k = j\n",
    "                    break\n",
    "            if k >= 0:\n",
    "                attn[:k+1] = 1\n",
    "                attn[k+1:] = 0\n",
    "            else:\n",
    "                attn[:] = 1\n",
    "        else:\n",
    "            attn[:] = 1\n",
    "\n",
    "        return {\n",
    "            \"input_values\": torch.from_numpy(y),\n",
    "            \"attention_mask\": torch.from_numpy(attn),\n",
    "            \"labels\": torch.tensor(label, dtype=torch.long),\n",
    "            \"task_group\": task_group,\n",
    "            \"sex_norm\": sex_norm,\n",
    "            \"clip_path\": clip_path,\n",
    "            \"speaker_id\": speaker_id,\n",
    "        }\n",
    "\n",
    "# Pads variable-length audio in a batch to the longest clip.\n",
    "def collate_fn(batch):\n",
    "    max_len = int(max(b[\"input_values\"].numel() for b in batch))\n",
    "    input_vals, attn_masks, labels = [], [], []\n",
    "    task_groups, sex_norms, clip_paths, speaker_ids = [], [], [], []\n",
    "    for b in batch:\n",
    "        x = b[\"input_values\"]\n",
    "        a = b[\"attention_mask\"]\n",
    "        pad = max_len - x.numel()\n",
    "        if pad > 0:\n",
    "            x = torch.cat([x, torch.zeros(pad, dtype=x.dtype)], dim=0)\n",
    "            a = torch.cat([a, torch.zeros(pad, dtype=a.dtype)], dim=0)\n",
    "        input_vals.append(x)\n",
    "        attn_masks.append(a)\n",
    "        labels.append(b[\"labels\"])\n",
    "        task_groups.append(b[\"task_group\"])\n",
    "        sex_norms.append(b[\"sex_norm\"])\n",
    "        clip_paths.append(b[\"clip_path\"])\n",
    "        speaker_ids.append(b[\"speaker_id\"])\n",
    "    return {\n",
    "        \"input_values\": torch.stack(input_vals, dim=0),\n",
    "        \"attention_mask\": torch.stack(attn_masks, dim=0),\n",
    "        \"labels\": torch.stack(labels, dim=0),\n",
    "        \"task_group\": task_groups,\n",
    "        \"sex_norm\": sex_norms,\n",
    "        \"clip_path\": clip_paths,\n",
    "        \"speaker_id\": speaker_ids,\n",
    "    }\n",
    "\n",
    "# -------------------------\n",
    "# 8) Two-head classifier (frozen backbone)\n",
    "# -------------------------\n",
    "# Uses a frozen Wav2Vec2 backbone and switches heads by task_group.\n",
    "class Wav2Vec2TwoHeadClassifier(nn.Module):\n",
    "    def __init__(self, ckpt: str, dropout_p: float):\n",
    "        super().__init__()\n",
    "        self.backbone = Wav2Vec2Model.from_pretrained(ckpt, use_safetensors=True)\n",
    "        for p in self.backbone.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "        H = int(self.backbone.config.hidden_size)\n",
    "        self.pre_vowel = nn.Sequential(nn.LayerNorm(H), nn.Dropout(float(dropout_p)))\n",
    "        self.pre_other = nn.Sequential(nn.LayerNorm(H), nn.Dropout(float(dropout_p)))\n",
    "        self.head_vowel = nn.Linear(H, 2)\n",
    "        self.head_other = nn.Linear(H, 2)\n",
    "\n",
    "    # Mean-pools frame features using the attention mask.\n",
    "    def masked_mean_pool(self, last_hidden, attn_mask_samples):\n",
    "        feat_mask = self.backbone._get_feature_vector_attention_mask(last_hidden.shape[1], attn_mask_samples)\n",
    "        feat_mask = feat_mask.to(last_hidden.device).unsqueeze(-1).type_as(last_hidden)\n",
    "        masked = last_hidden * feat_mask\n",
    "        denom = feat_mask.sum(dim=1).clamp(min=1.0)\n",
    "        return masked.sum(dim=1) / denom\n",
    "\n",
    "    # Runs heads in fp32 for stable probabilities even when AMP is enabled.\n",
    "    def _heads_fp32(self, x_any: torch.Tensor, head: nn.Module) -> torch.Tensor:\n",
    "        x = x_any.float()\n",
    "        if x.is_cuda:\n",
    "            with torch.amp.autocast(device_type=\"cuda\", enabled=False):\n",
    "                return head(x)\n",
    "        return head(x)\n",
    "\n",
    "    # Returns logits for the PD vs healthy classes, using the correct head per item.\n",
    "    def forward_logits(self, input_values, attention_mask, task_group):\n",
    "        with torch.no_grad():\n",
    "            out = self.backbone(input_values=input_values, attention_mask=attention_mask)\n",
    "            last_hidden = out.last_hidden_state\n",
    "\n",
    "        pooled = self.masked_mean_pool(last_hidden, attention_mask)\n",
    "\n",
    "        z_v = self.pre_vowel(pooled.float())\n",
    "        z_o = self.pre_other(pooled.float())\n",
    "\n",
    "        logits_v = self._heads_fp32(z_v, self.head_vowel)\n",
    "        logits_o = self._heads_fp32(z_o, self.head_other)\n",
    "\n",
    "        is_vowel = torch.tensor([tg == \"vowel\" for tg in task_group], device=pooled.device, dtype=torch.bool)\n",
    "        logits = logits_o.clone()\n",
    "        if is_vowel.any():\n",
    "            logits[is_vowel] = logits_v[is_vowel]\n",
    "        return logits\n",
    "\n",
    "# -------------------------\n",
    "# 9) Metrics and plotting helpers\n",
    "# -------------------------\n",
    "# AUROC is threshold-independent; other metrics use a chosen threshold.\n",
    "def compute_auc(y_true, y_prob):\n",
    "    y_true = np.asarray(y_true, dtype=np.int64)\n",
    "    y_prob = np.asarray(y_prob, dtype=np.float64)\n",
    "    if len(np.unique(y_true)) < 2:\n",
    "        return float(\"nan\")\n",
    "    return float(roc_auc_score(y_true, y_prob))\n",
    "\n",
    "def compute_threshold_metrics(y_true, y_prob, thr=0.5):\n",
    "    y_true = np.asarray(y_true, dtype=np.int64)\n",
    "    y_prob = np.asarray(y_prob, dtype=np.float64)\n",
    "    y_pred = (y_prob >= float(thr)).astype(np.int64)\n",
    "\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=[0, 1])\n",
    "    TN, FP, FN, TP = int(cm[0, 0]), int(cm[0, 1]), int(cm[1, 0]), int(cm[1, 1])\n",
    "\n",
    "    eps = 1e-12\n",
    "    acc = (TP + TN) / max(1, (TP + TN + FP + FN))\n",
    "    prec = TP / (TP + FP + eps)\n",
    "    rec = TP / (TP + FN + eps)     # sensitivity\n",
    "    f1 = 2 * prec * rec / (prec + rec + eps)\n",
    "    spec = TN / (TN + FP + eps)\n",
    "\n",
    "    # MCC can be undefined if predictions collapse to a single class.\n",
    "    try:\n",
    "        mcc = float(matthews_corrcoef(y_true, y_pred)) if len(np.unique(y_pred)) > 1 else float(\"nan\")\n",
    "    except Exception:\n",
    "        mcc = float(\"nan\")\n",
    "\n",
    "    # Fisher exact test on the 2x2 table (best-effort if scipy is available).\n",
    "    pval = float(\"nan\")\n",
    "    try:\n",
    "        from scipy.stats import fisher_exact  # type: ignore\n",
    "        _, pval = fisher_exact([[TN, FP], [FN, TP]], alternative=\"two-sided\")\n",
    "        pval = float(pval)\n",
    "    except Exception:\n",
    "        pval = float(\"nan\")\n",
    "\n",
    "    return {\n",
    "        \"threshold\": float(thr),\n",
    "        \"tn\": TN, \"fp\": FP, \"fn\": FN, \"tp\": TP,\n",
    "        \"accuracy\": float(acc),\n",
    "        \"precision\": float(prec),\n",
    "        \"recall\": float(rec),\n",
    "        \"f1\": float(f1),\n",
    "        \"sensitivity\": float(rec),\n",
    "        \"specificity\": float(spec),\n",
    "        \"mcc\": float(mcc),\n",
    "        \"p_value_fisher_two_sided\": float(pval),\n",
    "    }\n",
    "\n",
    "# Saves a simple ROC curve plot for a single seed run.\n",
    "def save_roc_curve_png(y_true, y_prob, out_png, title_suffix=\"Test\"):\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_prob)\n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr)\n",
    "    plt.plot([0, 1], [0, 1], linestyle=\"--\")\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(f\"ROC Curve ({title_suffix})\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_png, dpi=150)\n",
    "    plt.close()\n",
    "\n",
    "# Saves a labeled 2x2 confusion matrix image.\n",
    "def save_confusion_png(y_true, y_prob, out_png, thr=0.5, title_suffix=\"Test\"):\n",
    "    y_true = np.asarray(y_true, dtype=np.int64)\n",
    "    y_prob = np.asarray(y_prob, dtype=np.float64)\n",
    "    y_pred = (y_prob >= float(thr)).astype(int)\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=[0, 1])\n",
    "\n",
    "    plt.figure()\n",
    "    plt.imshow(cm)\n",
    "    plt.xticks([0, 1], [\"Healthy(0)\", \"PD(1)\"])\n",
    "    plt.yticks([0, 1], [\"Healthy(0)\", \"PD(1)\"])\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"True\")\n",
    "    plt.title(f\"Confusion Matrix ({title_suffix}, thr={thr:.4f})\")\n",
    "    for (i, j), v in np.ndenumerate(cm):\n",
    "        plt.text(j, i, str(v), ha=\"center\", va=\"center\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_png, dpi=150)\n",
    "    plt.close()\n",
    "\n",
    "# -------------------------\n",
    "# 9.5) Fairness (H3): signed ΔFNR = FNR(F) - FNR(M)\n",
    "# -------------------------\n",
    "# Computes FNR per sex group using PD-only cases, then returns signed and absolute gaps.\n",
    "def compute_fnr_by_group_signed(y_true, y_prob, groups, thr=0.5):\n",
    "    y_true = np.asarray(y_true, dtype=np.int64)\n",
    "    y_prob = np.asarray(y_prob, dtype=np.float64)\n",
    "    groups = np.asarray(list(groups), dtype=object)\n",
    "    y_pred = (y_prob >= float(thr)).astype(np.int64)\n",
    "\n",
    "    out = {}\n",
    "    for g in sorted(set(groups.tolist())):\n",
    "        mask_g = (groups == g)\n",
    "        if int(mask_g.sum()) == 0:\n",
    "            continue\n",
    "\n",
    "        pos_mask = mask_g & (y_true == 1)\n",
    "        n_pos = int(pos_mask.sum())\n",
    "        if n_pos == 0:\n",
    "            out[g] = {\"n_total\": int(mask_g.sum()), \"n_pos\": 0, \"tp\": 0, \"fn\": 0, \"fnr\": float(\"nan\")}\n",
    "            continue\n",
    "\n",
    "        tp = int(((y_pred == 1) & pos_mask).sum())\n",
    "        fn = int(((y_pred == 0) & pos_mask).sum())\n",
    "        fnr = float(fn / max(1, (fn + tp)))\n",
    "        out[g] = {\"n_total\": int(mask_g.sum()), \"n_pos\": int(n_pos), \"tp\": int(tp), \"fn\": int(fn), \"fnr\": float(fnr)}\n",
    "\n",
    "    fnr_m = out.get(\"M\", {}).get(\"fnr\", float(\"nan\"))\n",
    "    fnr_f = out.get(\"F\", {}).get(\"fnr\", float(\"nan\"))\n",
    "    if (not np.isnan(fnr_m)) and (not np.isnan(fnr_f)):\n",
    "        delta_signed = float(fnr_f - fnr_m)\n",
    "        delta_abs = float(abs(delta_signed))\n",
    "    else:\n",
    "        delta_signed = float(\"nan\")\n",
    "        delta_abs = float(\"nan\")\n",
    "\n",
    "    return out, delta_signed, delta_abs\n",
    "\n",
    "# Confusion counts are saved both overall and by sex for quick diagnostics.\n",
    "def compute_confusion_counts(y_true, y_prob, thr=0.5):\n",
    "    y_true = np.asarray(y_true, dtype=np.int64)\n",
    "    y_prob = np.asarray(y_prob, dtype=np.float64)\n",
    "    y_pred = (y_prob >= float(thr)).astype(np.int64)\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=[0, 1])\n",
    "    return {\"TN\": int(cm[0, 0]), \"FP\": int(cm[0, 1]), \"FN\": int(cm[1, 0]), \"TP\": int(cm[1, 1])}\n",
    "\n",
    "def compute_confusion_by_group(y_true, y_prob, groups, thr=0.5):\n",
    "    y_true = np.asarray(y_true, dtype=np.int64)\n",
    "    y_prob = np.asarray(y_prob, dtype=np.float64)\n",
    "    groups = np.asarray(list(groups), dtype=object)\n",
    "    out = {}\n",
    "    for g in sorted(set(groups.tolist())):\n",
    "        mask = (groups == g)\n",
    "        if int(mask.sum()) == 0:\n",
    "            continue\n",
    "        out[g] = {\"n\": int(mask.sum()), \"confusion\": compute_confusion_counts(y_true[mask], y_prob[mask], thr=thr)}\n",
    "    return out\n",
    "\n",
    "# -------------------------\n",
    "# 10) Seed control\n",
    "# -------------------------\n",
    "# Fixes random states to make runs repeatable per seed.\n",
    "def set_all_seeds(seed: int):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# -------------------------\n",
    "# 11) Select the most recent matching D7 enhanced trainval experiment\n",
    "# -------------------------\n",
    "# Looks under trainval_runs/exp_* for the newest folder whose name contains trainEnh3 and has all three heads + summary.\n",
    "TRAINVAL_ROOT = Path(D7_OUT_ROOT) / \"trainval_runs\"\n",
    "if not TRAINVAL_ROOT.exists():\n",
    "    raise FileNotFoundError(f\"Missing trainval_runs folder under D7_OUT_ROOT: {str(TRAINVAL_ROOT)}\")\n",
    "\n",
    "exp_dirs = sorted([p for p in TRAINVAL_ROOT.glob(\"exp_*\") if p.is_dir()], key=lambda p: p.stat().st_mtime, reverse=True)\n",
    "if not exp_dirs:\n",
    "    raise FileNotFoundError(f\"No exp_* folders found under: {str(TRAINVAL_ROOT)}\")\n",
    "\n",
    "train_dataset_id = \"D7\"  # expected naming from trainval code (run_D7_seedXXXX)\n",
    "\n",
    "def _is_enhanced_exp_dir(exp_path: Path, required_substring: str) -> bool:\n",
    "    return (required_substring.lower() in exp_path.name.lower())\n",
    "\n",
    "def _has_all_seeds_and_summary(exp_path: Path, dataset_id: str, seeds: list) -> bool:\n",
    "    summary_path = exp_path / \"summary_trainval.json\"\n",
    "    if not summary_path.exists():\n",
    "        return False\n",
    "    for s in seeds:\n",
    "        p = exp_path / f\"run_{dataset_id}_seed{s}\" / \"best_heads.pt\"\n",
    "        if not p.exists():\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "chosen_exp = None\n",
    "for ed in exp_dirs:\n",
    "    if not _is_enhanced_exp_dir(ed, REQUIRED_EXP_SUBSTRING):\n",
    "        continue\n",
    "    if _has_all_seeds_and_summary(ed, train_dataset_id, SEEDS):\n",
    "        chosen_exp = ed\n",
    "        break\n",
    "\n",
    "if chosen_exp is None:\n",
    "    sample = exp_dirs[0]\n",
    "    raise FileNotFoundError(\n",
    "        \"Could not find a recent D7 *ENHANCED* trainval experiment folder that:\\n\"\n",
    "        f\"  (1) contains substring '{REQUIRED_EXP_SUBSTRING}' (case-insensitive) in the exp folder name, and\\n\"\n",
    "        \"  (2) contains all 3 best_heads.pt files + summary_trainval.json.\\n\\n\"\n",
    "        f\"Most recent exp checked (for reference): {str(sample)}\"\n",
    "    )\n",
    "\n",
    "# Uses the full exp folder name as the tag so outputs are easy to trace back.\n",
    "FULL_TRAINVAL_EXP_TAG = chosen_exp.name\n",
    "TAG_SAFE = _sanitize_tag(FULL_TRAINVAL_EXP_TAG)\n",
    "RUN_PARENT_DIRNAME = f\"run_{TAG_SAFE}__{RUN_STAMP}\"\n",
    "\n",
    "# Output locations: one folder per run stamp, plus a stable per-tag folder for pointers.\n",
    "TEST_ROOT = Path(D7_OUT_ROOT) / \"monolingual_test_runs\"\n",
    "RUN_ROOT  = TEST_ROOT / RUN_PARENT_DIRNAME\n",
    "RUN_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "TAG_ROOT = TEST_ROOT / f\"run_{TAG_SAFE}\"\n",
    "TAG_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Builder-aligned config and logs folders (kept stable per tag).\n",
    "cfg_dir  = Path(D7_OUT_ROOT) / \"config\" / f\"D7_{TAG_SAFE}_on_D2_Test\"\n",
    "logs_dir = Path(D7_OUT_ROOT) / \"logs\"   / f\"D7_{TAG_SAFE}_on_D2_Test\"\n",
    "cfg_dir.mkdir(parents=True, exist_ok=True)\n",
    "logs_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "RUN_CONFIG_PATH       = cfg_dir / \"run_config.json\"\n",
    "WARNINGS_CSV_PATH     = logs_dir / \"preprocess_warnings.csv\"\n",
    "DATASET_SUMMARY_PATH  = logs_dir / \"dataset_summary.json\"\n",
    "\n",
    "print(\"\\nUsing D7 ENHANCED Train+Val experiment folder:\")\n",
    "print(\" \", str(chosen_exp))\n",
    "print(\"FULL_TRAINVAL_EXP_TAG:\", FULL_TRAINVAL_EXP_TAG)\n",
    "print(\"ENH_TAG:\", ENH_TAG)\n",
    "print(\"RUN_ROOT:\", str(RUN_ROOT))\n",
    "print(\"cfg_dir:\", str(cfg_dir))\n",
    "print(\"logs_dir:\", str(logs_dir))\n",
    "\n",
    "# --------- GUARD B ----------\n",
    "# Re-checks that all seed head files exist under the chosen experiment.\n",
    "for s in SEEDS:\n",
    "    p = chosen_exp / f\"run_{train_dataset_id}_seed{s}\" / \"best_heads.pt\"\n",
    "    if not p.exists():\n",
    "        raise RuntimeError(f\"Trainval artifact missing after choosing exp. Missing: {str(p)}\")\n",
    "\n",
    "# Loads the trainval summary used to fetch the shared mean threshold.\n",
    "summary_trainval_path = chosen_exp / \"summary_trainval.json\"\n",
    "with open(summary_trainval_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    d7_trainval_summary = json.load(f)\n",
    "\n",
    "# -------------------------\n",
    "# 11.5) Shared threshold from trainval summary\n",
    "# -------------------------\n",
    "# Uses the mean validation-optimal threshold for all test seeds; falls back to 0.5 if missing.\n",
    "val_opt_obj = (d7_trainval_summary or {}).get(\"val_optimal_threshold\", {}) or {}\n",
    "thr_mean_sd = (val_opt_obj.get(\"mean_sd\", {}) or {})\n",
    "\n",
    "def _get_mean_val_opt_threshold() -> float:\n",
    "    try:\n",
    "        return float(thr_mean_sd.get(\"mean\", float(\"nan\")))\n",
    "    except Exception:\n",
    "        return float(\"nan\")\n",
    "\n",
    "THR_MEAN_FROM_TRAINVAL = _get_mean_val_opt_threshold()\n",
    "\n",
    "if np.isnan(THR_MEAN_FROM_TRAINVAL):\n",
    "    THR_USED_GLOBAL = 0.5\n",
    "    THR_GLOBAL_NOTE = (\n",
    "        \"Mean val-opt threshold was missing/NaN in D7 enhanced summary_trainval.json. \"\n",
    "        \"Fallback: THR_USED_GLOBAL=0.5 for ALL seeds.\"\n",
    "    )\n",
    "else:\n",
    "    THR_USED_GLOBAL = float(THR_MEAN_FROM_TRAINVAL)\n",
    "    THR_GLOBAL_NOTE = None\n",
    "\n",
    "print(\"\\nVAL-opt threshold selection for TEST (GLOBAL):\")\n",
    "print(\"  Source: summary_trainval.json -> val_optimal_threshold.mean_sd.mean\")\n",
    "print(f\"  THR_USED_GLOBAL: {THR_USED_GLOBAL:.6f}\")\n",
    "if THR_GLOBAL_NOTE is not None:\n",
    "    print(\"  NOTE:\", THR_GLOBAL_NOTE)\n",
    "\n",
    "# -------------------------\n",
    "# 13) D2 test DataLoader\n",
    "# -------------------------\n",
    "# Builds the DataLoader used for all seeds.\n",
    "test_ds = AudioManifestDataset(test_df)\n",
    "test_loader = DataLoader(\n",
    "    test_ds,\n",
    "    batch_size=PER_DEVICE_BS,\n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=PIN_MEMORY,\n",
    "    collate_fn=collate_fn,\n",
    ")\n",
    "\n",
    "# -------------------------\n",
    "# 14) Warm-up read (sanity check)\n",
    "# -------------------------\n",
    "# Loads a few batches to catch DataLoader issues early.\n",
    "print(\"\\nWarm-up: loading up to 3 D2 TEST batches...\")\n",
    "t0 = time.time()\n",
    "\n",
    "def _warmup(loader, name):\n",
    "    nb = len(loader)\n",
    "    wb = min(3, nb)\n",
    "    if wb == 0:\n",
    "        raise RuntimeError(f\"{name} DataLoader has 0 batches. Check df length and PER_DEVICE_BS.\")\n",
    "    it = iter(loader)\n",
    "    for i in range(wb):\n",
    "        _ = next(it)\n",
    "        print(f\"  loaded warmup {name} batch {i+1}/{wb}\")\n",
    "\n",
    "_warmup(test_loader, \"D2 TEST\")\n",
    "print(f\"Warm-up done in {time.time()-t0:.2f}s\")\n",
    "\n",
    "# -------------------------\n",
    "# 15) Load saved heads into the model\n",
    "# -------------------------\n",
    "# Loads only the trained head and pre-head blocks; backbone stays pretrained and frozen.\n",
    "def load_heads_into_model(model: Wav2Vec2TwoHeadClassifier, best_heads_path: Path):\n",
    "    if not best_heads_path.exists():\n",
    "        raise FileNotFoundError(f\"Missing best_heads.pt: {str(best_heads_path)}\")\n",
    "    state = torch.load(str(best_heads_path), map_location=\"cpu\")\n",
    "    model.pre_vowel.load_state_dict(state[\"pre_vowel\"], strict=True)\n",
    "    model.pre_other.load_state_dict(state[\"pre_other\"], strict=True)\n",
    "    model.head_vowel.load_state_dict(state[\"head_vowel\"], strict=True)\n",
    "    model.head_other.load_state_dict(state[\"head_other\"], strict=True)\n",
    "    return model\n",
    "\n",
    "# -------------------------\n",
    "# 16) Inference helper with metadata\n",
    "# -------------------------\n",
    "# Runs inference and returns probabilities plus fields needed for predictions.csv.\n",
    "def _infer_probs_with_meta(loader, model, desc):\n",
    "    use_amp = bool(USE_AMP and DEVICE.type == \"cuda\")\n",
    "    all_probs, all_true, all_sex = [], [], []\n",
    "    all_clip, all_spk, all_task = [], [], []\n",
    "\n",
    "    pbar = tqdm(loader, desc=desc, dynamic_ncols=True)\n",
    "    with torch.inference_mode():\n",
    "        for batch in pbar:\n",
    "            input_values = batch[\"input_values\"].to(DEVICE, non_blocking=False)\n",
    "            attention_mask = batch[\"attention_mask\"].to(DEVICE, non_blocking=False)\n",
    "            labels = batch[\"labels\"].to(DEVICE, non_blocking=False)\n",
    "            task_group = batch[\"task_group\"]\n",
    "            sex_norm = batch[\"sex_norm\"]\n",
    "            clip_paths = batch[\"clip_path\"]\n",
    "            speaker_ids = batch[\"speaker_id\"]\n",
    "\n",
    "            with torch.amp.autocast(device_type=\"cuda\", enabled=use_amp):\n",
    "                logits = model.forward_logits(input_values, attention_mask, task_group)\n",
    "\n",
    "            # Class-1 probability is treated as PD probability.\n",
    "            probs = torch.softmax(logits.float(), dim=-1)[:, 1].detach().cpu().numpy().astype(np.float64)\n",
    "            all_probs.extend(probs.tolist())\n",
    "            all_true.extend(labels.detach().cpu().numpy().astype(np.int64).tolist())\n",
    "            all_sex.extend(list(sex_norm))\n",
    "            all_task.extend(list(task_group))\n",
    "            all_clip.extend(list(clip_paths))\n",
    "            all_spk.extend([(\"\" if (x is None or (isinstance(x, float) and np.isnan(x))) else str(x)) for x in speaker_ids])\n",
    "\n",
    "    return (\n",
    "        np.asarray(all_true, dtype=np.int64),\n",
    "        np.asarray(all_probs, dtype=np.float64),\n",
    "        np.asarray(all_sex, dtype=object),\n",
    "        np.asarray(all_clip, dtype=object),\n",
    "        np.asarray(all_spk, dtype=object),\n",
    "        np.asarray(all_task, dtype=object),\n",
    "    )\n",
    "\n",
    "# -------------------------\n",
    "# 17) Single-seed evaluation on D2 test\n",
    "# -------------------------\n",
    "# For each seed: load heads, run inference, compute metrics at THR_USED_GLOBAL, save plots and predictions.\n",
    "def run_test_once(seed: int):\n",
    "    set_all_seeds(seed)\n",
    "\n",
    "    run_dir = RUN_ROOT / f\"run_{train_dataset_id}_on_{d2_dataset_id}test_seed{seed}\"\n",
    "    run_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    best_heads_path = chosen_exp / f\"run_{train_dataset_id}_seed{seed}\" / \"best_heads.pt\"\n",
    "\n",
    "    print(f\"\\n[seed={seed}] Loading model + heads from:\")\n",
    "    print(\" \", str(best_heads_path))\n",
    "\n",
    "    model = Wav2Vec2TwoHeadClassifier(BACKBONE_CKPT, dropout_p=DROPOUT_P).to(DEVICE)\n",
    "    model = load_heads_into_model(model, best_heads_path)\n",
    "    model.eval()\n",
    "\n",
    "    thr_used = float(THR_USED_GLOBAL)\n",
    "    thr_note = THR_GLOBAL_NOTE\n",
    "\n",
    "    # Inference (with meta for predictions.csv).\n",
    "    yt_true, yt_prob, yt_sex, yt_clip, yt_spk, yt_task = _infer_probs_with_meta(\n",
    "        test_loader, model, desc=f\"[seed={seed}] Test (D2 TEST)\"\n",
    "    )\n",
    "    test_auc = compute_auc(yt_true, yt_prob)\n",
    "\n",
    "    # Metrics at the shared threshold.\n",
    "    thr_metrics_test = compute_threshold_metrics(yt_true, yt_prob, thr=thr_used)\n",
    "    fnr_by_sex, delta_f_minus_m, delta_abs = compute_fnr_by_group_signed(yt_true, yt_prob, yt_sex, thr=thr_used)\n",
    "    confusion_by_sex = compute_confusion_by_group(yt_true, yt_prob, yt_sex, thr=thr_used)\n",
    "\n",
    "    # Plots (overall).\n",
    "    roc_png = run_dir / \"roc_curve.png\"\n",
    "    cm_png  = run_dir / \"confusion_matrix.png\"\n",
    "    save_roc_curve_png(yt_true, yt_prob, str(roc_png), title_suffix=f\"D2 TEST (seed={seed})\")\n",
    "    save_confusion_png(yt_true, yt_prob, str(cm_png), thr=thr_used, title_suffix=f\"D2 TEST (seed={seed})\")\n",
    "\n",
    "    # Plots split by sex (M and F only).\n",
    "    cm_m_png = None\n",
    "    cm_f_png = None\n",
    "    mask_m = (yt_sex == \"M\")\n",
    "    mask_f = (yt_sex == \"F\")\n",
    "\n",
    "    if int(mask_m.sum()) > 0:\n",
    "        cm_m_png = run_dir / \"confusion_matrix_M.png\"\n",
    "        save_confusion_png(yt_true[mask_m], yt_prob[mask_m], str(cm_m_png), thr=thr_used, title_suffix=f\"D2 TEST SEX=M (seed={seed})\")\n",
    "\n",
    "    if int(mask_f.sum()) > 0:\n",
    "        cm_f_png = run_dir / \"confusion_matrix_F.png\"\n",
    "        save_confusion_png(yt_true[mask_f], yt_prob[mask_f], str(cm_f_png), thr=thr_used, title_suffix=f\"D2 TEST SEX=F (seed={seed})\")\n",
    "\n",
    "    # predictions.csv includes clip id fields plus model score and group fields.\n",
    "    pred_df = pd.DataFrame({\n",
    "        \"clip_path\": yt_clip.astype(str),\n",
    "        \"y_true\": yt_true.astype(int),\n",
    "        \"y_score\": yt_prob.astype(float),\n",
    "        \"sex_norm\": yt_sex.astype(str),\n",
    "        \"speaker_id\": yt_spk.astype(str),\n",
    "        \"task_group\": yt_task.astype(str),\n",
    "        \"seed\": int(seed),\n",
    "        \"trainval_exp_tag\": str(FULL_TRAINVAL_EXP_TAG),\n",
    "        \"run_stamp\": str(RUN_STAMP),\n",
    "        \"threshold_used_global\": float(thr_used),\n",
    "    })\n",
    "    pred_csv_path = run_dir / \"predictions.csv\"\n",
    "    pred_df.to_csv(pred_csv_path, index=False)\n",
    "\n",
    "    # metrics.json captures run settings, aggregate stats, fairness, and artifact paths for this seed.\n",
    "    metrics = {\n",
    "        \"enh_tag\": str(ENH_TAG),\n",
    "\n",
    "        \"train_dataset\": train_dataset_id,\n",
    "        \"test_dataset\": d2_dataset_id,\n",
    "        \"seed\": int(seed),\n",
    "\n",
    "        \"n_test\": int(len(test_df)),\n",
    "        \"label_counts_test\": test_df[\"label_num\"].value_counts(dropna=False).to_dict(),\n",
    "        \"sex_counts_test_norm\": test_df[\"sex_norm\"].value_counts(dropna=False).to_dict(),\n",
    "\n",
    "        \"test_auroc\": float(test_auc),\n",
    "\n",
    "        \"threshold_source\": \"D7 enhanced trainval summary_trainval.json -> val_optimal_threshold.mean_sd.mean\",\n",
    "        \"trainval_experiment_used\": str(chosen_exp),\n",
    "        \"trainval_exp_tag\": str(FULL_TRAINVAL_EXP_TAG),\n",
    "        \"trainval_summary_path\": str(summary_trainval_path),\n",
    "\n",
    "        \"test_threshold_used_global\": float(thr_used),\n",
    "        \"test_threshold_note_global\": thr_note,\n",
    "\n",
    "        \"threshold_metrics_test_at_thr_used\": thr_metrics_test,\n",
    "\n",
    "        \"fairness_test_at_thr_used\": {\n",
    "            \"definition\": \"H3: ΔFNR = FNR(F) - FNR(M), where FNR(sex)=FN/(FN+TP) computed on PD-only true labels at test_threshold_used_global.\",\n",
    "            \"threshold_used\": float(thr_used),\n",
    "            \"fnr_by_sex_norm\": fnr_by_sex,\n",
    "            \"delta_fnr_F_minus_M\": float(delta_f_minus_m),\n",
    "            \"delta_fnr_abs\": float(delta_abs),\n",
    "            \"note\": \"If n_PD for a sex is 0, its FNR is NaN and ΔFNR is NaN.\",\n",
    "            \"sex_normalization_note\": \"D2 mapping: exact 'male'->M and 'female'->F (case-sensitive); otherwise UNK.\",\n",
    "        },\n",
    "\n",
    "        \"confusion_by_sex_norm_at_thr_used\": confusion_by_sex,\n",
    "\n",
    "        \"artifacts\": {\n",
    "            \"predictions_csv\": str(pred_csv_path),\n",
    "            \"roc_curve_png\": str(roc_png),\n",
    "            \"confusion_matrix_png\": str(cm_png),\n",
    "            \"confusion_matrix_M_png\": str(cm_m_png) if cm_m_png is not None else None,\n",
    "            \"confusion_matrix_F_png\": str(cm_f_png) if cm_f_png is not None else None,\n",
    "        },\n",
    "\n",
    "        \"d7_out_root\": D7_OUT_ROOT,\n",
    "        \"d2_out_root\": D2_OUT_ROOT,\n",
    "        \"d2_manifest_all\": D2_MANIFEST_ALL,\n",
    "\n",
    "        \"best_heads_path\": str(best_heads_path),\n",
    "        \"backbone_ckpt\": BACKBONE_CKPT,\n",
    "        \"dropout_p\": float(DROPOUT_P),\n",
    "        \"timestamp\": time.strftime(\"%Y%m%d_%H%M%S\"),\n",
    "    }\n",
    "\n",
    "    metrics_path = run_dir / \"metrics.json\"\n",
    "    with open(metrics_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(metrics, f, indent=2)\n",
    "\n",
    "    print(f\"[seed={seed}] DONE | test_AUROC={test_auc:.6f}\")\n",
    "    print(f\"[seed={seed}] Threshold used (GLOBAL mean from D7 enhanced trainval): {thr_used:.6f}\")\n",
    "    print(f\"[seed={seed}] WROTE:\")\n",
    "    print(\" \", str(metrics_path))\n",
    "    print(\" \", str(pred_csv_path))\n",
    "    print(\" \", str(roc_png))\n",
    "    print(\" \", str(cm_png))\n",
    "\n",
    "    return {\n",
    "        \"seed\": int(seed),\n",
    "        \"thr_used\": float(thr_used),\n",
    "        \"thr_note\": thr_note,\n",
    "        \"test_auc\": float(test_auc),\n",
    "        \"thr_metrics_test\": thr_metrics_test,\n",
    "        \"fnr_by_sex\": fnr_by_sex,\n",
    "        \"delta_signed\": float(delta_f_minus_m),\n",
    "        \"delta_abs\": float(delta_abs),\n",
    "        \"run_dir\": str(run_dir),\n",
    "        \"predictions_csv\": str(pred_csv_path),\n",
    "    }\n",
    "\n",
    "# -------------------------\n",
    "# 18) Run all seeds and aggregate results\n",
    "# -------------------------\n",
    "# Reports AUROC mean with 95% CI (n=3) and other metrics as mean ± SD across seeds.\n",
    "results = []\n",
    "for seed in SEEDS:\n",
    "    results.append(run_test_once(seed))\n",
    "\n",
    "aurocs = [r[\"test_auc\"] for r in results]\n",
    "t_crit = 4.302652729911275  # df=2, 95% CI\n",
    "n = len(aurocs)\n",
    "mean_auc = float(np.mean(aurocs))\n",
    "std_auc = float(np.std(aurocs, ddof=1)) if n > 1 else 0.0\n",
    "half_width = float(t_crit * (std_auc / math.sqrt(n))) if n > 1 else 0.0\n",
    "ci95 = [float(mean_auc - half_width), float(mean_auc + half_width)]\n",
    "\n",
    "# Aggregates mean and SD for a list of values (NaN-safe).\n",
    "def _mean_sd(vals):\n",
    "    vals = np.asarray(vals, dtype=np.float64)\n",
    "    mu = float(np.nanmean(vals)) if np.any(~np.isnan(vals)) else float(\"nan\")\n",
    "    sd = float(np.nanstd(vals, ddof=1)) if np.sum(~np.isnan(vals)) > 1 else 0.0\n",
    "    return mu, sd\n",
    "\n",
    "thr_list = [r[\"thr_metrics_test\"] for r in results]\n",
    "keys = [\"accuracy\",\"precision\",\"recall\",\"f1\",\"sensitivity\",\"specificity\",\"mcc\",\"p_value_fisher_two_sided\"]\n",
    "agg = {}\n",
    "for k in keys:\n",
    "    v = [float(tm.get(k, float(\"nan\"))) for tm in thr_list]\n",
    "    mu, sd = _mean_sd(v)\n",
    "    agg[k] = {\n",
    "        \"mean\": float(mu),\n",
    "        \"sd\": float(sd),\n",
    "        \"values_by_seed\": {str(s): float(tm.get(k, float(\"nan\"))) for s, tm in zip(SEEDS, thr_list)},\n",
    "    }\n",
    "\n",
    "# Stores confusion counts per seed for quick cross-checks.\n",
    "cm_by_seed = {\n",
    "    str(s): {\"tn\": int(thr_list[i][\"tn\"]), \"fp\": int(thr_list[i][\"fp\"]), \"fn\": int(thr_list[i][\"fn\"]), \"tp\": int(thr_list[i][\"tp\"])}\n",
    "    for i, s in enumerate(SEEDS)\n",
    "}\n",
    "\n",
    "# Fairness aggregation across seeds.\n",
    "fnr_by_seed = {str(r[\"seed\"]): r[\"fnr_by_sex\"] for r in results}\n",
    "delta_signed_by_seed = {str(r[\"seed\"]): float(r[\"delta_signed\"]) for r in results}\n",
    "delta_abs_by_seed = {str(r[\"seed\"]): float(r[\"delta_abs\"]) for r in results}\n",
    "\n",
    "fnr_m_vals, fnr_f_vals = [], []\n",
    "d_signed_vals, d_abs_vals = [], []\n",
    "for r in results:\n",
    "    d = r[\"fnr_by_sex\"] or {}\n",
    "    fnr_m_vals.append(float(d.get(\"M\", {}).get(\"fnr\", float(\"nan\"))))\n",
    "    fnr_f_vals.append(float(d.get(\"F\", {}).get(\"fnr\", float(\"nan\"))))\n",
    "    d_signed_vals.append(float(r[\"delta_signed\"]))\n",
    "    d_abs_vals.append(float(r[\"delta_abs\"]))\n",
    "\n",
    "fnr_m_mean, fnr_m_sd = _mean_sd(fnr_m_vals)\n",
    "fnr_f_mean, fnr_f_sd = _mean_sd(fnr_f_vals)\n",
    "d_signed_mean, d_signed_sd = _mean_sd(d_signed_vals)\n",
    "d_abs_mean, d_abs_sd = _mean_sd(d_abs_vals)\n",
    "\n",
    "# Console summary for quick review.\n",
    "print(\"\\nTest AUROC by seed:\")\n",
    "for r in results:\n",
    "    print(f\"  seed {r['seed']}: {r['test_auc']:.6f}\")\n",
    "print(f\"\\nMean Test AUROC: {mean_auc:.6f}\")\n",
    "print(f\"95% CI (t, n=3): [{ci95[0]:.6f}, {ci95[1]:.6f}]\")\n",
    "\n",
    "print(\"\\nTEST threshold used (GLOBAL mean val-opt from D7 enhanced trainval):\")\n",
    "print(f\"  THR_USED_GLOBAL: {THR_USED_GLOBAL:.6f}\")\n",
    "if THR_GLOBAL_NOTE is not None:\n",
    "    print(\"  NOTE:\", THR_GLOBAL_NOTE)\n",
    "\n",
    "print(\"\\nThreshold metrics on D2 TEST @ THR_USED_GLOBAL (mean ± SD across seeds):\")\n",
    "for k in [\"accuracy\",\"precision\",\"sensitivity\",\"specificity\",\"f1\",\"mcc\"]:\n",
    "    print(f\"  {k}: {agg[k]['mean']:.6f} ± {agg[k]['sd']:.6f}\")\n",
    "print(\"  fisher_p_value_two_sided:\", f\"{agg['p_value_fisher_two_sided']['mean']:.6g} ± {agg['p_value_fisher_two_sided']['sd']:.6g}\")\n",
    "\n",
    "print(\"\\nFAIRNESS (H3) on D2 TEST @ THR_USED_GLOBAL across seeds (mean ± SD):\")\n",
    "print(f\"  FNR_M: {fnr_m_mean:.6f} ± {fnr_m_sd:.6f}\")\n",
    "print(f\"  FNR_F: {fnr_f_mean:.6f} ± {fnr_f_sd:.6f}\")\n",
    "print(f\"  ΔFNR (F - M): {d_signed_mean:.6f} ± {d_signed_sd:.6f}\")\n",
    "print(f\"  |ΔFNR|: {d_abs_mean:.6f} ± {d_abs_sd:.6f}\")\n",
    "\n",
    "# -------------------------\n",
    "# 19) Write summary and pointer files\n",
    "# -------------------------\n",
    "# Saves a latest summary (overwritten with backup), a last-run pointer, an append-only history line, and a tag pointer.\n",
    "summary_latest = {\n",
    "    \"enh_tag\": str(ENH_TAG),\n",
    "    \"train_dataset\": train_dataset_id,\n",
    "    \"test_dataset\": d2_dataset_id,\n",
    "\n",
    "    \"chosen_trainval_exp\": str(chosen_exp),\n",
    "    \"trainval_exp_tag\": str(FULL_TRAINVAL_EXP_TAG),\n",
    "    \"run_stamp\": str(RUN_STAMP),\n",
    "\n",
    "    \"threshold_used_global\": float(THR_USED_GLOBAL),\n",
    "    \"threshold_note_global\": THR_GLOBAL_NOTE,\n",
    "\n",
    "    \"test_auroc\": {\n",
    "        \"by_seed\": {str(r[\"seed\"]): float(r[\"test_auc\"]) for r in results},\n",
    "        \"mean\": float(mean_auc),\n",
    "        \"std\": float(std_auc),\n",
    "        \"ci95_t_n3\": [float(ci95[0]), float(ci95[1])],\n",
    "    },\n",
    "    \"threshold_metrics_test_at_thr_used\": agg,\n",
    "    \"confusion_counts_by_seed_at_thr_used\": cm_by_seed,\n",
    "\n",
    "    \"fairness_test_at_thr_used\": {\n",
    "        \"definition\": \"H3: ΔFNR = FNR(F) - FNR(M), where FNR(sex)=FN/(FN+TP) computed on PD-only true labels at threshold_used_global.\",\n",
    "        \"fnr_by_seed\": fnr_by_seed,\n",
    "        \"delta_fnr_F_minus_M_by_seed\": delta_signed_by_seed,\n",
    "        \"delta_fnr_abs_by_seed\": delta_abs_by_seed,\n",
    "        \"fnr_M_mean_sd\": {\"mean\": float(fnr_m_mean), \"sd\": float(fnr_m_sd)},\n",
    "        \"fnr_F_mean_sd\": {\"mean\": float(fnr_f_mean), \"sd\": float(fnr_f_sd)},\n",
    "        \"delta_signed_mean_sd\": {\"mean\": float(d_signed_mean), \"sd\": float(d_signed_sd)},\n",
    "        \"delta_abs_mean_sd\": {\"mean\": float(d_abs_mean), \"sd\": float(d_abs_sd)},\n",
    "    },\n",
    "\n",
    "    \"run_root\": str(RUN_ROOT),\n",
    "    \"runs\": {str(r[\"seed\"]): {\"run_dir\": r[\"run_dir\"], \"predictions_csv\": r[\"predictions_csv\"]} for r in results},\n",
    "}\n",
    "\n",
    "summary_latest_path = TEST_ROOT / \"summary_latest.json\"\n",
    "_backup_if_exists(summary_latest_path)\n",
    "with open(summary_latest_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(summary_latest, f, indent=2)\n",
    "\n",
    "# Kept for compatibility with prior notebooks (path is the same as summary_latest_path).\n",
    "summary_symlink_path = TEST_ROOT / \"summary_latest.json\"  # intentional overwrite already handled via backup\n",
    "# keep as-is\n",
    "\n",
    "# Stores the most recent run folder pointer (overwritten with backup).\n",
    "last_run_pointer_path = TEST_ROOT / \"last_run_pointer.json\"\n",
    "_backup_if_exists(last_run_pointer_path)\n",
    "last_run_pointer_obj = {\n",
    "    \"run_root\": str(RUN_ROOT),\n",
    "    \"run_parent_dirname\": str(RUN_PARENT_DIRNAME),\n",
    "    \"trainval_exp_tag\": str(FULL_TRAINVAL_EXP_TAG),\n",
    "    \"trainval_exp_path\": str(chosen_exp),\n",
    "    \"enh_tag\": str(ENH_TAG),\n",
    "    \"run_stamp\": str(RUN_STAMP),\n",
    "    \"timestamp\": time.strftime(\"%Y%m%d_%H%M%S\"),\n",
    "}\n",
    "with open(last_run_pointer_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(last_run_pointer_obj, f, indent=2)\n",
    "\n",
    "# Appends a one-line history record so older runs stay discoverable.\n",
    "history_index_path = TEST_ROOT / \"history_index.jsonl\"\n",
    "history_record = {\n",
    "    \"timestamp\": time.strftime(\"%Y%m%d_%H%M%S\"),\n",
    "    \"run_stamp\": str(RUN_STAMP),\n",
    "    \"run_root\": str(RUN_ROOT),\n",
    "    \"trainval_exp_tag\": str(FULL_TRAINVAL_EXP_TAG),\n",
    "    \"trainval_exp_path\": str(chosen_exp),\n",
    "    \"enh_tag\": str(ENH_TAG),\n",
    "    \"summary_latest_path\": str(summary_latest_path),\n",
    "}\n",
    "with open(history_index_path, \"a\", encoding=\"utf-8\") as f:\n",
    "    f.write(json.dumps(history_record) + \"\\n\")\n",
    "\n",
    "# Stores a tag-scoped pointer (stable location per experiment tag).\n",
    "tag_pointer_path = TAG_ROOT / \"tag_run_pointer.json\"\n",
    "tag_pointer_obj = dict(last_run_pointer_obj)\n",
    "tag_pointer_obj[\"tag_root\"] = str(TAG_ROOT)\n",
    "_backup_if_exists(tag_pointer_path)\n",
    "with open(tag_pointer_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(tag_pointer_obj, f, indent=2)\n",
    "\n",
    "print(\"\\nWROTE (index/pointers):\")\n",
    "print(\" \", str(summary_latest_path))\n",
    "print(\" \", str(last_run_pointer_path))\n",
    "print(\" \", str(history_index_path))\n",
    "print(\" \", str(tag_pointer_path))\n",
    "\n",
    "# -------------------------\n",
    "# 20) Builder-aligned config and logs\n",
    "# -------------------------\n",
    "# Writes a compact run_config plus minimal log placeholders for consistent folder structure.\n",
    "run_config_obj = {\n",
    "    \"mode\": f\"D7_{TAG_SAFE}_on_D2_Test\",\n",
    "    \"enh_tag\": str(ENH_TAG),\n",
    "    \"trainval_exp_tag\": str(FULL_TRAINVAL_EXP_TAG),\n",
    "    \"trainval_exp_path\": str(chosen_exp),\n",
    "    \"d7_out_root\": str(D7_OUT_ROOT),\n",
    "    \"d2_out_root\": str(D2_OUT_ROOT),\n",
    "    \"d2_manifest_all\": str(D2_MANIFEST_ALL),\n",
    "    \"seeds\": [int(s) for s in SEEDS],\n",
    "    \"per_device_bs\": int(PER_DEVICE_BS),\n",
    "    \"effective_bs\": int(PER_DEVICE_BS * GRAD_ACCUM),\n",
    "    \"use_amp\": bool(USE_AMP and DEVICE.type == \"cuda\"),\n",
    "    \"threshold_used_global\": float(THR_USED_GLOBAL),\n",
    "    \"run_stamp\": str(RUN_STAMP),\n",
    "    \"timestamp\": time.strftime(\"%Y%m%d_%H%M%S\"),\n",
    "}\n",
    "_backup_if_exists(RUN_CONFIG_PATH)\n",
    "with open(RUN_CONFIG_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(run_config_obj, f, indent=2)\n",
    "\n",
    "# Creates an empty warnings CSV if it does not exist (structure consistency).\n",
    "if not WARNINGS_CSV_PATH.exists():\n",
    "    WARNINGS_CSV_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
    "    pd.DataFrame(columns=[\"warning_type\",\"detail\"]).to_csv(WARNINGS_CSV_PATH, index=False)\n",
    "\n",
    "# Saves a small dataset summary for this evaluation run.\n",
    "dataset_summary_obj = {\n",
    "    \"mode\": f\"D7_{TAG_SAFE}_on_D2_Test\",\n",
    "    \"dataset_id\": str(d2_dataset_id),\n",
    "    \"split\": \"test\",\n",
    "    \"n_rows\": int(len(test_df)),\n",
    "    \"label_counts\": test_df[\"label_num\"].value_counts(dropna=False).to_dict(),\n",
    "    \"sex_counts_raw\": test_df[\"sex\"].value_counts(dropna=False).to_dict(),\n",
    "    \"sex_counts_norm\": test_df[\"sex_norm\"].value_counts(dropna=False).to_dict(),\n",
    "    \"timestamp\": time.strftime(\"%Y%m%d_%H%M%S\"),\n",
    "}\n",
    "_backup_if_exists(DATASET_SUMMARY_PATH)\n",
    "with open(DATASET_SUMMARY_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(dataset_summary_obj, f, indent=2)\n",
    "\n",
    "print(\"\\nWROTE (builder-aligned config/logs):\")\n",
    "print(\" \", str(RUN_CONFIG_PATH))\n",
    "print(\" \", str(WARNINGS_CSV_PATH))\n",
    "print(\" \", str(DATASET_SUMMARY_PATH))\n",
    "\n",
    "# -------------------------\n",
    "# 21) Free GPU memory (optional)\n",
    "# -------------------------\n",
    "# Clears cached CUDA allocations to reduce peak memory after the run.\n",
    "try:\n",
    "    torch.cuda.empty_cache()\n",
    "except Exception:\n",
    "    pass"
   ],
   "metadata": {
    "id": "83FwTiNai8Wa"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Ablation 1: Threshold Sweeps"
   ],
   "metadata": {
    "id": "Xkyt_Prp1dEO"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The following cell runs a **post-training decision threshold sweep** for the **D7 trainEnh1 model evaluated on the D2 test split**, without retraining or changing the model. It automatically finds the most recent D7 training and validation experiment whose folder name contains **“trainEnh1”** and checks that all required files are present for each of the three seeds, including the training summary and saved head weights. Using these fixed model heads, it runs inference once per seed on the D2 test data to get stable predicted probabilities, then evaluates how performance and fairness change as the decision threshold is varied from 0.01 to 0.99.\n",
    "\n",
    "The cell starts with basic setup and safety checks. It avoids issues from locally named files that could override core libraries, mounts Google Drive if needed, resolves dataset output paths, and prints key settings such as device type, batch size, and sweep resolution. It then loads the D2 `manifest_all.csv`, confirms it belongs to **dataset D2**, filters to the test split only, and prints basic label and sex counts. All referenced audio files are checked, and the run stops immediately if any are missing.\n",
    "\n",
    "For data preparation, each test clip is assigned to a simple task group: **vowel** for sustained vowel recordings and **other** for all remaining speech tasks. Sex metadata is standardized using D2’s original values, mapping `male` and `female` to **M** and **F**, with anything else treated as unknown. Audio is loaded at **16 kHz**, and attention masks are created so padded regions are ignored. For vowel clips, the mask also removes trailing near-silence so silence is not treated as useful signal.\n",
    "\n",
    "Inference uses the same model structure as training: a frozen speech feature extractor with two task-specific classification heads, one for vowel clips and one for non-vowel clips. For each seed, the saved head weights are loaded and the full D2 test set is processed once. The predicted Parkinson’s probabilities, along with true labels and sex information, are stored. **AUROC is computed for each seed and averaged across seeds** as a threshold-free reference before sweeping.\n",
    "\n",
    "With predictions fixed, the cell performs the threshold sweep. At each threshold, standard classification metrics are computed and then averaged across the three seeds, including sensitivity, specificity, accuracy, precision, F1 score, Matthews correlation coefficient, and Fisher’s exact test p-value. Fairness is evaluated by computing the false negative rate for Parkinson’s cases separately for males and females, then summarizing the sex difference as both a signed value (female minus male) and an absolute gap. The sweep table keeps both averaged results and per-seed values so differences across seeds remain visible.\n",
    "\n",
    "A single recommended threshold is chosen using a clear rule called **Policy B+**. This threshold minimizes the mean absolute sex gap in false negative rate while also meeting two requirements at the same time: mean sensitivity of at least **0.60** and mean specificity of at least **0.50**. If no threshold meets both conditions, the cell reports which requirement failed and selects the closest available alternative based on how far it misses the targets.\n",
    "\n",
    "All outputs are saved to a timestamped folder under the D7 threshold sweep directory. This includes the full sweep table, a summary explaining the selected experiment and threshold choice, and plots showing how sensitivity, specificity, and fairness change with the threshold, along with a trade-off curve that clearly marks the chosen operating point."
   ],
   "metadata": {
    "id": "tqu4kw4zty1H"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# =========================\n",
    "# Threshold Sweep With Fairness Guardrail (D7 trainEnh1 → D2 test)\n",
    "# =========================\n",
    "# Purpose: run inference once per seed using saved D7 trainEnh1 heads, then sweep decision thresholds on the same scores.\n",
    "# Inputs: saved head weights for three seeds, plus the D2 manifest (test split) pointing to clip files and labels.\n",
    "# Outputs: a sweep table (CSV), a summary (JSON), and a few simple plots saved in a new timestamped sweep folder.\n",
    "# =========================\n",
    "\n",
    "import os, json, math, random, time, warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import soundfile as sf\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from transformers import Wav2Vec2Model\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix, matthews_corrcoef\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# -------------------------\n",
    "# 0) Import safety check\n",
    "# -------------------------\n",
    "# Stops early if local files would override PyTorch or Transformers.\n",
    "if os.path.exists(\"/content/torch.py\") or os.path.exists(\"/content/torch/__init__.py\"):\n",
    "    raise RuntimeError(\"Found local /content/torch.py or /content/torch/ that shadows PyTorch. Rename/remove it and restart runtime.\")\n",
    "if os.path.exists(\"/content/transformers.py\") or os.path.exists(\"/content/transformers/__init__.py\"):\n",
    "    raise RuntimeError(\"Found local /content/transformers.py or /content/transformers/ that shadows Hugging Face Transformers. Rename/remove it and restart runtime.\")\n",
    "\n",
    "# -------------------------\n",
    "# 1) Drive mount (best-effort)\n",
    "# -------------------------\n",
    "# Mounts Google Drive if running in Colab and not already mounted.\n",
    "try:\n",
    "    from google.colab import drive  # type: ignore\n",
    "    if not os.path.isdir(\"/content/drive/MyDrive\"):\n",
    "        drive.mount(\"/content/drive\")\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# -------------------------\n",
    "# 2) Root paths\n",
    "# -------------------------\n",
    "# Uses existing notebook globals if present; otherwise uses the fallbacks below.\n",
    "D7_OUT_ROOT_FALLBACK = \"/content/drive/MyDrive/AI_PD_Project/Datasets/D7-Multilingual (D1_D4_D5v2_D6)/preprocessed_v1\"\n",
    "D2_OUT_ROOT_FALLBACK = \"/content/drive/MyDrive/AI_PD_Project/Datasets/D2-Slovak (EWA-DB)/EWA-DB/preprocessed_v1\"\n",
    "\n",
    "D7_OUT_ROOT = str(globals().get(\"D7_OUT_ROOT\", D7_OUT_ROOT_FALLBACK))\n",
    "D2_OUT_ROOT = str(globals().get(\"D2_OUT_ROOT\", D2_OUT_ROOT_FALLBACK))\n",
    "D2_MANIFEST_ALL = f\"{D2_OUT_ROOT}/manifests/manifest_all.csv\"\n",
    "\n",
    "# Exposes roots for other cells that might reuse them.\n",
    "globals()[\"D7_OUT_ROOT\"] = D7_OUT_ROOT\n",
    "globals()[\"D2_OUT_ROOT\"] = D2_OUT_ROOT\n",
    "\n",
    "# -------------------------\n",
    "# 3) Run configuration\n",
    "# -------------------------\n",
    "# Core settings: seeds, backbone name, expected audio sample rate, and inference batching.\n",
    "SEEDS          = [1337, 2024, 7777]\n",
    "BACKBONE_CKPT  = \"facebook/wav2vec2-base\"\n",
    "SR_EXPECTED    = 16000\n",
    "TINY_THRESH    = 1e-4\n",
    "\n",
    "PER_DEVICE_BS  = 16\n",
    "NUM_WORKERS    = 0\n",
    "PIN_MEMORY     = False\n",
    "USE_AMP        = True\n",
    "DROPOUT_P      = 0.2\n",
    "\n",
    "# Experiment selector: matches the exp_* folder name tag for trainEnh1.\n",
    "REQUIRED_EXP_SUBSTRING = \"trainEnh1\"  # case-insensitive match against exp_* folder name\n",
    "\n",
    "# Policy B+: reduce sex FNR gap while keeping sensitivity and specificity above minimums.\n",
    "TARGET_SENS = 0.60\n",
    "MIN_SPEC    = 0.50   # avoids trivial thresholds that label nearly everything as PD\n",
    "POLICY_TEXT = (\n",
    "    \"Policy B+: minimize mean(|ΔFNR|) subject to \"\n",
    "    f\"mean(sensitivity) >= {TARGET_SENS:.2f} AND mean(specificity) >= {MIN_SPEC:.2f}\"\n",
    ")\n",
    "\n",
    "# Threshold grid for sweeping.\n",
    "THR_MIN, THR_MAX, THR_STEPS = 0.01, 0.99, 199\n",
    "THR_GRID = np.linspace(THR_MIN, THR_MAX, THR_STEPS).astype(np.float64)\n",
    "\n",
    "# Device selection and minor numeric tuning.\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "try:\n",
    "    torch.set_float32_matmul_precision(\"high\")\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# Keeps output readable by filtering common non-critical warnings.\n",
    "warnings.filterwarnings(\"ignore\", message=\"None of the inputs have requires_grad=True. Gradients will be None\")\n",
    "warnings.filterwarnings(\"ignore\", message=\"Passing `gradient_checkpointing` to a config initialization is deprecated\")\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"huggingface_hub\")\n",
    "\n",
    "# Prints key settings for quick verification.\n",
    "print(\"D7_OUT_ROOT:\", D7_OUT_ROOT)\n",
    "print(\"D2_OUT_ROOT:\", D2_OUT_ROOT)\n",
    "print(\"D2_MANIFEST_ALL:\", D2_MANIFEST_ALL)\n",
    "print(\"DEVICE:\", DEVICE)\n",
    "if DEVICE.type == \"cuda\":\n",
    "    print(\"GPU:\", torch.cuda.get_device_name(0))\n",
    "print(\"PER_DEVICE_BS:\", PER_DEVICE_BS)\n",
    "print(\"NUM_WORKERS:\", NUM_WORKERS, \"| PIN_MEMORY:\", PIN_MEMORY)\n",
    "print(\"USE_AMP:\", bool(USE_AMP and DEVICE.type == \"cuda\"))\n",
    "print(\"Required exp substring (case-insensitive):\", REQUIRED_EXP_SUBSTRING)\n",
    "print(\"Policy B+ constraints:\")\n",
    "print(\"  mean sensitivity >=\", TARGET_SENS)\n",
    "print(\"  mean specificity >=\", MIN_SPEC)\n",
    "print(f\"Threshold sweep grid: {THR_MIN:.2f}..{THR_MAX:.2f} with {THR_STEPS} steps\")\n",
    "\n",
    "# -------------------------\n",
    "# 4) Load D2 manifest and keep only test split\n",
    "# -------------------------\n",
    "# Reads the D2 manifest, checks required columns, confirms it is actually D2, then filters to split == \"test\".\n",
    "if not os.path.exists(D2_MANIFEST_ALL):\n",
    "    raise FileNotFoundError(f\"Missing D2 manifest_all.csv: {D2_MANIFEST_ALL}\")\n",
    "\n",
    "m_all = pd.read_csv(D2_MANIFEST_ALL)\n",
    "\n",
    "req_cols = {\"split\", \"clip_path\", \"label_num\", \"task\", \"sex\", \"age\"}\n",
    "missing = [c for c in sorted(req_cols) if c not in m_all.columns]\n",
    "if missing:\n",
    "    raise ValueError(f\"D2 manifest missing required columns: {missing}. Found: {list(m_all.columns)}\")\n",
    "\n",
    "# Uses the most common dataset label (when present) and filters to that dataset.\n",
    "if \"dataset\" in m_all.columns and m_all[\"dataset\"].notna().any():\n",
    "    d2_dataset_id = str(m_all[\"dataset\"].astype(str).value_counts(dropna=True).idxmax())\n",
    "    m_all = m_all[m_all[\"dataset\"].astype(str) == d2_dataset_id].copy()\n",
    "else:\n",
    "    d2_dataset_id = \"DX\"\n",
    "\n",
    "# Hard stop if the manifest is not D2.\n",
    "if d2_dataset_id != \"D2\":\n",
    "    raise RuntimeError(\n",
    "        f\"Expected D2 dataset_id=='D2' but got {d2_dataset_id!r}. \"\n",
    "        \"This usually means D2_OUT_ROOT is wrong or the manifest is not D2. \"\n",
    "        f\"D2_OUT_ROOT={D2_OUT_ROOT}\"\n",
    "    )\n",
    "\n",
    "# Keeps a stable set of columns used downstream (fills missing ones with NaN).\n",
    "keep_cols = [\"clip_path\", \"label_num\", \"task\", \"speaker_id\", \"sex\", \"age\", \"duration_sec\", \"split\"]\n",
    "for c in keep_cols:\n",
    "    if c not in m_all.columns:\n",
    "        m_all[c] = np.nan\n",
    "m_all = m_all[keep_cols].copy()\n",
    "\n",
    "# Test split only.\n",
    "test_df = m_all[m_all[\"split\"].astype(str) == \"test\"].reset_index(drop=True)\n",
    "\n",
    "print(f\"\\nD2 dataset inferred: {d2_dataset_id}\")\n",
    "print(f\"D2 TEST rows: {len(test_df)}\")\n",
    "print(\"D2 TEST label counts:\", test_df[\"label_num\"].value_counts(dropna=False).to_dict())\n",
    "print(\"D2 TEST sex counts (raw):\", test_df[\"sex\"].value_counts(dropna=False).to_dict())\n",
    "\n",
    "if len(test_df) == 0:\n",
    "    raise RuntimeError(\"After filtering to split=='test', D2 manifest has 0 rows.\")\n",
    "\n",
    "# -------------------------\n",
    "# 5) Fail fast: confirm audio files exist\n",
    "# -------------------------\n",
    "# Checks for missing clip paths early to avoid long runs that fail late.\n",
    "def _fail_fast_missing_paths(df: pd.DataFrame, name: str):\n",
    "    missing_paths = []\n",
    "    for p in tqdm(df[\"clip_path\"].astype(str).tolist(), desc=f\"Check {name} clip_path exists\", dynamic_ncols=True):\n",
    "        if not os.path.exists(p):\n",
    "            missing_paths.append(p)\n",
    "            if len(missing_paths) >= 10:\n",
    "                break\n",
    "    if missing_paths:\n",
    "        raise FileNotFoundError(f\"{name}: missing clip_path(s). Examples: {missing_paths}\")\n",
    "\n",
    "_fail_fast_missing_paths(test_df, \"D2 TEST\")\n",
    "\n",
    "# -------------------------\n",
    "# 6) Task grouping (vowel vs other)\n",
    "# -------------------------\n",
    "# Maps each clip to the head it should use during inference.\n",
    "def _task_group(task_val) -> str:\n",
    "    return \"vowel\" if str(task_val) == \"vowl\" else \"other\"\n",
    "\n",
    "test_df[\"task_group\"] = test_df[\"task\"].apply(_task_group)\n",
    "\n",
    "# -------------------------\n",
    "# 6.5) Sex normalization\n",
    "# -------------------------\n",
    "# Normalizes D2 \"male\"/\"female\" into \"M\"/\"F\" and maps anything else to \"UNK\".\n",
    "def normalize_sex_d2_case_sensitive(val) -> str:\n",
    "    if pd.isna(val):\n",
    "        return \"UNK\"\n",
    "    if val == \"male\":\n",
    "        return \"M\"\n",
    "    if val == \"female\":\n",
    "        return \"F\"\n",
    "    return \"UNK\"\n",
    "\n",
    "test_df[\"sex_norm\"] = test_df[\"sex\"].apply(normalize_sex_d2_case_sensitive)\n",
    "print(\"D2 TEST sex counts (normalized):\", test_df[\"sex_norm\"].value_counts(dropna=False).to_dict())\n",
    "if (test_df[\"sex_norm\"] == \"UNK\").any():\n",
    "    print(\"NOTE: Some D2 'sex' values were not exactly 'male'/'female' and were mapped to 'UNK'.\")\n",
    "\n",
    "# -------------------------\n",
    "# 7) Dataset and collator\n",
    "# -------------------------\n",
    "# Builds model inputs from audio files and creates an attention mask for pooling.\n",
    "# For vowel clips, the mask removes trailing padding-like silence based on a tiny amplitude threshold.\n",
    "class AudioManifestDataset(Dataset):\n",
    "    def __init__(self, df: pd.DataFrame):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        row = self.df.iloc[idx]\n",
    "        clip_path = str(row[\"clip_path\"])\n",
    "        label = int(row[\"label_num\"])\n",
    "        task_group = str(row[\"task_group\"])\n",
    "        sex_norm = str(row[\"sex_norm\"])\n",
    "\n",
    "        if not os.path.exists(clip_path):\n",
    "            raise FileNotFoundError(f\"Missing clip_path: {clip_path}\")\n",
    "\n",
    "        # Reads audio and converts to mono float32.\n",
    "        y, sr = sf.read(clip_path, always_2d=False)\n",
    "        if isinstance(y, np.ndarray) and y.ndim > 1:\n",
    "            y = np.mean(y, axis=1)\n",
    "        y = np.asarray(y, dtype=np.float32)\n",
    "\n",
    "        # Enforces a single sample rate so the backbone sees consistent inputs.\n",
    "        if int(sr) != SR_EXPECTED:\n",
    "            raise ValueError(f\"Sample rate mismatch (expected {SR_EXPECTED}): {clip_path} has sr={sr}\")\n",
    "\n",
    "        # Attention mask: 1 = keep, 0 = ignore during pooling.\n",
    "        attn = np.ones((len(y),), dtype=np.int64)\n",
    "        if task_group == \"vowel\":\n",
    "            # Finds the last non-tiny sample and ignores anything after it.\n",
    "            k = -1\n",
    "            for j in range(len(y) - 1, -1, -1):\n",
    "                if abs(float(y[j])) > TINY_THRESH:\n",
    "                    k = j\n",
    "                    break\n",
    "            if k >= 0:\n",
    "                attn[:k+1] = 1\n",
    "                attn[k+1:] = 0\n",
    "            else:\n",
    "                attn[:] = 1\n",
    "        else:\n",
    "            attn[:] = 1\n",
    "\n",
    "        return {\n",
    "            \"input_values\": torch.from_numpy(y),\n",
    "            \"attention_mask\": torch.from_numpy(attn),\n",
    "            \"labels\": torch.tensor(label, dtype=torch.long),\n",
    "            \"task_group\": task_group,\n",
    "            \"sex_norm\": sex_norm,\n",
    "        }\n",
    "\n",
    "# Pads variable-length audio in a batch to the longest clip length.\n",
    "def collate_fn(batch):\n",
    "    max_len = int(max(b[\"input_values\"].numel() for b in batch))\n",
    "    input_vals, attn_masks, labels, task_groups, sex_norms = [], [], [], [], []\n",
    "    for b in batch:\n",
    "        x = b[\"input_values\"]\n",
    "        a = b[\"attention_mask\"]\n",
    "        pad = max_len - x.numel()\n",
    "        if pad > 0:\n",
    "            x = torch.cat([x, torch.zeros(pad, dtype=x.dtype)], dim=0)\n",
    "            a = torch.cat([a, torch.zeros(pad, dtype=a.dtype)], dim=0)\n",
    "        input_vals.append(x)\n",
    "        attn_masks.append(a)\n",
    "        labels.append(b[\"labels\"])\n",
    "        task_groups.append(b[\"task_group\"])\n",
    "        sex_norms.append(b[\"sex_norm\"])\n",
    "    return {\n",
    "        \"input_values\": torch.stack(input_vals, dim=0),\n",
    "        \"attention_mask\": torch.stack(attn_masks, dim=0),\n",
    "        \"labels\": torch.stack(labels, dim=0),\n",
    "        \"task_group\": task_groups,\n",
    "        \"sex_norm\": sex_norms,\n",
    "    }\n",
    "\n",
    "# DataLoader over the D2 test split.\n",
    "test_ds = AudioManifestDataset(test_df)\n",
    "test_loader = DataLoader(\n",
    "    test_ds,\n",
    "    batch_size=PER_DEVICE_BS,\n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=PIN_MEMORY,\n",
    "    collate_fn=collate_fn,\n",
    ")\n",
    "\n",
    "# -------------------------\n",
    "# 8) Model (frozen backbone + two heads)\n",
    "# -------------------------\n",
    "# Loads a frozen Wav2Vec2 backbone and switches between vowel and other heads per clip.\n",
    "class Wav2Vec2TwoHeadClassifier(nn.Module):\n",
    "    def __init__(self, ckpt: str, dropout_p: float):\n",
    "        super().__init__()\n",
    "        self.backbone = Wav2Vec2Model.from_pretrained(ckpt, use_safetensors=True)\n",
    "        for p in self.backbone.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "        H = int(self.backbone.config.hidden_size)\n",
    "        self.pre_vowel = nn.Sequential(nn.LayerNorm(H), nn.Dropout(float(dropout_p)))\n",
    "        self.pre_other = nn.Sequential(nn.LayerNorm(H), nn.Dropout(float(dropout_p)))\n",
    "        self.head_vowel = nn.Linear(H, 2)\n",
    "        self.head_other = nn.Linear(H, 2)\n",
    "\n",
    "    # Mean-pools frame features using the attention mask (ignores masked regions).\n",
    "    def masked_mean_pool(self, last_hidden, attn_mask_samples):\n",
    "        feat_mask = self.backbone._get_feature_vector_attention_mask(last_hidden.shape[1], attn_mask_samples)\n",
    "        feat_mask = feat_mask.to(last_hidden.device).unsqueeze(-1).type_as(last_hidden)\n",
    "        masked = last_hidden * feat_mask\n",
    "        denom = feat_mask.sum(dim=1).clamp(min=1.0)\n",
    "        return masked.sum(dim=1) / denom\n",
    "\n",
    "    # Runs the head in fp32 for stability even when AMP is enabled.\n",
    "    def _heads_fp32(self, x_any: torch.Tensor, head: nn.Module) -> torch.Tensor:\n",
    "        x = x_any.float()\n",
    "        if x.is_cuda:\n",
    "            with torch.amp.autocast(device_type=\"cuda\", enabled=False):\n",
    "                return head(x)\n",
    "        return head(x)\n",
    "\n",
    "    # Produces logits using the head selected by task_group.\n",
    "    def forward_logits(self, input_values, attention_mask, task_group):\n",
    "        with torch.no_grad():\n",
    "            out = self.backbone(input_values=input_values, attention_mask=attention_mask)\n",
    "            last_hidden = out.last_hidden_state\n",
    "\n",
    "        pooled = self.masked_mean_pool(last_hidden, attention_mask)\n",
    "        z_v = self.pre_vowel(pooled.float())\n",
    "        z_o = self.pre_other(pooled.float())\n",
    "\n",
    "        logits_v = self._heads_fp32(z_v, self.head_vowel)\n",
    "        logits_o = self._heads_fp32(z_o, self.head_other)\n",
    "\n",
    "        is_vowel = torch.tensor([tg == \"vowel\" for tg in task_group], device=pooled.device, dtype=torch.bool)\n",
    "        logits = logits_o.clone()\n",
    "        if is_vowel.any():\n",
    "            logits[is_vowel] = logits_v[is_vowel]\n",
    "        return logits\n",
    "\n",
    "# Loads only the head weights saved during training (backbone stays the pretrained frozen model).\n",
    "def load_heads_into_model(model: Wav2Vec2TwoHeadClassifier, best_heads_path: Path):\n",
    "    if not best_heads_path.exists():\n",
    "        raise FileNotFoundError(f\"Missing best_heads.pt: {str(best_heads_path)}\")\n",
    "    state = torch.load(str(best_heads_path), map_location=\"cpu\")\n",
    "    model.pre_vowel.load_state_dict(state[\"pre_vowel\"], strict=True)\n",
    "    model.pre_other.load_state_dict(state[\"pre_other\"], strict=True)\n",
    "    model.head_vowel.load_state_dict(state[\"head_vowel\"], strict=True)\n",
    "    model.head_other.load_state_dict(state[\"head_other\"], strict=True)\n",
    "    return model\n",
    "\n",
    "# -------------------------\n",
    "# 9) Seed control and single-pass inference\n",
    "# -------------------------\n",
    "# Runs inference once per seed and returns fixed probabilities, labels, and sex tags for threshold sweeping.\n",
    "def set_all_seeds(seed: int):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "def infer_once(seed: int, chosen_exp: Path, train_dataset_id: str):\n",
    "    set_all_seeds(seed)\n",
    "    best_heads_path = chosen_exp / f\"run_{train_dataset_id}_seed{seed}\" / \"best_heads.pt\"\n",
    "\n",
    "    model = Wav2Vec2TwoHeadClassifier(BACKBONE_CKPT, dropout_p=DROPOUT_P).to(DEVICE)\n",
    "    model = load_heads_into_model(model, best_heads_path)\n",
    "    model.eval()\n",
    "\n",
    "    use_amp = bool(USE_AMP and DEVICE.type == \"cuda\")\n",
    "    all_probs, all_true, all_sex = [], [], []\n",
    "\n",
    "    # Forward pass over D2 test (no thresholding here).\n",
    "    pbar = tqdm(test_loader, desc=f\"[seed={seed}] Inference D2 TEST\", dynamic_ncols=True)\n",
    "    with torch.inference_mode():\n",
    "        for batch in pbar:\n",
    "            input_values = batch[\"input_values\"].to(DEVICE, non_blocking=False)\n",
    "            attention_mask = batch[\"attention_mask\"].to(DEVICE, non_blocking=False)\n",
    "            labels = batch[\"labels\"].to(DEVICE, non_blocking=False)\n",
    "            task_group = batch[\"task_group\"]\n",
    "            sex_norm = batch[\"sex_norm\"]\n",
    "\n",
    "            with torch.amp.autocast(device_type=\"cuda\", enabled=use_amp):\n",
    "                logits = model.forward_logits(input_values, attention_mask, task_group)\n",
    "\n",
    "            # Probability for PD is the softmax class-1 probability.\n",
    "            probs = torch.softmax(logits.float(), dim=-1)[:, 1].detach().cpu().numpy().astype(np.float64)\n",
    "            all_probs.extend(probs.tolist())\n",
    "            all_true.extend(labels.detach().cpu().numpy().astype(np.int64).tolist())\n",
    "            all_sex.extend(list(sex_norm))\n",
    "\n",
    "    y_true = np.asarray(all_true, dtype=np.int64)\n",
    "    y_prob = np.asarray(all_probs, dtype=np.float64)\n",
    "    y_sex  = np.asarray(all_sex, dtype=object)\n",
    "\n",
    "    # AUROC is reported per seed as a threshold-independent reference.\n",
    "    auc = float(\"nan\")\n",
    "    if len(np.unique(y_true)) >= 2:\n",
    "        auc = float(roc_auc_score(y_true, y_prob))\n",
    "\n",
    "    return {\n",
    "        \"seed\": int(seed),\n",
    "        \"best_heads_path\": str(best_heads_path),\n",
    "        \"y_true\": y_true,\n",
    "        \"y_prob\": y_prob,\n",
    "        \"y_sex\": y_sex,\n",
    "        \"auroc\": float(auc),\n",
    "    }\n",
    "\n",
    "# -------------------------\n",
    "# 10) Metric helpers for threshold sweep\n",
    "# -------------------------\n",
    "# Computes standard metrics from a thresholded prediction and sex-based FNR gap on PD-only cases.\n",
    "def confusion_counts(y_true, y_prob, thr):\n",
    "    y_pred = (y_prob >= float(thr)).astype(np.int64)\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=[0, 1])\n",
    "    TN, FP, FN, TP = int(cm[0, 0]), int(cm[0, 1]), int(cm[1, 0]), int(cm[1, 1])\n",
    "    return TN, FP, FN, TP\n",
    "\n",
    "def threshold_metrics(y_true, y_prob, thr):\n",
    "    TN, FP, FN, TP = confusion_counts(y_true, y_prob, thr)\n",
    "    eps = 1e-12\n",
    "    acc  = (TP + TN) / max(1, (TP + TN + FP + FN))\n",
    "    prec = TP / (TP + FP + eps)\n",
    "    sens = TP / (TP + FN + eps)  # recall/sensitivity\n",
    "    spec = TN / (TN + FP + eps)\n",
    "    f1   = 2 * prec * sens / (prec + sens + eps)\n",
    "\n",
    "    # MCC can be undefined if predictions collapse to a single class.\n",
    "    try:\n",
    "        y_pred = (y_prob >= float(thr)).astype(np.int64)\n",
    "        mcc = float(matthews_corrcoef(y_true, y_pred)) if len(np.unique(y_pred)) > 1 else float(\"nan\")\n",
    "    except Exception:\n",
    "        mcc = float(\"nan\")\n",
    "\n",
    "    # Fisher exact test on the 2x2 table (best-effort if scipy is available).\n",
    "    pval = float(\"nan\")\n",
    "    try:\n",
    "        from scipy.stats import fisher_exact  # type: ignore\n",
    "        _, pval = fisher_exact([[TN, FP], [FN, TP]], alternative=\"two-sided\")\n",
    "        pval = float(pval)\n",
    "    except Exception:\n",
    "        pval = float(\"nan\")\n",
    "\n",
    "    return {\n",
    "        \"tn\": TN, \"fp\": FP, \"fn\": FN, \"tp\": TP,\n",
    "        \"accuracy\": float(acc),\n",
    "        \"precision\": float(prec),\n",
    "        \"sensitivity\": float(sens),\n",
    "        \"specificity\": float(spec),\n",
    "        \"f1\": float(f1),\n",
    "        \"mcc\": float(mcc),\n",
    "        \"fisher_p_two_sided\": float(pval),\n",
    "    }\n",
    "\n",
    "def fnr_by_sex_signed_delta(y_true, y_prob, y_sex, thr):\n",
    "    # FNR is computed within each sex group on true PD cases only: FN / (FN + TP).\n",
    "    y_pred = (y_prob >= float(thr)).astype(np.int64)\n",
    "    out = {}\n",
    "\n",
    "    for g in [\"M\", \"F\"]:\n",
    "        mask_g = (y_sex == g)\n",
    "        pos_mask = mask_g & (y_true == 1)\n",
    "        n_pos = int(pos_mask.sum())\n",
    "        if n_pos == 0:\n",
    "            out[g] = {\"n_pos\": 0, \"fn\": 0, \"tp\": 0, \"fnr\": float(\"nan\")}\n",
    "            continue\n",
    "        tp = int(((y_pred == 1) & pos_mask).sum())\n",
    "        fn = int(((y_pred == 0) & pos_mask).sum())\n",
    "        fnr = float(fn / max(1, (fn + tp)))\n",
    "        out[g] = {\"n_pos\": int(n_pos), \"fn\": int(fn), \"tp\": int(tp), \"fnr\": float(fnr)}\n",
    "\n",
    "    fnr_m = out[\"M\"][\"fnr\"]\n",
    "    fnr_f = out[\"F\"][\"fnr\"]\n",
    "    if (not np.isnan(fnr_m)) and (not np.isnan(fnr_f)):\n",
    "        delta = float(fnr_f - fnr_m)     # ΔFNR = F - M (signed)\n",
    "        absd  = float(abs(delta))\n",
    "    else:\n",
    "        delta = float(\"nan\")\n",
    "        absd  = float(\"nan\")\n",
    "\n",
    "    return out, delta, absd\n",
    "\n",
    "# Small helpers to aggregate values across seeds while tolerating NaNs.\n",
    "def mean_sd(vals):\n",
    "    vals = np.asarray(vals, dtype=np.float64)\n",
    "    mu = float(np.nanmean(vals)) if np.any(~np.isnan(vals)) else float(\"nan\")\n",
    "    sd = float(np.nanstd(vals, ddof=1)) if np.sum(~np.isnan(vals)) > 1 else 0.0\n",
    "    return mu, sd\n",
    "\n",
    "def safe_nanmean(x):\n",
    "    x = np.asarray(x, dtype=np.float64)\n",
    "    if np.any(~np.isnan(x)):\n",
    "        return float(np.nanmean(x))\n",
    "    return float(\"nan\")\n",
    "\n",
    "def safe_nansd(x):\n",
    "    x = np.asarray(x, dtype=np.float64)\n",
    "    if np.sum(~np.isnan(x)) > 1:\n",
    "        return float(np.nanstd(x, ddof=1))\n",
    "    return 0.0\n",
    "\n",
    "# -------------------------\n",
    "# 11) Find the most recent matching trainval experiment\n",
    "# -------------------------\n",
    "# Selects the newest exp_* folder that matches REQUIRED_EXP_SUBSTRING and has all seed heads plus summary_trainval.json.\n",
    "TRAINVAL_ROOT = Path(D7_OUT_ROOT) / \"trainval_runs\"\n",
    "if not TRAINVAL_ROOT.exists():\n",
    "    raise FileNotFoundError(f\"Missing trainval_runs folder under D7_OUT_ROOT: {str(TRAINVAL_ROOT)}\")\n",
    "\n",
    "exp_dirs = sorted([p for p in TRAINVAL_ROOT.glob(\"exp_*\") if p.is_dir()], key=lambda p: p.stat().st_mtime, reverse=True)\n",
    "if not exp_dirs:\n",
    "    raise FileNotFoundError(f\"No exp_* folders found under: {str(TRAINVAL_ROOT)}\")\n",
    "\n",
    "train_dataset_id = \"D7\"\n",
    "\n",
    "def is_match(exp_path: Path, required_substring: str) -> bool:\n",
    "    return required_substring.lower() in exp_path.name.lower()\n",
    "\n",
    "def has_all_seeds_and_summary(exp_path: Path, dataset_id: str, seeds: list) -> bool:\n",
    "    if not (exp_path / \"summary_trainval.json\").exists():\n",
    "        return False\n",
    "    for s in seeds:\n",
    "        p = exp_path / f\"run_{dataset_id}_seed{s}\" / \"best_heads.pt\"\n",
    "        if not p.exists():\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "chosen_exp = None\n",
    "for ed in exp_dirs:\n",
    "    if not is_match(ed, REQUIRED_EXP_SUBSTRING):\n",
    "        continue\n",
    "    if has_all_seeds_and_summary(ed, train_dataset_id, SEEDS):\n",
    "        chosen_exp = ed\n",
    "        break\n",
    "\n",
    "if chosen_exp is None:\n",
    "    examples = [p.name for p in exp_dirs[:10]]\n",
    "    raise FileNotFoundError(\n",
    "        \"Could not find a recent D7 trainval experiment folder that:\\n\"\n",
    "        f\"  (1) contains substring '{REQUIRED_EXP_SUBSTRING}' (case-insensitive) in the exp folder name, and\\n\"\n",
    "        \"  (2) contains all 3 best_heads.pt files + summary_trainval.json.\\n\"\n",
    "        f\"Checked under: {str(TRAINVAL_ROOT)}\\n\\n\"\n",
    "        f\"Example exp_* folder names (most recent first):\\n  - \" + \"\\n  - \".join(examples) + \"\\n\\n\"\n",
    "        \"Fix: set REQUIRED_EXP_SUBSTRING to match the exact tag used in the exp folder name (underscores vs camelCase matter).\"\n",
    "    )\n",
    "\n",
    "print(\"\\nUsing trainval experiment folder:\")\n",
    "print(\" \", str(chosen_exp))\n",
    "\n",
    "# Loads the trainval summary only to print the mean Youden-J threshold as a reference.\n",
    "summary_trainval_path = chosen_exp / \"summary_trainval.json\"\n",
    "with open(summary_trainval_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    trainval_summary = json.load(f)\n",
    "\n",
    "val_opt = ((trainval_summary or {}).get(\"val_optimal_threshold\", {}) or {})\n",
    "thr_mean = float((((val_opt.get(\"mean_sd\", {}) or {}).get(\"mean\", float(\"nan\")))))\n",
    "print(\"\\nTrainval mean val-opt threshold (Youden J) for reference:\")\n",
    "print(\"  summary_trainval.json -> val_optimal_threshold.mean_sd.mean =\", f\"{thr_mean:.6f}\" if not np.isnan(thr_mean) else \"nan\")\n",
    "\n",
    "# -------------------------\n",
    "# 12) Create output folder for this sweep run\n",
    "# -------------------------\n",
    "# Uses a timestamp so each sweep run writes to a fresh folder.\n",
    "TS_ROOT = Path(D7_OUT_ROOT) / \"threshold_sweeps\"\n",
    "TS_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "timestamp = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "OUT_DIR = TS_ROOT / f\"run_D7_{REQUIRED_EXP_SUBSTRING}_on_D2test_{timestamp}\"\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# -------------------------\n",
    "# 13) Inference once per seed\n",
    "# -------------------------\n",
    "# Produces fixed prediction probabilities used by all threshold evaluations.\n",
    "print(\"\\nRunning inference ONCE per seed (then sweep thresholds on fixed predictions)...\")\n",
    "seed_payloads = []\n",
    "for s in SEEDS:\n",
    "    seed_payloads.append(infer_once(s, chosen_exp, train_dataset_id))\n",
    "\n",
    "print(\"\\nAUROC by seed (ranking metric, threshold-independent):\")\n",
    "for sp in seed_payloads:\n",
    "    print(f\"  seed {sp['seed']}: AUROC={sp['auroc']:.6f}\")\n",
    "auc_mean, auc_sd = mean_sd([sp[\"auroc\"] for sp in seed_payloads])\n",
    "print(f\"Mean AUROC: {auc_mean:.6f} ± {auc_sd:.6f}\")\n",
    "\n",
    "# -------------------------\n",
    "# 14) Threshold sweep across seeds\n",
    "# -------------------------\n",
    "# For each threshold: compute metrics per seed, then take the mean across seeds.\n",
    "rows = []\n",
    "for thr in tqdm(THR_GRID, desc=\"Threshold sweep\", dynamic_ncols=True):\n",
    "    sens_list, absd_list, signd_list = [], [], []\n",
    "    acc_list, prec_list, spec_list, f1_list, mcc_list, p_list = [], [], [], [], [], []\n",
    "\n",
    "    fnr_m_list, fnr_f_list = [], []\n",
    "    n_pd_m_list, n_pd_f_list = [], []\n",
    "\n",
    "    for sp in seed_payloads:\n",
    "        y_true = sp[\"y_true\"]\n",
    "        y_prob = sp[\"y_prob\"]\n",
    "        y_sex  = sp[\"y_sex\"]\n",
    "\n",
    "        tm = threshold_metrics(y_true, y_prob, thr)\n",
    "        fnr_by_sex, delta_signed, delta_abs = fnr_by_sex_signed_delta(y_true, y_prob, y_sex, thr)\n",
    "\n",
    "        sens_list.append(tm[\"sensitivity\"])\n",
    "        acc_list.append(tm[\"accuracy\"])\n",
    "        prec_list.append(tm[\"precision\"])\n",
    "        spec_list.append(tm[\"specificity\"])\n",
    "        f1_list.append(tm[\"f1\"])\n",
    "        mcc_list.append(tm[\"mcc\"])\n",
    "        p_list.append(tm[\"fisher_p_two_sided\"])\n",
    "\n",
    "        signd_list.append(delta_signed)\n",
    "        absd_list.append(delta_abs)\n",
    "\n",
    "        fnr_m_list.append(float(fnr_by_sex[\"M\"][\"fnr\"]))\n",
    "        fnr_f_list.append(float(fnr_by_sex[\"F\"][\"fnr\"]))\n",
    "        n_pd_m_list.append(float(fnr_by_sex[\"M\"][\"n_pos\"]))\n",
    "        n_pd_f_list.append(float(fnr_by_sex[\"F\"][\"n_pos\"]))\n",
    "\n",
    "    row = {\n",
    "        \"threshold\": float(thr),\n",
    "\n",
    "        \"mean_sensitivity\": safe_nanmean(sens_list),\n",
    "        \"sd_sensitivity\": safe_nansd(sens_list),\n",
    "\n",
    "        \"mean_specificity\": safe_nanmean(spec_list),\n",
    "        \"sd_specificity\": safe_nansd(spec_list),\n",
    "\n",
    "        \"mean_abs_deltaFNR\": safe_nanmean(absd_list),\n",
    "        \"sd_abs_deltaFNR\": safe_nansd(absd_list),\n",
    "\n",
    "        \"mean_signed_deltaFNR_F_minus_M\": safe_nanmean(signd_list),\n",
    "        \"sd_signed_deltaFNR\": safe_nansd(signd_list),\n",
    "\n",
    "        \"mean_accuracy\": safe_nanmean(acc_list),\n",
    "        \"mean_precision\": safe_nanmean(prec_list),\n",
    "        \"mean_f1\": safe_nanmean(f1_list),\n",
    "        \"mean_mcc\": safe_nanmean(mcc_list),\n",
    "        \"mean_fisher_p_two_sided\": safe_nanmean(p_list),\n",
    "\n",
    "        \"mean_FNR_M\": safe_nanmean(fnr_m_list),\n",
    "        \"mean_FNR_F\": safe_nanmean(fnr_f_list),\n",
    "        \"n_PD_M_each_seed\": json.dumps({str(SEEDS[i]): int(n_pd_m_list[i]) for i in range(len(SEEDS))}),\n",
    "        \"n_PD_F_each_seed\": json.dumps({str(SEEDS[i]): int(n_pd_f_list[i]) for i in range(len(SEEDS))}),\n",
    "    }\n",
    "\n",
    "    # Stores per-seed values to explain the means later.\n",
    "    row[\"sensitivity_by_seed\"] = json.dumps({str(sp[\"seed\"]): float(sens_list[i]) for i, sp in enumerate(seed_payloads)})\n",
    "    row[\"specificity_by_seed\"] = json.dumps({str(sp[\"seed\"]): float(spec_list[i]) for i, sp in enumerate(seed_payloads)})\n",
    "    row[\"abs_deltaFNR_by_seed\"] = json.dumps({str(sp[\"seed\"]): float(absd_list[i]) for i, sp in enumerate(seed_payloads)})\n",
    "    row[\"signed_deltaFNR_by_seed\"] = json.dumps({str(sp[\"seed\"]): float(signd_list[i]) for i, sp in enumerate(seed_payloads)})\n",
    "\n",
    "    rows.append(row)\n",
    "\n",
    "sweep_df = pd.DataFrame(rows)\n",
    "\n",
    "# -------------------------\n",
    "# 15) Choose threshold using Policy B+\n",
    "# -------------------------\n",
    "# 1) If both constraints are reachable: choose the eligible threshold with the smallest mean |ΔFNR|.\n",
    "# 2) Otherwise: choose the threshold with the smallest total shortfall from the constraints, and report the gaps.\n",
    "max_mean_sens = float(sweep_df[\"mean_sensitivity\"].max())\n",
    "thr_at_max_sens = float(sweep_df.loc[sweep_df[\"mean_sensitivity\"].idxmax(), \"threshold\"])\n",
    "\n",
    "max_mean_spec = float(sweep_df[\"mean_specificity\"].max())\n",
    "thr_at_max_spec = float(sweep_df.loc[sweep_df[\"mean_specificity\"].idxmax(), \"threshold\"])\n",
    "\n",
    "eligible = sweep_df[\n",
    "    (sweep_df[\"mean_sensitivity\"] >= TARGET_SENS) &\n",
    "    (sweep_df[\"mean_specificity\"] >= MIN_SPEC)\n",
    "].copy()\n",
    "\n",
    "constraint_reached = (len(eligible) > 0)\n",
    "\n",
    "# Summarizes whether a threshold is short on sensitivity and or specificity.\n",
    "def _constraint_status_at_row(row_dict):\n",
    "    sens_gap = float(TARGET_SENS - row_dict[\"mean_sensitivity\"])\n",
    "    spec_gap = float(MIN_SPEC - row_dict[\"mean_specificity\"])\n",
    "    return {\n",
    "        \"sens_ok\": bool(row_dict[\"mean_sensitivity\"] >= TARGET_SENS),\n",
    "        \"spec_ok\": bool(row_dict[\"mean_specificity\"] >= MIN_SPEC),\n",
    "        \"sens_gap_needed\": float(max(0.0, sens_gap)),\n",
    "        \"spec_gap_needed\": float(max(0.0, spec_gap)),\n",
    "    }\n",
    "\n",
    "if constraint_reached:\n",
    "    # Picks the smallest mean |ΔFNR| among thresholds that meet both constraints.\n",
    "    eligible = eligible.sort_values(\n",
    "        by=[\"mean_abs_deltaFNR\", \"mean_sensitivity\", \"mean_specificity\", \"threshold\"],\n",
    "        ascending=[True, False, False, True]\n",
    "    )\n",
    "    chosen = eligible.iloc[0].to_dict()\n",
    "\n",
    "    policy_note = (\n",
    "        \"Constraints reached:\\n\"\n",
    "        f\"  - mean(sensitivity) >= {TARGET_SENS:.2f}\\n\"\n",
    "        f\"  - mean(specificity) >= {MIN_SPEC:.2f}\\n\"\n",
    "        \"Chosen threshold minimizes mean(|ΔFNR|) among eligible thresholds \"\n",
    "        \"(tie-breakers: higher sensitivity, higher specificity, then lower threshold).\"\n",
    "    )\n",
    "else:\n",
    "    # If no threshold meets both constraints, selects the closest option and explains what failed.\n",
    "    can_reach_sens = bool((sweep_df[\"mean_sensitivity\"] >= TARGET_SENS).any())\n",
    "    can_reach_spec = bool((sweep_df[\"mean_specificity\"] >= MIN_SPEC).any())\n",
    "\n",
    "    working_df = sweep_df.copy()\n",
    "\n",
    "    # If a constraint is unreachable, it is not required in the subset filter.\n",
    "    subset = working_df.copy()\n",
    "    if can_reach_sens:\n",
    "        subset = subset[subset[\"mean_sensitivity\"] >= TARGET_SENS].copy()\n",
    "    if can_reach_spec:\n",
    "        subset = subset[subset[\"mean_specificity\"] >= MIN_SPEC].copy()\n",
    "\n",
    "    if len(subset) == 0:\n",
    "        subset = working_df.copy()\n",
    "\n",
    "    # Defines how far below each constraint the mean values are (0 if satisfied).\n",
    "    subset[\"_sens_gap\"] = np.maximum(0.0, TARGET_SENS - subset[\"mean_sensitivity\"])\n",
    "    subset[\"_spec_gap\"] = np.maximum(0.0, MIN_SPEC - subset[\"mean_specificity\"])\n",
    "    subset[\"_total_gap\"] = subset[\"_sens_gap\"] + subset[\"_spec_gap\"]\n",
    "\n",
    "    # Chooses the smallest total gap, then smallest mean |ΔFNR|, then higher sensitivity and specificity.\n",
    "    subset = subset.sort_values(\n",
    "        by=[\"_total_gap\", \"mean_abs_deltaFNR\", \"mean_sensitivity\", \"mean_specificity\", \"threshold\"],\n",
    "        ascending=[True, True, False, False, True]\n",
    "    )\n",
    "\n",
    "    chosen = subset.iloc[0].drop(labels=[\"_sens_gap\", \"_spec_gap\", \"_total_gap\"]).to_dict()\n",
    "    status = _constraint_status_at_row(chosen)\n",
    "\n",
    "    failed_parts = []\n",
    "    if not can_reach_sens:\n",
    "        failed_parts.append(\n",
    "            f\"mean(sensitivity) >= {TARGET_SENS:.2f} (UNREACHABLE on this grid; max was {max_mean_sens:.6f} at thr={thr_at_max_sens:.4f})\"\n",
    "        )\n",
    "    if not can_reach_spec:\n",
    "        failed_parts.append(\n",
    "            f\"mean(specificity) >= {MIN_SPEC:.2f} (UNREACHABLE on this grid; max was {max_mean_spec:.6f} at thr={thr_at_max_spec:.4f})\"\n",
    "        )\n",
    "    if not failed_parts:\n",
    "        failed_parts.append(\n",
    "            \"Both constraints are individually reachable, but no single threshold meets BOTH at the same time on this grid.\"\n",
    "        )\n",
    "\n",
    "    policy_note = (\n",
    "        \"Constraints NOT jointly reachable.\\n\"\n",
    "        \"What failed:\\n\"\n",
    "        \"  - \" + \"\\n  - \".join(failed_parts) + \"\\n\"\n",
    "        \"Returned the threshold that minimizes total constraint violation (sum of gaps), then minimizes mean(|ΔFNR|), \"\n",
    "        \"then prefers higher sensitivity and higher specificity.\\n\"\n",
    "        f\"At the chosen threshold, remaining gaps are:\\n\"\n",
    "        f\"  - sensitivity gap needed: {status['sens_gap_needed']:.6f}\\n\"\n",
    "        f\"  - specificity gap needed: {status['spec_gap_needed']:.6f}\"\n",
    "    )\n",
    "\n",
    "print(\"\\n================ POLICY RESULT ================\")\n",
    "print(POLICY_TEXT)\n",
    "print(\"Constraint reached (both)?:\", bool(constraint_reached))\n",
    "print(\"Max achievable mean(sensitivity) on grid:\", f\"{max_mean_sens:.6f}\", \"| at threshold:\", f\"{thr_at_max_sens:.4f}\")\n",
    "print(\"Max achievable mean(specificity) on grid:\", f\"{max_mean_spec:.6f}\", \"| at threshold:\", f\"{thr_at_max_spec:.4f}\")\n",
    "\n",
    "print(\"\\nChosen threshold:\", f\"{float(chosen['threshold']):.4f}\")\n",
    "print(\"Chosen mean(sensitivity):\", f\"{float(chosen['mean_sensitivity']):.6f}\")\n",
    "print(\"Chosen mean(specificity):\", f\"{float(chosen['mean_specificity']):.6f}\")\n",
    "print(\"Chosen mean(|ΔFNR|):\", f\"{float(chosen['mean_abs_deltaFNR']):.6f}\")\n",
    "print(\"Chosen mean signed ΔFNR (F-M):\", f\"{float(chosen['mean_signed_deltaFNR_F_minus_M']):.6f}\")\n",
    "\n",
    "print(\"\\nDetails (no black box):\")\n",
    "print(policy_note)\n",
    "\n",
    "# -------------------------\n",
    "# 16) Save sweep results\n",
    "# -------------------------\n",
    "# Writes the full sweep table (all thresholds) and a compact JSON summary (chosen threshold + key stats).\n",
    "sweep_csv = OUT_DIR / \"sweep_table.csv\"\n",
    "sweep_df.to_csv(sweep_csv, index=False)\n",
    "\n",
    "summary = {\n",
    "    \"train_dataset\": \"D7\",\n",
    "    \"test_dataset\": \"D2\",\n",
    "    \"split_swept\": \"D2 TEST\",\n",
    "    \"required_exp_substring_case_insensitive\": REQUIRED_EXP_SUBSTRING,\n",
    "    \"trainval_experiment_used\": str(chosen_exp),\n",
    "    \"trainval_summary_path\": str(summary_trainval_path),\n",
    "    \"seeds\": SEEDS,\n",
    "    \"policy\": POLICY_TEXT,\n",
    "    \"constraints\": {\n",
    "        \"target_mean_sensitivity\": float(TARGET_SENS),\n",
    "        \"min_mean_specificity\": float(MIN_SPEC),\n",
    "    },\n",
    "    \"constraint_reached_both\": bool(constraint_reached),\n",
    "    \"max_mean_sensitivity_on_grid\": float(max_mean_sens),\n",
    "    \"threshold_at_max_mean_sensitivity\": float(thr_at_max_sens),\n",
    "    \"max_mean_specificity_on_grid\": float(max_mean_spec),\n",
    "    \"threshold_at_max_mean_specificity\": float(thr_at_max_spec),\n",
    "    \"youdenJ_mean_threshold_reference\": (None if np.isnan(thr_mean) else float(thr_mean)),\n",
    "    \"chosen_threshold\": float(chosen[\"threshold\"]),\n",
    "    \"chosen_metrics\": {\n",
    "        \"mean_sensitivity\": float(chosen[\"mean_sensitivity\"]),\n",
    "        \"mean_specificity\": float(chosen[\"mean_specificity\"]),\n",
    "        \"mean_abs_deltaFNR\": float(chosen[\"mean_abs_deltaFNR\"]),\n",
    "        \"mean_signed_deltaFNR_F_minus_M\": float(chosen[\"mean_signed_deltaFNR_F_minus_M\"]),\n",
    "        \"mean_accuracy\": float(chosen[\"mean_accuracy\"]),\n",
    "        \"mean_precision\": float(chosen[\"mean_precision\"]),\n",
    "        \"mean_f1\": float(chosen[\"mean_f1\"]),\n",
    "        \"mean_mcc\": float(chosen[\"mean_mcc\"]),\n",
    "        \"mean_fisher_p_two_sided\": float(chosen[\"mean_fisher_p_two_sided\"]),\n",
    "        \"mean_FNR_M\": float(chosen[\"mean_FNR_M\"]),\n",
    "        \"mean_FNR_F\": float(chosen[\"mean_FNR_F\"]),\n",
    "        \"sensitivity_by_seed\": chosen.get(\"sensitivity_by_seed\", \"\"),\n",
    "        \"specificity_by_seed\": chosen.get(\"specificity_by_seed\", \"\"),\n",
    "        \"abs_deltaFNR_by_seed\": chosen.get(\"abs_deltaFNR_by_seed\", \"\"),\n",
    "        \"signed_deltaFNR_by_seed\": chosen.get(\"signed_deltaFNR_by_seed\", \"\"),\n",
    "        \"n_PD_M_each_seed\": chosen.get(\"n_PD_M_each_seed\", \"\"),\n",
    "        \"n_PD_F_each_seed\": chosen.get(\"n_PD_F_each_seed\", \"\"),\n",
    "    },\n",
    "    \"policy_note_transparent\": policy_note,\n",
    "    \"paths\": {\n",
    "        \"out_dir\": str(OUT_DIR),\n",
    "        \"sweep_table_csv\": str(sweep_csv),\n",
    "    },\n",
    "    \"timestamp\": timestamp,\n",
    "}\n",
    "\n",
    "summary_json = OUT_DIR / \"sweep_summary.json\"\n",
    "with open(summary_json, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "# -------------------------\n",
    "# 17) Plots\n",
    "# -------------------------\n",
    "# Saves quick visuals for: sensitivity, specificity, |ΔFNR| vs threshold, and the sensitivity–fairness tradeoff.\n",
    "plt.figure()\n",
    "plt.plot(sweep_df[\"threshold\"].values, sweep_df[\"mean_sensitivity\"].values)\n",
    "plt.axhline(TARGET_SENS, linestyle=\"--\")\n",
    "plt.axvline(float(chosen[\"threshold\"]), linestyle=\"--\")\n",
    "plt.xlabel(\"Threshold\")\n",
    "plt.ylabel(\"Mean Sensitivity (across seeds)\")\n",
    "plt.title(\"Threshold Sweep: Mean Sensitivity vs Threshold (D7 trainEnh1 → D2 TEST)\")\n",
    "plt.tight_layout()\n",
    "p1 = OUT_DIR / \"sweep_sensitivity_vs_threshold.png\"\n",
    "plt.savefig(p1, dpi=150)\n",
    "plt.close()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(sweep_df[\"threshold\"].values, sweep_df[\"mean_specificity\"].values)\n",
    "plt.axhline(MIN_SPEC, linestyle=\"--\")\n",
    "plt.axvline(float(chosen[\"threshold\"]), linestyle=\"--\")\n",
    "plt.xlabel(\"Threshold\")\n",
    "plt.ylabel(\"Mean Specificity (across seeds)\")\n",
    "plt.title(\"Threshold Sweep: Mean Specificity vs Threshold (D7 trainEnh1 → D2 TEST)\")\n",
    "plt.tight_layout()\n",
    "p1b = OUT_DIR / \"sweep_specificity_vs_threshold.png\"\n",
    "plt.savefig(p1b, dpi=150)\n",
    "plt.close()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(sweep_df[\"threshold\"].values, sweep_df[\"mean_abs_deltaFNR\"].values)\n",
    "plt.axvline(float(chosen[\"threshold\"]), linestyle=\"--\")\n",
    "plt.xlabel(\"Threshold\")\n",
    "plt.ylabel(\"Mean |ΔFNR| (across seeds)\")\n",
    "plt.title(\"Threshold Sweep: Mean |ΔFNR| vs Threshold (D7 trainEnh1 → D2 TEST)\")\n",
    "plt.tight_layout()\n",
    "p2 = OUT_DIR / \"sweep_abs_deltaFNR_vs_threshold.png\"\n",
    "plt.savefig(p2, dpi=150)\n",
    "plt.close()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(sweep_df[\"mean_sensitivity\"].values, sweep_df[\"mean_abs_deltaFNR\"].values)\n",
    "plt.scatter([float(chosen[\"mean_sensitivity\"])], [float(chosen[\"mean_abs_deltaFNR\"])])\n",
    "plt.axvline(TARGET_SENS, linestyle=\"--\")\n",
    "plt.xlabel(\"Mean Sensitivity (across seeds)\")\n",
    "plt.ylabel(\"Mean |ΔFNR| (across seeds)\")\n",
    "plt.title(\"Threshold Tradeoff: Sensitivity vs |ΔFNR| (D7 trainEnh1 → D2 TEST)\")\n",
    "plt.tight_layout()\n",
    "p3 = OUT_DIR / \"sweep_tradeoff.png\"\n",
    "plt.savefig(p3, dpi=150)\n",
    "plt.close()\n",
    "\n",
    "print(\"\\n================ SAVED OUTPUTS ================\")\n",
    "print(\"OUT_DIR:\", str(OUT_DIR))\n",
    "print(\"Saved:\", str(sweep_csv))\n",
    "print(\"Saved:\", str(summary_json))\n",
    "print(\"Saved plots:\")\n",
    "print(\" \", str(p1))\n",
    "print(\" \", str(p1b))\n",
    "print(\" \", str(p2))\n",
    "print(\" \", str(p3))\n",
    "print(\"\\nDone.\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "f724bba148c64237ab20c7fea778aa9e",
      "bcec623d7d4841ecbea798609b621a0a",
      "474c9878e3a0425daebffa2970555bbb",
      "e3523ec0ffbe464ba4d100120c554db3",
      "504201f4a2e347bc9e5ef4520762e966",
      "95bdb96709aa4829baf8eb6be446b807",
      "154917ddb0a34c1283cbf88057d8c902",
      "a6d6b20c623b4ef1b3056a59664d055c",
      "10c0b4be49954d00a6d5881d201788ad",
      "63db9b90f77244b99df1e740f0b5e6cd",
      "af9bd404b7964ba1a40c2f1896e2a71a",
      "04725ca7a5d04a6e9d0bb272cd7cbc10",
      "8497709c7a584ad58eb35c9b19975eb9",
      "9442a87351b64560ae470f2af8adcc8b",
      "530824398f73405b81b3005cf6e1138b",
      "bb94bc4592fe44b68341bc4b303db141",
      "2ccaaafdfe364b24b74b27675625ab33",
      "0555620939754eafbb24fea17caee009",
      "2b44b9682bac4d91adaf51087448ef66",
      "bd446d59ff0a493e891312a3ff154c21",
      "2766feec4067423391e7e13c3c87ddd4",
      "55971f67a64747a99f76e55f179baa0f",
      "76414fb997764a3ab51f5c8dc36ec058",
      "a491139a77fa42389ea9853110e6a72a",
      "253b16b058d942cd8edf6cab6a756c03",
      "762c1a4273e34b12bfce6abab15ffc87",
      "bc995e99114c4109b0cf08f994738a3e",
      "f7ab44724838440bb31d28ef8dfbc014",
      "a5dcab5147cb455496686b91c253c2b7",
      "00dd5ae353bb419cb160d61d6d340b0d",
      "13cdc0f365b64d5e8e3dc519432b984a",
      "c9788678dd45483db6b07b52167bafd6",
      "902b31122be7433d85882f84fd5da3d1",
      "91fc1f1190c74eee956b216c23822e5a",
      "4fc59581c3e44fbf8ecf88207c4e98ab",
      "50bb567a5248486fab4814300d1a18c4",
      "4d72609304e94e0ead0ec7b0ffdb1ab3",
      "824b252ad385468f9593ddafa075bf5d",
      "9888445a847847c79be6f62a12cf6b0a",
      "823949417e144f4c803e2dfc3eba4e2f",
      "9c4edca1dcb44ad9b42b4b5bce7b5f66",
      "985acbf4bf134fa29fda721c5e0868f6",
      "0f11b649e6414e1fb14cc72e587582f6",
      "75c01eecc1d84537bda0bc5aaf4d058e",
      "7d3b93af5aa747938b64bb27766e1245",
      "ecfe0d6cbd784a6c88f29f618fcb0896",
      "08990ba162514c36a49e5925290877e7",
      "0c408ff41612443f9f0d126e0916efe3",
      "bcda1819ba844784a2c1391c83b8b263",
      "43416d0fef094bf895977cfbef6ae03d",
      "0d473d8e6473478e82eeb8d16fdd7464",
      "90836430f0554c539dc707bd47c99829",
      "e34fed244eab4a0589cbe8639a9e349c",
      "bd7829ffd98b48e68556c318a823212d",
      "8e0e6a9a36034051827200e4027fd0bc"
     ]
    },
    "id": "pVZRmL09-8XX"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "The following cell runs a **post-training decision threshold sweep** for the **D7 trainEnh2 model evaluated on the D2 test split**, using models that have already been trained. No retraining is done in this step. The cell automatically finds the most recent D7 training and validation experiment whose folder name contains **“trainEnh2”** and checks that all required files exist for each of the three seeds, including the training summary and saved head weights. Using these fixed models, it runs one full inference pass per seed on the D2 test data to generate Parkinson’s probability scores, then studies how performance and sex-based fairness change as the decision threshold moves from 0.01 to 0.99.\n",
    "\n",
    "The cell starts with basic setup and safety checks to ensure the run is clean and repeatable. It avoids import conflicts from locally named files, mounts Google Drive if needed, resolves the dataset paths, and prints key settings such as the compute device, batch size, threshold range, and policy targets. It then loads the D2 `manifest_all.csv`, confirms that required columns are present, verifies that the data belong to **dataset D2**, and filters the data to the test split only. Before inference begins, it checks that all referenced audio files exist and reports progress during this check.\n",
    "\n",
    "Each test clip is assigned to a simple task group: **vowel** for sustained vowel recordings and **other** for all remaining speech tasks. Sex labels are standardized from the original D2 values by mapping `male` to **M** and `female` to **F**, with any other value treated as unknown. Audio is loaded at **16 kHz**, and attention masks are created so padded regions are ignored. For vowel clips, the mask also removes trailing near-silence so non-speech segments have less effect on the results.\n",
    "\n",
    "Inference uses the same model structure as during training: a frozen speech feature extractor with two task-specific heads, one for vowel clips and one for non-vowel clips. For each seed, the saved head weights are loaded and the full D2 test set is processed once. The predicted probabilities are stored together with the true labels and sex information. **AUROC is computed for each seed and averaged across seeds** to provide a threshold-free reference before applying any decision threshold.\n",
    "\n",
    "With predictions fixed, the cell performs the threshold sweep. At each threshold, standard classification metrics are calculated and averaged across the three seeds, including accuracy, precision, sensitivity, specificity, F1 score, Matthews correlation coefficient, and Fisher’s exact test p-value. Fairness is evaluated by comparing false negative rates for Parkinson’s cases between females and males, reported as both a signed difference and an absolute gap. The sweep table includes both the averaged values and the per-seed results so differences across seeds remain visible.\n",
    "\n",
    "A single operating threshold is then chosen using a clear rule called **Policy B+**. This threshold minimizes the mean absolute sex gap in false negative rate while also meeting two requirements at the same time: mean sensitivity of at least **0.60** and mean specificity of at least **0.50**. If no threshold meets both conditions, the cell reports which requirement failed and selects the closest available alternative based on how far the results miss the targets.\n",
    "\n",
    "All outputs are saved to a timestamped folder under the D7 threshold sweep directory. This includes the full sweep table, a summary describing the selected experiment and threshold choice, and plots that show how sensitivity, specificity, and fairness change across thresholds, along with a tradeoff curve that clearly marks the chosen operating point."
   ],
   "metadata": {
    "id": "m5dPkc1XtOLK"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# =========================\n",
    "# Threshold Sweep With Fairness Guardrail (D7 trainEnh2 → D2 test)\n",
    "# =========================\n",
    "# Purpose: run inference once per seed using saved D7 trainEnh2 heads, then sweep decision thresholds on the same scores.\n",
    "# Inputs: (1) saved model heads for the three seeds, (2) D2 manifest with test split clip paths and labels.\n",
    "# Outputs: sweep_table.csv, sweep_summary.json, and a few sweep plots saved under a new timestamped sweep folder.\n",
    "# =========================\n",
    "\n",
    "import os, json, math, random, time, warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import soundfile as sf\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from transformers import Wav2Vec2Model\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix, matthews_corrcoef\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# -------------------------\n",
    "# 0) Import safety check\n",
    "# -------------------------\n",
    "# Stops early if local files would override real PyTorch or Transformers imports.\n",
    "if os.path.exists(\"/content/torch.py\") or os.path.exists(\"/content/torch/__init__.py\"):\n",
    "    raise RuntimeError(\"Found local /content/torch.py or /content/torch/ that shadows PyTorch. Rename/remove it and restart runtime.\")\n",
    "if os.path.exists(\"/content/transformers.py\") or os.path.exists(\"/content/transformers/__init__.py\"):\n",
    "    raise RuntimeError(\"Found local /content/transformers.py or /content/transformers/ that shadows Hugging Face Transformers. Rename/remove it and restart runtime.\")\n",
    "\n",
    "# -------------------------\n",
    "# 1) Drive mount (best-effort)\n",
    "# -------------------------\n",
    "# Mounts Google Drive if running in Colab and not already mounted.\n",
    "try:\n",
    "    from google.colab import drive  # type: ignore\n",
    "    if not os.path.isdir(\"/content/drive/MyDrive\"):\n",
    "        drive.mount(\"/content/drive\")\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# -------------------------\n",
    "# 2) Root paths\n",
    "# -------------------------\n",
    "# Uses existing notebook globals if present; otherwise uses the fallbacks below.\n",
    "D7_OUT_ROOT_FALLBACK = \"/content/drive/MyDrive/AI_PD_Project/Datasets/D7-Multilingual (D1_D4_D5v2_D6)/preprocessed_v1\"\n",
    "D2_OUT_ROOT_FALLBACK = \"/content/drive/MyDrive/AI_PD_Project/Datasets/D2-Slovak (EWA-DB)/EWA-DB/preprocessed_v1\"\n",
    "\n",
    "D7_OUT_ROOT = str(globals().get(\"D7_OUT_ROOT\", D7_OUT_ROOT_FALLBACK))\n",
    "D2_OUT_ROOT = str(globals().get(\"D2_OUT_ROOT\", D2_OUT_ROOT_FALLBACK))\n",
    "D2_MANIFEST_ALL = f\"{D2_OUT_ROOT}/manifests/manifest_all.csv\"\n",
    "\n",
    "# Exposes these roots for downstream cells.\n",
    "globals()[\"D7_OUT_ROOT\"] = D7_OUT_ROOT\n",
    "globals()[\"D2_OUT_ROOT\"] = D2_OUT_ROOT\n",
    "\n",
    "# -------------------------\n",
    "# 3) Run configuration\n",
    "# -------------------------\n",
    "# Core settings: seeds, backbone, audio requirements, and inference batch sizing.\n",
    "SEEDS          = [1337, 2024, 7777]\n",
    "BACKBONE_CKPT  = \"facebook/wav2vec2-base\"\n",
    "SR_EXPECTED    = 16000\n",
    "TINY_THRESH    = 1e-4\n",
    "\n",
    "PER_DEVICE_BS  = 16\n",
    "NUM_WORKERS    = 0\n",
    "PIN_MEMORY     = False\n",
    "USE_AMP        = True\n",
    "DROPOUT_P      = 0.2\n",
    "\n",
    "# Experiment selector: matches the exp_* folder name for trainEnh2.\n",
    "REQUIRED_EXP_SUBSTRING = \"trainEnh2\"  # case-insensitive match against exp_* folder name\n",
    "\n",
    "# Policy B+: choose a threshold that reduces sex FNR gap while keeping sensitivity and specificity above minimums.\n",
    "TARGET_SENS = 0.60\n",
    "MIN_SPEC    = 0.50   # avoids trivial thresholds that label nearly everything as PD\n",
    "POLICY_TEXT = (\n",
    "    \"Policy B+: minimize mean(|ΔFNR|) subject to \"\n",
    "    f\"mean(sensitivity) >= {TARGET_SENS:.2f} AND mean(specificity) >= {MIN_SPEC:.2f}\"\n",
    ")\n",
    "\n",
    "# Threshold grid for sweeping.\n",
    "THR_MIN, THR_MAX, THR_STEPS = 0.01, 0.99, 199\n",
    "THR_GRID = np.linspace(THR_MIN, THR_MAX, THR_STEPS).astype(np.float64)\n",
    "\n",
    "# Device selection and minor numeric tuning.\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "try:\n",
    "    torch.set_float32_matmul_precision(\"high\")\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# Keeps output readable by filtering common non-critical warnings.\n",
    "warnings.filterwarnings(\"ignore\", message=\"None of the inputs have requires_grad=True. Gradients will be None\")\n",
    "warnings.filterwarnings(\"ignore\", message=\"Passing `gradient_checkpointing` to a config initialization is deprecated\")\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"huggingface_hub\")\n",
    "\n",
    "# Prints key settings for quick verification.\n",
    "print(\"D7_OUT_ROOT:\", D7_OUT_ROOT)\n",
    "print(\"D2_OUT_ROOT:\", D2_OUT_ROOT)\n",
    "print(\"D2_MANIFEST_ALL:\", D2_MANIFEST_ALL)\n",
    "print(\"DEVICE:\", DEVICE)\n",
    "if DEVICE.type == \"cuda\":\n",
    "    print(\"GPU:\", torch.cuda.get_device_name(0))\n",
    "print(\"PER_DEVICE_BS:\", PER_DEVICE_BS)\n",
    "print(\"NUM_WORKERS:\", NUM_WORKERS, \"| PIN_MEMORY:\", PIN_MEMORY)\n",
    "print(\"USE_AMP:\", bool(USE_AMP and DEVICE.type == \"cuda\"))\n",
    "print(\"Required exp substring (case-insensitive):\", REQUIRED_EXP_SUBSTRING)\n",
    "print(\"Policy B+ constraints:\")\n",
    "print(\"  mean sensitivity >=\", TARGET_SENS)\n",
    "print(\"  mean specificity >=\", MIN_SPEC)\n",
    "print(f\"Threshold sweep grid: {THR_MIN:.2f}..{THR_MAX:.2f} with {THR_STEPS} steps\")\n",
    "\n",
    "# -------------------------\n",
    "# 4) Load D2 manifest and keep only test split\n",
    "# -------------------------\n",
    "# Reads the D2 manifest, checks required columns, confirms it is actually D2, then filters to split == \"test\".\n",
    "if not os.path.exists(D2_MANIFEST_ALL):\n",
    "    raise FileNotFoundError(f\"Missing D2 manifest_all.csv: {D2_MANIFEST_ALL}\")\n",
    "\n",
    "m_all = pd.read_csv(D2_MANIFEST_ALL)\n",
    "\n",
    "req_cols = {\"split\", \"clip_path\", \"label_num\", \"task\", \"sex\", \"age\"}\n",
    "missing = [c for c in sorted(req_cols) if c not in m_all.columns]\n",
    "if missing:\n",
    "    raise ValueError(f\"D2 manifest missing required columns: {missing}. Found: {list(m_all.columns)}\")\n",
    "\n",
    "# Uses the most common dataset label (when present) and filters to that dataset.\n",
    "if \"dataset\" in m_all.columns and m_all[\"dataset\"].notna().any():\n",
    "    d2_dataset_id = str(m_all[\"dataset\"].astype(str).value_counts(dropna=True).idxmax())\n",
    "    m_all = m_all[m_all[\"dataset\"].astype(str) == d2_dataset_id].copy()\n",
    "else:\n",
    "    d2_dataset_id = \"DX\"\n",
    "\n",
    "# Hard stop if the manifest is not D2.\n",
    "if d2_dataset_id != \"D2\":\n",
    "    raise RuntimeError(\n",
    "        f\"Expected D2 dataset_id=='D2' but got {d2_dataset_id!r}. \"\n",
    "        \"This usually means D2_OUT_ROOT is wrong or the manifest is not D2. \"\n",
    "        f\"D2_OUT_ROOT={D2_OUT_ROOT}\"\n",
    "    )\n",
    "\n",
    "# Keeps a stable set of columns used downstream (fills missing ones with NaN).\n",
    "keep_cols = [\"clip_path\", \"label_num\", \"task\", \"speaker_id\", \"sex\", \"age\", \"duration_sec\", \"split\"]\n",
    "for c in keep_cols:\n",
    "    if c not in m_all.columns:\n",
    "        m_all[c] = np.nan\n",
    "m_all = m_all[keep_cols].copy()\n",
    "\n",
    "# Test split only.\n",
    "test_df = m_all[m_all[\"split\"].astype(str) == \"test\"].reset_index(drop=True)\n",
    "\n",
    "print(f\"\\nD2 dataset inferred: {d2_dataset_id}\")\n",
    "print(f\"D2 TEST rows: {len(test_df)}\")\n",
    "print(\"D2 TEST label counts:\", test_df[\"label_num\"].value_counts(dropna=False).to_dict())\n",
    "print(\"D2 TEST sex counts (raw):\", test_df[\"sex\"].value_counts(dropna=False).to_dict())\n",
    "\n",
    "if len(test_df) == 0:\n",
    "    raise RuntimeError(\"After filtering to split=='test', D2 manifest has 0 rows.\")\n",
    "\n",
    "# -------------------------\n",
    "# 5) Fail fast: confirm audio files exist\n",
    "# -------------------------\n",
    "# Checks for missing clip paths early to avoid long runs that fail late.\n",
    "def _fail_fast_missing_paths(df: pd.DataFrame, name: str):\n",
    "    missing_paths = []\n",
    "    for p in tqdm(df[\"clip_path\"].astype(str).tolist(), desc=f\"Check {name} clip_path exists\", dynamic_ncols=True):\n",
    "        if not os.path.exists(p):\n",
    "            missing_paths.append(p)\n",
    "            if len(missing_paths) >= 10:\n",
    "                break\n",
    "    if missing_paths:\n",
    "        raise FileNotFoundError(f\"{name}: missing clip_path(s). Examples: {missing_paths}\")\n",
    "\n",
    "_fail_fast_missing_paths(test_df, \"D2 TEST\")\n",
    "\n",
    "# -------------------------\n",
    "# 6) Task grouping (vowel vs other)\n",
    "# -------------------------\n",
    "# Maps each clip to the head it should use during inference.\n",
    "def _task_group(task_val) -> str:\n",
    "    return \"vowel\" if str(task_val) == \"vowl\" else \"other\"\n",
    "\n",
    "test_df[\"task_group\"] = test_df[\"task\"].apply(_task_group)\n",
    "\n",
    "# -------------------------\n",
    "# 6.5) Sex normalization\n",
    "# -------------------------\n",
    "# Normalizes D2 \"male\"/\"female\" into \"M\"/\"F\" and maps anything else to \"UNK\".\n",
    "def normalize_sex_d2_case_sensitive(val) -> str:\n",
    "    if pd.isna(val):\n",
    "        return \"UNK\"\n",
    "    if val == \"male\":\n",
    "        return \"M\"\n",
    "    if val == \"female\":\n",
    "        return \"F\"\n",
    "    return \"UNK\"\n",
    "\n",
    "test_df[\"sex_norm\"] = test_df[\"sex\"].apply(normalize_sex_d2_case_sensitive)\n",
    "print(\"D2 TEST sex counts (normalized):\", test_df[\"sex_norm\"].value_counts(dropna=False).to_dict())\n",
    "if (test_df[\"sex_norm\"] == \"UNK\").any():\n",
    "    print(\"NOTE: Some D2 'sex' values were not exactly 'male'/'female' and were mapped to 'UNK'.\")\n",
    "\n",
    "# -------------------------\n",
    "# 7) Dataset and collator\n",
    "# -------------------------\n",
    "# Builds model inputs from audio files and creates an attention mask for pooling.\n",
    "# For vowel clips, the mask removes trailing padding-like silence based on a tiny amplitude threshold.\n",
    "class AudioManifestDataset(Dataset):\n",
    "    def __init__(self, df: pd.DataFrame):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        row = self.df.iloc[idx]\n",
    "        clip_path = str(row[\"clip_path\"])\n",
    "        label = int(row[\"label_num\"])\n",
    "        task_group = str(row[\"task_group\"])\n",
    "        sex_norm = str(row[\"sex_norm\"])\n",
    "\n",
    "        if not os.path.exists(clip_path):\n",
    "            raise FileNotFoundError(f\"Missing clip_path: {clip_path}\")\n",
    "\n",
    "        y, sr = sf.read(clip_path, always_2d=False)\n",
    "        if isinstance(y, np.ndarray) and y.ndim > 1:\n",
    "            y = np.mean(y, axis=1)\n",
    "        y = np.asarray(y, dtype=np.float32)\n",
    "\n",
    "        if int(sr) != SR_EXPECTED:\n",
    "            raise ValueError(f\"Sample rate mismatch (expected {SR_EXPECTED}): {clip_path} has sr={sr}\")\n",
    "\n",
    "        # Attention mask: 1 = keep, 0 = ignore during pooling.\n",
    "        attn = np.ones((len(y),), dtype=np.int64)\n",
    "        if task_group == \"vowel\":\n",
    "            # Find last non-tiny sample and ignore anything after it.\n",
    "            k = -1\n",
    "            for j in range(len(y) - 1, -1, -1):\n",
    "                if abs(float(y[j])) > TINY_THRESH:\n",
    "                    k = j\n",
    "                    break\n",
    "            if k >= 0:\n",
    "                attn[:k+1] = 1\n",
    "                attn[k+1:] = 0\n",
    "            else:\n",
    "                attn[:] = 1\n",
    "        else:\n",
    "            attn[:] = 1\n",
    "\n",
    "        return {\n",
    "            \"input_values\": torch.from_numpy(y),\n",
    "            \"attention_mask\": torch.from_numpy(attn),\n",
    "            \"labels\": torch.tensor(label, dtype=torch.long),\n",
    "            \"task_group\": task_group,\n",
    "            \"sex_norm\": sex_norm,\n",
    "        }\n",
    "\n",
    "# Pads variable-length audio in a batch to the longest clip length.\n",
    "def collate_fn(batch):\n",
    "    max_len = int(max(b[\"input_values\"].numel() for b in batch))\n",
    "    input_vals, attn_masks, labels, task_groups, sex_norms = [], [], [], [], []\n",
    "    for b in batch:\n",
    "        x = b[\"input_values\"]\n",
    "        a = b[\"attention_mask\"]\n",
    "        pad = max_len - x.numel()\n",
    "        if pad > 0:\n",
    "            x = torch.cat([x, torch.zeros(pad, dtype=x.dtype)], dim=0)\n",
    "            a = torch.cat([a, torch.zeros(pad, dtype=a.dtype)], dim=0)\n",
    "        input_vals.append(x)\n",
    "        attn_masks.append(a)\n",
    "        labels.append(b[\"labels\"])\n",
    "        task_groups.append(b[\"task_group\"])\n",
    "        sex_norms.append(b[\"sex_norm\"])\n",
    "    return {\n",
    "        \"input_values\": torch.stack(input_vals, dim=0),\n",
    "        \"attention_mask\": torch.stack(attn_masks, dim=0),\n",
    "        \"labels\": torch.stack(labels, dim=0),\n",
    "        \"task_group\": task_groups,\n",
    "        \"sex_norm\": sex_norms,\n",
    "    }\n",
    "\n",
    "# DataLoader over the D2 test split.\n",
    "test_ds = AudioManifestDataset(test_df)\n",
    "test_loader = DataLoader(\n",
    "    test_ds,\n",
    "    batch_size=PER_DEVICE_BS,\n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=PIN_MEMORY,\n",
    "    collate_fn=collate_fn,\n",
    ")\n",
    "\n",
    "# -------------------------\n",
    "# 8) Model definition (frozen backbone + two heads)\n",
    "# -------------------------\n",
    "# Loads a frozen Wav2Vec2 backbone and switches between vowel and other heads per clip.\n",
    "class Wav2Vec2TwoHeadClassifier(nn.Module):\n",
    "    def __init__(self, ckpt: str, dropout_p: float):\n",
    "        super().__init__()\n",
    "        self.backbone = Wav2Vec2Model.from_pretrained(ckpt, use_safetensors=True)\n",
    "        for p in self.backbone.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "        H = int(self.backbone.config.hidden_size)\n",
    "        self.pre_vowel = nn.Sequential(nn.LayerNorm(H), nn.Dropout(float(dropout_p)))\n",
    "        self.pre_other = nn.Sequential(nn.LayerNorm(H), nn.Dropout(float(dropout_p)))\n",
    "        self.head_vowel = nn.Linear(H, 2)\n",
    "        self.head_other = nn.Linear(H, 2)\n",
    "\n",
    "    # Mean-pools frame features using the attention mask (ignores masked regions).\n",
    "    def masked_mean_pool(self, last_hidden, attn_mask_samples):\n",
    "        feat_mask = self.backbone._get_feature_vector_attention_mask(last_hidden.shape[1], attn_mask_samples)\n",
    "        feat_mask = feat_mask.to(last_hidden.device).unsqueeze(-1).type_as(last_hidden)\n",
    "        masked = last_hidden * feat_mask\n",
    "        denom = feat_mask.sum(dim=1).clamp(min=1.0)\n",
    "        return masked.sum(dim=1) / denom\n",
    "\n",
    "    # Runs the head in fp32 for stability even when AMP is enabled.\n",
    "    def _heads_fp32(self, x_any: torch.Tensor, head: nn.Module) -> torch.Tensor:\n",
    "        x = x_any.float()\n",
    "        if x.is_cuda:\n",
    "            with torch.amp.autocast(device_type=\"cuda\", enabled=False):\n",
    "                return head(x)\n",
    "        return head(x)\n",
    "\n",
    "    # Produces logits for each sample using the appropriate head (vowel or other).\n",
    "    def forward_logits(self, input_values, attention_mask, task_group):\n",
    "        with torch.no_grad():\n",
    "            out = self.backbone(input_values=input_values, attention_mask=attention_mask)\n",
    "            last_hidden = out.last_hidden_state\n",
    "\n",
    "        pooled = self.masked_mean_pool(last_hidden, attention_mask)\n",
    "        z_v = self.pre_vowel(pooled.float())\n",
    "        z_o = self.pre_other(pooled.float())\n",
    "\n",
    "        logits_v = self._heads_fp32(z_v, self.head_vowel)\n",
    "        logits_o = self._heads_fp32(z_o, self.head_other)\n",
    "\n",
    "        is_vowel = torch.tensor([tg == \"vowel\" for tg in task_group], device=pooled.device, dtype=torch.bool)\n",
    "        logits = logits_o.clone()\n",
    "        if is_vowel.any():\n",
    "            logits[is_vowel] = logits_v[is_vowel]\n",
    "        return logits\n",
    "\n",
    "# Loads only the head weights saved during training (backbone stays the pretrained frozen model).\n",
    "def load_heads_into_model(model: Wav2Vec2TwoHeadClassifier, best_heads_path: Path):\n",
    "    if not best_heads_path.exists():\n",
    "        raise FileNotFoundError(f\"Missing best_heads.pt: {str(best_heads_path)}\")\n",
    "    state = torch.load(str(best_heads_path), map_location=\"cpu\")\n",
    "    model.pre_vowel.load_state_dict(state[\"pre_vowel\"], strict=True)\n",
    "    model.pre_other.load_state_dict(state[\"pre_other\"], strict=True)\n",
    "    model.head_vowel.load_state_dict(state[\"head_vowel\"], strict=True)\n",
    "    model.head_other.load_state_dict(state[\"head_other\"], strict=True)\n",
    "    return model\n",
    "\n",
    "# -------------------------\n",
    "# 9) Seed control and single-pass inference\n",
    "# -------------------------\n",
    "# Runs inference once per seed and returns fixed probabilities, labels, and sex tags for threshold sweeping.\n",
    "def set_all_seeds(seed: int):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "def infer_once(seed: int, chosen_exp: Path, train_dataset_id: str):\n",
    "    set_all_seeds(seed)\n",
    "    best_heads_path = chosen_exp / f\"run_{train_dataset_id}_seed{seed}\" / \"best_heads.pt\"\n",
    "\n",
    "    model = Wav2Vec2TwoHeadClassifier(BACKBONE_CKPT, dropout_p=DROPOUT_P).to(DEVICE)\n",
    "    model = load_heads_into_model(model, best_heads_path)\n",
    "    model.eval()\n",
    "\n",
    "    use_amp = bool(USE_AMP and DEVICE.type == \"cuda\")\n",
    "    all_probs, all_true, all_sex = [], [], []\n",
    "\n",
    "    # Forward pass over D2 test once (no thresholding here).\n",
    "    pbar = tqdm(test_loader, desc=f\"[seed={seed}] Inference D2 TEST\", dynamic_ncols=True)\n",
    "    with torch.inference_mode():\n",
    "        for batch in pbar:\n",
    "            input_values = batch[\"input_values\"].to(DEVICE, non_blocking=False)\n",
    "            attention_mask = batch[\"attention_mask\"].to(DEVICE, non_blocking=False)\n",
    "            labels = batch[\"labels\"].to(DEVICE, non_blocking=False)\n",
    "            task_group = batch[\"task_group\"]\n",
    "            sex_norm = batch[\"sex_norm\"]\n",
    "\n",
    "            with torch.amp.autocast(device_type=\"cuda\", enabled=use_amp):\n",
    "                logits = model.forward_logits(input_values, attention_mask, task_group)\n",
    "\n",
    "            # Probability for PD is the softmax class-1 probability.\n",
    "            probs = torch.softmax(logits.float(), dim=-1)[:, 1].detach().cpu().numpy().astype(np.float64)\n",
    "            all_probs.extend(probs.tolist())\n",
    "            all_true.extend(labels.detach().cpu().numpy().astype(np.int64).tolist())\n",
    "            all_sex.extend(list(sex_norm))\n",
    "\n",
    "    y_true = np.asarray(all_true, dtype=np.int64)\n",
    "    y_prob = np.asarray(all_probs, dtype=np.float64)\n",
    "    y_sex  = np.asarray(all_sex, dtype=object)\n",
    "\n",
    "    # AUROC is reported per seed as a threshold-independent reference.\n",
    "    auc = float(\"nan\")\n",
    "    if len(np.unique(y_true)) >= 2:\n",
    "        auc = float(roc_auc_score(y_true, y_prob))\n",
    "\n",
    "    return {\n",
    "        \"seed\": int(seed),\n",
    "        \"best_heads_path\": str(best_heads_path),\n",
    "        \"y_true\": y_true,\n",
    "        \"y_prob\": y_prob,\n",
    "        \"y_sex\": y_sex,\n",
    "        \"auroc\": float(auc),\n",
    "    }\n",
    "\n",
    "# -------------------------\n",
    "# 10) Metric helpers for sweeping thresholds\n",
    "# -------------------------\n",
    "# Computes standard metrics from a thresholded prediction and sex-based FNR gap on PD-only cases.\n",
    "def confusion_counts(y_true, y_prob, thr):\n",
    "    y_pred = (y_prob >= float(thr)).astype(np.int64)\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=[0, 1])\n",
    "    TN, FP, FN, TP = int(cm[0, 0]), int(cm[0, 1]), int(cm[1, 0]), int(cm[1, 1])\n",
    "    return TN, FP, FN, TP\n",
    "\n",
    "def threshold_metrics(y_true, y_prob, thr):\n",
    "    TN, FP, FN, TP = confusion_counts(y_true, y_prob, thr)\n",
    "    eps = 1e-12\n",
    "    acc  = (TP + TN) / max(1, (TP + TN + FP + FN))\n",
    "    prec = TP / (TP + FP + eps)\n",
    "    sens = TP / (TP + FN + eps)  # recall/sensitivity\n",
    "    spec = TN / (TN + FP + eps)\n",
    "    f1   = 2 * prec * sens / (prec + sens + eps)\n",
    "\n",
    "    # MCC may be undefined if predictions collapse into a single class.\n",
    "    try:\n",
    "        y_pred = (y_prob >= float(thr)).astype(np.int64)\n",
    "        mcc = float(matthews_corrcoef(y_true, y_pred)) if len(np.unique(y_pred)) > 1 else float(\"nan\")\n",
    "    except Exception:\n",
    "        mcc = float(\"nan\")\n",
    "\n",
    "    # Fisher exact test on the 2x2 table (best-effort if scipy is available).\n",
    "    pval = float(\"nan\")\n",
    "    try:\n",
    "        from scipy.stats import fisher_exact  # type: ignore\n",
    "        _, pval = fisher_exact([[TN, FP], [FN, TP]], alternative=\"two-sided\")\n",
    "        pval = float(pval)\n",
    "    except Exception:\n",
    "        pval = float(\"nan\")\n",
    "\n",
    "    return {\n",
    "        \"tn\": TN, \"fp\": FP, \"fn\": FN, \"tp\": TP,\n",
    "        \"accuracy\": float(acc),\n",
    "        \"precision\": float(prec),\n",
    "        \"sensitivity\": float(sens),\n",
    "        \"specificity\": float(spec),\n",
    "        \"f1\": float(f1),\n",
    "        \"mcc\": float(mcc),\n",
    "        \"fisher_p_two_sided\": float(pval),\n",
    "    }\n",
    "\n",
    "def fnr_by_sex_signed_delta(y_true, y_prob, y_sex, thr):\n",
    "    # FNR is computed within each sex group on true PD cases only: FN / (FN + TP).\n",
    "    y_pred = (y_prob >= float(thr)).astype(np.int64)\n",
    "    out = {}\n",
    "\n",
    "    for g in [\"M\", \"F\"]:\n",
    "        mask_g = (y_sex == g)\n",
    "        pos_mask = mask_g & (y_true == 1)\n",
    "        n_pos = int(pos_mask.sum())\n",
    "        if n_pos == 0:\n",
    "            out[g] = {\"n_pos\": 0, \"fn\": 0, \"tp\": 0, \"fnr\": float(\"nan\")}\n",
    "            continue\n",
    "        tp = int(((y_pred == 1) & pos_mask).sum())\n",
    "        fn = int(((y_pred == 0) & pos_mask).sum())\n",
    "        fnr = float(fn / max(1, (fn + tp)))\n",
    "        out[g] = {\"n_pos\": int(n_pos), \"fn\": int(fn), \"tp\": int(tp), \"fnr\": float(fnr)}\n",
    "\n",
    "    fnr_m = out[\"M\"][\"fnr\"]\n",
    "    fnr_f = out[\"F\"][\"fnr\"]\n",
    "    if (not np.isnan(fnr_m)) and (not np.isnan(fnr_f)):\n",
    "        delta = float(fnr_f - fnr_m)     # ΔFNR = F - M (signed)\n",
    "        absd  = float(abs(delta))\n",
    "    else:\n",
    "        delta = float(\"nan\")\n",
    "        absd  = float(\"nan\")\n",
    "\n",
    "    return out, delta, absd\n",
    "\n",
    "# Small helpers to aggregate values across seeds while tolerating NaNs.\n",
    "def mean_sd(vals):\n",
    "    vals = np.asarray(vals, dtype=np.float64)\n",
    "    mu = float(np.nanmean(vals)) if np.any(~np.isnan(vals)) else float(\"nan\")\n",
    "    sd = float(np.nanstd(vals, ddof=1)) if np.sum(~np.isnan(vals)) > 1 else 0.0\n",
    "    return mu, sd\n",
    "\n",
    "def safe_nanmean(x):\n",
    "    x = np.asarray(x, dtype=np.float64)\n",
    "    if np.any(~np.isnan(x)):\n",
    "        return float(np.nanmean(x))\n",
    "    return float(\"nan\")\n",
    "\n",
    "def safe_nansd(x):\n",
    "    x = np.asarray(x, dtype=np.float64)\n",
    "    if np.sum(~np.isnan(x)) > 1:\n",
    "        return float(np.nanstd(x, ddof=1))\n",
    "    return 0.0\n",
    "\n",
    "# -------------------------\n",
    "# 11) Find the most recent matching trainval experiment\n",
    "# -------------------------\n",
    "# Selects the newest exp_* folder that matches REQUIRED_EXP_SUBSTRING and has all seed heads plus summary_trainval.json.\n",
    "TRAINVAL_ROOT = Path(D7_OUT_ROOT) / \"trainval_runs\"\n",
    "if not TRAINVAL_ROOT.exists():\n",
    "    raise FileNotFoundError(f\"Missing trainval_runs folder under D7_OUT_ROOT: {str(TRAINVAL_ROOT)}\")\n",
    "\n",
    "exp_dirs = sorted([p for p in TRAINVAL_ROOT.glob(\"exp_*\") if p.is_dir()], key=lambda p: p.stat().st_mtime, reverse=True)\n",
    "if not exp_dirs:\n",
    "    raise FileNotFoundError(f\"No exp_* folders found under: {str(TRAINVAL_ROOT)}\")\n",
    "\n",
    "train_dataset_id = \"D7\"\n",
    "\n",
    "def is_match(exp_path: Path, required_substring: str) -> bool:\n",
    "    return required_substring.lower() in exp_path.name.lower()\n",
    "\n",
    "def has_all_seeds_and_summary(exp_path: Path, dataset_id: str, seeds: list) -> bool:\n",
    "    if not (exp_path / \"summary_trainval.json\").exists():\n",
    "        return False\n",
    "    for s in seeds:\n",
    "        p = exp_path / f\"run_{dataset_id}_seed{s}\" / \"best_heads.pt\"\n",
    "        if not p.exists():\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "chosen_exp = None\n",
    "for ed in exp_dirs:\n",
    "    if not is_match(ed, REQUIRED_EXP_SUBSTRING):\n",
    "        continue\n",
    "    if has_all_seeds_and_summary(ed, train_dataset_id, SEEDS):\n",
    "        chosen_exp = ed\n",
    "        break\n",
    "\n",
    "if chosen_exp is None:\n",
    "    examples = [p.name for p in exp_dirs[:10]]\n",
    "    raise FileNotFoundError(\n",
    "        \"Could not find a recent D7 trainval experiment folder that:\\n\"\n",
    "        f\"  (1) contains substring '{REQUIRED_EXP_SUBSTRING}' (case-insensitive) in the exp folder name, and\\n\"\n",
    "        \"  (2) contains all 3 best_heads.pt files + summary_trainval.json.\\n\"\n",
    "        f\"Checked under: {str(TRAINVAL_ROOT)}\\n\\n\"\n",
    "        f\"Example exp_* folder names (most recent first):\\n  - \" + \"\\n  - \".join(examples) + \"\\n\\n\"\n",
    "        \"Fix: set REQUIRED_EXP_SUBSTRING to match the exact tag used in the exp folder name.\"\n",
    "    )\n",
    "\n",
    "print(\"\\nUsing trainval experiment folder:\")\n",
    "print(\" \", str(chosen_exp))\n",
    "\n",
    "# Loads the trainval summary only to print the mean Youden-J threshold as a reference.\n",
    "summary_trainval_path = chosen_exp / \"summary_trainval.json\"\n",
    "with open(summary_trainval_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    trainval_summary = json.load(f)\n",
    "\n",
    "val_opt = ((trainval_summary or {}).get(\"val_optimal_threshold\", {}) or {})\n",
    "thr_mean = float((((val_opt.get(\"mean_sd\", {}) or {}).get(\"mean\", float(\"nan\")))))\n",
    "print(\"\\nTrainval mean val-opt threshold (Youden J) for reference:\")\n",
    "print(\"  summary_trainval.json -> val_optimal_threshold.mean_sd.mean =\", f\"{thr_mean:.6f}\" if not np.isnan(thr_mean) else \"nan\")\n",
    "\n",
    "# -------------------------\n",
    "# 12) Create output folder for this sweep run\n",
    "# -------------------------\n",
    "# Uses a timestamp so each sweep run writes to a fresh folder.\n",
    "TS_ROOT = Path(D7_OUT_ROOT) / \"threshold_sweeps\"\n",
    "TS_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "timestamp = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "OUT_DIR = TS_ROOT / f\"run_D7_{REQUIRED_EXP_SUBSTRING}_on_D2test_{timestamp}\"\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# -------------------------\n",
    "# 13) Inference once per seed\n",
    "# -------------------------\n",
    "# Produces fixed prediction probabilities used by all threshold evaluations.\n",
    "print(\"\\nRunning inference ONCE per seed (then sweep thresholds on fixed predictions)...\")\n",
    "seed_payloads = []\n",
    "for s in SEEDS:\n",
    "    seed_payloads.append(infer_once(s, chosen_exp, train_dataset_id))\n",
    "\n",
    "print(\"\\nAUROC by seed (ranking metric, threshold-independent):\")\n",
    "for sp in seed_payloads:\n",
    "    print(f\"  seed {sp['seed']}: AUROC={sp['auroc']:.6f}\")\n",
    "auc_mean, auc_sd = mean_sd([sp[\"auroc\"] for sp in seed_payloads])\n",
    "print(f\"Mean AUROC: {auc_mean:.6f} ± {auc_sd:.6f}\")\n",
    "\n",
    "# -------------------------\n",
    "# 14) Threshold sweep across seeds\n",
    "# -------------------------\n",
    "# For each threshold: compute metrics per seed, then take the mean across seeds.\n",
    "rows = []\n",
    "for thr in tqdm(THR_GRID, desc=\"Threshold sweep\", dynamic_ncols=True):\n",
    "    sens_list, absd_list, signd_list = [], [], []\n",
    "    acc_list, prec_list, spec_list, f1_list, mcc_list, p_list = [], [], [], [], [], []\n",
    "\n",
    "    fnr_m_list, fnr_f_list = [], []\n",
    "    n_pd_m_list, n_pd_f_list = [], []\n",
    "\n",
    "    for sp in seed_payloads:\n",
    "        y_true = sp[\"y_true\"]\n",
    "        y_prob = sp[\"y_prob\"]\n",
    "        y_sex  = sp[\"y_sex\"]\n",
    "\n",
    "        tm = threshold_metrics(y_true, y_prob, thr)\n",
    "        fnr_by_sex, delta_signed, delta_abs = fnr_by_sex_signed_delta(y_true, y_prob, y_sex, thr)\n",
    "\n",
    "        sens_list.append(tm[\"sensitivity\"])\n",
    "        acc_list.append(tm[\"accuracy\"])\n",
    "        prec_list.append(tm[\"precision\"])\n",
    "        spec_list.append(tm[\"specificity\"])\n",
    "        f1_list.append(tm[\"f1\"])\n",
    "        mcc_list.append(tm[\"mcc\"])\n",
    "        p_list.append(tm[\"fisher_p_two_sided\"])\n",
    "\n",
    "        signd_list.append(delta_signed)\n",
    "        absd_list.append(delta_abs)\n",
    "\n",
    "        fnr_m_list.append(float(fnr_by_sex[\"M\"][\"fnr\"]))\n",
    "        fnr_f_list.append(float(fnr_by_sex[\"F\"][\"fnr\"]))\n",
    "        n_pd_m_list.append(float(fnr_by_sex[\"M\"][\"n_pos\"]))\n",
    "        n_pd_f_list.append(float(fnr_by_sex[\"F\"][\"n_pos\"]))\n",
    "\n",
    "    row = {\n",
    "        \"threshold\": float(thr),\n",
    "\n",
    "        \"mean_sensitivity\": safe_nanmean(sens_list),\n",
    "        \"sd_sensitivity\": safe_nansd(sens_list),\n",
    "\n",
    "        \"mean_specificity\": safe_nanmean(spec_list),\n",
    "        \"sd_specificity\": safe_nansd(spec_list),\n",
    "\n",
    "        \"mean_abs_deltaFNR\": safe_nanmean(absd_list),\n",
    "        \"sd_abs_deltaFNR\": safe_nansd(absd_list),\n",
    "\n",
    "        \"mean_signed_deltaFNR_F_minus_M\": safe_nanmean(signd_list),\n",
    "        \"sd_signed_deltaFNR\": safe_nansd(signd_list),\n",
    "\n",
    "        \"mean_accuracy\": safe_nanmean(acc_list),\n",
    "        \"mean_precision\": safe_nanmean(prec_list),\n",
    "        \"mean_f1\": safe_nanmean(f1_list),\n",
    "        \"mean_mcc\": safe_nanmean(mcc_list),\n",
    "        \"mean_fisher_p_two_sided\": safe_nanmean(p_list),\n",
    "\n",
    "        \"mean_FNR_M\": safe_nanmean(fnr_m_list),\n",
    "        \"mean_FNR_F\": safe_nanmean(fnr_f_list),\n",
    "        \"n_PD_M_each_seed\": json.dumps({str(SEEDS[i]): int(n_pd_m_list[i]) for i in range(len(SEEDS))}),\n",
    "        \"n_PD_F_each_seed\": json.dumps({str(SEEDS[i]): int(n_pd_f_list[i]) for i in range(len(SEEDS))}),\n",
    "    }\n",
    "\n",
    "    # Stores per-seed values to explain the means later.\n",
    "    row[\"sensitivity_by_seed\"] = json.dumps({str(sp[\"seed\"]): float(sens_list[i]) for i, sp in enumerate(seed_payloads)})\n",
    "    row[\"specificity_by_seed\"] = json.dumps({str(sp[\"seed\"]): float(spec_list[i]) for i, sp in enumerate(seed_payloads)})\n",
    "    row[\"abs_deltaFNR_by_seed\"] = json.dumps({str(sp[\"seed\"]): float(absd_list[i]) for i, sp in enumerate(seed_payloads)})\n",
    "    row[\"signed_deltaFNR_by_seed\"] = json.dumps({str(sp[\"seed\"]): float(signd_list[i]) for i, sp in enumerate(seed_payloads)})\n",
    "\n",
    "    rows.append(row)\n",
    "\n",
    "sweep_df = pd.DataFrame(rows)\n",
    "\n",
    "# -------------------------\n",
    "# 15) Pick threshold using Policy B+\n",
    "# -------------------------\n",
    "# 1) If both constraints are reachable: choose the eligible threshold with the smallest mean |ΔFNR|.\n",
    "# 2) Otherwise: choose the threshold with the smallest total shortfall from the constraints, and report the gaps.\n",
    "max_mean_sens = float(sweep_df[\"mean_sensitivity\"].max())\n",
    "thr_at_max_sens = float(sweep_df.loc[sweep_df[\"mean_sensitivity\"].idxmax(), \"threshold\"])\n",
    "\n",
    "max_mean_spec = float(sweep_df[\"mean_specificity\"].max())\n",
    "thr_at_max_spec = float(sweep_df.loc[sweep_df[\"mean_specificity\"].idxmax(), \"threshold\"])\n",
    "\n",
    "eligible = sweep_df[\n",
    "    (sweep_df[\"mean_sensitivity\"] >= TARGET_SENS) &\n",
    "    (sweep_df[\"mean_specificity\"] >= MIN_SPEC)\n",
    "].copy()\n",
    "\n",
    "constraint_reached = (len(eligible) > 0)\n",
    "\n",
    "def _constraint_status_at_row(row_dict):\n",
    "    sens_gap = float(TARGET_SENS - row_dict[\"mean_sensitivity\"])\n",
    "    spec_gap = float(MIN_SPEC - row_dict[\"mean_specificity\"])\n",
    "    return {\n",
    "        \"sens_ok\": bool(row_dict[\"mean_sensitivity\"] >= TARGET_SENS),\n",
    "        \"spec_ok\": bool(row_dict[\"mean_specificity\"] >= MIN_SPEC),\n",
    "        \"sens_gap_needed\": float(max(0.0, sens_gap)),\n",
    "        \"spec_gap_needed\": float(max(0.0, spec_gap)),\n",
    "    }\n",
    "\n",
    "if constraint_reached:\n",
    "    eligible = eligible.sort_values(\n",
    "        by=[\"mean_abs_deltaFNR\", \"mean_sensitivity\", \"mean_specificity\", \"threshold\"],\n",
    "        ascending=[True, False, False, True]\n",
    "    )\n",
    "    chosen = eligible.iloc[0].to_dict()\n",
    "\n",
    "    policy_note = (\n",
    "        \"Constraints reached:\\n\"\n",
    "        f\"  - mean(sensitivity) >= {TARGET_SENS:.2f}\\n\"\n",
    "        f\"  - mean(specificity) >= {MIN_SPEC:.2f}\\n\"\n",
    "        \"Chosen threshold minimizes mean(|ΔFNR|) among eligible thresholds \"\n",
    "        \"(tie-breakers: higher sensitivity, higher specificity, then lower threshold).\"\n",
    "    )\n",
    "else:\n",
    "    can_reach_sens = bool((sweep_df[\"mean_sensitivity\"] >= TARGET_SENS).any())\n",
    "    can_reach_spec = bool((sweep_df[\"mean_specificity\"] >= MIN_SPEC).any())\n",
    "\n",
    "    working_df = sweep_df.copy()\n",
    "    subset = working_df.copy()\n",
    "    if can_reach_sens:\n",
    "        subset = subset[subset[\"mean_sensitivity\"] >= TARGET_SENS].copy()\n",
    "    if can_reach_spec:\n",
    "        subset = subset[subset[\"mean_specificity\"] >= MIN_SPEC].copy()\n",
    "\n",
    "    if len(subset) == 0:\n",
    "        subset = working_df.copy()\n",
    "\n",
    "    subset[\"_sens_gap\"] = np.maximum(0.0, TARGET_SENS - subset[\"mean_sensitivity\"])\n",
    "    subset[\"_spec_gap\"] = np.maximum(0.0, MIN_SPEC - subset[\"mean_specificity\"])\n",
    "    subset[\"_total_gap\"] = subset[\"_sens_gap\"] + subset[\"_spec_gap\"]\n",
    "\n",
    "    subset = subset.sort_values(\n",
    "        by=[\"_total_gap\", \"mean_abs_deltaFNR\", \"mean_sensitivity\", \"mean_specificity\", \"threshold\"],\n",
    "        ascending=[True, True, False, False, True]\n",
    "    )\n",
    "\n",
    "    chosen = subset.iloc[0].drop(labels=[\"_sens_gap\", \"_spec_gap\", \"_total_gap\"]).to_dict()\n",
    "    status = _constraint_status_at_row(chosen)\n",
    "\n",
    "    failed_parts = []\n",
    "    if not can_reach_sens:\n",
    "        failed_parts.append(\n",
    "            f\"mean(sensitivity) >= {TARGET_SENS:.2f} (UNREACHABLE on this grid; max was {max_mean_sens:.6f} at thr={thr_at_max_sens:.4f})\"\n",
    "        )\n",
    "    if not can_reach_spec:\n",
    "        failed_parts.append(\n",
    "            f\"mean(specificity) >= {MIN_SPEC:.2f} (UNREACHABLE on this grid; max was {max_mean_spec:.6f} at thr={thr_at_max_spec:.4f})\"\n",
    "        )\n",
    "    if not failed_parts:\n",
    "        failed_parts.append(\n",
    "            \"Both constraints are individually reachable, but no single threshold meets BOTH at the same time on this grid.\"\n",
    "        )\n",
    "\n",
    "    policy_note = (\n",
    "        \"Constraints NOT jointly reachable.\\n\"\n",
    "        \"What failed:\\n\"\n",
    "        \"  - \" + \"\\n  - \".join(failed_parts) + \"\\n\"\n",
    "        \"Returned the threshold that minimizes total constraint violation (sum of gaps), then minimizes mean(|ΔFNR|), \"\n",
    "        \"then prefers higher sensitivity and higher specificity.\\n\"\n",
    "        f\"At the chosen threshold, remaining gaps are:\\n\"\n",
    "        f\"  - sensitivity gap needed: {status['sens_gap_needed']:.6f}\\n\"\n",
    "        f\"  - specificity gap needed: {status['spec_gap_needed']:.6f}\"\n",
    "    )\n",
    "\n",
    "print(\"\\n================ POLICY RESULT ================\")\n",
    "print(POLICY_TEXT)\n",
    "print(\"Constraint reached (both)?:\", bool(constraint_reached))\n",
    "print(\"Max achievable mean(sensitivity) on grid:\", f\"{max_mean_sens:.6f}\", \"| at threshold:\", f\"{thr_at_max_sens:.4f}\")\n",
    "print(\"Max achievable mean(specificity) on grid:\", f\"{max_mean_spec:.6f}\", \"| at threshold:\", f\"{thr_at_max_spec:.4f}\")\n",
    "\n",
    "print(\"\\nChosen threshold:\", f\"{float(chosen['threshold']):.4f}\")\n",
    "print(\"Chosen mean(sensitivity):\", f\"{float(chosen['mean_sensitivity']):.6f}\")\n",
    "print(\"Chosen mean(specificity):\", f\"{float(chosen['mean_specificity']):.6f}\")\n",
    "print(\"Chosen mean(|ΔFNR|):\", f\"{float(chosen['mean_abs_deltaFNR']):.6f}\")\n",
    "print(\"Chosen mean signed ΔFNR (F-M):\", f\"{float(chosen['mean_signed_deltaFNR_F_minus_M']):.6f}\")\n",
    "\n",
    "print(\"\\nDetails (no black box):\")\n",
    "print(policy_note)\n",
    "\n",
    "# -------------------------\n",
    "# 16) Save sweep results\n",
    "# -------------------------\n",
    "# Writes the full sweep table (all thresholds) and a compact JSON summary (chosen threshold + key stats).\n",
    "sweep_csv = OUT_DIR / \"sweep_table.csv\"\n",
    "sweep_df.to_csv(sweep_csv, index=False)\n",
    "\n",
    "summary = {\n",
    "    \"train_dataset\": \"D7\",\n",
    "    \"test_dataset\": \"D2\",\n",
    "    \"split_swept\": \"D2 TEST\",\n",
    "    \"required_exp_substring_case_insensitive\": REQUIRED_EXP_SUBSTRING,\n",
    "    \"trainval_experiment_used\": str(chosen_exp),\n",
    "    \"trainval_summary_path\": str(summary_trainval_path),\n",
    "    \"seeds\": SEEDS,\n",
    "    \"policy\": POLICY_TEXT,\n",
    "    \"constraints\": {\n",
    "        \"target_mean_sensitivity\": float(TARGET_SENS),\n",
    "        \"min_mean_specificity\": float(MIN_SPEC),\n",
    "    },\n",
    "    \"constraint_reached_both\": bool(constraint_reached),\n",
    "    \"max_mean_sensitivity_on_grid\": float(max_mean_sens),\n",
    "    \"threshold_at_max_mean_sensitivity\": float(thr_at_max_sens),\n",
    "    \"max_mean_specificity_on_grid\": float(max_mean_spec),\n",
    "    \"threshold_at_max_mean_specificity\": float(thr_at_max_spec),\n",
    "    \"youdenJ_mean_threshold_reference\": (None if np.isnan(thr_mean) else float(thr_mean)),\n",
    "    \"chosen_threshold\": float(chosen[\"threshold\"]),\n",
    "    \"chosen_metrics\": {\n",
    "        \"mean_sensitivity\": float(chosen[\"mean_sensitivity\"]),\n",
    "        \"mean_specificity\": float(chosen[\"mean_specificity\"]),\n",
    "        \"mean_abs_deltaFNR\": float(chosen[\"mean_abs_deltaFNR\"]),\n",
    "        \"mean_signed_deltaFNR_F_minus_M\": float(chosen[\"mean_signed_deltaFNR_F_minus_M\"]),\n",
    "        \"mean_accuracy\": float(chosen[\"mean_accuracy\"]),\n",
    "        \"mean_precision\": float(chosen[\"mean_precision\"]),\n",
    "        \"mean_f1\": float(chosen[\"mean_f1\"]),\n",
    "        \"mean_mcc\": float(chosen[\"mean_mcc\"]),\n",
    "        \"mean_fisher_p_two_sided\": float(chosen[\"mean_fisher_p_two_sided\"]),\n",
    "        \"mean_FNR_M\": float(chosen[\"mean_FNR_M\"]),\n",
    "        \"mean_FNR_F\": float(chosen[\"mean_FNR_F\"]),\n",
    "        \"sensitivity_by_seed\": chosen.get(\"sensitivity_by_seed\", \"\"),\n",
    "        \"specificity_by_seed\": chosen.get(\"specificity_by_seed\", \"\"),\n",
    "        \"abs_deltaFNR_by_seed\": chosen.get(\"abs_deltaFNR_by_seed\", \"\"),\n",
    "        \"signed_deltaFNR_by_seed\": chosen.get(\"signed_deltaFNR_by_seed\", \"\"),\n",
    "        \"n_PD_M_each_seed\": chosen.get(\"n_PD_M_each_seed\", \"\"),\n",
    "        \"n_PD_F_each_seed\": chosen.get(\"n_PD_F_each_seed\", \"\"),\n",
    "    },\n",
    "    \"policy_note_transparent\": policy_note,\n",
    "    \"paths\": {\n",
    "        \"out_dir\": str(OUT_DIR),\n",
    "        \"sweep_table_csv\": str(sweep_csv),\n",
    "    },\n",
    "    \"timestamp\": timestamp,\n",
    "}\n",
    "\n",
    "summary_json = OUT_DIR / \"sweep_summary.json\"\n",
    "with open(summary_json, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "# -------------------------\n",
    "# 17) Plots\n",
    "# -------------------------\n",
    "# Saves quick visuals for: sensitivity, specificity, |ΔFNR| vs threshold, and the sensitivity–fairness tradeoff.\n",
    "plt.figure()\n",
    "plt.plot(sweep_df[\"threshold\"].values, sweep_df[\"mean_sensitivity\"].values)\n",
    "plt.axhline(TARGET_SENS, linestyle=\"--\")\n",
    "plt.axvline(float(chosen[\"threshold\"]), linestyle=\"--\")\n",
    "plt.xlabel(\"Threshold\")\n",
    "plt.ylabel(\"Mean Sensitivity (across seeds)\")\n",
    "plt.title(f\"Threshold Sweep: Mean Sensitivity vs Threshold (D7 {REQUIRED_EXP_SUBSTRING} → D2 TEST)\")\n",
    "plt.tight_layout()\n",
    "p1 = OUT_DIR / \"sweep_sensitivity_vs_threshold.png\"\n",
    "plt.savefig(p1, dpi=150)\n",
    "plt.close()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(sweep_df[\"threshold\"].values, sweep_df[\"mean_specificity\"].values)\n",
    "plt.axhline(MIN_SPEC, linestyle=\"--\")\n",
    "plt.axvline(float(chosen[\"threshold\"]), linestyle=\"--\")\n",
    "plt.xlabel(\"Threshold\")\n",
    "plt.ylabel(\"Mean Specificity (across seeds)\")\n",
    "plt.title(f\"Threshold Sweep: Mean Specificity vs Threshold (D7 {REQUIRED_EXP_SUBSTRING} → D2 TEST)\")\n",
    "plt.tight_layout()\n",
    "p1b = OUT_DIR / \"sweep_specificity_vs_threshold.png\"\n",
    "plt.savefig(p1b, dpi=150)\n",
    "plt.close()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(sweep_df[\"threshold\"].values, sweep_df[\"mean_abs_deltaFNR\"].values)\n",
    "plt.axvline(float(chosen[\"threshold\"]), linestyle=\"--\")\n",
    "plt.xlabel(\"Threshold\")\n",
    "plt.ylabel(\"Mean |ΔFNR| (across seeds)\")\n",
    "plt.title(f\"Threshold Sweep: Mean |ΔFNR| vs Threshold (D7 {REQUIRED_EXP_SUBSTRING} → D2 TEST)\")\n",
    "plt.tight_layout()\n",
    "p2 = OUT_DIR / \"sweep_abs_deltaFNR_vs_threshold.png\"\n",
    "plt.savefig(p2, dpi=150)\n",
    "plt.close()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(sweep_df[\"mean_sensitivity\"].values, sweep_df[\"mean_abs_deltaFNR\"].values)\n",
    "plt.scatter([float(chosen[\"mean_sensitivity\"])], [float(chosen[\"mean_abs_deltaFNR\"])])\n",
    "plt.axvline(TARGET_SENS, linestyle=\"--\")\n",
    "plt.xlabel(\"Mean Sensitivity (across seeds)\")\n",
    "plt.ylabel(\"Mean |ΔFNR| (across seeds)\")\n",
    "plt.title(f\"Threshold Tradeoff: Sensitivity vs |ΔFNR| (D7 {REQUIRED_EXP_SUBSTRING} → D2 TEST)\")\n",
    "plt.tight_layout()\n",
    "p3 = OUT_DIR / \"sweep_tradeoff.png\"\n",
    "plt.savefig(p3, dpi=150)\n",
    "plt.close()\n",
    "\n",
    "print(\"\\n================ SAVED OUTPUTS ================\")\n",
    "print(\"OUT_DIR:\", str(OUT_DIR))\n",
    "print(\"Saved:\", str(sweep_csv))\n",
    "print(\"Saved:\", str(summary_json))\n",
    "print(\"Saved plots:\")\n",
    "print(\" \", str(p1))\n",
    "print(\" \", str(p1b))\n",
    "print(\" \", str(p2))\n",
    "print(\" \", str(p3))\n",
    "print(\"\\nDone.\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "74fe4623801f4b0b9fa527f7db940730",
      "3ccd12f7dd714f9c938d15a9f55996fc",
      "e5ddfa13c31f4132ba9731af574b6ddb",
      "9031c59cb2e14aecb84d86a7ef49fcb3",
      "af6100fe946348488c0f152681dc8f00",
      "c5a2b508e6e34a07bdc48e779a390066",
      "387c494a493344fb96e921c1e1199f5a",
      "3accda18f2254b6d95e41205324c8c72",
      "21dfc204952b49dd89aaa87b60d1e11f",
      "b2d94020babc4963a19114227c816d00",
      "dae3005bfc154b37aab9eb30b2467a4b",
      "e8711eaf4c69409993049db0ed2f2881",
      "037b43c71363494f8582bc8970a08a31",
      "56f2f121a3504118ac24123b33abc611",
      "00a49d9d271b478d91bf98b7eaa97a20",
      "09a52fc693884473b38b301e0fa849ec",
      "96a15726617d4adc820431c203a0bc34",
      "f719411a527f41349c484b1fed445f30",
      "6ce694d4e40a4136a85936725d46e6da",
      "2b5c988489cb4366b01420364bcdc316",
      "205657f71e854c5387e059a5ba3721a0",
      "f99046a15b6b4fdab99b0b0075262f0d",
      "27eb4a5fe31749e78c83756157d3b567",
      "508378aa0ac744b5ba50df96cb96b8f4",
      "db8acc991c4447e3b717394f8fd55632",
      "4d70d6b6318f459588df8ca785ae9a91",
      "b6c18ca81f534e73ac52d6b75bc3d0e9",
      "f191f9a7340143a98950212208e76fcb",
      "c00cbf8f24954e8099624a5d3923a32e",
      "55b9f4bc611c4218b8ec1cc477d5551a",
      "4ff3e0f1c6ae4539aeacf37188cd9346",
      "96ca8ba3118a4d3c9d5996be78e7e0c7",
      "432af1830cae4fe1adfbe82905a61d4a",
      "4ea8714e85974445ac875bb3cfea486b",
      "f1ecd69599524485b74d18326fb8ca63",
      "237506795bbd4972bf4223f64fd544f2",
      "268d573215dc40618a64ce205a761c8e",
      "387b4958cb3743879de1fb62a270fd84",
      "81390b8d0acd4cdb8945e84f2a72d59e",
      "6ef2f91edef44e849c8cefb554d0994f",
      "b012f6f7ec9047c58b26f0f906e608b8",
      "cf8c94d9b086440dad923219801fadc0",
      "231171f1ec914c67810974ed123780ea",
      "45b2c6fe41a44e3687cac8d43bf9dadd",
      "e52dd72d7fbf477b9387220ee061d048",
      "d49225eb4595484794f53da89ec1160f",
      "f97fcfaa51694de1b65155c6bb1a074e",
      "c1a39030660143748fbd5bceb1ee6ff5",
      "464b0b7b34e04b85bc1a511d53bbd17c",
      "fdafe38a73c74beeab741d05346e917f",
      "d9033bd6915148288604161a7d7adf75",
      "e1b787850d9d44288e3d59e2147d2c96",
      "b6f7a9249c074f21bc98e202ba9708b3",
      "7877157fcd04466cb1b21b6de763498b",
      "cba747f3ce3d4cadb194e2178391e303"
     ]
    },
    "id": "MpOL6SKnYZx4"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "The following cell runs a **post-training threshold selection study** for the **D7 trainEnh3 model evaluated on the D2 test set**, using models that are already trained. No retraining is done. The cell finds the most recent D7 training experiment linked to **trainEnh3**, runs inference once per random seed to get a fixed set of prediction scores, and then studies how performance and sex-based fairness change as the decision threshold varies. The goal is to choose one operating threshold using a clear rule called **Policy B+**, which focuses on fairness while still meeting minimum sensitivity and specificity targets.\n",
    "\n",
    "The cell starts with setup checks to ensure a clean run and to avoid import issues caused by locally named files. It mounts Google Drive if needed, resolves dataset paths from existing variables or safe defaults, and defines all fixed settings used in the study. These include the three random seeds, the speech backbone checkpoint, the required 16 kHz sample rate, batch size, evaluation precision settings, the text used to identify the correct training experiment, and the threshold grid from 0.01 to 0.99. The Policy B+ rules are also defined here: among thresholds that meet minimum mean sensitivity and specificity across seeds, preference is given to the one with the smallest average sex gap in false negative rate.\n",
    "\n",
    "Next, the D2 `manifest_all.csv` file is loaded and checked for required columns and correct dataset identity. The data are filtered to the **test split only**, and basic counts by label and sex are printed. A fail-fast check confirms that all listed audio files exist before any model work begins. Two standardized fields are added for later analysis: a simple task group that separates sustained vowel clips from all other speech tasks, and a normalized sex field with values **M**, **F**, or **UNK**, based on the original D2 labels. A dataset and data loader are then created to load audio files, enforce the expected sample rate, and build attention masks that ignore padded regions, with extra handling to reduce the effect of trailing silence in vowel clips.\n",
    "\n",
    "The cell then finds the correct trained model heads by searching the training and validation runs for the most recent folder whose name includes the trainEnh3 identifier and that contains all required files. The selected experiment folder is printed, and the validation-derived threshold saved during training is read only as a reference. A new timestamped output folder is created so results from this sweep do not overwrite earlier runs.\n",
    "\n",
    "Inference is run once per seed. For each seed, random states are fixed for repeatability, the frozen speech backbone and two task-specific heads are rebuilt, and the saved head weights are loaded. The model is then run on the full D2 test set to produce one probability score per clip. **AUROC is reported for each seed and summarized across seeds** as a threshold-independent performance check. The true labels, predicted probabilities, and sex labels are stored and reused for all threshold analysis.\n",
    "\n",
    "The threshold sweep then evaluates every value in the predefined grid. For each threshold and each seed, standard classification metrics are computed, including accuracy, precision, sensitivity, specificity, F1 score, Matthews correlation coefficient, and Fisher’s exact test p-value. Fairness is measured using false negative rates among Parkinson’s cases, calculated separately for males and females, along with the signed and absolute sex gaps. Results are combined across seeds, keeping both average values and variability, while also preserving per-seed results. All threshold results are collected into a single table.\n",
    "\n",
    "Finally, **Policy B+** is applied in a transparent way. The cell first checks whether any thresholds meet both the sensitivity and specificity requirements. If so, it selects the threshold with the smallest average absolute sex gap, using higher sensitivity and specificity as tie-breakers. If no threshold meets both requirements, it reports which conditions failed and selects the closest available option based on how far the results fall short, again favoring smaller fairness gaps and stronger performance. The chosen threshold and its main metrics are printed with a short explanation.\n",
    "\n",
    "All outputs are written to the timestamped sweep folder. These include the full sweep table, a summary file explaining the threshold choice, and plots showing how sensitivity, specificity, and fairness change with threshold, along with a trade-off curve that highlights the selected operating point."
   ],
   "metadata": {
    "id": "ilDQL3vfs_Ag"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# =========================\n",
    "# Post Hoc Threshold Sweep With Fairness Guardrail\n",
    "# =========================\n",
    "# Runs inference one time per seed using an existing trained D7 trainEnh3 head,\n",
    "# then sweeps decision thresholds on the fixed probabilities to study tradeoffs.\n",
    "# Inputs: saved trained heads (per seed) and the D2 test manifest with clip paths.\n",
    "# Outputs: a sweep table, a short JSON summary, and a few PNG plots saved in a new sweep folder.\n",
    "# =========================\n",
    "\n",
    "import os, json, math, random, time, warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import soundfile as sf\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from transformers import Wav2Vec2Model\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix, matthews_corrcoef\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# -------------------------\n",
    "# Safety checks for common Colab import conflicts\n",
    "# -------------------------\n",
    "# Stop early if local files would override real PyTorch or Transformers imports.\n",
    "if os.path.exists(\"/content/torch.py\") or os.path.exists(\"/content/torch/__init__.py\"):\n",
    "    raise RuntimeError(\"Found local /content/torch.py or /content/torch/ that shadows PyTorch. Rename/remove it and restart runtime.\")\n",
    "if os.path.exists(\"/content/transformers.py\") or os.path.exists(\"/content/transformers/__init__.py\"):\n",
    "    raise RuntimeError(\"Found local /content/transformers.py or /content/transformers/ that shadows Hugging Face Transformers. Rename/remove it and restart runtime.\")\n",
    "\n",
    "# -------------------------\n",
    "# Drive mount (best-effort)\n",
    "# -------------------------\n",
    "# Mount Drive if running in Colab and not already mounted.\n",
    "try:\n",
    "    from google.colab import drive  # type: ignore\n",
    "    if not os.path.isdir(\"/content/drive/MyDrive\"):\n",
    "        drive.mount(\"/content/drive\")\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# -------------------------\n",
    "# Root paths used by this run\n",
    "# -------------------------\n",
    "# Use notebook globals if already set, otherwise fall back to the defaults below.\n",
    "D7_OUT_ROOT_FALLBACK = \"/content/drive/MyDrive/AI_PD_Project/Datasets/D7-Multilingual (D1_D4_D5v2_D6)/preprocessed_v1\"\n",
    "D2_OUT_ROOT_FALLBACK = \"/content/drive/MyDrive/AI_PD_Project/Datasets/D2-Slovak (EWA-DB)/EWA-DB/preprocessed_v1\"\n",
    "\n",
    "D7_OUT_ROOT = str(globals().get(\"D7_OUT_ROOT\", D7_OUT_ROOT_FALLBACK))\n",
    "D2_OUT_ROOT = str(globals().get(\"D2_OUT_ROOT\", D2_OUT_ROOT_FALLBACK))\n",
    "D2_MANIFEST_ALL = f\"{D2_OUT_ROOT}/manifests/manifest_all.csv\"\n",
    "\n",
    "# Re-export for downstream cells that may rely on these globals.\n",
    "globals()[\"D7_OUT_ROOT\"] = D7_OUT_ROOT\n",
    "globals()[\"D2_OUT_ROOT\"] = D2_OUT_ROOT\n",
    "\n",
    "# -------------------------\n",
    "# Fixed run settings\n",
    "# -------------------------\n",
    "# Seeds: run inference once per seed, then average metrics across seeds during the sweep.\n",
    "SEEDS          = [1337, 2024, 7777]\n",
    "\n",
    "# Backbone and audio expectations must match training.\n",
    "BACKBONE_CKPT  = \"facebook/wav2vec2-base\"\n",
    "SR_EXPECTED    = 16000\n",
    "TINY_THRESH    = 1e-4\n",
    "\n",
    "# DataLoader and inference settings.\n",
    "PER_DEVICE_BS  = 16\n",
    "NUM_WORKERS    = 0\n",
    "PIN_MEMORY     = False\n",
    "USE_AMP        = True\n",
    "DROPOUT_P      = 0.2\n",
    "\n",
    "# Which trainval experiment to load (matches exp_* folder name).\n",
    "REQUIRED_EXP_SUBSTRING = \"trainEnh3\"  # case-insensitive match against exp_* folder name\n",
    "\n",
    "# Threshold selection rule: prioritize small sex fairness gap while keeping performance acceptable.\n",
    "TARGET_SENS = 0.60\n",
    "MIN_SPEC    = 0.50\n",
    "POLICY_TEXT = (\n",
    "    \"Policy B+: minimize mean(|ΔFNR|) subject to \"\n",
    "    f\"mean(sensitivity) >= {TARGET_SENS:.2f} AND mean(specificity) >= {MIN_SPEC:.2f}\"\n",
    ")\n",
    "\n",
    "# Threshold grid for the sweep.\n",
    "THR_MIN, THR_MAX, THR_STEPS = 0.01, 0.99, 199\n",
    "THR_GRID = np.linspace(THR_MIN, THR_MAX, THR_STEPS).astype(np.float64)\n",
    "\n",
    "# Compute device selection.\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "try:\n",
    "    torch.set_float32_matmul_precision(\"high\")\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# Reduce noisy warnings for cleaner notebook output.\n",
    "warnings.filterwarnings(\"ignore\", message=\"None of the inputs have requires_grad=True. Gradients will be None\")\n",
    "warnings.filterwarnings(\"ignore\", message=\"Passing `gradient_checkpointing` to a config initialization is deprecated\")\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"huggingface_hub\")\n",
    "\n",
    "# Print key settings for quick verification.\n",
    "print(\"D7_OUT_ROOT:\", D7_OUT_ROOT)\n",
    "print(\"D2_OUT_ROOT:\", D2_OUT_ROOT)\n",
    "print(\"D2_MANIFEST_ALL:\", D2_MANIFEST_ALL)\n",
    "print(\"DEVICE:\", DEVICE)\n",
    "if DEVICE.type == \"cuda\":\n",
    "    print(\"GPU:\", torch.cuda.get_device_name(0))\n",
    "print(\"PER_DEVICE_BS:\", PER_DEVICE_BS)\n",
    "print(\"NUM_WORKERS:\", NUM_WORKERS, \"| PIN_MEMORY:\", PIN_MEMORY)\n",
    "print(\"USE_AMP:\", bool(USE_AMP and DEVICE.type == \"cuda\"))\n",
    "print(\"Required exp substring (case-insensitive):\", REQUIRED_EXP_SUBSTRING)\n",
    "print(\"Policy B+ constraints:\")\n",
    "print(\"  mean sensitivity >=\", TARGET_SENS)\n",
    "print(\"  mean specificity >=\", MIN_SPEC)\n",
    "print(f\"Threshold sweep grid: {THR_MIN:.2f}..{THR_MAX:.2f} with {THR_STEPS} steps\")\n",
    "\n",
    "# -------------------------\n",
    "# Load D2 manifest and keep only the test split\n",
    "# -------------------------\n",
    "# Read the manifest, validate required columns, confirm the dataset is D2, then filter to split == \"test\".\n",
    "if not os.path.exists(D2_MANIFEST_ALL):\n",
    "    raise FileNotFoundError(f\"Missing D2 manifest_all.csv: {D2_MANIFEST_ALL}\")\n",
    "\n",
    "m_all = pd.read_csv(D2_MANIFEST_ALL)\n",
    "\n",
    "req_cols = {\"split\", \"clip_path\", \"label_num\", \"task\", \"sex\", \"age\"}\n",
    "missing = [c for c in sorted(req_cols) if c not in m_all.columns]\n",
    "if missing:\n",
    "    raise ValueError(f\"D2 manifest missing required columns: {missing}. Found: {list(m_all.columns)}\")\n",
    "\n",
    "# Identify dataset id when available, then restrict to that dataset block.\n",
    "if \"dataset\" in m_all.columns and m_all[\"dataset\"].notna().any():\n",
    "    d2_dataset_id = str(m_all[\"dataset\"].astype(str).value_counts(dropna=True).idxmax())\n",
    "    m_all = m_all[m_all[\"dataset\"].astype(str) == d2_dataset_id].copy()\n",
    "else:\n",
    "    d2_dataset_id = \"DX\"\n",
    "\n",
    "# Hard guard: prevent accidentally running on the wrong manifest.\n",
    "if d2_dataset_id != \"D2\":\n",
    "    raise RuntimeError(\n",
    "        f\"Expected D2 dataset_id=='D2' but got {d2_dataset_id!r}. \"\n",
    "        \"This usually means D2_OUT_ROOT is wrong or the manifest is not D2. \"\n",
    "        f\"D2_OUT_ROOT={D2_OUT_ROOT}\"\n",
    "    )\n",
    "\n",
    "# Keep a consistent set of columns used by the dataset class.\n",
    "keep_cols = [\"clip_path\", \"label_num\", \"task\", \"speaker_id\", \"sex\", \"age\", \"duration_sec\", \"split\"]\n",
    "for c in keep_cols:\n",
    "    if c not in m_all.columns:\n",
    "        m_all[c] = np.nan\n",
    "m_all = m_all[keep_cols].copy()\n",
    "\n",
    "test_df = m_all[m_all[\"split\"].astype(str) == \"test\"].reset_index(drop=True)\n",
    "\n",
    "print(f\"\\nD2 dataset inferred: {d2_dataset_id}\")\n",
    "print(f\"D2 TEST rows: {len(test_df)}\")\n",
    "print(\"D2 TEST label counts:\", test_df[\"label_num\"].value_counts(dropna=False).to_dict())\n",
    "print(\"D2 TEST sex counts (raw):\", test_df[\"sex\"].value_counts(dropna=False).to_dict())\n",
    "\n",
    "if len(test_df) == 0:\n",
    "    raise RuntimeError(\"After filtering to split=='test', D2 manifest has 0 rows.\")\n",
    "\n",
    "# -------------------------\n",
    "# Fail fast if audio files are missing\n",
    "# -------------------------\n",
    "# Checks a few missing paths quickly (up to 10 examples) before spending time on inference.\n",
    "def _fail_fast_missing_paths(df: pd.DataFrame, name: str):\n",
    "    missing_paths = []\n",
    "    for p in tqdm(df[\"clip_path\"].astype(str).tolist(), desc=f\"Check {name} clip_path exists\", dynamic_ncols=True):\n",
    "        if not os.path.exists(p):\n",
    "            missing_paths.append(p)\n",
    "            if len(missing_paths) >= 10:\n",
    "                break\n",
    "    if missing_paths:\n",
    "        raise FileNotFoundError(f\"{name}: missing clip_path(s). Examples: {missing_paths}\")\n",
    "\n",
    "_fail_fast_missing_paths(test_df, \"D2 TEST\")\n",
    "\n",
    "# -------------------------\n",
    "# Add analysis-friendly columns (task group, normalized sex)\n",
    "# -------------------------\n",
    "# task_group is used to choose the correct head (vowel vs other).\n",
    "def _task_group(task_val) -> str:\n",
    "    return \"vowel\" if str(task_val) == \"vowl\" else \"other\"\n",
    "\n",
    "test_df[\"task_group\"] = test_df[\"task\"].apply(_task_group)\n",
    "\n",
    "# Normalize sex using D2's exact encoding (\"male\"/\"female\"); anything else becomes UNK.\n",
    "def normalize_sex_d2_case_sensitive(val) -> str:\n",
    "    if pd.isna(val):\n",
    "        return \"UNK\"\n",
    "    if val == \"male\":\n",
    "        return \"M\"\n",
    "    if val == \"female\":\n",
    "        return \"F\"\n",
    "    return \"UNK\"\n",
    "\n",
    "test_df[\"sex_norm\"] = test_df[\"sex\"].apply(normalize_sex_d2_case_sensitive)\n",
    "print(\"D2 TEST sex counts (normalized):\", test_df[\"sex_norm\"].value_counts(dropna=False).to_dict())\n",
    "if (test_df[\"sex_norm\"] == \"UNK\").any():\n",
    "    print(\"NOTE: Some D2 'sex' values were not exactly 'male'/'female' and were mapped to 'UNK'.\")\n",
    "\n",
    "# -------------------------\n",
    "# Dataset and collator for variable-length audio\n",
    "# -------------------------\n",
    "# Builds per-clip tensors and an attention mask. For vowel clips, the attention mask ignores trailing padding/silence.\n",
    "class AudioManifestDataset(Dataset):\n",
    "    def __init__(self, df: pd.DataFrame):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        row = self.df.iloc[idx]\n",
    "        clip_path = str(row[\"clip_path\"])\n",
    "        label = int(row[\"label_num\"])\n",
    "        task_group = str(row[\"task_group\"])\n",
    "        sex_norm = str(row[\"sex_norm\"])\n",
    "\n",
    "        if not os.path.exists(clip_path):\n",
    "            raise FileNotFoundError(f\"Missing clip_path: {clip_path}\")\n",
    "\n",
    "        y, sr = sf.read(clip_path, always_2d=False)\n",
    "        if isinstance(y, np.ndarray) and y.ndim > 1:\n",
    "            y = np.mean(y, axis=1)\n",
    "        y = np.asarray(y, dtype=np.float32)\n",
    "\n",
    "        if int(sr) != SR_EXPECTED:\n",
    "            raise ValueError(f\"Sample rate mismatch (expected {SR_EXPECTED}): {clip_path} has sr={sr}\")\n",
    "\n",
    "        # Attention mask marks which samples should count during pooling.\n",
    "        attn = np.ones((len(y),), dtype=np.int64)\n",
    "        if task_group == \"vowel\":\n",
    "            # Find last non-tiny sample, then mask out the remainder as padding/silence.\n",
    "            k = -1\n",
    "            for j in range(len(y) - 1, -1, -1):\n",
    "                if abs(float(y[j])) > TINY_THRESH:\n",
    "                    k = j\n",
    "                    break\n",
    "            if k >= 0:\n",
    "                attn[:k+1] = 1\n",
    "                attn[k+1:] = 0\n",
    "            else:\n",
    "                attn[:] = 1\n",
    "        else:\n",
    "            # Non-vowel clips are treated as fully valid audio.\n",
    "            attn[:] = 1\n",
    "\n",
    "        return {\n",
    "            \"input_values\": torch.from_numpy(y),\n",
    "            \"attention_mask\": torch.from_numpy(attn),\n",
    "            \"labels\": torch.tensor(label, dtype=torch.long),\n",
    "            \"task_group\": task_group,\n",
    "            \"sex_norm\": sex_norm,\n",
    "        }\n",
    "\n",
    "# Pads a batch to the longest clip length and stacks tensors for model input.\n",
    "def collate_fn(batch):\n",
    "    max_len = int(max(b[\"input_values\"].numel() for b in batch))\n",
    "    input_vals, attn_masks, labels, task_groups, sex_norms = [], [], [], [], []\n",
    "    for b in batch:\n",
    "        x = b[\"input_values\"]\n",
    "        a = b[\"attention_mask\"]\n",
    "        pad = max_len - x.numel()\n",
    "        if pad > 0:\n",
    "            x = torch.cat([x, torch.zeros(pad, dtype=x.dtype)], dim=0)\n",
    "            a = torch.cat([a, torch.zeros(pad, dtype=a.dtype)], dim=0)\n",
    "        input_vals.append(x)\n",
    "        attn_masks.append(a)\n",
    "        labels.append(b[\"labels\"])\n",
    "        task_groups.append(b[\"task_group\"])\n",
    "        sex_norms.append(b[\"sex_norm\"])\n",
    "    return {\n",
    "        \"input_values\": torch.stack(input_vals, dim=0),\n",
    "        \"attention_mask\": torch.stack(attn_masks, dim=0),\n",
    "        \"labels\": torch.stack(labels, dim=0),\n",
    "        \"task_group\": task_groups,\n",
    "        \"sex_norm\": sex_norms,\n",
    "    }\n",
    "\n",
    "# DataLoader over the D2 test split.\n",
    "test_ds = AudioManifestDataset(test_df)\n",
    "test_loader = DataLoader(\n",
    "    test_ds,\n",
    "    batch_size=PER_DEVICE_BS,\n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=PIN_MEMORY,\n",
    "    collate_fn=collate_fn,\n",
    ")\n",
    "\n",
    "# -------------------------\n",
    "# Model definition (backbone + two task heads)\n",
    "# -------------------------\n",
    "# Uses a frozen Wav2Vec2 backbone and switches between vowel and other heads per clip.\n",
    "class Wav2Vec2TwoHeadClassifier(nn.Module):\n",
    "    def __init__(self, ckpt: str, dropout_p: float):\n",
    "        super().__init__()\n",
    "        self.backbone = Wav2Vec2Model.from_pretrained(ckpt, use_safetensors=True)\n",
    "        for p in self.backbone.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "        H = int(self.backbone.config.hidden_size)\n",
    "        self.pre_vowel = nn.Sequential(nn.LayerNorm(H), nn.Dropout(float(dropout_p)))\n",
    "        self.pre_other = nn.Sequential(nn.LayerNorm(H), nn.Dropout(float(dropout_p)))\n",
    "        self.head_vowel = nn.Linear(H, 2)\n",
    "        self.head_other = nn.Linear(H, 2)\n",
    "\n",
    "    # Pools frame-level features into one vector per clip, ignoring masked-out regions.\n",
    "    def masked_mean_pool(self, last_hidden, attn_mask_samples):\n",
    "        feat_mask = self.backbone._get_feature_vector_attention_mask(last_hidden.shape[1], attn_mask_samples)\n",
    "        feat_mask = feat_mask.to(last_hidden.device).unsqueeze(-1).type_as(last_hidden)\n",
    "        masked = last_hidden * feat_mask\n",
    "        denom = feat_mask.sum(dim=1).clamp(min=1.0)\n",
    "        return masked.sum(dim=1) / denom\n",
    "\n",
    "    # Runs heads in fp32 to avoid mixed-precision edge cases.\n",
    "    def _heads_fp32(self, x_any: torch.Tensor, head: nn.Module) -> torch.Tensor:\n",
    "        x = x_any.float()\n",
    "        if x.is_cuda:\n",
    "            with torch.amp.autocast(device_type=\"cuda\", enabled=False):\n",
    "                return head(x)\n",
    "        return head(x)\n",
    "\n",
    "    # Produces logits for PD vs HC, choosing the correct head based on task_group.\n",
    "    def forward_logits(self, input_values, attention_mask, task_group):\n",
    "        with torch.no_grad():\n",
    "            out = self.backbone(input_values=input_values, attention_mask=attention_mask)\n",
    "            last_hidden = out.last_hidden_state\n",
    "\n",
    "        pooled = self.masked_mean_pool(last_hidden, attention_mask)\n",
    "        z_v = self.pre_vowel(pooled.float())\n",
    "        z_o = self.pre_other(pooled.float())\n",
    "\n",
    "        logits_v = self._heads_fp32(z_v, self.head_vowel)\n",
    "        logits_o = self._heads_fp32(z_o, self.head_other)\n",
    "\n",
    "        is_vowel = torch.tensor([tg == \"vowel\" for tg in task_group], device=pooled.device, dtype=torch.bool)\n",
    "        logits = logits_o.clone()\n",
    "        if is_vowel.any():\n",
    "            logits[is_vowel] = logits_v[is_vowel]\n",
    "        return logits\n",
    "\n",
    "# Load only the trained head weights from best_heads.pt into the model.\n",
    "def load_heads_into_model(model: Wav2Vec2TwoHeadClassifier, best_heads_path: Path):\n",
    "    if not best_heads_path.exists():\n",
    "        raise FileNotFoundError(f\"Missing best_heads.pt: {str(best_heads_path)}\")\n",
    "    state = torch.load(str(best_heads_path), map_location=\"cpu\")\n",
    "    model.pre_vowel.load_state_dict(state[\"pre_vowel\"], strict=True)\n",
    "    model.pre_other.load_state_dict(state[\"pre_other\"], strict=True)\n",
    "    model.head_vowel.load_state_dict(state[\"head_vowel\"], strict=True)\n",
    "    model.head_other.load_state_dict(state[\"head_other\"], strict=True)\n",
    "    return model\n",
    "\n",
    "# -------------------------\n",
    "# Deterministic setup and one-pass inference per seed\n",
    "# -------------------------\n",
    "# Inference returns fixed probabilities, labels, and sex labels, used later for threshold sweeping.\n",
    "def set_all_seeds(seed: int):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "def infer_once(seed: int, chosen_exp: Path, train_dataset_id: str):\n",
    "    set_all_seeds(seed)\n",
    "    best_heads_path = chosen_exp / f\"run_{train_dataset_id}_seed{seed}\" / \"best_heads.pt\"\n",
    "\n",
    "    model = Wav2Vec2TwoHeadClassifier(BACKBONE_CKPT, dropout_p=DROPOUT_P).to(DEVICE)\n",
    "    model = load_heads_into_model(model, best_heads_path)\n",
    "    model.eval()\n",
    "\n",
    "    use_amp = bool(USE_AMP and DEVICE.type == \"cuda\")\n",
    "    all_probs, all_true, all_sex = [], [], []\n",
    "\n",
    "    # Single forward pass over the test set for this seed.\n",
    "    pbar = tqdm(test_loader, desc=f\"[seed={seed}] Inference D2 TEST\", dynamic_ncols=True)\n",
    "    with torch.inference_mode():\n",
    "        for batch in pbar:\n",
    "            input_values = batch[\"input_values\"].to(DEVICE, non_blocking=False)\n",
    "            attention_mask = batch[\"attention_mask\"].to(DEVICE, non_blocking=False)\n",
    "            labels = batch[\"labels\"].to(DEVICE, non_blocking=False)\n",
    "            task_group = batch[\"task_group\"]\n",
    "            sex_norm = batch[\"sex_norm\"]\n",
    "\n",
    "            with torch.amp.autocast(device_type=\"cuda\", enabled=use_amp):\n",
    "                logits = model.forward_logits(input_values, attention_mask, task_group)\n",
    "\n",
    "            probs = torch.softmax(logits.float(), dim=-1)[:, 1].detach().cpu().numpy().astype(np.float64)\n",
    "            all_probs.extend(probs.tolist())\n",
    "            all_true.extend(labels.detach().cpu().numpy().astype(np.int64).tolist())\n",
    "            all_sex.extend(list(sex_norm))\n",
    "\n",
    "    y_true = np.asarray(all_true, dtype=np.int64)\n",
    "    y_prob = np.asarray(all_probs, dtype=np.float64)\n",
    "    y_sex  = np.asarray(all_sex, dtype=object)\n",
    "\n",
    "    # AUROC is reported for reference since it does not depend on threshold.\n",
    "    auc = float(\"nan\")\n",
    "    if len(np.unique(y_true)) >= 2:\n",
    "        auc = float(roc_auc_score(y_true, y_prob))\n",
    "\n",
    "    return {\n",
    "        \"seed\": int(seed),\n",
    "        \"best_heads_path\": str(best_heads_path),\n",
    "        \"y_true\": y_true,\n",
    "        \"y_prob\": y_prob,\n",
    "        \"y_sex\": y_sex,\n",
    "        \"auroc\": float(auc),\n",
    "    }\n",
    "\n",
    "# -------------------------\n",
    "# Threshold-dependent metrics and fairness helper\n",
    "# -------------------------\n",
    "# Provides standard classification metrics and a sex-based ΔFNR (F minus M) on PD-only cases.\n",
    "def confusion_counts(y_true, y_prob, thr):\n",
    "    y_pred = (y_prob >= float(thr)).astype(np.int64)\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=[0, 1])\n",
    "    TN, FP, FN, TP = int(cm[0, 0]), int(cm[0, 1]), int(cm[1, 0]), int(cm[1, 1])\n",
    "    return TN, FP, FN, TP\n",
    "\n",
    "def threshold_metrics(y_true, y_prob, thr):\n",
    "    TN, FP, FN, TP = confusion_counts(y_true, y_prob, thr)\n",
    "    eps = 1e-12\n",
    "    acc  = (TP + TN) / max(1, (TP + TN + FP + FN))\n",
    "    prec = TP / (TP + FP + eps)\n",
    "    sens = TP / (TP + FN + eps)\n",
    "    spec = TN / (TN + FP + eps)\n",
    "    f1   = 2 * prec * sens / (prec + sens + eps)\n",
    "\n",
    "    # MCC may be undefined if predictions are all one class.\n",
    "    try:\n",
    "        y_pred = (y_prob >= float(thr)).astype(np.int64)\n",
    "        mcc = float(matthews_corrcoef(y_true, y_pred)) if len(np.unique(y_pred)) > 1 else float(\"nan\")\n",
    "    except Exception:\n",
    "        mcc = float(\"nan\")\n",
    "\n",
    "    # Fisher test on the 2x2 confusion table (may fail if scipy not available).\n",
    "    pval = float(\"nan\")\n",
    "    try:\n",
    "        from scipy.stats import fisher_exact  # type: ignore\n",
    "        _, pval = fisher_exact([[TN, FP], [FN, TP]], alternative=\"two-sided\")\n",
    "        pval = float(pval)\n",
    "    except Exception:\n",
    "        pval = float(\"nan\")\n",
    "\n",
    "    return {\n",
    "        \"tn\": TN, \"fp\": FP, \"fn\": FN, \"tp\": TP,\n",
    "        \"accuracy\": float(acc),\n",
    "        \"precision\": float(prec),\n",
    "        \"sensitivity\": float(sens),\n",
    "        \"specificity\": float(spec),\n",
    "        \"f1\": float(f1),\n",
    "        \"mcc\": float(mcc),\n",
    "        \"fisher_p_two_sided\": float(pval),\n",
    "    }\n",
    "\n",
    "def fnr_by_sex_signed_delta(y_true, y_prob, y_sex, thr):\n",
    "    # FNR is computed on true PD cases within each sex: FN / (FN + TP).\n",
    "    y_pred = (y_prob >= float(thr)).astype(np.int64)\n",
    "    out = {}\n",
    "\n",
    "    for g in [\"M\", \"F\"]:\n",
    "        mask_g = (y_sex == g)\n",
    "        pos_mask = mask_g & (y_true == 1)\n",
    "        n_pos = int(pos_mask.sum())\n",
    "        if n_pos == 0:\n",
    "            out[g] = {\"n_pos\": 0, \"fn\": 0, \"tp\": 0, \"fnr\": float(\"nan\")}\n",
    "            continue\n",
    "        tp = int(((y_pred == 1) & pos_mask).sum())\n",
    "        fn = int(((y_pred == 0) & pos_mask).sum())\n",
    "        fnr = float(fn / max(1, (fn + tp)))\n",
    "        out[g] = {\"n_pos\": int(n_pos), \"fn\": int(fn), \"tp\": int(tp), \"fnr\": float(fnr)}\n",
    "\n",
    "    fnr_m = out[\"M\"][\"fnr\"]\n",
    "    fnr_f = out[\"F\"][\"fnr\"]\n",
    "    if (not np.isnan(fnr_m)) and (not np.isnan(fnr_f)):\n",
    "        delta = float(fnr_f - fnr_m)     # ΔFNR = F - M\n",
    "        absd  = float(abs(delta))\n",
    "    else:\n",
    "        delta = float(\"nan\")\n",
    "        absd  = float(\"nan\")\n",
    "\n",
    "    return out, delta, absd\n",
    "\n",
    "# Small utilities to summarize across seeds while tolerating NaNs.\n",
    "def mean_sd(vals):\n",
    "    vals = np.asarray(vals, dtype=np.float64)\n",
    "    mu = float(np.nanmean(vals)) if np.any(~np.isnan(vals)) else float(\"nan\")\n",
    "    sd = float(np.nanstd(vals, ddof=1)) if np.sum(~np.isnan(vals)) > 1 else 0.0\n",
    "    return mu, sd\n",
    "\n",
    "def safe_nanmean(x):\n",
    "    x = np.asarray(x, dtype=np.float64)\n",
    "    if np.any(~np.isnan(x)):\n",
    "        return float(np.nanmean(x))\n",
    "    return float(\"nan\")\n",
    "\n",
    "def safe_nansd(x):\n",
    "    x = np.asarray(x, dtype=np.float64)\n",
    "    if np.sum(~np.isnan(x)) > 1:\n",
    "        return float(np.nanstd(x, ddof=1))\n",
    "    return 0.0\n",
    "\n",
    "# -------------------------\n",
    "# Locate the most recent matching trainval experiment\n",
    "# -------------------------\n",
    "# Finds the newest exp_* folder that matches REQUIRED_EXP_SUBSTRING and contains all needed artifacts.\n",
    "TRAINVAL_ROOT = Path(D7_OUT_ROOT) / \"trainval_runs\"\n",
    "if not TRAINVAL_ROOT.exists():\n",
    "    raise FileNotFoundError(f\"Missing trainval_runs folder under D7_OUT_ROOT: {str(TRAINVAL_ROOT)}\")\n",
    "\n",
    "exp_dirs = sorted([p for p in TRAINVAL_ROOT.glob(\"exp_*\") if p.is_dir()], key=lambda p: p.stat().st_mtime, reverse=True)\n",
    "if not exp_dirs:\n",
    "    raise FileNotFoundError(f\"No exp_* folders found under: {str(TRAINVAL_ROOT)}\")\n",
    "\n",
    "train_dataset_id = \"D7\"\n",
    "\n",
    "def is_match(exp_path: Path, required_substring: str) -> bool:\n",
    "    return required_substring.lower() in exp_path.name.lower()\n",
    "\n",
    "def has_all_seeds_and_summary(exp_path: Path, dataset_id: str, seeds: list) -> bool:\n",
    "    if not (exp_path / \"summary_trainval.json\").exists():\n",
    "        return False\n",
    "    for s in seeds:\n",
    "        p = exp_path / f\"run_{dataset_id}_seed{s}\" / \"best_heads.pt\"\n",
    "        if not p.exists():\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "chosen_exp = None\n",
    "for ed in exp_dirs:\n",
    "    if not is_match(ed, REQUIRED_EXP_SUBSTRING):\n",
    "        continue\n",
    "    if has_all_seeds_and_summary(ed, train_dataset_id, SEEDS):\n",
    "        chosen_exp = ed\n",
    "        break\n",
    "\n",
    "if chosen_exp is None:\n",
    "    examples = [p.name for p in exp_dirs[:10]]\n",
    "    raise FileNotFoundError(\n",
    "        \"Could not find a recent D7 trainval experiment folder that:\\n\"\n",
    "        f\"  (1) contains substring '{REQUIRED_EXP_SUBSTRING}' (case-insensitive) in the exp folder name, and\\n\"\n",
    "        \"  (2) contains all 3 best_heads.pt files + summary_trainval.json.\\n\"\n",
    "        f\"Checked under: {str(TRAINVAL_ROOT)}\\n\\n\"\n",
    "        f\"Example exp_* folder names (most recent first):\\n  - \" + \"\\n  - \".join(examples) + \"\\n\\n\"\n",
    "        \"Fix: set REQUIRED_EXP_SUBSTRING to match the exact tag used in the exp folder name.\"\n",
    "    )\n",
    "\n",
    "print(\"\\nUsing trainval experiment folder:\")\n",
    "print(\" \", str(chosen_exp))\n",
    "\n",
    "# Load the trainval summary only to show the mean Youden-J threshold as a reference point.\n",
    "summary_trainval_path = chosen_exp / \"summary_trainval.json\"\n",
    "with open(summary_trainval_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    trainval_summary = json.load(f)\n",
    "\n",
    "val_opt = ((trainval_summary or {}).get(\"val_optimal_threshold\", {}) or {})\n",
    "thr_mean = float((((val_opt.get(\"mean_sd\", {}) or {}).get(\"mean\", float(\"nan\")))))\n",
    "print(\"\\nTrainval mean val-opt threshold (Youden J) for reference:\")\n",
    "print(\"  summary_trainval.json -> val_optimal_threshold.mean_sd.mean =\", f\"{thr_mean:.6f}\" if not np.isnan(thr_mean) else \"nan\")\n",
    "\n",
    "# -------------------------\n",
    "# Create a new output folder for this sweep run\n",
    "# -------------------------\n",
    "# Writes sweep table, summary JSON, and plots into a timestamped folder to avoid overwrites.\n",
    "TS_ROOT = Path(D7_OUT_ROOT) / \"threshold_sweeps\"\n",
    "TS_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "timestamp = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "OUT_DIR = TS_ROOT / f\"run_D7_{REQUIRED_EXP_SUBSTRING}_on_D2test_{timestamp}\"\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# -------------------------\n",
    "# Inference once per seed (fixed predictions)\n",
    "# -------------------------\n",
    "# Produces y_prob arrays per seed; all later threshold results reuse these probabilities.\n",
    "print(\"\\nRunning inference ONCE per seed (then sweep thresholds on fixed predictions)...\")\n",
    "seed_payloads = []\n",
    "for s in SEEDS:\n",
    "    seed_payloads.append(infer_once(s, chosen_exp, train_dataset_id))\n",
    "\n",
    "print(\"\\nAUROC by seed (ranking metric, threshold-independent):\")\n",
    "for sp in seed_payloads:\n",
    "    print(f\"  seed {sp['seed']}: AUROC={sp['auroc']:.6f}\")\n",
    "auc_mean, auc_sd = mean_sd([sp[\"auroc\"] for sp in seed_payloads])\n",
    "print(f\"Mean AUROC: {auc_mean:.6f} ± {auc_sd:.6f}\")\n",
    "\n",
    "# -------------------------\n",
    "# Sweep thresholds and aggregate across seeds\n",
    "# -------------------------\n",
    "# For each threshold, compute mean sensitivity, mean specificity, and mean |ΔFNR| across seeds.\n",
    "rows = []\n",
    "for thr in tqdm(THR_GRID, desc=\"Threshold sweep\", dynamic_ncols=True):\n",
    "    sens_list, absd_list, signd_list = [], [], []\n",
    "    acc_list, prec_list, spec_list, f1_list, mcc_list, p_list = [], [], [], [], [], []\n",
    "\n",
    "    fnr_m_list, fnr_f_list = [], []\n",
    "    n_pd_m_list, n_pd_f_list = [], []\n",
    "\n",
    "    for sp in seed_payloads:\n",
    "        y_true = sp[\"y_true\"]\n",
    "        y_prob = sp[\"y_prob\"]\n",
    "        y_sex  = sp[\"y_sex\"]\n",
    "\n",
    "        tm = threshold_metrics(y_true, y_prob, thr)\n",
    "        fnr_by_sex, delta_signed, delta_abs = fnr_by_sex_signed_delta(y_true, y_prob, y_sex, thr)\n",
    "\n",
    "        sens_list.append(tm[\"sensitivity\"])\n",
    "        acc_list.append(tm[\"accuracy\"])\n",
    "        prec_list.append(tm[\"precision\"])\n",
    "        spec_list.append(tm[\"specificity\"])\n",
    "        f1_list.append(tm[\"f1\"])\n",
    "        mcc_list.append(tm[\"mcc\"])\n",
    "        p_list.append(tm[\"fisher_p_two_sided\"])\n",
    "\n",
    "        signd_list.append(delta_signed)\n",
    "        absd_list.append(delta_abs)\n",
    "\n",
    "        fnr_m_list.append(float(fnr_by_sex[\"M\"][\"fnr\"]))\n",
    "        fnr_f_list.append(float(fnr_by_sex[\"F\"][\"fnr\"]))\n",
    "        n_pd_m_list.append(float(fnr_by_sex[\"M\"][\"n_pos\"]))\n",
    "        n_pd_f_list.append(float(fnr_by_sex[\"F\"][\"n_pos\"]))\n",
    "\n",
    "    # One row per threshold, with means across seeds and seed-level details stored as JSON strings.\n",
    "    row = {\n",
    "        \"threshold\": float(thr),\n",
    "\n",
    "        \"mean_sensitivity\": safe_nanmean(sens_list),\n",
    "        \"sd_sensitivity\": safe_nansd(sens_list),\n",
    "\n",
    "        \"mean_specificity\": safe_nanmean(spec_list),\n",
    "        \"sd_specificity\": safe_nansd(spec_list),\n",
    "\n",
    "        \"mean_abs_deltaFNR\": safe_nanmean(absd_list),\n",
    "        \"sd_abs_deltaFNR\": safe_nansd(absd_list),\n",
    "\n",
    "        \"mean_signed_deltaFNR_F_minus_M\": safe_nanmean(signd_list),\n",
    "        \"sd_signed_deltaFNR\": safe_nansd(signd_list),\n",
    "\n",
    "        \"mean_accuracy\": safe_nanmean(acc_list),\n",
    "        \"mean_precision\": safe_nanmean(prec_list),\n",
    "        \"mean_f1\": safe_nanmean(f1_list),\n",
    "        \"mean_mcc\": safe_nanmean(mcc_list),\n",
    "        \"mean_fisher_p_two_sided\": safe_nanmean(p_list),\n",
    "\n",
    "        \"mean_FNR_M\": safe_nanmean(fnr_m_list),\n",
    "        \"mean_FNR_F\": safe_nanmean(fnr_f_list),\n",
    "        \"n_PD_M_each_seed\": json.dumps({str(SEEDS[i]): int(n_pd_m_list[i]) for i in range(len(SEEDS))}),\n",
    "        \"n_PD_F_each_seed\": json.dumps({str(SEEDS[i]): int(n_pd_f_list[i]) for i in range(len(SEEDS))}),\n",
    "    }\n",
    "\n",
    "    # Store per-seed values to explain the averages when needed.\n",
    "    row[\"sensitivity_by_seed\"] = json.dumps({str(sp[\"seed\"]): float(sens_list[i]) for i, sp in enumerate(seed_payloads)})\n",
    "    row[\"specificity_by_seed\"] = json.dumps({str(sp[\"seed\"]): float(spec_list[i]) for i, sp in enumerate(seed_payloads)})\n",
    "    row[\"abs_deltaFNR_by_seed\"] = json.dumps({str(sp[\"seed\"]): float(absd_list[i]) for i, sp in enumerate(seed_payloads)})\n",
    "    row[\"signed_deltaFNR_by_seed\"] = json.dumps({str(sp[\"seed\"]): float(signd_list[i]) for i, sp in enumerate(seed_payloads)})\n",
    "\n",
    "    rows.append(row)\n",
    "\n",
    "sweep_df = pd.DataFrame(rows)\n",
    "\n",
    "# -------------------------\n",
    "# Choose a threshold using Policy B+ and explain the decision\n",
    "# -------------------------\n",
    "# Policy B+ prefers thresholds with low mean |ΔFNR| but rejects thresholds that break sensitivity/specificity constraints.\n",
    "max_mean_sens = float(sweep_df[\"mean_sensitivity\"].max())\n",
    "thr_at_max_sens = float(sweep_df.loc[sweep_df[\"mean_sensitivity\"].idxmax(), \"threshold\"])\n",
    "\n",
    "max_mean_spec = float(sweep_df[\"mean_specificity\"].max())\n",
    "thr_at_max_spec = float(sweep_df.loc[sweep_df[\"mean_specificity\"].idxmax(), \"threshold\"])\n",
    "\n",
    "eligible = sweep_df[\n",
    "    (sweep_df[\"mean_sensitivity\"] >= TARGET_SENS) &\n",
    "    (sweep_df[\"mean_specificity\"] >= MIN_SPEC)\n",
    "].copy()\n",
    "\n",
    "constraint_reached = (len(eligible) > 0)\n",
    "\n",
    "# Summarizes which constraint is met and how far below the target the other is (if any).\n",
    "def _constraint_status_at_row(row_dict):\n",
    "    sens_gap = float(TARGET_SENS - row_dict[\"mean_sensitivity\"])\n",
    "    spec_gap = float(MIN_SPEC - row_dict[\"mean_specificity\"])\n",
    "    return {\n",
    "        \"sens_ok\": bool(row_dict[\"mean_sensitivity\"] >= TARGET_SENS),\n",
    "        \"spec_ok\": bool(row_dict[\"mean_specificity\"] >= MIN_SPEC),\n",
    "        \"sens_gap_needed\": float(max(0.0, sens_gap)),\n",
    "        \"spec_gap_needed\": float(max(0.0, spec_gap)),\n",
    "    }\n",
    "\n",
    "if constraint_reached:\n",
    "    # Select best eligible threshold: lowest mean |ΔFNR|, then stronger sensitivity/specificity, then lower threshold.\n",
    "    eligible = eligible.sort_values(\n",
    "        by=[\"mean_abs_deltaFNR\", \"mean_sensitivity\", \"mean_specificity\", \"threshold\"],\n",
    "        ascending=[True, False, False, True]\n",
    "    )\n",
    "    chosen = eligible.iloc[0].to_dict()\n",
    "\n",
    "    policy_note = (\n",
    "        \"Constraints reached:\\n\"\n",
    "        f\"  - mean(sensitivity) >= {TARGET_SENS:.2f}\\n\"\n",
    "        f\"  - mean(specificity) >= {MIN_SPEC:.2f}\\n\"\n",
    "        \"Chosen threshold minimizes mean(|ΔFNR|) among eligible thresholds \"\n",
    "        \"(tie-breakers: higher sensitivity, higher specificity, then lower threshold).\"\n",
    "    )\n",
    "else:\n",
    "    # If constraints cannot be met together, pick the closest option and state the gaps clearly.\n",
    "    can_reach_sens = bool((sweep_df[\"mean_sensitivity\"] >= TARGET_SENS).any())\n",
    "    can_reach_spec = bool((sweep_df[\"mean_specificity\"] >= MIN_SPEC).any())\n",
    "\n",
    "    working_df = sweep_df.copy()\n",
    "    subset = working_df.copy()\n",
    "    if can_reach_sens:\n",
    "        subset = subset[subset[\"mean_sensitivity\"] >= TARGET_SENS].copy()\n",
    "    if can_reach_spec:\n",
    "        subset = subset[subset[\"mean_specificity\"] >= MIN_SPEC].copy()\n",
    "\n",
    "    if len(subset) == 0:\n",
    "        subset = working_df.copy()\n",
    "\n",
    "    subset[\"_sens_gap\"] = np.maximum(0.0, TARGET_SENS - subset[\"mean_sensitivity\"])\n",
    "    subset[\"_spec_gap\"] = np.maximum(0.0, MIN_SPEC - subset[\"mean_specificity\"])\n",
    "    subset[\"_total_gap\"] = subset[\"_sens_gap\"] + subset[\"_spec_gap\"]\n",
    "\n",
    "    subset = subset.sort_values(\n",
    "        by=[\"_total_gap\", \"mean_abs_deltaFNR\", \"mean_sensitivity\", \"mean_specificity\", \"threshold\"],\n",
    "        ascending=[True, True, False, False, True]\n",
    "    )\n",
    "\n",
    "    chosen = subset.iloc[0].drop(labels=[\"_sens_gap\", \"_spec_gap\", \"_total_gap\"]).to_dict()\n",
    "    status = _constraint_status_at_row(chosen)\n",
    "\n",
    "    failed_parts = []\n",
    "    if not can_reach_sens:\n",
    "        failed_parts.append(\n",
    "            f\"mean(sensitivity) >= {TARGET_SENS:.2f} (UNREACHABLE on this grid; max was {max_mean_sens:.6f} at thr={thr_at_max_sens:.4f})\"\n",
    "        )\n",
    "    if not can_reach_spec:\n",
    "        failed_parts.append(\n",
    "            f\"mean(specificity) >= {MIN_SPEC:.2f} (UNREACHABLE on this grid; max was {max_mean_spec:.6f} at thr={thr_at_max_spec:.4f})\"\n",
    "        )\n",
    "    if not failed_parts:\n",
    "        failed_parts.append(\n",
    "            \"Both constraints are individually reachable, but no single threshold meets BOTH at the same time on this grid.\"\n",
    "        )\n",
    "\n",
    "    policy_note = (\n",
    "        \"Constraints NOT jointly reachable.\\n\"\n",
    "        \"What failed:\\n\"\n",
    "        \"  - \" + \"\\n  - \".join(failed_parts) + \"\\n\"\n",
    "        \"Returned the threshold that minimizes total constraint violation (sum of gaps), then minimizes mean(|ΔFNR|), \"\n",
    "        \"then prefers higher sensitivity and higher specificity.\\n\"\n",
    "        f\"At the chosen threshold, remaining gaps are:\\n\"\n",
    "        f\"  - sensitivity gap needed: {status['sens_gap_needed']:.6f}\\n\"\n",
    "        f\"  - specificity gap needed: {status['spec_gap_needed']:.6f}\"\n",
    "    )\n",
    "\n",
    "# Console output stays as-is; comments only clarify the intent.\n",
    "print(\"\\n================ POLICY RESULT ================\")\n",
    "print(POLICY_TEXT)\n",
    "print(\"Constraint reached (both)?:\", bool(constraint_reached))\n",
    "print(\"Max achievable mean(sensitivity) on grid:\", f\"{max_mean_sens:.6f}\", \"| at threshold:\", f\"{thr_at_max_sens:.4f}\")\n",
    "print(\"Max achievable mean(specificity) on grid:\", f\"{max_mean_spec:.6f}\", \"| at threshold:\", f\"{thr_at_max_spec:.4f}\")\n",
    "\n",
    "print(\"\\nChosen threshold:\", f\"{float(chosen['threshold']):.4f}\")\n",
    "print(\"Chosen mean(sensitivity):\", f\"{float(chosen['mean_sensitivity']):.6f}\")\n",
    "print(\"Chosen mean(specificity):\", f\"{float(chosen['mean_specificity']):.6f}\")\n",
    "print(\"Chosen mean(|ΔFNR|):\", f\"{float(chosen['mean_abs_deltaFNR']):.6f}\")\n",
    "print(\"Chosen mean signed ΔFNR (F-M):\", f\"{float(chosen['mean_signed_deltaFNR_F_minus_M']):.6f}\")\n",
    "\n",
    "print(\"\\nDetails:\")\n",
    "print(policy_note)\n",
    "\n",
    "# -------------------------\n",
    "# Save sweep table and a compact summary JSON\n",
    "# -------------------------\n",
    "# Writes the full sweep grid as CSV, plus a summary JSON with the chosen threshold and key metrics.\n",
    "sweep_csv = OUT_DIR / \"sweep_table.csv\"\n",
    "sweep_df.to_csv(sweep_csv, index=False)\n",
    "\n",
    "summary = {\n",
    "    \"train_dataset\": \"D7\",\n",
    "    \"test_dataset\": \"D2\",\n",
    "    \"split_swept\": \"D2 TEST\",\n",
    "    \"required_exp_substring_case_insensitive\": REQUIRED_EXP_SUBSTRING,\n",
    "    \"trainval_experiment_used\": str(chosen_exp),\n",
    "    \"trainval_summary_path\": str(summary_trainval_path),\n",
    "    \"seeds\": SEEDS,\n",
    "    \"policy\": POLICY_TEXT,\n",
    "    \"constraints\": {\n",
    "        \"target_mean_sensitivity\": float(TARGET_SENS),\n",
    "        \"min_mean_specificity\": float(MIN_SPEC),\n",
    "    },\n",
    "    \"constraint_reached_both\": bool(constraint_reached),\n",
    "    \"max_mean_sensitivity_on_grid\": float(max_mean_sens),\n",
    "    \"threshold_at_max_mean_sensitivity\": float(thr_at_max_sens),\n",
    "    \"max_mean_specificity_on_grid\": float(max_mean_spec),\n",
    "    \"threshold_at_max_mean_specificity\": float(thr_at_max_spec),\n",
    "    \"youdenJ_mean_threshold_reference\": (None if np.isnan(thr_mean) else float(thr_mean)),\n",
    "    \"chosen_threshold\": float(chosen[\"threshold\"]),\n",
    "    \"chosen_metrics\": {\n",
    "        \"mean_sensitivity\": float(chosen[\"mean_sensitivity\"]),\n",
    "        \"mean_specificity\": float(chosen[\"mean_specificity\"]),\n",
    "        \"mean_abs_deltaFNR\": float(chosen[\"mean_abs_deltaFNR\"]),\n",
    "        \"mean_signed_deltaFNR_F_minus_M\": float(chosen[\"mean_signed_deltaFNR_F_minus_M\"]),\n",
    "        \"mean_accuracy\": float(chosen[\"mean_accuracy\"]),\n",
    "        \"mean_precision\": float(chosen[\"mean_precision\"]),\n",
    "        \"mean_f1\": float(chosen[\"mean_f1\"]),\n",
    "        \"mean_mcc\": float(chosen[\"mean_mcc\"]),\n",
    "        \"mean_fisher_p_two_sided\": float(chosen[\"mean_fisher_p_two_sided\"]),\n",
    "        \"mean_FNR_M\": float(chosen[\"mean_FNR_M\"]),\n",
    "        \"mean_FNR_F\": float(chosen[\"mean_FNR_F\"]),\n",
    "        \"sensitivity_by_seed\": chosen.get(\"sensitivity_by_seed\", \"\"),\n",
    "        \"specificity_by_seed\": chosen.get(\"specificity_by_seed\", \"\"),\n",
    "        \"abs_deltaFNR_by_seed\": chosen.get(\"abs_deltaFNR_by_seed\", \"\"),\n",
    "        \"signed_deltaFNR_by_seed\": chosen.get(\"signed_deltaFNR_by_seed\", \"\"),\n",
    "        \"n_PD_M_each_seed\": chosen.get(\"n_PD_M_each_seed\", \"\"),\n",
    "        \"n_PD_F_each_seed\": chosen.get(\"n_PD_F_each_seed\", \"\"),\n",
    "    },\n",
    "    \"policy_note_transparent\": policy_note,\n",
    "    \"paths\": {\n",
    "        \"out_dir\": str(OUT_DIR),\n",
    "        \"sweep_table_csv\": str(sweep_csv),\n",
    "    },\n",
    "    \"timestamp\": timestamp,\n",
    "}\n",
    "\n",
    "summary_json = OUT_DIR / \"sweep_summary.json\"\n",
    "with open(summary_json, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "# -------------------------\n",
    "# Plots for quick visual checks\n",
    "# -------------------------\n",
    "# Saves four simple plots: sensitivity vs threshold, specificity vs threshold, |ΔFNR| vs threshold, and the tradeoff curve.\n",
    "plt.figure()\n",
    "plt.plot(sweep_df[\"threshold\"].values, sweep_df[\"mean_sensitivity\"].values)\n",
    "plt.axhline(TARGET_SENS, linestyle=\"--\")\n",
    "plt.axvline(float(chosen[\"threshold\"]), linestyle=\"--\")\n",
    "plt.xlabel(\"Threshold\")\n",
    "plt.ylabel(\"Mean Sensitivity (across seeds)\")\n",
    "plt.title(\"Threshold Sweep: Mean Sensitivity vs Threshold (D7 trainEnh3 → D2 TEST)\")\n",
    "plt.tight_layout()\n",
    "p1 = OUT_DIR / \"sweep_sensitivity_vs_threshold.png\"\n",
    "plt.savefig(p1, dpi=150)\n",
    "plt.close()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(sweep_df[\"threshold\"].values, sweep_df[\"mean_specificity\"].values)\n",
    "plt.axhline(MIN_SPEC, linestyle=\"--\")\n",
    "plt.axvline(float(chosen[\"threshold\"]), linestyle=\"--\")\n",
    "plt.xlabel(\"Threshold\")\n",
    "plt.ylabel(\"Mean Specificity (across seeds)\")\n",
    "plt.title(\"Threshold Sweep: Mean Specificity vs Threshold (D7 trainEnh3 → D2 TEST)\")\n",
    "plt.tight_layout()\n",
    "p1b = OUT_DIR / \"sweep_specificity_vs_threshold.png\"\n",
    "plt.savefig(p1b, dpi=150)\n",
    "plt.close()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(sweep_df[\"threshold\"].values, sweep_df[\"mean_abs_deltaFNR\"].values)\n",
    "plt.axvline(float(chosen[\"threshold\"]), linestyle=\"--\")\n",
    "plt.xlabel(\"Threshold\")\n",
    "plt.ylabel(\"Mean |ΔFNR| (across seeds)\")\n",
    "plt.title(\"Threshold Sweep: Mean |ΔFNR| vs Threshold (D7 trainEnh3 → D2 TEST)\")\n",
    "plt.tight_layout()\n",
    "p2 = OUT_DIR / \"sweep_abs_deltaFNR_vs_threshold.png\"\n",
    "plt.savefig(p2, dpi=150)\n",
    "plt.close()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(sweep_df[\"mean_sensitivity\"].values, sweep_df[\"mean_abs_deltaFNR\"].values)\n",
    "plt.scatter([float(chosen[\"mean_sensitivity\"])], [float(chosen[\"mean_abs_deltaFNR\"])])\n",
    "plt.axvline(TARGET_SENS, linestyle=\"--\")\n",
    "plt.xlabel(\"Mean Sensitivity (across seeds)\")\n",
    "plt.ylabel(\"Mean |ΔFNR| (across seeds)\")\n",
    "plt.title(\"Threshold Tradeoff: Sensitivity vs |ΔFNR| (D7 trainEnh3 → D2 TEST)\")\n",
    "plt.tight_layout()\n",
    "p3 = OUT_DIR / \"sweep_tradeoff.png\"\n",
    "plt.savefig(p3, dpi=150)\n",
    "plt.close()\n",
    "\n",
    "# Final file list for quick confirmation.\n",
    "print(\"\\n================ SAVED OUTPUTS ================\")\n",
    "print(\"OUT_DIR:\", str(OUT_DIR))\n",
    "print(\"Saved:\", str(sweep_csv))\n",
    "print(\"Saved:\", str(summary_json))\n",
    "print(\"Saved plots:\")\n",
    "print(\" \", str(p1))\n",
    "print(\" \", str(p1b))\n",
    "print(\" \", str(p2))\n",
    "print(\" \", str(p3))\n",
    "print(\"\\nDone.\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "3e5460c445594caa9b5a307a03f9dfda",
      "9b8bf32dd26f4727b644338534d397db",
      "7a096d00ea6b4e9fa2fa958290955470",
      "dd41e288ff9c4c2baa406d5064e9de66",
      "0fcd2dabeb4c4794a4ce540305f24f08",
      "4e9dd0a505be46fa91e2107cb142fb73",
      "8d91bf3dea2d4e92a18a6db32d27bf85",
      "51236fab0bdd4ba1830c0ab66b3efd98",
      "9b1628c86df541bb895d89ee114dfe9d",
      "20fa4884a344429689a0c54869c13154",
      "1d35a28036364f879c41b015b7b20e46",
      "31cf7b39d5c54c2b88ae61b2b2e017a6",
      "ad07af955a924566b34944cefc009ad1",
      "50db36a457e8457eb208b401458019fa",
      "544173b23e694d03b3bff5f3bb1b901a",
      "f0549cad233442fbb7ddd670247c7116",
      "8a7fe2a9d20f4b669cff06977a1ed891",
      "67810049b2034cb2a91667cb801f1df2",
      "6a35a59bdf104f669629439dd638df45",
      "46073dd935a344a48df847716f6a4d96",
      "3618134c35d54364ba110c35d0baebc0",
      "83d83b19407942e8b28cda6527230249",
      "c462cf9496bf459c9c01803efacc38cd",
      "416c9667919e405f83c64202ac5f73cf",
      "92877cd5f9694a6f913eb3e8e6ba0cd8",
      "fdd646fc4dc94a02ae5ccf118c0a0d54",
      "ccec1ea6fff9482daf477315fee8979a",
      "2d73952c446c482fabccdbde08f73054",
      "738d1584998544f4851a245a91e89698",
      "a84c4a92225749a2ad04a60ade47f4bc",
      "c514d86547484dd1b8545a0d05b04825",
      "c449c32c392c4036bf6541a6f683c01a",
      "b81feecdcf2543d1ac988d76a9817983",
      "2d2ea5303dc7473fbe39823c34e7b8de",
      "4068d86c33ec457eb0a3631573c9ef98",
      "04eb6f701486413580dd4a9fb6771df2",
      "3af4e5c577834abda86a4d1ef2ad18fd",
      "d30a63da00034cdeb4d70217362f9547",
      "164ea840b86b421599ef661e0792e243",
      "da4ad9d723cf4ac89a1c348637943e2f",
      "2ef942f4a3d248578520567558764721",
      "09513955d38c4d489d4fc71de6f95ec3",
      "1d6feacf25b34eecaa94e22e4ec9ce0a",
      "f3a5da946adf411b9df058eb962b4bed",
      "4aa666f77aae4746b9979895a909e1e6",
      "8b90b6a8f48f4d35a788617efc6253f3",
      "7ae4baffaf0f40c9ab0d555670383e37",
      "e2a1a7a5a6c84df693f4eb6695098bd2",
      "d831d63e31fd4dc9a7205487a2f4a6f2",
      "af879e3fafdd4c55a9b06015d7f3f42e",
      "5840e98eee8c409e9014a1e8d0dbf8cf",
      "6fde7cd2da194704893828e014bb7c8f",
      "e359d35a47384d56a6c9887a217ae436",
      "6c1af526c7cc4e619c11eff0cdcee75d",
      "6d4aaf7b6ade4a19b648c122121e5ad1"
     ]
    },
    "id": "K9xwCoMGZLEh"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "#Ablation 2: Task-Head Ablation: Vowel vs Non-Vowel Speech Contributions"
   ],
   "metadata": {
    "id": "qZHKAnjA8e-v"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The following cell prepares the **mechanistic evaluation** workspace and builds a dependable run registry so subsequent analysis cells can find the correct prediction files without guessing folder names. It mounts Google Drive if needed, defines the main dataset roots for D7 and D2, and lists the three fixed random seeds used in all test runs. It also sets a single shared output directory, `.../mechanistic_eval/`, where each model condition (base, trainEnh1, trainEnh2, trainEnh3) will have its own subfolder.\n",
    "\n",
    "Next, the cell creates a small registry called `RUNS` that points to one `tag_run_pointer.json` file for each condition. These pointer files act as stable identifiers for the runs. A helper function reads each pointer file and reliably determines the expected `predictions.csv` path for each seed, without scanning folders. It follows a clear order: it first looks for an explicit `predictions_csv_by_seed` entry, then a `runs` dictionary with per-seed information, then a `seed_run_dirs` list, and finally falls back to a standard path under the tag folder (`TAG_ROOT/run_D7_on_D2test_seed{seed}/predictions.csv`). This fallback allows the base run to work even if the pointer file contains only minimal information.\n",
    "\n",
    "For each condition, the cell creates the matching output directory under `mechanistic_eval/<tag>/` and prints a clear summary that shows the pointer file path, the resolved tag and run roots, and the resolved `predictions.csv` path for each seed. It then checks that every resolved `predictions.csv` file exists. If any file is missing, it prints the exact seed and path that failed and stops with an error. When all checks pass, the cell builds a final dictionary called `RUNS_RESOLVED`. For each condition, this dictionary stores the pointer path, resolved roots, per-seed prediction file paths, and the output folder for mechanistic evaluation. The cell finishes by confirming that `RUNS_RESOLVED` is ready for use by the later scoring and ablation cells.\n"
   ],
   "metadata": {
    "id": "fLbFXT-PrvKz"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# =========================\n",
    "# Mechanistic evaluation run registry (paths + pointer-based resolution)\n",
    "# -------------------------\n",
    "# Purpose\n",
    "# - Centralize condition names and pointer files for base and three enhanced draws\n",
    "# - Resolve the exact per-seed predictions.csv locations without searching folders\n",
    "#\n",
    "# Inputs\n",
    "# - Root folders for D7 outputs and D2 dataset\n",
    "# - One tag_run_pointer.json per condition\n",
    "#\n",
    "# Outputs\n",
    "# - MECH_EVAL_ROOT folder created (one subfolder per condition)\n",
    "# - RUNS_RESOLVED dictionary populated and printed (per-seed predictions.csv paths)\n",
    "# =========================\n",
    "\n",
    "import os, json\n",
    "from pathlib import Path\n",
    "\n",
    "# -------------------------\n",
    "# Drive mount (Colab)\n",
    "# -------------------------\n",
    "# Input: Google Drive availability in the runtime\n",
    "# Output: /content/drive/MyDrive becomes accessible for reading and writing files\n",
    "if not os.path.isdir(\"/content/drive/MyDrive\"):\n",
    "    from google.colab import drive  # type: ignore\n",
    "    drive.mount(\"/content/drive\")\n",
    "\n",
    "# -------------------------\n",
    "# Dataset roots\n",
    "# -------------------------\n",
    "# Inputs: D7 output root (contains test runs and trainval runs) and D2 root (contains manifest and clips)\n",
    "# Outputs: used by later cells for reading predictions and writing mechanistic eval artifacts\n",
    "D7_OUT_ROOT = \"/content/drive/MyDrive/AI_PD_Project/Datasets/D7-Multilingual (D1_D4_D5v2_D6)/preprocessed_v1\"\n",
    "D2_OUT_ROOT = \"/content/drive/MyDrive/AI_PD_Project/Datasets/D2-Slovak (EWA-DB)/EWA-DB/preprocessed_v1\"\n",
    "\n",
    "# -------------------------\n",
    "# Seeds used throughout evaluation\n",
    "# -------------------------\n",
    "# Input: fixed seed list\n",
    "# Output: defines which per-seed prediction files must exist for each condition\n",
    "SEEDS = [1337, 2024, 7777]\n",
    "\n",
    "# -------------------------\n",
    "# Condition registry (pointer JSON locations)\n",
    "# -------------------------\n",
    "# Input: one pointer JSON per condition\n",
    "# Output: RUNS dict used to build RUNS_RESOLVED\n",
    "def _latest_pointer_json(search_root: str, required_substrings: list) -> str:\n",
    "    search_root = str(search_root).strip()\n",
    "    root = Path(search_root)\n",
    "    if not root.exists():\n",
    "        raise FileNotFoundError(f\"Search root not found: {search_root}\")\n",
    "\n",
    "    candidates = []\n",
    "    for p in root.rglob(\"tag_run_pointer.json\"):\n",
    "        sp = str(p)\n",
    "        ok = True\n",
    "        for sub in required_substrings:\n",
    "            if sub and (sub.lower() not in sp.lower()):\n",
    "                ok = False\n",
    "                break\n",
    "        if ok:\n",
    "            candidates.append(p)\n",
    "\n",
    "    if not candidates:\n",
    "        req = \", \".join([s for s in required_substrings if s])\n",
    "        raise FileNotFoundError(\n",
    "            f\"No tag_run_pointer.json found under {search_root} matching: [{req}]\"\n",
    "        )\n",
    "\n",
    "    candidates.sort(key=lambda x: x.stat().st_mtime, reverse=True)\n",
    "    return str(candidates[0])\n",
    "\n",
    "RUNS = {\n",
    "    \"base\": {\n",
    "        \"pointer_json\": _latest_pointer_json(\n",
    "            f\"{D7_OUT_ROOT}/multilingual_test_runs\",\n",
    "            [\"base\"],\n",
    "        ),\n",
    "    },\n",
    "    \"trainEnh1\": {\n",
    "        \"pointer_json\": _latest_pointer_json(\n",
    "            f\"{D7_OUT_ROOT}/monolingual_test_runs\",\n",
    "            [\"trainEnh1\"],\n",
    "        ),\n",
    "    },\n",
    "    \"trainEnh2\": {\n",
    "        \"pointer_json\": _latest_pointer_json(\n",
    "            f\"{D7_OUT_ROOT}/monolingual_test_runs\",\n",
    "            [\"trainEnh2\"],\n",
    "        ),\n",
    "    },\n",
    "    \"trainEnh3\": {\n",
    "        \"pointer_json\": _latest_pointer_json(\n",
    "            f\"{D7_OUT_ROOT}/monolingual_test_runs\",\n",
    "            [\"trainEnh3\"],\n",
    "        ),\n",
    "    },\n",
    "}\n",
    "\n",
    "# -------------------------\n",
    "# Shared output root for mechanistic evaluation\n",
    "# -------------------------\n",
    "# Input: D7_OUT_ROOT\n",
    "# Output: MECH_EVAL_ROOT created and used for all downstream artifacts\n",
    "MECH_EVAL_ROOT = f\"{D7_OUT_ROOT}/mechanistic_eval\"\n",
    "os.makedirs(MECH_EVAL_ROOT, exist_ok=True)\n",
    "\n",
    "# -------------------------\n",
    "# Pointer parsing and deterministic path resolution\n",
    "# -------------------------\n",
    "# Inputs: pointer JSON contents + seed list\n",
    "# Output: resolved per-seed predictions.csv paths with no folder searching\n",
    "def _read_json(path: str) -> dict:\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def resolve_predictions_paths_from_pointer(pointer_json_path: str, seeds: list) -> dict:\n",
    "    \"\"\"\n",
    "    Resolve per-seed predictions.csv paths using known pointer schemas.\n",
    "\n",
    "    Inputs\n",
    "    - pointer_json_path: path to tag_run_pointer.json\n",
    "    - seeds: list of expected seeds\n",
    "\n",
    "    Outputs\n",
    "    - run_root: best-effort run root (falls back to tag_root)\n",
    "    - tag_root: folder that contains the pointer file\n",
    "    - predictions_by_seed: dict(seed -> predictions.csv path)\n",
    "\n",
    "    Resolution order:\n",
    "      1) predictions_csv_by_seed\n",
    "      2) runs[seed][\"predictions_csv\"]\n",
    "      3) seed_run_dirs entries (append /predictions.csv)\n",
    "      4) fallback under tag_root/run_D7_on_D2test_seed{seed}/predictions.csv\n",
    "    \"\"\"\n",
    "    ptr = _read_json(pointer_json_path)\n",
    "\n",
    "    pointer_path = Path(pointer_json_path)\n",
    "    TAG_ROOT = pointer_path.parent  # tag folder that owns this pointer file\n",
    "\n",
    "    # Prefer explicit run_root fields when present, else use TAG_ROOT\n",
    "    run_root = (\n",
    "        ptr.get(\"run_root\")\n",
    "        or ptr.get(\"run_dir\")\n",
    "        or ptr.get(\"root\")\n",
    "        or ptr.get(\"test_run_root\")\n",
    "        or None\n",
    "    )\n",
    "    if run_root is None:\n",
    "        run_root = str(TAG_ROOT)\n",
    "\n",
    "    run_root = str(run_root).strip()\n",
    "\n",
    "    predictions_by_seed = {}\n",
    "\n",
    "    # (1) Direct map: predictions_csv_by_seed = {\"1337\": \".../predictions.csv\", ...}\n",
    "    pbs = ptr.get(\"predictions_csv_by_seed\")\n",
    "    if isinstance(pbs, dict):\n",
    "        for s in seeds:\n",
    "            v = pbs.get(str(s))\n",
    "            if isinstance(v, str) and len(v) > 0:\n",
    "                predictions_by_seed[s] = v\n",
    "\n",
    "    # (2) Nested map: runs = {\"1337\": {\"predictions_csv\": \"...\"}, ...}\n",
    "    if not predictions_by_seed:\n",
    "        runs = ptr.get(\"runs\")\n",
    "        if isinstance(runs, dict):\n",
    "            for s in seeds:\n",
    "                d = runs.get(str(s))\n",
    "                if isinstance(d, dict):\n",
    "                    v = d.get(\"predictions_csv\")\n",
    "                    if isinstance(v, str) and len(v) > 0:\n",
    "                        predictions_by_seed[s] = v\n",
    "\n",
    "    # (3) List of per-seed run dirs: seed_run_dirs = [\"...seed1337\", ...]\n",
    "    if len(predictions_by_seed) < len(seeds):\n",
    "        srd = ptr.get(\"seed_run_dirs\")\n",
    "        if isinstance(srd, list):\n",
    "            for item in srd:\n",
    "                if not isinstance(item, str):\n",
    "                    continue\n",
    "                for s in seeds:\n",
    "                    if f\"seed{s}\" in item:\n",
    "                        predictions_by_seed.setdefault(s, str(Path(item) / \"predictions.csv\"))\n",
    "\n",
    "    # (4) Fallback to the standard per-seed folder name under TAG_ROOT\n",
    "    for s in seeds:\n",
    "        if s not in predictions_by_seed:\n",
    "            predictions_by_seed[s] = str(TAG_ROOT / f\"run_D7_on_D2test_seed{s}\" / \"predictions.csv\")\n",
    "\n",
    "    return {\n",
    "        \"run_root\": run_root,\n",
    "        \"tag_root\": str(TAG_ROOT),\n",
    "        \"predictions_by_seed\": predictions_by_seed,\n",
    "    }\n",
    "\n",
    "def _best_existing_predictions_path(pointer_json_path: str, seeds: list) -> dict:\n",
    "    \"\"\"\n",
    "    Resolve predictions.csv paths, but if the pointer's TAG_ROOT does not contain the files,\n",
    "    fall back to searching for the newest folder under the same test-run parent that:\n",
    "      - contains the same tag string (e.g., trainEnh3)\n",
    "      - contains predictions.csv for all requested seeds\n",
    "    \"\"\"\n",
    "    resolved = resolve_predictions_paths_from_pointer(pointer_json_path, seeds)\n",
    "\n",
    "    # If all exist, done\n",
    "    all_ok = True\n",
    "    for s in seeds:\n",
    "        if not os.path.isfile(resolved[\"predictions_by_seed\"][s]):\n",
    "            all_ok = False\n",
    "            break\n",
    "    if all_ok:\n",
    "        return resolved\n",
    "\n",
    "    pointer_path = Path(pointer_json_path)\n",
    "    tag_root = pointer_path.parent\n",
    "    parent = tag_root.parent  # monolingual_test_runs/ or multilingual_test_runs/\n",
    "    tag_name = tag_root.name.lower()\n",
    "\n",
    "    # Identify a stable tag token to match (trainEnh3 / trainEnh2 / trainEnh1 / base)\n",
    "    want = None\n",
    "    for tok in [\"trainenh3\", \"trainenh2\", \"trainenh1\", \"base\"]:\n",
    "        if tok in tag_name:\n",
    "            want = tok\n",
    "            break\n",
    "    if want is None:\n",
    "        return resolved\n",
    "\n",
    "    candidates = []\n",
    "    for d in parent.iterdir():\n",
    "        if not d.is_dir():\n",
    "            continue\n",
    "        dn = d.name.lower()\n",
    "        if want not in dn:\n",
    "            continue\n",
    "\n",
    "        # Prefer directories that are actually test run roots, not just tag folders\n",
    "        ok = True\n",
    "        for s in seeds:\n",
    "            p = d / f\"run_D7_on_D2test_seed{s}\" / \"predictions.csv\"\n",
    "            if not p.is_file():\n",
    "                ok = False\n",
    "                break\n",
    "        if ok:\n",
    "            candidates.append(d)\n",
    "\n",
    "    if not candidates:\n",
    "        # Last attempt: search one level deeper (handles tag_root__timestamp patterns)\n",
    "        for d in parent.iterdir():\n",
    "            if not d.is_dir():\n",
    "                continue\n",
    "            dn = d.name.lower()\n",
    "            if want not in dn:\n",
    "                continue\n",
    "            ok = True\n",
    "            for s in seeds:\n",
    "                found = False\n",
    "                for p in d.rglob(f\"run_D7_on_D2test_seed{s}/predictions.csv\"):\n",
    "                    if p.is_file():\n",
    "                        found = True\n",
    "                        break\n",
    "                if not found:\n",
    "                    ok = False\n",
    "                    break\n",
    "            if ok:\n",
    "                candidates.append(d)\n",
    "\n",
    "    if not candidates:\n",
    "        return resolved\n",
    "\n",
    "    candidates.sort(key=lambda x: x.stat().st_mtime, reverse=True)\n",
    "    best = candidates[0]\n",
    "\n",
    "    resolved[\"run_root\"] = str(best)\n",
    "    resolved[\"tag_root\"] = str(best)\n",
    "    resolved[\"predictions_by_seed\"] = {\n",
    "        s: str(best / f\"run_D7_on_D2test_seed{s}\" / \"predictions.csv\") for s in seeds\n",
    "    }\n",
    "    return resolved\n",
    "\n",
    "# -------------------------\n",
    "# Validate pointers and predictions, then build RUNS_RESOLVED\n",
    "# -------------------------\n",
    "# Inputs: RUNS dict (conditions) + resolved predictions paths\n",
    "# Outputs: RUNS_RESOLVED dict used directly by later evaluation cells\n",
    "print(\"D7_OUT_ROOT:\", D7_OUT_ROOT)\n",
    "print(\"D2_OUT_ROOT:\", D2_OUT_ROOT)\n",
    "print(\"SEEDS:\", SEEDS)\n",
    "print(\"MECH_EVAL_ROOT:\", MECH_EVAL_ROOT)\n",
    "print()\n",
    "\n",
    "RUNS_RESOLVED = {}\n",
    "\n",
    "for tag, info in RUNS.items():\n",
    "    pointer_path = str(info[\"pointer_json\"]).strip()\n",
    "\n",
    "    # Guard: pointer JSON must exist for this condition\n",
    "    if not os.path.isfile(pointer_path):\n",
    "        raise FileNotFoundError(f\"[{tag}] pointer_json not found: {pointer_path}\")\n",
    "\n",
    "    resolved = _best_existing_predictions_path(pointer_path, SEEDS)\n",
    "\n",
    "    # Per-condition output folder for mechanistic eval artifacts\n",
    "    out_dir = os.path.join(MECH_EVAL_ROOT, tag)\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    # Guard: predictions.csv must already exist for all seeds (this cell runs after tests)\n",
    "    missing = []\n",
    "    for s, p in resolved[\"predictions_by_seed\"].items():\n",
    "        if not os.path.isfile(p):\n",
    "            missing.append((s, p))\n",
    "\n",
    "    print(f\"[{tag}]\")\n",
    "    print(\"  pointer_json:\", pointer_path)\n",
    "    print(\"  tag_root   :\", resolved[\"tag_root\"])\n",
    "    print(\"  run_root   :\", resolved[\"run_root\"])\n",
    "    print(\"  out_dir    :\", out_dir)\n",
    "    print(\"  predictions_by_seed:\")\n",
    "    for s in SEEDS:\n",
    "        print(f\"    seed{s}: {resolved['predictions_by_seed'][s]}\")\n",
    "    if missing:\n",
    "        print(\"  !!! MISSING predictions.csv for:\")\n",
    "        for s, p in missing:\n",
    "            print(f\"    seed{s}: {p}\")\n",
    "        raise FileNotFoundError(f\"[{tag}] Missing predictions.csv files (see above).\")\n",
    "    print()\n",
    "\n",
    "    RUNS_RESOLVED[tag] = {\n",
    "        \"tag\": tag,\n",
    "        \"pointer_json\": pointer_path,\n",
    "        \"tag_root\": resolved[\"tag_root\"],\n",
    "        \"run_root\": resolved[\"run_root\"],\n",
    "        \"predictions_by_seed\": resolved[\"predictions_by_seed\"],\n",
    "        \"out_dir\": out_dir,\n",
    "    }\n",
    "\n",
    "print(\"Cell 1 complete: RUNS_RESOLVED is ready for downstream scoring/ablation cells.\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "v_XcBh078yfX"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "The following cell runs a **data ablation analysis** using prediction files that already exist. It does not run any new model inference. It assumes that the run mapping (`RUNS_RESOLVED`), the mechanistic evaluation root folder (`MECH_EVAL_ROOT`), and the list of seeds (`SEEDS`) are already defined from the earlier setup cell, and it stops immediately with a clear error if any of these are missing. Using the resolved paths, it loads each condition’s saved `predictions.csv` file for all three seeds (Base, trainEnh1, trainEnh2, trainEnh3) and evaluates results separately for the two clip types: `task_group == \"vowel\"` and `task_group == \"other\"`.\n",
    "\n",
    "For each condition and seed, the cell first checks that the predictions file contains all required columns, including `task_group`, `sex_norm`, and `threshold_used_global`. It confirms that the `seed` value stored in the file matches the expected seed, to avoid mixing results from different runs, and verifies that `threshold_used_global` is the same for all rows in that file. This stored threshold is then used to turn probabilities into binary predictions, so all models are evaluated in the same way they were originally tested. Within each task group, the cell computes AUROC (set to NaN if it cannot be computed because only one class is present) and standard threshold-based metrics at the stored threshold: accuracy, precision, sensitivity (recall), specificity, F1 score, MCC, and the full confusion matrix (TN, FP, FN, TP). It also computes a sex-based fairness metric within each task group using only Parkinson’s cases: false negative rate (FNR) for males and females separately, the signed gap ΔFNR = FNR(F) − FNR(M), and the absolute gap. These fairness calculations use only rows with sex values `M` and `F`, and automatically exclude `UNK`.\n",
    "\n",
    "All results are saved in three forms. First, a single long-format table called `combined_taskgroup_metrics.csv` is written under the mechanistic evaluation root, with one row per condition, seed, and task group containing all key metrics and counts. Second, for each condition, a JSON summary file named `taskgroup_eval_normal.json` is written under `.../mechanistic_eval/<tag>/`. This file includes detailed per-seed results as well as mean and standard deviation summaries across the three seeds for each task group. Third, for each condition, a confusion-matrix table called `taskgroup_confusion_tables.csv` is written in the same folder so threshold outcomes can be reviewed later without rerunning the analysis.\n",
    "\n",
    "The cell also creates visual summaries and saves them as PNG files. It generates four comparison plots under `.../mechanistic_eval/plots_all_conditions/`: AUROC by condition for vowel clips and for other clips, and ΔFNR by condition for vowel clips and for other clips. Each plot shows individual seed results as slightly jittered points, along with a mean line and error bars showing the standard deviation across seeds. In addition, the cell creates a single PD-denominator plot that is global rather than condition-specific. This plot shows the number of PD males and PD females for vowel and other clips, reflecting that these counts come from the fixed D2 test set and do not change across conditions. Finally, to keep each condition folder complete on its own, the cell copies all generated plots from the shared plots folder into `.../mechanistic_eval/<tag>/plots/` for every condition."
   ],
   "metadata": {
    "id": "xw_GEKXDrlog"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# =========================\n",
    "# Data ablation from saved predictions (no new inference) + summary plots\n",
    "# -------------------------\n",
    "# Purpose\n",
    "# - Compare performance separately on vowel clips vs other clips using the already-saved predictions.\n",
    "# - Report both accuracy style metrics and a sex-based fairness gap (ΔFNR on PD-only clips).\n",
    "#\n",
    "# Inputs\n",
    "# - RUNS_RESOLVED (from Cell 1): per condition, per seed path to predictions.csv\n",
    "# - Each predictions.csv must include y_true, y_score, task_group, sex_norm, threshold_used_global\n",
    "#\n",
    "# Outputs\n",
    "# - Per condition:\n",
    "#   - taskgroup_eval_normal.json (per seed metrics + mean±SD summary)\n",
    "#   - taskgroup_confusion_tables.csv (confusion counts per seed and task_group)\n",
    "# - Combined:\n",
    "#   - combined_taskgroup_metrics.csv (one row per condition, seed, task_group)\n",
    "# - Plots:\n",
    "#   - AUROC by condition (vowel and other, separate figures)\n",
    "#   - ΔFNR by condition (vowel and other, separate figures)\n",
    "#   - One global PD denominator plot (vowel vs other) since denominators do not change by condition\n",
    "# =========================\n",
    "\n",
    "import os, json\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score,\n",
    "    confusion_matrix,\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    matthews_corrcoef,\n",
    ")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# -------------------------\n",
    "# 0) Guards (this cell depends on Cell 1 variables)\n",
    "# -------------------------\n",
    "if \"RUNS_RESOLVED\" not in globals():\n",
    "    raise RuntimeError(\"RUNS_RESOLVED not found. Run Cell 1 first in the same runtime.\")\n",
    "if \"MECH_EVAL_ROOT\" not in globals():\n",
    "    raise RuntimeError(\"MECH_EVAL_ROOT not found. Run Cell 1 first in the same runtime.\")\n",
    "if \"SEEDS\" not in globals():\n",
    "    raise RuntimeError(\"SEEDS not found. Run Cell 1 first in the same runtime.\")\n",
    "\n",
    "# -------------------------\n",
    "# 0.5) Plot readability defaults (150% text size)\n",
    "# -------------------------\n",
    "# Inputs: none (affects matplotlib defaults only)\n",
    "# Outputs: larger text across ALL plots generated in this cell (AUROC, ΔFNR, PD denominators)\n",
    "plt.rcParams.update({\n",
    "    \"font.size\": plt.rcParams.get(\"font.size\", 10) * 1.5,\n",
    "    \"axes.titlesize\": plt.rcParams.get(\"axes.titlesize\", plt.rcParams.get(\"font.size\", 10)) * 1.5,\n",
    "    \"axes.labelsize\": plt.rcParams.get(\"axes.labelsize\", plt.rcParams.get(\"font.size\", 10)) * 1.5,\n",
    "    \"xtick.labelsize\": plt.rcParams.get(\"xtick.labelsize\", plt.rcParams.get(\"font.size\", 10)) * 1.5,\n",
    "    \"ytick.labelsize\": plt.rcParams.get(\"ytick.labelsize\", plt.rcParams.get(\"font.size\", 10)) * 1.5,\n",
    "    \"legend.fontsize\": plt.rcParams.get(\"legend.fontsize\", plt.rcParams.get(\"font.size\", 10)) * 1.5,\n",
    "})\n",
    "\n",
    "# -------------------------\n",
    "# 1) Expected schema and evaluation slices\n",
    "# -------------------------\n",
    "# Inputs: required columns present in predictions.csv\n",
    "# Outputs: constants used for validation, slicing, and plot locations\n",
    "REQUIRED_COLS = [\n",
    "    \"clip_path\",\n",
    "    \"y_true\",\n",
    "    \"y_score\",\n",
    "    \"sex_norm\",\n",
    "    \"speaker_id\",\n",
    "    \"task_group\",\n",
    "    \"seed\",\n",
    "    \"threshold_used_global\",\n",
    "]\n",
    "TASK_GROUPS = [\"vowel\", \"other\"]\n",
    "SEX_LEVELS = [\"M\", \"F\"]  # UNK excluded from sex-specific fairness metrics\n",
    "\n",
    "PLOTS_ALL_DIR = os.path.join(MECH_EVAL_ROOT, \"plots_all_conditions\")\n",
    "os.makedirs(PLOTS_ALL_DIR, exist_ok=True)\n",
    "\n",
    "# -------------------------\n",
    "# 2) Helper functions (metrics + safety checks)\n",
    "# -------------------------\n",
    "# Inputs: arrays or small DataFrames\n",
    "# Outputs: scalar metrics, summaries, and validation errors when needed\n",
    "def _auc_or_nan(y_true, y_score):\n",
    "    # AUROC needs both classes; return NaN if the slice is single-class\n",
    "    y_true = np.asarray(y_true).astype(int)\n",
    "    if len(np.unique(y_true)) < 2:\n",
    "        return np.nan\n",
    "    return float(roc_auc_score(y_true, y_score))\n",
    "\n",
    "def _specificity_from_cm(cm):\n",
    "    # Specificity = TN / (TN + FP)\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    denom = tn + fp\n",
    "    return float(tn / denom) if denom > 0 else np.nan\n",
    "\n",
    "def _fnr_on_pd_only(y_true, y_pred):\n",
    "    # FNR computed only on PD clips: FN / (FN + TP)\n",
    "    y_true = np.asarray(y_true).astype(int)\n",
    "    y_pred = np.asarray(y_pred).astype(int)\n",
    "    mask = (y_true == 1)\n",
    "    if int(mask.sum()) == 0:\n",
    "        return np.nan, 0\n",
    "    fn = int((y_pred[mask] == 0).sum())\n",
    "    tp = int((y_pred[mask] == 1).sum())\n",
    "    denom = fn + tp\n",
    "    fnr = float(fn / denom) if denom > 0 else np.nan\n",
    "    return fnr, int(mask.sum())\n",
    "\n",
    "def _mean_sd(values):\n",
    "    # Mean and sample SD (ddof=1) over non-NaN values\n",
    "    vals = np.asarray(values, dtype=float)\n",
    "    vals = vals[~np.isnan(vals)]\n",
    "    if len(vals) == 0:\n",
    "        return {\"mean\": np.nan, \"sd\": np.nan, \"n\": 0}\n",
    "    if len(vals) == 1:\n",
    "        return {\"mean\": float(vals[0]), \"sd\": 0.0, \"n\": 1}\n",
    "    return {\"mean\": float(np.mean(vals)), \"sd\": float(np.std(vals, ddof=1)), \"n\": int(len(vals))}\n",
    "\n",
    "def _verify_required_cols(df, path):\n",
    "    # Fail fast if predictions.csv schema is not as expected\n",
    "    missing = [c for c in REQUIRED_COLS if c not in df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"Missing required columns in {path}: {missing}\")\n",
    "\n",
    "def _read_predictions_csv(path):\n",
    "    # Read and validate one predictions.csv\n",
    "    df = pd.read_csv(path)\n",
    "    _verify_required_cols(df, path)\n",
    "    return df\n",
    "\n",
    "def compute_metrics_for_subset(df_subset, threshold):\n",
    "    \"\"\"\n",
    "    Compute metrics for one slice (one seed, one task_group).\n",
    "    Returns:\n",
    "    - AUROC\n",
    "    - threshold-based metrics (confusion + common scores)\n",
    "    - fairness: FNR by sex on PD-only clips and ΔFNR = FNR(F) − FNR(M)\n",
    "    \"\"\"\n",
    "    y_true = df_subset[\"y_true\"].astype(int).to_numpy()\n",
    "    y_score = df_subset[\"y_score\"].astype(float).to_numpy()\n",
    "    y_pred = (y_score >= threshold).astype(int)\n",
    "\n",
    "    auc = _auc_or_nan(y_true, y_score)\n",
    "\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=[0, 1])\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "    acc = float(accuracy_score(y_true, y_pred))\n",
    "    prec = float(precision_score(y_true, y_pred, zero_division=0))\n",
    "    rec = float(recall_score(y_true, y_pred, zero_division=0))  # sensitivity/recall\n",
    "    f1v = float(f1_score(y_true, y_pred, zero_division=0))\n",
    "    spec = _specificity_from_cm(cm)\n",
    "    mcc = float(matthews_corrcoef(y_true, y_pred)) if (len(np.unique(y_true)) > 1 or len(np.unique(y_pred)) > 1) else np.nan\n",
    "\n",
    "    # Fairness: compute FNR separately for M and F on PD-only clips\n",
    "    fnr_vals = {}\n",
    "    n_pd_vals = {}\n",
    "    for sex in SEX_LEVELS:\n",
    "        dsex = df_subset[df_subset[\"sex_norm\"] == sex]\n",
    "        y_t = dsex[\"y_true\"].astype(int).to_numpy()\n",
    "        y_p = (dsex[\"y_score\"].astype(float).to_numpy() >= threshold).astype(int)\n",
    "        fnr, n_pd = _fnr_on_pd_only(y_t, y_p)\n",
    "        fnr_vals[sex] = fnr\n",
    "        n_pd_vals[sex] = n_pd\n",
    "\n",
    "    delta_fnr = np.nan\n",
    "    abs_delta_fnr = np.nan\n",
    "    if not np.isnan(fnr_vals[\"F\"]) and not np.isnan(fnr_vals[\"M\"]):\n",
    "        delta_fnr = float(fnr_vals[\"F\"] - fnr_vals[\"M\"])\n",
    "        abs_delta_fnr = float(abs(delta_fnr))\n",
    "\n",
    "    return {\n",
    "        \"auroc\": auc,\n",
    "        \"threshold\": float(threshold),\n",
    "        \"metrics_at_threshold\": {\n",
    "            \"accuracy\": acc,\n",
    "            \"precision\": prec,\n",
    "            \"sensitivity_recall\": rec,\n",
    "            \"specificity\": spec,\n",
    "            \"f1\": f1v,\n",
    "            \"mcc\": mcc,\n",
    "            \"confusion\": {\"TN\": int(tn), \"FP\": int(fp), \"FN\": int(fn), \"TP\": int(tp)},\n",
    "            \"n\": int(len(df_subset)),\n",
    "        },\n",
    "        \"fairness\": {\n",
    "            \"FNR_M\": fnr_vals[\"M\"],\n",
    "            \"FNR_F\": fnr_vals[\"F\"],\n",
    "            \"delta_FNR_F_minus_M\": delta_fnr,\n",
    "            \"abs_delta_FNR\": abs_delta_fnr,\n",
    "            \"n_PD_M\": int(n_pd_vals[\"M\"]),\n",
    "            \"n_PD_F\": int(n_pd_vals[\"F\"]),\n",
    "        },\n",
    "    }\n",
    "\n",
    "# -------------------------\n",
    "# 3) Main scoring loop (condition → seed → task_group)\n",
    "# -------------------------\n",
    "# Inputs: RUNS_RESOLVED and SEEDS\n",
    "# Outputs: per-condition JSON summaries and two combined in-memory tables for CSV + plots\n",
    "all_rows = []       # one row per condition/seed/task_group\n",
    "all_conf_rows = []  # confusion counts per condition/seed/task_group\n",
    "\n",
    "for tag, info in RUNS_RESOLVED.items():\n",
    "    out_dir = info[\"out_dir\"]\n",
    "    plots_dir = os.path.join(out_dir, \"plots\")\n",
    "    os.makedirs(plots_dir, exist_ok=True)\n",
    "\n",
    "    per_seed = {}\n",
    "\n",
    "    for seed in SEEDS:\n",
    "        pred_path = info[\"predictions_by_seed\"][seed]\n",
    "        df = _read_predictions_csv(pred_path)\n",
    "\n",
    "        # Sanity: ensure the file contains exactly one seed and it matches the expected seed\n",
    "        unique_seeds = sorted(set(df[\"seed\"].astype(int).tolist()))\n",
    "        if len(unique_seeds) != 1 or unique_seeds[0] != int(seed):\n",
    "            raise ValueError(f\"[{tag}] Seed mismatch in {pred_path}. Found seeds {unique_seeds}, expected {seed}.\")\n",
    "\n",
    "        # Threshold is read from the file and must be constant within the seed file\n",
    "        thr_unique = df[\"threshold_used_global\"].astype(float).to_numpy()\n",
    "        thr_unique_rounded = sorted(set([round(float(x), 12) for x in thr_unique]))\n",
    "        if len(thr_unique_rounded) != 1:\n",
    "            raise ValueError(\n",
    "                f\"[{tag}] threshold_used_global is not constant within seed {seed} in {pred_path}. \"\n",
    "                f\"Unique (rounded) thresholds: {thr_unique_rounded[:10]}\"\n",
    "            )\n",
    "        threshold = float(thr_unique_rounded[0])\n",
    "\n",
    "        per_seed.setdefault(seed, {})\n",
    "\n",
    "        # Score vowel-only and other-only slices separately\n",
    "        for tg in TASK_GROUPS:\n",
    "            dfg = df[df[\"task_group\"] == tg].copy()\n",
    "            m = compute_metrics_for_subset(dfg, threshold)\n",
    "            per_seed[seed][tg] = m\n",
    "\n",
    "            # Flat row for combined CSV and plotting\n",
    "            all_rows.append({\n",
    "                \"tag\": tag,\n",
    "                \"seed\": int(seed),\n",
    "                \"task_group\": tg,\n",
    "                \"threshold_used_global\": float(threshold),\n",
    "                \"auroc\": m[\"auroc\"],\n",
    "                \"accuracy\": m[\"metrics_at_threshold\"][\"accuracy\"],\n",
    "                \"precision\": m[\"metrics_at_threshold\"][\"precision\"],\n",
    "                \"sensitivity_recall\": m[\"metrics_at_threshold\"][\"sensitivity_recall\"],\n",
    "                \"specificity\": m[\"metrics_at_threshold\"][\"specificity\"],\n",
    "                \"f1\": m[\"metrics_at_threshold\"][\"f1\"],\n",
    "                \"mcc\": m[\"metrics_at_threshold\"][\"mcc\"],\n",
    "                \"FNR_M\": m[\"fairness\"][\"FNR_M\"],\n",
    "                \"FNR_F\": m[\"fairness\"][\"FNR_F\"],\n",
    "                \"delta_FNR_F_minus_M\": m[\"fairness\"][\"delta_FNR_F_minus_M\"],\n",
    "                \"abs_delta_FNR\": m[\"fairness\"][\"abs_delta_FNR\"],\n",
    "                \"n_PD_M\": int(m[\"fairness\"][\"n_PD_M\"]),\n",
    "                \"n_PD_F\": int(m[\"fairness\"][\"n_PD_F\"]),\n",
    "                \"n_rows\": int(m[\"metrics_at_threshold\"][\"n\"]),\n",
    "                \"predictions_csv\": pred_path,\n",
    "            })\n",
    "\n",
    "            # Confusion table row for easier inspection\n",
    "            conf = m[\"metrics_at_threshold\"][\"confusion\"]\n",
    "            all_conf_rows.append({\n",
    "                \"tag\": tag,\n",
    "                \"seed\": int(seed),\n",
    "                \"task_group\": tg,\n",
    "                \"threshold_used_global\": float(threshold),\n",
    "                **conf,\n",
    "                \"n_rows\": int(m[\"metrics_at_threshold\"][\"n\"]),\n",
    "                \"predictions_csv\": pred_path,\n",
    "            })\n",
    "\n",
    "    # Mean ± SD summary across seeds for each task_group\n",
    "    summary = {}\n",
    "    for tg in TASK_GROUPS:\n",
    "        summary[tg] = {\n",
    "            \"auroc\": _mean_sd([per_seed[s][tg][\"auroc\"] for s in SEEDS]),\n",
    "            \"accuracy\": _mean_sd([per_seed[s][tg][\"metrics_at_threshold\"][\"accuracy\"] for s in SEEDS]),\n",
    "            \"precision\": _mean_sd([per_seed[s][tg][\"metrics_at_threshold\"][\"precision\"] for s in SEEDS]),\n",
    "            \"sensitivity_recall\": _mean_sd([per_seed[s][tg][\"metrics_at_threshold\"][\"sensitivity_recall\"] for s in SEEDS]),\n",
    "            \"specificity\": _mean_sd([per_seed[s][tg][\"metrics_at_threshold\"][\"specificity\"] for s in SEEDS]),\n",
    "            \"f1\": _mean_sd([per_seed[s][tg][\"metrics_at_threshold\"][\"f1\"] for s in SEEDS]),\n",
    "            \"mcc\": _mean_sd([per_seed[s][tg][\"metrics_at_threshold\"][\"mcc\"] for s in SEEDS]),\n",
    "            \"FNR_M\": _mean_sd([per_seed[s][tg][\"fairness\"][\"FNR_M\"] for s in SEEDS]),\n",
    "            \"FNR_F\": _mean_sd([per_seed[s][tg][\"fairness\"][\"FNR_F\"] for s in SEEDS]),\n",
    "            \"delta_FNR_F_minus_M\": _mean_sd([per_seed[s][tg][\"fairness\"][\"delta_FNR_F_minus_M\"] for s in SEEDS]),\n",
    "            \"abs_delta_FNR\": _mean_sd([per_seed[s][tg][\"fairness\"][\"abs_delta_FNR\"] for s in SEEDS]),\n",
    "            \"n_PD_M\": {\"values_by_seed\": {str(s): int(per_seed[s][tg][\"fairness\"][\"n_PD_M\"]) for s in SEEDS}},\n",
    "            \"n_PD_F\": {\"values_by_seed\": {str(s): int(per_seed[s][tg][\"fairness\"][\"n_PD_F\"]) for s in SEEDS}},\n",
    "        }\n",
    "\n",
    "    # Per-condition JSON output written under its mechanistic evaluation folder\n",
    "    out_json = {\n",
    "        \"tag\": tag,\n",
    "        \"mode\": \"normal_routing_existing_predictions\",\n",
    "        \"seeds\": [int(s) for s in SEEDS],\n",
    "        \"per_seed\": per_seed,\n",
    "        \"summary_mean_sd_across_seeds\": summary,\n",
    "        \"out_dir\": out_dir,\n",
    "    }\n",
    "    with open(os.path.join(out_dir, \"taskgroup_eval_normal.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(out_json, f, indent=2)\n",
    "\n",
    "# -------------------------\n",
    "# 4) Write combined CSV outputs\n",
    "# -------------------------\n",
    "# Inputs: all_rows and all_conf_rows collected above\n",
    "# Outputs: combined_taskgroup_metrics.csv under MECH_EVAL_ROOT, plus per-tag confusion CSVs\n",
    "combined_df = pd.DataFrame(all_rows)\n",
    "combined_csv_path = os.path.join(MECH_EVAL_ROOT, \"combined_taskgroup_metrics.csv\")\n",
    "combined_df.to_csv(combined_csv_path, index=False)\n",
    "\n",
    "conf_df = pd.DataFrame(all_conf_rows)\n",
    "for tag in RUNS_RESOLVED.keys():\n",
    "    out_dir = RUNS_RESOLVED[tag][\"out_dir\"]\n",
    "    conf_df_tag = conf_df[conf_df[\"tag\"] == tag].copy()\n",
    "    conf_df_tag.to_csv(os.path.join(out_dir, \"taskgroup_confusion_tables.csv\"), index=False)\n",
    "\n",
    "print(\"Wrote:\")\n",
    "print(\"  \", combined_csv_path)\n",
    "for tag in RUNS_RESOLVED.keys():\n",
    "    print(\"  \", os.path.join(RUNS_RESOLVED[tag][\"out_dir\"], \"taskgroup_eval_normal.json\"))\n",
    "    print(\"  \", os.path.join(RUNS_RESOLVED[tag][\"out_dir\"], \"taskgroup_confusion_tables.csv\"))\n",
    "\n",
    "# -------------------------\n",
    "# 5) Plotting helpers (seed points + mean±SD)\n",
    "# -------------------------\n",
    "# Inputs: combined_df filtered to one task_group\n",
    "# Outputs: PNG figures saved to the shared plots folder\n",
    "def _condition_order(tags):\n",
    "    # Stable ordering for readability across plots\n",
    "    preferred = [\"base\", \"trainEnh1\", \"trainEnh2\", \"trainEnh3\"]\n",
    "    ordered = [t for t in preferred if t in tags] + [t for t in tags if t not in preferred]\n",
    "    return ordered\n",
    "\n",
    "def _jitter(xs, seed_index, jitter_scale=0.06):\n",
    "    # Small horizontal offset so seed points do not overlap\n",
    "    offsets = [-1, 0, 1]\n",
    "    o = offsets[seed_index % len(offsets)]\n",
    "    return xs + o * jitter_scale\n",
    "\n",
    "def plot_metric_across_conditions(df_in, metric_col, title, ylabel, out_path):\n",
    "    # Plot per-seed points and mean±SD across seeds for one metric\n",
    "    tags = _condition_order(sorted(df_in[\"tag\"].unique().tolist()))\n",
    "    x = np.arange(len(tags), dtype=float)\n",
    "\n",
    "    plt.figure(figsize=(10, 4.5))\n",
    "\n",
    "    # Seed points\n",
    "    for si, seed in enumerate(SEEDS):\n",
    "        vals = []\n",
    "        for t in tags:\n",
    "            sub = df_in[(df_in[\"tag\"] == t) & (df_in[\"seed\"] == seed)]\n",
    "            v = sub[metric_col].iloc[0] if len(sub) == 1 else np.nan\n",
    "            vals.append(v)\n",
    "        plt.scatter(_jitter(x, si), vals, label=f\"seed{seed}\", s=28)\n",
    "\n",
    "    # Mean ± SD across seeds\n",
    "    means = []\n",
    "    sds = []\n",
    "    for t in tags:\n",
    "        sub = df_in[df_in[\"tag\"] == t][metric_col].astype(float).to_numpy()\n",
    "        means.append(np.nanmean(sub) if np.any(~np.isnan(sub)) else np.nan)\n",
    "        sds.append(np.nanstd(sub, ddof=1) if np.sum(~np.isnan(sub)) >= 2 else 0.0)\n",
    "\n",
    "    plt.errorbar(x, means, yerr=sds, fmt=\"o-\", capsize=4)\n",
    "\n",
    "    plt.xticks(x, tags, rotation=0)\n",
    "    plt.title(title)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.grid(True, axis=\"y\", alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_path, dpi=200)\n",
    "    plt.close()\n",
    "\n",
    "def plot_pd_denoms_by_taskgroup_global(combined_df_in, title, out_path):\n",
    "    \"\"\"\n",
    "    Global PD denominator plot.\n",
    "    - X axis: task_group (vowel vs other)\n",
    "    - Bars: PD counts for M and F\n",
    "    Denominators should match across conditions because task_group and sex come from the dataset, not the model.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(8, 4.5))\n",
    "\n",
    "    xlabels = TASK_GROUPS\n",
    "    x = np.arange(len(xlabels), dtype=float)\n",
    "    width = 0.35\n",
    "\n",
    "    n_m = []\n",
    "    n_f = []\n",
    "\n",
    "    for tg in TASK_GROUPS:\n",
    "        sub = combined_df_in[combined_df_in[\"task_group\"] == tg].copy()\n",
    "\n",
    "        # Robust aggregation (should be identical across tag and seed rows)\n",
    "        n_m.append(float(np.nanmedian(sub[\"n_PD_M\"].astype(float).to_numpy())))\n",
    "        n_f.append(float(np.nanmedian(sub[\"n_PD_F\"].astype(float).to_numpy())))\n",
    "\n",
    "    plt.bar(x - width/2, n_m, width=width, label=\"n_PD_M\")\n",
    "    plt.bar(x + width/2, n_f, width=width, label=\"n_PD_F\")\n",
    "\n",
    "    plt.xticks(x, xlabels, rotation=0)\n",
    "    plt.title(title)\n",
    "    plt.ylabel(\"Count (PD only)\")\n",
    "    plt.grid(True, axis=\"y\", alpha=0.3)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_path, dpi=200)\n",
    "    plt.close()\n",
    "\n",
    "# -------------------------\n",
    "# 6) Generate plots requested for this cell\n",
    "# -------------------------\n",
    "# Inputs: combined_df written above\n",
    "# Outputs: 5 PNG files saved under plots_all_conditions\n",
    "# (two AUROC, two ΔFNR, one global denominator plot)\n",
    "\n",
    "# AUROC by condition (separate plots for vowel and other)\n",
    "for tg in TASK_GROUPS:\n",
    "    df_tg = combined_df[combined_df[\"task_group\"] == tg].copy()\n",
    "    auroc_path = os.path.join(PLOTS_ALL_DIR, f\"auroc_by_condition__taskgroup_{tg}.png\")\n",
    "    plot_metric_across_conditions(\n",
    "        df_tg,\n",
    "        metric_col=\"auroc\",\n",
    "        title=f\"AUROC by condition (task_group = {tg})\",\n",
    "        ylabel=\"AUROC\",\n",
    "        out_path=auroc_path,\n",
    "    )\n",
    "\n",
    "# ΔFNR by condition (separate plots for vowel and other)\n",
    "for tg in TASK_GROUPS:\n",
    "    df_tg = combined_df[combined_df[\"task_group\"] == tg].copy()\n",
    "    gap_path = os.path.join(PLOTS_ALL_DIR, f\"deltaFNR_F_minus_M_by_condition__taskgroup_{tg}.png\")\n",
    "    plot_metric_across_conditions(\n",
    "        df_tg,\n",
    "        metric_col=\"delta_FNR_F_minus_M\",\n",
    "        title=f\"ΔFNR (FNR_F − FNR_M) by condition (task_group = {tg})\",\n",
    "        ylabel=\"ΔFNR (signed)\",\n",
    "        out_path=gap_path,\n",
    "    )\n",
    "\n",
    "# PD denominators (single global plot: vowel vs other)\n",
    "denom_global_path = os.path.join(PLOTS_ALL_DIR, \"pd_denominators_by_taskgroup_global.png\")\n",
    "plot_pd_denoms_by_taskgroup_global(\n",
    "    combined_df,\n",
    "    title=\"PD denominators by task group (global, D2 test)\",\n",
    "    out_path=denom_global_path,\n",
    ")\n",
    "\n",
    "print(\"Wrote plots under:\")\n",
    "print(\"  \", PLOTS_ALL_DIR)\n",
    "print(\"  \", denom_global_path)\n",
    "\n",
    "# -------------------------\n",
    "# 7) Copy plots into each condition folder for convenience\n",
    "# -------------------------\n",
    "# Inputs: PNGs in plots_all_conditions\n",
    "# Outputs: same PNGs duplicated into each <condition>/plots/ folder\n",
    "PLOTS_TO_COPY = []\n",
    "\n",
    "for tg in TASK_GROUPS:\n",
    "    PLOTS_TO_COPY.append(f\"auroc_by_condition__taskgroup_{tg}.png\")\n",
    "    PLOTS_TO_COPY.append(f\"deltaFNR_F_minus_M_by_condition__taskgroup_{tg}.png\")\n",
    "\n",
    "PLOTS_TO_COPY.append(\"pd_denominators_by_taskgroup_global.png\")\n",
    "\n",
    "for tag in RUNS_RESOLVED.keys():\n",
    "    plots_dir = os.path.join(RUNS_RESOLVED[tag][\"out_dir\"], \"plots\")\n",
    "    os.makedirs(plots_dir, exist_ok=True)\n",
    "    for fname in PLOTS_TO_COPY:\n",
    "        src = os.path.join(PLOTS_ALL_DIR, fname)\n",
    "        dst = os.path.join(plots_dir, fname)\n",
    "        with open(src, \"rb\") as fsrc, open(dst, \"wb\") as fdst:\n",
    "            fdst.write(fsrc.read())\n",
    "\n",
    "print(\"Copied plots into each tag's plots/ folder as well.\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4N274SXZF0BC"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Ablation 3: Forced Task-Head Routing Ablation"
   ],
   "metadata": {
    "id": "1tX4_f0UdgaC"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The following cell runs a focused **mechanistic evaluation** to see how much the model depends on the choice of task head (vowel head versus other head) when scoring D2 test audio. It produces the five required summary plots. Unlike earlier plotting-only cells, this cell **does run new inference**. It loads already trained model heads for the Base model and for the three enhanced models (trainEnh1, trainEnh2, trainEnh3), then re-scores the same D2 test clips multiple times while deliberately forcing the model to use one head or the other.\n",
    "\n",
    "First, the cell makes sure Google Drive is mounted, then rebuilds a clean and deterministic run map (`RUNS_RESOLVED`) using the provided `tag_run_pointer.json` files and confirmed train–validation experiment folders. This avoids relying on outputs from earlier cells and avoids broad folder searches. For each condition (base, trainEnh1, trainEnh2, trainEnh3), it resolves the locations of the existing “normal” `predictions.csv` files for each seed, checks that those files exist, and sets up a dedicated output folder under `.../mechanistic_eval/<tag>/`.\n",
    "\n",
    "Next, the cell loads the D2 master manifest (`manifests/manifest_all.csv`), filters it to **only the D2 test split**, and standardizes two fields needed for analysis. Sex is normalized to `M`, `F`, or `UNK` using D2’s original `male` and `female` values, and each clip is assigned a **task group**: `vowel` only if `task == \"vowl\"`, otherwise `other`. A PyTorch dataset and data loader are created to read audio from `clip_path` and build an attention mask. For vowel clips, the mask turns off after the last meaningful audio sample so padded silence does not affect the model. For other clips, the mask stays fully on. Audio and masks are padded consistently within each batch.\n",
    "\n",
    "For each condition and each of the three seeds, the cell loads the saved heads (`best_heads.pt`) from the selected train–validation experiment and attaches them to a frozen wav2vec2 backbone. It reads the **single global threshold actually used in the normal test run** from the existing `predictions.csv` (`threshold_used_global`) and confirms that it is constant within that file. It then runs two new inference passes over the full D2 test set. One pass forces the **vowel head for every clip** and writes `predictions_force_vowel.csv`. The other pass forces the **other head for every clip** and writes `predictions_force_other.csv`. Both files include the same core fields as the normal predictions (clip path, true label, predicted probability, sex, speaker id, task group, seed), plus extra fields for traceability, such as which head was forced, which global threshold was used, which train–validation run the heads came from, and a run timestamp. All forced outputs are written under `.../mechanistic_eval/<tag>/forced_head_runs/run_D7_on_D2test_seed{seed}/` so they never overwrite the original test results.\n",
    "\n",
    "After inference, the cell computes performance and fairness metrics for three routing modes for each condition and seed: **normal routing**, **force_vowel**, and **force_other**. Metrics are computed separately for vowel-only clips and other-only clips. Performance is measured using AUROC, which is recorded as NaN if a subset contains only one class. Fairness is measured using the **sex-based difference in false negative rate among Parkinson’s cases only**, defined as ΔFNR = FNR(F) − FNR(M), evaluated at the same global threshold used in the normal test run. All results are collected into a long-format table (`forced_head_metrics_long.csv`) under `mechanistic_eval/` and into per-condition JSON summaries (`forced_head_eval_summary.json`) under each condition folder. A registry file (`forced_head_registry.json`) is also written to record exactly which prediction files and thresholds were used for each seed.\n",
    "\n",
    "Finally, the cell generates the five required plots and saves them as PNG files under `.../mechanistic_eval/plots_all_conditions/`. Four plots show grouped bar charts (mean with standard deviation across the three seeds) for AUROC and ΔFNR across the four conditions, with the three routing modes shown side by side, separately for vowel-only clips and other-only clips. The fifth plot shows “head sensitivity” by computing **AUROC(force vowel) − AUROC(force other)** for each condition and seed, and plotting results for vowel clips and other clips with mean and standard deviation across seeds. At the end, the cell attempts to unassign the Colab runtime, matching the behavior of the test-only cells, so the session is released cleanly after completion."
   ],
   "metadata": {
    "id": "ZGV35m2WrcW5"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# =========================\n",
    "# Task-head ablation on D2 test (new inference) + required mechanistic plots\n",
    "# -------------------------\n",
    "# Purpose\n",
    "# - Quantify how much performance and fairness depend on which head is used.\n",
    "# - Re-run inference on the D2 test split while forcing:\n",
    "#     (1) vowel head for every clip\n",
    "#     (2) other head for every clip\n",
    "# - Compare those to the normal routed mode using the same global threshold per seed.\n",
    "#\n",
    "# Inputs\n",
    "# - Pointer JSON files that identify where existing per-seed predictions.csv live\n",
    "# - D2 manifest (test split) and audio clips\n",
    "# - Trained heads (best_heads.pt) from each condition and seed\n",
    "#\n",
    "# Outputs\n",
    "# - Per seed, per condition: predictions_force_vowel.csv and predictions_force_other.csv\n",
    "# - Long-form metrics table (CSV) and per-condition JSON summaries\n",
    "# - Five PNG plots saved under a shared plots folder\n",
    "# - Runtime unassigned at the end\n",
    "# =========================\n",
    "\n",
    "import os\n",
    "import json\n",
    "import math\n",
    "import time\n",
    "import warnings\n",
    "import re\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any, List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import soundfile as sf\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# -------------------------\n",
    "# Drive mount (Colab)\n",
    "# -------------------------\n",
    "# Ensures files are reachable even if this runtime started fresh.\n",
    "if not os.path.isdir(\"/content/drive/MyDrive\"):\n",
    "    from google.colab import drive  # type: ignore\n",
    "    drive.mount(\"/content/drive\")\n",
    "\n",
    "# -------------------------\n",
    "# Fixed inputs (roots, seeds, and run identifiers)\n",
    "# -------------------------\n",
    "# Defines where to read D2 data from and where to write mechanistic evaluation outputs.\n",
    "D7_OUT_ROOT = \"/content/drive/MyDrive/AI_PD_Project/Datasets/D7-Multilingual (D1_D4_D5v2_D6)/preprocessed_v1\"\n",
    "D2_OUT_ROOT = \"/content/drive/MyDrive/AI_PD_Project/Datasets/D2-Slovak (EWA-DB)/EWA-DB/preprocessed_v1\"\n",
    "SEEDS = [1337, 2024, 7777]\n",
    "\n",
    "# Output root for all forced-head artifacts and plots.\n",
    "MECH_EVAL_ROOT = f\"{D7_OUT_ROOT}/mechanistic_eval\"\n",
    "Path(MECH_EVAL_ROOT).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def _parse_run_stamp_from_name(name: str) -> Tuple[int, int]:\n",
    "    \"\"\"\n",
    "    Tries to parse ..._YYYYMMDD_HHMMSS from a folder name.\n",
    "    Returns (yyyymmdd, hhmmss) as ints, or (0, 0) if not found.\n",
    "    \"\"\"\n",
    "    m = re.search(r\"_(\\d{8})_(\\d{6})(?:\\D|$)\", str(name))\n",
    "    if not m:\n",
    "        return (0, 0)\n",
    "    return (int(m.group(1)), int(m.group(2)))\n",
    "\n",
    "def _sort_key_latest(p: Path) -> Tuple[int, int, float]:\n",
    "    a, b = _parse_run_stamp_from_name(p.name)\n",
    "    try:\n",
    "        mt = p.stat().st_mtime\n",
    "    except Exception:\n",
    "        mt = 0.0\n",
    "    return (a, b, mt)\n",
    "\n",
    "def _find_latest_dir(parent: Path, required_substrings: List[str], must_have_relpaths: List[str]) -> Path:\n",
    "    \"\"\"\n",
    "    Picks the latest directory under parent whose name contains all required_substrings\n",
    "    (case-insensitive), and that contains all must_have_relpaths.\n",
    "    \"\"\"\n",
    "    if not parent.is_dir():\n",
    "        raise FileNotFoundError(f\"Missing directory: {str(parent)}\")\n",
    "\n",
    "    req = [s.lower() for s in required_substrings if str(s).strip()]\n",
    "    candidates = []\n",
    "    for d in parent.iterdir():\n",
    "        if not d.is_dir():\n",
    "            continue\n",
    "        nm = d.name.lower()\n",
    "        ok = True\n",
    "        for s in req:\n",
    "            if s not in nm:\n",
    "                ok = False\n",
    "                break\n",
    "        if not ok:\n",
    "            continue\n",
    "        for rel in must_have_relpaths:\n",
    "            if not (d / rel).exists():\n",
    "                ok = False\n",
    "                break\n",
    "        if ok:\n",
    "            candidates.append(d)\n",
    "\n",
    "    if not candidates:\n",
    "        raise FileNotFoundError(\n",
    "            f\"No matching run directory found under: {str(parent)} \"\n",
    "            f\"required_substrings={required_substrings} must_have_relpaths={must_have_relpaths}\"\n",
    "        )\n",
    "\n",
    "    candidates.sort(key=_sort_key_latest, reverse=True)\n",
    "    return candidates[0]\n",
    "\n",
    "def _find_latest_trainval_exp(parent: Path, tag: str, seeds: List[int]) -> Path:\n",
    "    \"\"\"\n",
    "    Finds the latest trainval exp directory that contains required best_heads.pt files.\n",
    "    Selection rules:\n",
    "    - For trainEnh*: folder name must contain that tag (case-insensitive).\n",
    "    - For base: folder name must NOT contain 'trainenh' (case-insensitive).\n",
    "    \"\"\"\n",
    "    if not parent.is_dir():\n",
    "        raise FileNotFoundError(f\"Missing directory: {str(parent)}\")\n",
    "\n",
    "    tag_l = tag.lower().strip()\n",
    "    candidates = []\n",
    "    for d in parent.iterdir():\n",
    "        if not d.is_dir():\n",
    "            continue\n",
    "        nm = d.name.lower()\n",
    "\n",
    "        if tag_l == \"base\":\n",
    "            if \"trainenh\" in nm:\n",
    "                continue\n",
    "        else:\n",
    "            if tag_l not in nm:\n",
    "                continue\n",
    "\n",
    "        ok = True\n",
    "        for s in seeds:\n",
    "            rel = Path(f\"run_D7_seed{s}\") / \"best_heads.pt\"\n",
    "            if not (d / rel).is_file():\n",
    "                ok = False\n",
    "                break\n",
    "        if ok:\n",
    "            candidates.append(d)\n",
    "\n",
    "    if not candidates:\n",
    "        raise FileNotFoundError(\n",
    "            f\"No matching trainval exp dir found under: {str(parent)} for tag={tag} \"\n",
    "            f\"that contains best_heads.pt for seeds={seeds}\"\n",
    "        )\n",
    "\n",
    "    candidates.sort(key=_sort_key_latest, reverse=True)\n",
    "    return candidates[0]\n",
    "\n",
    "# Tag pointers (one per condition). Used to rebuild run roots deterministically.\n",
    "RUN_POINTERS = {}\n",
    "_multitest_parent = Path(D7_OUT_ROOT) / \"multilingual_test_runs\"\n",
    "_monotest_parent = Path(D7_OUT_ROOT) / \"monolingual_test_runs\"\n",
    "\n",
    "RUN_POINTERS[\"base\"] = str(_find_latest_dir(\n",
    "    parent=_multitest_parent,\n",
    "    required_substrings=[\"base\"],\n",
    "    must_have_relpaths=[\"tag_run_pointer.json\"],\n",
    ") / \"tag_run_pointer.json\")\n",
    "\n",
    "for k in [\"trainEnh1\", \"trainEnh2\", \"trainEnh3\"]:\n",
    "    RUN_POINTERS[k] = str(_find_latest_dir(\n",
    "        parent=_monotest_parent,\n",
    "        required_substrings=[k],\n",
    "        must_have_relpaths=[\"tag_run_pointer.json\"],\n",
    "    ) / \"tag_run_pointer.json\")\n",
    "\n",
    "# Trainval experiment folders used to load best_heads.pt for each seed.\n",
    "TRVAL_EXP_DIRS = {}\n",
    "_trval_parent = Path(D7_OUT_ROOT) / \"trainval_runs\"\n",
    "for tag in [\"base\", \"trainEnh1\", \"trainEnh2\", \"trainEnh3\"]:\n",
    "    TRVAL_EXP_DIRS[tag] = str(_find_latest_trainval_exp(_trval_parent, tag=tag, seeds=SEEDS))\n",
    "\n",
    "# -------------------------\n",
    "# Resolve run roots and normal predictions.csv paths from pointer JSONs\n",
    "# -------------------------\n",
    "# Reconstructs where the existing predictions.csv files live, without relying on earlier cells.\n",
    "def _load_json(path: str) -> Dict[str, Any]:\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def resolve_run_root_from_pointer(pointer_path: str) -> Tuple[str, str]:\n",
    "    \"\"\"\n",
    "    Returns (tag_root, run_root).\n",
    "    - tag_root: folder that contains tag_run_pointer.json\n",
    "    - run_root: folder that contains per-seed run subfolders with predictions.csv\n",
    "    \"\"\"\n",
    "    tag_root = str(Path(pointer_path).parent)\n",
    "    j = _load_json(pointer_path)\n",
    "\n",
    "    # Accept several common key names seen in run pointers.\n",
    "    for k in [\"run_root\", \"run_dir\", \"test_run_root\", \"root\", \"resolved_run_root\", \"resolved_test_run_root\"]:\n",
    "        v = j.get(k, None)\n",
    "        if isinstance(v, str) and v.strip():\n",
    "            v2 = v.strip()\n",
    "            if v2.startswith(\"/\"):\n",
    "                return tag_root, v2\n",
    "            return tag_root, str(Path(tag_root) / v2)\n",
    "\n",
    "    # Fallback: tag_root is used directly (typical for base layout).\n",
    "    return tag_root, tag_root\n",
    "\n",
    "def build_predictions_paths(run_root: str, seeds: List[int]) -> Dict[int, str]:\n",
    "    \"\"\"\n",
    "    Builds the expected per-seed predictions.csv paths under a resolved run folder.\n",
    "    Fails fast if any seed file is missing.\n",
    "    \"\"\"\n",
    "    out = {}\n",
    "    for s in seeds:\n",
    "        p = str(Path(run_root) / f\"run_D7_on_D2test_seed{s}\" / \"predictions.csv\")\n",
    "        if not os.path.isfile(p):\n",
    "            raise FileNotFoundError(f\"predictions.csv not found for seed{s} at: {p}\")\n",
    "        out[int(s)] = p\n",
    "    return out\n",
    "\n",
    "RUNS_RESOLVED: Dict[str, Any] = {}\n",
    "\n",
    "# Basic existence checks (no silent fallback).\n",
    "for tag, p in RUN_POINTERS.items():\n",
    "    if not os.path.isfile(p):\n",
    "        raise FileNotFoundError(f\"[{tag}] pointer_json not found: {p}\")\n",
    "for tag, p in TRVAL_EXP_DIRS.items():\n",
    "    if not os.path.isdir(p):\n",
    "        raise FileNotFoundError(f\"[{tag}] trainval exp dir not found: {p}\")\n",
    "\n",
    "# Build a resolved record per condition: where to read normal preds and where to write new forced-head outputs.\n",
    "for tag, pointer_path in RUN_POINTERS.items():\n",
    "    tag_root, run_root = resolve_run_root_from_pointer(pointer_path)\n",
    "    RUNS_RESOLVED[tag] = {\n",
    "        \"pointer_json\": pointer_path,\n",
    "        \"tag_root\": tag_root,\n",
    "        \"run_root\": run_root,\n",
    "        \"out_dir\": str(Path(MECH_EVAL_ROOT) / tag),\n",
    "        \"predictions_by_seed\": build_predictions_paths(run_root, SEEDS),\n",
    "        \"trainval_exp_dir\": TRVAL_EXP_DIRS[tag],\n",
    "    }\n",
    "\n",
    "print(\"\\nRUNS_RESOLVED rebuilt (this runtime):\")\n",
    "for tag in [\"base\", \"trainEnh1\", \"trainEnh2\", \"trainEnh3\"]:\n",
    "    print(f\"  [{tag}] run_root: {RUNS_RESOLVED[tag]['run_root']}\")\n",
    "\n",
    "# -------------------------\n",
    "# Inference settings (kept consistent with prior runs)\n",
    "# -------------------------\n",
    "# Controls model backbone, sampling rate, batching, and AMP behavior.\n",
    "BACKBONE_CKPT = \"facebook/wav2vec2-base\"  # If another backbone was used elsewhere, update here once.\n",
    "DROPOUT_P     = 0.2\n",
    "SR_EXPECTED   = 16000\n",
    "TINY_THRESH   = 1e-4\n",
    "\n",
    "PER_DEVICE_BS = 16\n",
    "NUM_WORKERS   = 0\n",
    "PIN_MEMORY    = False\n",
    "USE_AMP       = True\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"DEVICE:\", DEVICE, \"| USE_AMP:\", bool(USE_AMP and DEVICE.type == \"cuda\"))\n",
    "\n",
    "# -------------------------\n",
    "# D2 test set loader (labels, sex, and task grouping)\n",
    "# -------------------------\n",
    "# Loads the manifest, filters to split=test, and prepares a DataLoader that yields:\n",
    "# - waveform samples\n",
    "# - attention mask\n",
    "# - labels and metadata needed for slicing metrics\n",
    "D2_MANIFEST_ALL = f\"{D2_OUT_ROOT}/manifests/manifest_all.csv\"\n",
    "if not os.path.isfile(D2_MANIFEST_ALL):\n",
    "    raise FileNotFoundError(f\"Missing D2 manifest_all.csv: {D2_MANIFEST_ALL}\")\n",
    "\n",
    "m_all = pd.read_csv(D2_MANIFEST_ALL)\n",
    "req_cols = {\"split\", \"clip_path\", \"label_num\", \"task\", \"sex\"}\n",
    "missing = [c for c in sorted(req_cols) if c not in m_all.columns]\n",
    "if missing:\n",
    "    raise ValueError(f\"D2 manifest missing required columns: {missing}\")\n",
    "\n",
    "m_test = m_all[m_all[\"split\"].astype(str) == \"test\"].copy()\n",
    "if len(m_test) == 0:\n",
    "    raise ValueError(\"No rows found where split == 'test' in D2 manifest_all.csv\")\n",
    "\n",
    "# Normalizes D2 sex values into M/F/UNK.\n",
    "def _sex_norm_from_d2(sex_val: Any) -> str:\n",
    "    s = \"\" if sex_val is None else str(sex_val).strip().lower()\n",
    "    if s == \"male\":\n",
    "        return \"M\"\n",
    "    if s == \"female\":\n",
    "        return \"F\"\n",
    "    return \"UNK\"\n",
    "\n",
    "# Converts D2 task into the two groups used by the two-head model.\n",
    "def _task_group_from_task(task_val: Any) -> str:\n",
    "    t = \"\" if task_val is None else str(task_val).strip().lower()\n",
    "    return \"vowel\" if t == \"vowl\" else \"other\"\n",
    "\n",
    "m_test[\"sex_norm\"] = m_test[\"sex\"].apply(_sex_norm_from_d2)\n",
    "m_test[\"task_group\"] = m_test[\"task\"].apply(_task_group_from_task)\n",
    "m_test[\"y_true\"] = m_test[\"label_num\"].astype(int)\n",
    "if \"speaker_id\" not in m_test.columns:\n",
    "    m_test[\"speaker_id\"] = \"\"\n",
    "\n",
    "print(\"D2 TEST rows:\", len(m_test))\n",
    "print(\"Task group counts:\", m_test[\"task_group\"].value_counts().to_dict())\n",
    "\n",
    "class D2TestDataset(Dataset):\n",
    "    # Loads audio and constructs an attention mask aligned with the padding rule.\n",
    "    def __init__(self, df: pd.DataFrame):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return int(len(self.df))\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Dict[str, Any]:\n",
    "        r = self.df.iloc[int(idx)]\n",
    "        clip_path = str(r[\"clip_path\"])\n",
    "        if not os.path.exists(clip_path):\n",
    "            raise FileNotFoundError(f\"Missing clip_path: {clip_path}\")\n",
    "\n",
    "        y, sr = sf.read(clip_path, always_2d=False)\n",
    "        if isinstance(y, np.ndarray) and y.ndim > 1:\n",
    "            y = np.mean(y, axis=1)\n",
    "        y = np.asarray(y, dtype=np.float32)\n",
    "\n",
    "        if int(sr) != int(SR_EXPECTED):\n",
    "            raise ValueError(f\"Sample rate mismatch (expected {SR_EXPECTED}): {clip_path} has sr={sr}\")\n",
    "\n",
    "        task_group = str(r[\"task_group\"])\n",
    "\n",
    "        # Attention mask behavior:\n",
    "        # - vowel: mask trailing near-zero padding so it does not affect pooling\n",
    "        # - other: keep all samples active\n",
    "        attn = np.ones((len(y),), dtype=np.int64)\n",
    "        if task_group == \"vowel\":\n",
    "            k = -1\n",
    "            for j in range(len(y) - 1, -1, -1):\n",
    "                if abs(float(y[j])) > float(TINY_THRESH):\n",
    "                    k = j\n",
    "                    break\n",
    "            if k >= 0:\n",
    "                attn[:k+1] = 1\n",
    "                attn[k+1:] = 0\n",
    "            else:\n",
    "                attn[:] = 1\n",
    "        else:\n",
    "            attn[:] = 1\n",
    "\n",
    "        sid = r.get(\"speaker_id\", \"\")\n",
    "        sid = \"\" if (sid is None or (isinstance(sid, float) and np.isnan(sid))) else str(sid)\n",
    "\n",
    "        return {\n",
    "            \"input_values\": torch.from_numpy(y),\n",
    "            \"attention_mask\": torch.from_numpy(attn),\n",
    "            \"labels\": torch.tensor(int(r[\"y_true\"]), dtype=torch.long),\n",
    "            \"task_group\": task_group,\n",
    "            \"sex_norm\": str(r[\"sex_norm\"]),\n",
    "            \"clip_path\": clip_path,\n",
    "            \"speaker_id\": sid,\n",
    "        }\n",
    "\n",
    "def collate_fn(batch: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
    "    # Pads variable-length audio to a batch max length while padding the attention mask to match.\n",
    "    max_len = int(max(b[\"input_values\"].numel() for b in batch))\n",
    "    xs, ams, ys = [], [], []\n",
    "    tgs, sexs, cps, sids = [], [], [], []\n",
    "\n",
    "    for b in batch:\n",
    "        x = b[\"input_values\"]\n",
    "        a = b[\"attention_mask\"]\n",
    "        pad = max_len - x.numel()\n",
    "        if pad > 0:\n",
    "            x = torch.cat([x, torch.zeros(pad, dtype=x.dtype)], dim=0)\n",
    "            a = torch.cat([a, torch.zeros(pad, dtype=a.dtype)], dim=0)\n",
    "        xs.append(x)\n",
    "        ams.append(a)\n",
    "        ys.append(b[\"labels\"])\n",
    "        tgs.append(b[\"task_group\"])\n",
    "        sexs.append(b[\"sex_norm\"])\n",
    "        cps.append(b[\"clip_path\"])\n",
    "        sids.append(b[\"speaker_id\"])\n",
    "\n",
    "    return {\n",
    "        \"input_values\": torch.stack(xs, dim=0),\n",
    "        \"attention_mask\": torch.stack(ams, dim=0),\n",
    "        \"labels\": torch.stack(ys, dim=0),\n",
    "        \"task_group\": tgs,\n",
    "        \"sex_norm\": sexs,\n",
    "        \"clip_path\": cps,\n",
    "        \"speaker_id\": sids,\n",
    "    }\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    D2TestDataset(m_test),\n",
    "    batch_size=int(PER_DEVICE_BS),\n",
    "    shuffle=False,\n",
    "    num_workers=int(NUM_WORKERS),\n",
    "    pin_memory=bool(PIN_MEMORY),\n",
    "    collate_fn=collate_fn,\n",
    ")\n",
    "\n",
    "# -------------------------\n",
    "# Two-head model definition + head loading\n",
    "# -------------------------\n",
    "# Backbone is frozen; only the two heads are loaded from best_heads.pt.\n",
    "from transformers import Wav2Vec2Model\n",
    "\n",
    "class Wav2Vec2TwoHeadClassifier(nn.Module):\n",
    "    def __init__(self, ckpt: str, dropout_p: float):\n",
    "        super().__init__()\n",
    "        self.backbone = Wav2Vec2Model.from_pretrained(str(ckpt), use_safetensors=True)\n",
    "        for p in self.backbone.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "        H = int(self.backbone.config.hidden_size)\n",
    "        self.pre_vowel = nn.Sequential(nn.LayerNorm(H), nn.Dropout(float(dropout_p)))\n",
    "        self.pre_other = nn.Sequential(nn.LayerNorm(H), nn.Dropout(float(dropout_p)))\n",
    "        self.head_vowel = nn.Linear(H, 2)\n",
    "        self.head_other = nn.Linear(H, 2)\n",
    "\n",
    "    def masked_mean_pool(self, last_hidden: torch.Tensor, attn_mask_samples: torch.Tensor) -> torch.Tensor:\n",
    "        feat_mask = self.backbone._get_feature_vector_attention_mask(last_hidden.shape[1], attn_mask_samples)\n",
    "        feat_mask = feat_mask.to(last_hidden.device).unsqueeze(-1).type_as(last_hidden)\n",
    "        masked = last_hidden * feat_mask\n",
    "        denom = feat_mask.sum(dim=1).clamp(min=1.0)\n",
    "        return masked.sum(dim=1) / denom\n",
    "\n",
    "    def _heads_fp32(self, x_any: torch.Tensor, head: nn.Module) -> torch.Tensor:\n",
    "        # Ensures head matmul runs in FP32 even when AMP is enabled.\n",
    "        x = x_any.float()\n",
    "        if x.is_cuda:\n",
    "            with torch.amp.autocast(device_type=\"cuda\", enabled=False):\n",
    "                return head(x)\n",
    "        return head(x)\n",
    "\n",
    "    def forward_both_logits(self, input_values: torch.Tensor, attention_mask: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        # Computes both heads for every clip, then returns both logits for forced selection.\n",
    "        with torch.no_grad():\n",
    "            out = self.backbone(input_values=input_values, attention_mask=attention_mask)\n",
    "            last_hidden = out.last_hidden_state\n",
    "        pooled = self.masked_mean_pool(last_hidden, attention_mask)\n",
    "        z_v = self.pre_vowel(pooled.float())\n",
    "        z_o = self.pre_other(pooled.float())\n",
    "        logits_v = self._heads_fp32(z_v, self.head_vowel)\n",
    "        logits_o = self._heads_fp32(z_o, self.head_other)\n",
    "        return logits_v, logits_o\n",
    "\n",
    "def load_heads_into_model(model: nn.Module, best_heads_path: str) -> nn.Module:\n",
    "    # Loads saved head weights; supports both wrapped and raw state_dict formats.\n",
    "    obj = torch.load(best_heads_path, map_location=\"cpu\")\n",
    "    if isinstance(obj, dict) and \"state_dict\" in obj and isinstance(obj[\"state_dict\"], dict):\n",
    "        sd = obj[\"state_dict\"]\n",
    "    elif isinstance(obj, dict):\n",
    "        sd = obj\n",
    "    else:\n",
    "        raise RuntimeError(f\"Unsupported best_heads.pt format: {best_heads_path}\")\n",
    "    model.load_state_dict(sd, strict=False)\n",
    "    return model\n",
    "\n",
    "# -------------------------\n",
    "# Threshold used for scoring (read from normal predictions.csv)\n",
    "# -------------------------\n",
    "# Uses the single global threshold saved during the normal test run for that seed.\n",
    "def read_thr_used_global(pred_csv_path: str) -> float:\n",
    "    df = pd.read_csv(pred_csv_path)\n",
    "    if \"threshold_used_global\" not in df.columns:\n",
    "        raise KeyError(f\"threshold_used_global column missing in: {pred_csv_path}\")\n",
    "    vals = pd.to_numeric(df[\"threshold_used_global\"], errors=\"coerce\").dropna().unique()\n",
    "    if len(vals) == 0:\n",
    "        raise ValueError(f\"No usable threshold_used_global values in: {pred_csv_path}\")\n",
    "    if len(vals) > 1:\n",
    "        raise RuntimeError(f\"threshold_used_global not constant in: {pred_csv_path}. Unique: {vals[:10]}\")\n",
    "    return float(vals[0])\n",
    "\n",
    "# -------------------------\n",
    "# Forced-head inference and CSV writer\n",
    "# -------------------------\n",
    "# Runs inference once per forced mode and writes a predictions CSV with metadata for analysis.\n",
    "def infer_forced(loader: DataLoader, model: Wav2Vec2TwoHeadClassifier, forced_head: str, desc: str):\n",
    "    if forced_head not in [\"vowel\", \"other\"]:\n",
    "        raise ValueError(\"forced_head must be 'vowel' or 'other'\")\n",
    "\n",
    "    amp_ok = bool(USE_AMP and DEVICE.type == \"cuda\")\n",
    "    y_true_all, y_score_all = [], []\n",
    "    sex_all, tg_all, clip_all, spk_all = [], [], [], []\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(loader, desc=desc):\n",
    "            x = batch[\"input_values\"].to(DEVICE, non_blocking=True)\n",
    "            a = batch[\"attention_mask\"].to(DEVICE, non_blocking=True)\n",
    "            y = batch[\"labels\"].detach().cpu().numpy().astype(np.int64)\n",
    "\n",
    "            if amp_ok:\n",
    "                with torch.amp.autocast(device_type=\"cuda\", enabled=True):\n",
    "                    lv, lo = model.forward_both_logits(x, a)\n",
    "            else:\n",
    "                lv, lo = model.forward_both_logits(x, a)\n",
    "\n",
    "            logits = lv if forced_head == \"vowel\" else lo\n",
    "            p = torch.softmax(logits.float(), dim=-1)[:, 1].detach().cpu().numpy().astype(np.float64)\n",
    "\n",
    "            y_true_all.extend(y.tolist())\n",
    "            y_score_all.extend(p.tolist())\n",
    "            sex_all.extend(list(batch[\"sex_norm\"]))\n",
    "            tg_all.extend(list(batch[\"task_group\"]))\n",
    "            clip_all.extend(list(batch[\"clip_path\"]))\n",
    "            spk_all.extend(list(batch[\"speaker_id\"]))\n",
    "\n",
    "    return (\n",
    "        np.asarray(clip_all, dtype=object),\n",
    "        np.asarray(y_true_all, dtype=np.int64),\n",
    "        np.asarray(y_score_all, dtype=np.float64),\n",
    "        np.asarray(sex_all, dtype=object),\n",
    "        np.asarray(spk_all, dtype=object),\n",
    "        np.asarray(tg_all, dtype=object),\n",
    "    )\n",
    "\n",
    "def write_force_csv(out_csv: str, clip_path, y_true, y_score, sex_norm, speaker_id, task_group, seed, forced_head, thr, trainval_exp_tag, run_stamp):\n",
    "    # Saves forced-head predictions with enough fields to reproduce slicing and scoring later.\n",
    "    df = pd.DataFrame({\n",
    "        \"clip_path\": clip_path,\n",
    "        \"y_true\": y_true,\n",
    "        \"y_score\": y_score,\n",
    "        \"sex_norm\": sex_norm,\n",
    "        \"speaker_id\": speaker_id,\n",
    "        \"task_group\": task_group,\n",
    "        \"seed\": int(seed),\n",
    "        \"forced_head\": str(forced_head),\n",
    "        \"threshold_used_global\": float(thr),\n",
    "        \"trainval_exp_tag\": str(trainval_exp_tag),\n",
    "        \"run_stamp\": str(run_stamp),\n",
    "    })\n",
    "    Path(out_csv).parent.mkdir(parents=True, exist_ok=True)\n",
    "    df.to_csv(out_csv, index=False)\n",
    "\n",
    "# -------------------------\n",
    "# Metrics helpers (AUROC and ΔFNR)\n",
    "# -------------------------\n",
    "# Computes AUROC per task group, plus ΔFNR across sex within the PD subset.\n",
    "def compute_auc(y_true: np.ndarray, y_prob: np.ndarray) -> float:\n",
    "    # AUROC is undefined if only one class is present; return NaN to keep the pipeline running.\n",
    "    if len(np.unique(y_true)) < 2:\n",
    "        return float(\"nan\")\n",
    "    return float(roc_auc_score(y_true, y_prob))\n",
    "\n",
    "def fairness_delta_fnr_pd_only(y_true: np.ndarray, y_prob: np.ndarray, sex_norm: np.ndarray, thr: float) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Computes false negative rate (FNR) within PD-only clips for M and F, then:\n",
    "      ΔFNR = FNR(F) − FNR(M)\n",
    "    \"\"\"\n",
    "    y_true = np.asarray(y_true, dtype=np.int64)\n",
    "    y_prob = np.asarray(y_prob, dtype=np.float64)\n",
    "    sex_norm = np.asarray(sex_norm, dtype=object)\n",
    "\n",
    "    pd_mask = (y_true == 1)\n",
    "    out = {\"fnr_M\": float(\"nan\"), \"fnr_F\": float(\"nan\"), \"delta_f_minus_m\": float(\"nan\"),\n",
    "           \"n_pd_M\": 0, \"n_pd_F\": 0}\n",
    "\n",
    "    for g in [\"M\", \"F\"]:\n",
    "        gm = pd_mask & (sex_norm == g)\n",
    "        n_pd = int(gm.sum())\n",
    "        out[f\"n_pd_{g}\"] = n_pd\n",
    "        if n_pd == 0:\n",
    "            continue\n",
    "        pred = (y_prob[gm] >= float(thr)).astype(np.int64)\n",
    "        fn = int((pred == 0).sum())\n",
    "        out[f\"fnr_{g}\"] = float(fn / n_pd)\n",
    "\n",
    "    if not np.isnan(out[\"fnr_M\"]) and not np.isnan(out[\"fnr_F\"]):\n",
    "        out[\"delta_f_minus_m\"] = float(out[\"fnr_F\"] - out[\"fnr_M\"])\n",
    "    return out\n",
    "\n",
    "def score_df_by_taskgroup(df: pd.DataFrame, thr: float) -> Dict[str, Any]:\n",
    "    # Returns AUROC and ΔFNR metrics separately for vowel clips and other clips.\n",
    "    out = {}\n",
    "    for tg in [\"vowel\", \"other\"]:\n",
    "        sub = df[df[\"task_group\"].astype(str) == tg].copy()\n",
    "        yt = sub[\"y_true\"].to_numpy(dtype=np.int64)\n",
    "        yp = sub[\"y_score\"].to_numpy(dtype=np.float64)\n",
    "        sx = sub[\"sex_norm\"].to_numpy(dtype=object)\n",
    "        out[tg] = {\n",
    "            \"n\": int(len(sub)),\n",
    "            \"auc\": compute_auc(yt, yp),\n",
    "            \"delta_fnr\": fairness_delta_fnr_pd_only(yt, yp, sx, thr),\n",
    "        }\n",
    "    return out\n",
    "\n",
    "def mean_sd(arr: np.ndarray) -> Tuple[float, float]:\n",
    "    # Convenience for plotting mean ± SD across the three seeds.\n",
    "    arr = np.asarray(arr, dtype=np.float64)\n",
    "    return float(np.nanmean(arr)), float(np.nanstd(arr, ddof=0))\n",
    "\n",
    "# -------------------------\n",
    "# Forced-head inference for all conditions and seeds\n",
    "# -------------------------\n",
    "# For each condition and seed:\n",
    "# - read the seed's saved threshold from normal predictions.csv\n",
    "# - load best_heads.pt\n",
    "# - write predictions_force_vowel.csv and predictions_force_other.csv\n",
    "RUN_STAMP = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "PLOTS_ALL_DIR = str(Path(MECH_EVAL_ROOT) / \"plots_all_conditions\")\n",
    "Path(PLOTS_ALL_DIR).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "forced_index = {}\n",
    "t0 = time.time()\n",
    "\n",
    "for tag in [\"base\", \"trainEnh1\", \"trainEnh2\", \"trainEnh3\"]:\n",
    "    forced_index[tag] = {\"seeds\": SEEDS, \"files_by_seed\": {}}\n",
    "\n",
    "    exp_dir = Path(TRVAL_EXP_DIRS[tag])\n",
    "    trainval_exp_tag = exp_dir.name\n",
    "\n",
    "    for seed in SEEDS:\n",
    "        normal_pred = RUNS_RESOLVED[tag][\"predictions_by_seed\"][seed]\n",
    "        thr = read_thr_used_global(normal_pred)\n",
    "\n",
    "        best_heads = exp_dir / f\"run_D7_seed{seed}\" / \"best_heads.pt\"\n",
    "        if not best_heads.exists():\n",
    "            raise FileNotFoundError(f\"[{tag}] best_heads.pt missing for seed{seed}: {str(best_heads)}\")\n",
    "\n",
    "        out_run_dir = Path(MECH_EVAL_ROOT) / tag / \"forced_head_runs\" / f\"run_D7_on_D2test_seed{seed}\"\n",
    "        out_force_v = out_run_dir / \"predictions_force_vowel.csv\"\n",
    "        out_force_o = out_run_dir / \"predictions_force_other.csv\"\n",
    "\n",
    "        # Build model and load the trained heads for this seed.\n",
    "        model = Wav2Vec2TwoHeadClassifier(BACKBONE_CKPT, dropout_p=DROPOUT_P).to(DEVICE)\n",
    "        model = load_heads_into_model(model, str(best_heads))\n",
    "\n",
    "        # Forced vowel head for all clips.\n",
    "        cp, yt, yp, sx, sid, tg = infer_forced(test_loader, model, \"vowel\", f\"[{tag} seed{seed}] FORCE_VOWEL_HEAD\")\n",
    "        write_force_csv(str(out_force_v), cp, yt, yp, sx, sid, tg, seed, \"vowel\", thr, trainval_exp_tag, RUN_STAMP)\n",
    "\n",
    "        # Forced other head for all clips.\n",
    "        cp2, yt2, yp2, sx2, sid2, tg2 = infer_forced(test_loader, model, \"other\", f\"[{tag} seed{seed}] FORCE_OTHER_HEAD\")\n",
    "        write_force_csv(str(out_force_o), cp2, yt2, yp2, sx2, sid2, tg2, seed, \"other\", thr, trainval_exp_tag, RUN_STAMP)\n",
    "\n",
    "        forced_index[tag][\"files_by_seed\"][str(seed)] = {\n",
    "            \"threshold_used_global\": float(thr),\n",
    "            \"predictions_normal\": str(normal_pred),\n",
    "            \"predictions_force_vowel\": str(out_force_v),\n",
    "            \"predictions_force_other\": str(out_force_o),\n",
    "        }\n",
    "\n",
    "# Registry file listing all produced outputs (paths and thresholds).\n",
    "registry_path = Path(MECH_EVAL_ROOT) / \"forced_head_registry.json\"\n",
    "with open(registry_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(forced_index, f, indent=2)\n",
    "print(\"\\nWROTE:\", str(registry_path))\n",
    "print(\"Forced inference total seconds:\", time.time() - t0)\n",
    "\n",
    "# -------------------------\n",
    "# Score all modes and write a long metrics table\n",
    "# -------------------------\n",
    "# Produces one row per: condition × seed × routing_mode × task_group.\n",
    "rows = []\n",
    "for tag in [\"base\", \"trainEnh1\", \"trainEnh2\", \"trainEnh3\"]:\n",
    "    for seed in SEEDS:\n",
    "        info = forced_index[tag][\"files_by_seed\"][str(seed)]\n",
    "        thr = float(info[\"threshold_used_global\"])\n",
    "\n",
    "        df_n = pd.read_csv(info[\"predictions_normal\"])\n",
    "        df_v = pd.read_csv(info[\"predictions_force_vowel\"])\n",
    "        df_o = pd.read_csv(info[\"predictions_force_other\"])\n",
    "\n",
    "        # Verify the needed columns exist for scoring and slicing.\n",
    "        for name, df in [(\"normal\", df_n), (\"force_vowel\", df_v), (\"force_other\", df_o)]:\n",
    "            need = [\"y_true\", \"y_score\", \"sex_norm\", \"task_group\"]\n",
    "            miss = [c for c in need if c not in df.columns]\n",
    "            if miss:\n",
    "                raise KeyError(f\"[{tag} seed{seed} {name}] missing required cols {miss}\")\n",
    "\n",
    "        for mode, df in [(\"normal\", df_n), (\"force_vowel\", df_v), (\"force_other\", df_o)]:\n",
    "            scored = score_df_by_taskgroup(df, thr)\n",
    "            for tg in [\"vowel\", \"other\"]:\n",
    "                rows.append({\n",
    "                    \"condition\": tag,\n",
    "                    \"seed\": int(seed),\n",
    "                    \"routing_mode\": mode,\n",
    "                    \"task_group\": tg,\n",
    "                    \"auc\": float(scored[tg][\"auc\"]),\n",
    "                    \"delta_fnr_f_minus_m\": float(scored[tg][\"delta_fnr\"][\"delta_f_minus_m\"]),\n",
    "                    \"n_pd_m\": int(scored[tg][\"delta_fnr\"][\"n_pd_M\"]),\n",
    "                    \"n_pd_f\": int(scored[tg][\"delta_fnr\"][\"n_pd_F\"]),\n",
    "                    \"threshold_used_global\": float(thr),\n",
    "                })\n",
    "\n",
    "metrics_long = pd.DataFrame(rows)\n",
    "metrics_csv = Path(MECH_EVAL_ROOT) / \"forced_head_metrics_long.csv\"\n",
    "metrics_long.to_csv(metrics_csv, index=False)\n",
    "print(\"WROTE:\", str(metrics_csv))\n",
    "\n",
    "# -------------------------\n",
    "# Per-condition JSON summaries\n",
    "# -------------------------\n",
    "# Writes a compact per-seed summary for each condition so results stay localized.\n",
    "for tag in [\"base\", \"trainEnh1\", \"trainEnh2\", \"trainEnh3\"]:\n",
    "    out_json = Path(MECH_EVAL_ROOT) / tag / \"forced_head_eval_summary.json\"\n",
    "    pack = {\"tag\": tag, \"run_stamp\": RUN_STAMP, \"seeds\": SEEDS, \"by_seed\": {}}\n",
    "\n",
    "    for seed in SEEDS:\n",
    "        info = forced_index[tag][\"files_by_seed\"][str(seed)]\n",
    "        thr = float(info[\"threshold_used_global\"])\n",
    "\n",
    "        df_n = pd.read_csv(info[\"predictions_normal\"])\n",
    "        df_v = pd.read_csv(info[\"predictions_force_vowel\"])\n",
    "        df_o = pd.read_csv(info[\"predictions_force_other\"])\n",
    "\n",
    "        pack[\"by_seed\"][str(seed)] = {\n",
    "            \"threshold_used_global\": thr,\n",
    "            \"normal\": score_df_by_taskgroup(df_n, thr),\n",
    "            \"force_vowel\": score_df_by_taskgroup(df_v, thr),\n",
    "            \"force_other\": score_df_by_taskgroup(df_o, thr),\n",
    "            \"files\": info,\n",
    "        }\n",
    "\n",
    "    with open(out_json, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(pack, f, indent=2)\n",
    "    print(\"WROTE:\", str(out_json))\n",
    "\n",
    "# -------------------------\n",
    "# Plot helpers (mean ± SD across seeds)\n",
    "# -------------------------\n",
    "# Aggregates across seeds so each bar or point represents mean performance with variability.\n",
    "CONDS = [\"base\", \"trainEnh1\", \"trainEnh2\", \"trainEnh3\"]\n",
    "MODES = [\"normal\", \"force_vowel\", \"force_other\"]\n",
    "\n",
    "def summarize_metric(task_group: str, metric_col: str) -> Dict[Tuple[str, str], Tuple[float, float]]:\n",
    "    \"\"\"\n",
    "    Returns {(condition, routing_mode): (mean, sd)} over seeds.\n",
    "    metric_col: \"auc\" or \"delta_fnr_f_minus_m\"\n",
    "    \"\"\"\n",
    "    out = {}\n",
    "    for cond in CONDS:\n",
    "        for mode in MODES:\n",
    "            sub = metrics_long[\n",
    "                (metrics_long[\"condition\"] == cond) &\n",
    "                (metrics_long[\"routing_mode\"] == mode) &\n",
    "                (metrics_long[\"task_group\"] == task_group)\n",
    "            ]\n",
    "            vals = sub[metric_col].to_numpy(dtype=np.float64)\n",
    "            out[(cond, mode)] = mean_sd(vals)\n",
    "    return out\n",
    "\n",
    "def plot_grouped_bars(task_group: str, metric_col: str, ylabel: str, title: str, out_png: str):\n",
    "    # Creates one grouped bar chart per task group with three routing modes.\n",
    "    summ = summarize_metric(task_group, metric_col)\n",
    "    x = np.arange(len(CONDS))\n",
    "    bar_w = 0.24\n",
    "    offsets = {\"normal\": -bar_w, \"force_vowel\": 0.0, \"force_other\": bar_w}\n",
    "\n",
    "    plt.figure(figsize=(12, 4.8))\n",
    "    for mode in MODES:\n",
    "        means = [summ[(c, mode)][0] for c in CONDS]\n",
    "        sds   = [summ[(c, mode)][1] for c in CONDS]\n",
    "        plt.bar(x + offsets[mode], means, width=bar_w, yerr=sds, capsize=3, label=mode)\n",
    "\n",
    "    plt.xticks(x, CONDS)\n",
    "    plt.xlabel(\"condition\")\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_png, dpi=200)\n",
    "    plt.close()\n",
    "    print(\"WROTE:\", out_png)\n",
    "\n",
    "# -------------------------\n",
    "# Required plots (5 total)\n",
    "# -------------------------\n",
    "# (1) AUROC by condition and routing mode — vowel-only\n",
    "plot_grouped_bars(\n",
    "    task_group=\"vowel\",\n",
    "    metric_col=\"auc\",\n",
    "    ylabel=\"AUROC (mean ± SD across seeds)\",\n",
    "    title=\"AUROC by condition and routing mode — vowel-only\",\n",
    "    out_png=str(Path(PLOTS_ALL_DIR) / \"auroc_by_condition_vowel_only.png\"),\n",
    ")\n",
    "\n",
    "# (2) AUROC by condition and routing mode — other-only\n",
    "plot_grouped_bars(\n",
    "    task_group=\"other\",\n",
    "    metric_col=\"auc\",\n",
    "    ylabel=\"AUROC (mean ± SD across seeds)\",\n",
    "    title=\"AUROC by condition and routing mode — other-only\",\n",
    "    out_png=str(Path(PLOTS_ALL_DIR) / \"auroc_by_condition_other_only.png\"),\n",
    ")\n",
    "\n",
    "# (3) ΔFNR by condition and routing mode — vowel-only\n",
    "plot_grouped_bars(\n",
    "    task_group=\"vowel\",\n",
    "    metric_col=\"delta_fnr_f_minus_m\",\n",
    "    ylabel=\"ΔFNR = FNR(F) − FNR(M) (mean ± SD across seeds)\",\n",
    "    title=\"ΔFNR by condition and routing mode — vowel-only\",\n",
    "    out_png=str(Path(PLOTS_ALL_DIR) / \"deltaFNR_by_condition_vowel_only.png\"),\n",
    ")\n",
    "\n",
    "# (4) ΔFNR by condition and routing mode — other-only\n",
    "plot_grouped_bars(\n",
    "    task_group=\"other\",\n",
    "    metric_col=\"delta_fnr_f_minus_m\",\n",
    "    ylabel=\"ΔFNR = FNR(F) − FNR(M) (mean ± SD across seeds)\",\n",
    "    title=\"ΔFNR by condition and routing mode — other-only\",\n",
    "    out_png=str(Path(PLOTS_ALL_DIR) / \"deltaFNR_by_condition_other_only.png\"),\n",
    ")\n",
    "\n",
    "# (5) Head sensitivity: AUROC(force vowel) − AUROC(force other), shown for vowel clips and other clips\n",
    "sens_rows = []\n",
    "for cond in CONDS:\n",
    "    for seed in SEEDS:\n",
    "        for tg in [\"vowel\", \"other\"]:\n",
    "            a_v = metrics_long[(metrics_long[\"condition\"] == cond) & (metrics_long[\"seed\"] == seed) &\n",
    "                               (metrics_long[\"routing_mode\"] == \"force_vowel\") & (metrics_long[\"task_group\"] == tg)][\"auc\"].iloc[0]\n",
    "            a_o = metrics_long[(metrics_long[\"condition\"] == cond) & (metrics_long[\"seed\"] == seed) &\n",
    "                               (metrics_long[\"routing_mode\"] == \"force_other\") & (metrics_long[\"task_group\"] == tg)][\"auc\"].iloc[0]\n",
    "            sens_rows.append({\"condition\": cond, \"seed\": seed, \"task_group\": tg, \"sens\": float(a_v - a_o)})\n",
    "\n",
    "sens_df = pd.DataFrame(sens_rows)\n",
    "\n",
    "sens_png = str(Path(PLOTS_ALL_DIR) / \"head_sensitivity_auc_forceV_minus_forceO.png\")\n",
    "plt.figure(figsize=(12, 4.8))\n",
    "x = np.arange(len(CONDS))\n",
    "for tg in [\"vowel\", \"other\"]:\n",
    "    sub = sens_df[sens_df[\"task_group\"] == tg]\n",
    "    means = []\n",
    "    sds = []\n",
    "    for cond in CONDS:\n",
    "        vals = sub[sub[\"condition\"] == cond][\"sens\"].to_numpy(dtype=np.float64)\n",
    "        m, sd = mean_sd(vals)\n",
    "        means.append(m)\n",
    "        sds.append(sd)\n",
    "    plt.errorbar(x, means, yerr=sds, fmt=\"o-\", capsize=3, label=tg)\n",
    "plt.axhline(0.0, linewidth=1)\n",
    "plt.xticks(x, CONDS)\n",
    "plt.xlabel(\"condition\")\n",
    "plt.ylabel(\"AUROC(force vowel) − AUROC(force other) (mean ± SD)\")\n",
    "plt.title(\"Head sensitivity by task group\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(sens_png, dpi=200)\n",
    "plt.close()\n",
    "print(\"WROTE:\", sens_png)\n",
    "\n",
    "# -------------------------\n",
    "# Runtime unload (same pattern as other test-only cells)\n",
    "# -------------------------\n",
    "try:\n",
    "    from google.colab import runtime  # type: ignore\n",
    "    print(\"\\nUnassigning runtime (Colab)...\")\n",
    "    runtime.unassign()\n",
    "except Exception as e:\n",
    "    print(\"\\nNOTE: Could not unassign runtime. Reason:\", repr(e))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "1839209231164245a89891c7045ad740",
      "540b89a6ed9748478d7a8be90625a033",
      "cebce03e99bc481b8e22c0a7c1e7dd31",
      "c60a4fb1e0a545939cee7d13adda8ece",
      "9e6f88d2ab6e425392d50b7bf38a69e5",
      "f547ca0447014bf2a5f598b8b399b657",
      "adf72ab9ccbe4774a89246a30838ccc0",
      "80b2a9bc072b48bc8265c11a5e63f3a4",
      "bbd1f55abac74d66ad31585996683ac6",
      "8bd8adeac2da4db3b4cd18aa9e5ecab7",
      "1c9f80f9b74e4289a5f2d172cc19518b",
      "b7c9b704d3b841e28e80734f5e31ec37",
      "5621506b8f5f4922a14928637c80db38",
      "2a2d891adf934481b7458ba381ed8eba",
      "265679f0b8154ab491692811c88b264d",
      "e44262ba09d94d3aad84defb986c970c",
      "4029a6c97a584d949bda1537b9a25bf9",
      "eb98fc478dda44f1950cd90771e05010",
      "c88f7ac0b11e421faf1c9b400fab71c7",
      "0cc8a9a9a27240228752c7712621d674",
      "54ecd4406e234fe89b7f30a122c21f3c",
      "590d0260e23049d7a3462cb88532be67",
      "c43de49e9b6c425ea8c371e946eea264",
      "ac848b8eb22c45f4a1234aad2a5b5f3b",
      "42d18b26ccd2410ca1b3173f2762af31",
      "03a917a0da3b45f8afb2a16daf950478",
      "e50f9e3f8604414689f4a366c235b847",
      "e0a150e990ae4d23a394d741168f3ebc",
      "238c68287f2e423aaa31ef8f6d071706",
      "2b18bf0ca85c4324813a940ddcbacbe2",
      "65892e9b132d449ab8795f5fc5ed56ee",
      "f199fc5aba2d4140bb108b7bf4147449",
      "1c45241ab09b46b7819984e5f8f72635",
      "85bb159df6ba43ea89a4ce2a033155bf",
      "df807ffe38b443488b13f65099f99273",
      "6e2b3419d1fe4d25b21d2624affc9580",
      "51ed6ba8009c4d2e8964f0d6f6814a59",
      "b5da654fe0c84c389eb979f3041c3f4a",
      "dd8ae4574cb647f28ae2e81aa80ecfc6",
      "4eb05632e0ed4df9991b2f10f12feab1",
      "2aeebf306b254063a91de069c4f3c2ce",
      "6f800a93ddfe4ad2954fde0eb9e90e06",
      "92900370394c4755a6a49f9770c3f60d",
      "e09bac0019034478ac74e79dbf0b3e02",
      "a844db5698084818b99edd39e16ac00b",
      "59acf2e539a24811b33bfe81f8bc22a2",
      "202166d86f234755afb0e4dcfbe31c3e",
      "b1c49c0a76dd4cd8b380db8b1f0d9e43",
      "04f0c30913a842a7b86fe791a0ce2f4c",
      "873a8086650c44c7bfbf65392e82df68",
      "ee30a120aaff42639f76fba370a0b0af",
      "ed9ebd09e00c4c158b8fdef70323bdb9",
      "b474540ac384445bb3dbeba479faf65b",
      "829b64b4298448d7bdff4073cb78989e",
      "bffe3ecc73354255b51ae3099736e2b2",
      "cab526f1d2db4652a545966704ca530c",
      "8805d59a25ee4fc59abf4473a2e8c99e",
      "2e848c0abd1d4d27bbd7b458b13af165",
      "477eb61b751b493886471c93054fd43f",
      "e18f9bce951d4d72b7d934575bcd45f0",
      "7d13a79a0fac4d1e8d05c8ddb1ddf76f",
      "73b08f731f0943e3a3470b23828ee6bb",
      "4602396b6c564aacabf1302d21f716a4",
      "c72929a85ff94b199ca7e31bfdcf8cd7",
      "b6f61ff867dd4c08859d5abcb99e6273",
      "1ff8cd00fdaa4726bfd80c3c616e9441",
      "feebee87791c457cb127fca01e89f877",
      "97ce5e37f246427cba1c29757223938e",
      "c56228bfaf41433993df35c42b2b4f24",
      "0c6aa4d718994ea7acee2964160110c3",
      "679dc91a08604697a966f870408f0f23",
      "9b87a5574fe143a484c0ea9409fdb8d9",
      "c79b92d2def14e76ab3e16ddb3d8d833",
      "1ec2389ff12f4417b87c96c97ea84aba",
      "b000d7c260f846f18997e46ab385eb43",
      "806c1eb9db164f5eba6e50f9236ca834",
      "1e2ed27d7ac047b793dc2184271cb7e9",
      "275731de521641568fb52014dcad665a",
      "36897d73bcc248a6a26e3e4e3c525077",
      "8c0831a7164845dead2199656dc2ea9e",
      "e0eec640e33349788c73f5cc96f5f2af",
      "b8e525a01cc84a39a1e0a862963c253b",
      "e9049ad20a4c4ed59befcd0f34c28522",
      "d45d58e6da9742bfac1c3a8ca787a0e2",
      "726e80bfc1904a6c8a2edebee11ff7b9",
      "610f34f2a6bb4fa1b1c2e461e0caac83",
      "6fa3b024ed344104b656e1cee16c1f56",
      "b650b85f98b14282a95bd4633b6eaf44",
      "f0126b3bc49e42b1b50eb4ef9966756b",
      "3ae3617f4d984afdb88cbf3c84a27ac8",
      "c1d5b85a7b57410e9daf7a4f300b422a",
      "21cbc9678acb4beaaecd851946662374",
      "e6d0fcb95d9e4958b8b2996ccb91726a",
      "b3360bea63074668a3d9003129c9028c",
      "29fc512e9acb4b289182efddd3961f1e",
      "f269ecd7124041ddac6297021bba0568",
      "32e9a382807d47eda435280871676795",
      "ae53fa40b50b47d98e20b80952ebfa47",
      "419700917fa6496e8ba749306cf80922",
      "c8b96896b16f444294ea497890351679",
      "0030ad99e1224319bc0ce6fc4bd365f7",
      "554b739f8bd940769680f9bc35740728",
      "9c4a487cd2254a3d908ca76dbcd4b5a8",
      "54a016f40c8d41558a75b3a2c950ccb2",
      "949afa29ad1043188deb7c5f41d9e42c",
      "63160f51f17f43d585a2ca97db331e88",
      "1a60736323f146ba8fdd3beb376d23b7",
      "8307eaefc5964d15a53c99c1a951a05a",
      "96aa6e0e77144a5babca8b5ec61fe0ec",
      "a73df074951d48a8bb8777a06ac93be8",
      "da793f736384434ab4845b455969021b",
      "e27fb5b448fe48f587f7e7126363bc5d",
      "170c4ca8774b4874998f67dd5cf78421",
      "c8d4a5296aac44a691d1e1101c74a849",
      "5d94a6e9801e439093f6446468815aa6",
      "31e181b4b40b4bdd9cc39d81301a0073",
      "80a12bb5adeb4580ab73f856333b561a",
      "0351217b2987443fbcea04b973aa300b",
      "af84ba3c5277443bb5b6df8c37377a42",
      "7247a5830f974420967bb16ba5455cc5",
      "05f85f43101641ba9913bd1e28f0f32f",
      "5d5525e87f5f4844a081295cfc3288cb",
      "6a0bcf7a11494f3fac5a4db82db2d38d",
      "3e413562117b4ab8b8762345040b4b36",
      "d415412b487c4dc48b3c1ef74544afc4",
      "7b223044222c4b9b93a49e3dec9c7b44",
      "a1c497f6fcaa4f23b56922f1ee99377b",
      "00896bc5c7724c3e83cae83a3fc0229a",
      "c6deefe1905248b4a90ece5c565f8254",
      "bf7b2490e37b4725854c48ae31a35faf",
      "8a25d752ea3547fabab51a399de0ca0b",
      "a1cf133f5abf48dfb943311f053e55ed",
      "28981d58fa5c48b8bd6645a7c9d2e9e4",
      "eeebdea37afb46628152312f07ba4637",
      "69433ccb702a4b4a982f4497985836bf",
      "71ab0b1700f44b4897f3d4e2a9f1dd68",
      "091d06eccbc94e37b907338130d0c320",
      "68b800f0c2ac481cb9b2f021bf5508e6",
      "26c52d2e13df44609a4c931d1c21a7ca",
      "2d79cdf272604f1db84b5bcf59119761",
      "9c689635daba4ae19aece783f7ee779b",
      "0a50dac0d83047328a15d5bf224692be",
      "ad285fe90f3549eb8759132edda29663",
      "cf78a7a26354452cbc335555e8dfe4c4",
      "a8b3ee8001dd4e7e9759b16b15823f39",
      "7ab316b1d3b94582beff9a2ae15685a0",
      "1ff4a5066f884bd8bdd26f1596759ca5",
      "16e5f787fa4d44cdabf971404baa35ad",
      "d4bf3339bfb149dbbeddabebedce72bd",
      "ae8091bd67ba4fda9e87bad548c92da4",
      "aac0c3723ce34c92bf6b312a8c19e40b",
      "0c6abb4f3cb146fe9ad926888636da2c",
      "0341ab5a69904e249067649746519ebe",
      "9631e5b500bd46cbbafdab876515056f",
      "8f8dc15af8074fed801f55ca66ad5faf",
      "9fc626400aff4fadb9d71cae6dd8677e",
      "43f4e25c0c96448093fb35f550716a31",
      "3c8d7e831a7241fcb96ba786f7b9b2d2",
      "c220b8a79ed140a0a20738500abec9c1",
      "5dc2021a185f4a4aa01ebb44df9b3433",
      "d4df7e81b88b458fb22c99eca8e926b8",
      "e46bcc295c1f472c979e17b4a7da6418",
      "e212491a1c384dacb5d5d65e3d057d0a",
      "b4ce85eb32cf4f9f9166ad23b466e90a",
      "2be6584bea7a46809fe02063bd180c41",
      "1e84a3d681bc4cd894648e7bffe48ca6",
      "739f90564acd4f658631bbd6d509eb13",
      "5cf2fbf94dc74493b993a794dc7a36a7",
      "d046c5c47b3c4e35b0626a85ad29f594",
      "748c37698f2c419c8f5174f3fe9be89e",
      "a7426733e9a9401eb0e35320a69c18b0",
      "6209244fe8e74ebcb0e0606d02d423d7",
      "e556216d142747ac8cba62d14e9e6c51",
      "28eca465f1f84156bc728a74c6e77461",
      "16a573bf43f848789f02f10d1849714c",
      "97dcdc29f898493684e04b9deccf81b3",
      "c33164ba2fda410ca61d53afa78df8ae",
      "4050550ff6bf43cda219fcbb1ac1cbf9",
      "7e044a1376da42d48b794be801075629",
      "2475f5af27054ccd9a0f08ab728293bb",
      "3b020aa5ae6a43e78ff5799f3b680d8a",
      "09eb77f207484d8a9902c934e6f94540",
      "fe3178d8f38e4aa297610abc28184a5a",
      "a25d53dcf0814c3ea8430e8155c33774",
      "df4605fa51424da99a7f1b2fa21596ad",
      "281ed93ddb8142c29540cec9cef12b46",
      "e0939f8f083d46feb95b27369ca8c917",
      "1bf3e330221a44a780eb5e08df07693d",
      "ff164dbba20b4759a91732b1d2de3b5a",
      "1dbd9a63a9c84a2dbe6b98f550c6bb78",
      "e62a1225596d4e048322620b14b2e540",
      "7e76bad62a02415ab79aaf0bef03aef3",
      "0017a06f2d604e1587258b2b1987c3ca",
      "c7d8a91b08824944a6c34de4b6dcce9d",
      "0cf0912532c745789a33e58bc87645e2",
      "7076593461c84c4ca3c47100ea1c09be",
      "9987c3b466704a7ab71336b95f7a73ca",
      "6e1ed0c77c6c4633bb145884db92f771",
      "0fafa6eb7df3447e874b359cab21120c",
      "f2e7827a57ba4f4892374496c524c328",
      "3fb78054da714162994ac171bc00b1d9",
      "2b5661e65cd14781bb963a9ce6edc916",
      "30ceeb2b3f04420096e426f34ab0dc98",
      "1d83c810232342c5ac2e6f8b17065564",
      "5904e5c2329641389379b44a486f58b0",
      "db48ca9d7eed4a50b2ebe16dd3582207",
      "dd17f6d966ba420bb98eab547425bbb9",
      "2cc92ae50ece42dd93ba5f626df4f7c3",
      "5ae37abe3c254673b883eeef54c889e1",
      "b3e9c251f2cd4835b3a42abd05820594",
      "ea36e289478443138046e85bcaf5b1d1",
      "85d9befa470943a8af09aaa619032a0a",
      "c756c7ee6904405f85368435a4ffcd92",
      "2380e0ca62a0403aa86c05f30f9f9878",
      "1c7341b4aa9549099f9213f6012de213",
      "a1eb2638799147f5a329605acb16bbba",
      "20ddbc4fa67c44a38ad77f451c179354",
      "b0d9c2efe82141b8a07ad1c364f05019",
      "2863a52516c345009fac78de471df443",
      "a56fa70d5c7b4458b16df83cf49f56e0",
      "0281a49cd07e4c60b7d9c101a08a7ba2",
      "5b85a5208423440aaa33dc105601878a",
      "3840da303e1d4662bbe342a7c93be0d6",
      "86e04dbae37e4c6294df78e3aeece95c",
      "7eb95cdb9b2145fbab48259ccc7d93a5",
      "3291ddde660648889967cd0930d79cc5",
      "9bb862a82ead4ed78d37497e4c03312e",
      "d583e3bea76a47c58d7cd20249e3fd71",
      "34fbc329fd284316a0f6ffd5bbcdb24d",
      "fcfa1475f577483888b79954bd9ca1df",
      "64e0f8b545054efd88567f8f805aea60",
      "3dd5181c281747928d965b61c6334657",
      "bacf465bba3f4acb8d6ceb8e300c06c2",
      "be34742a368a465987b032ffb40314f3",
      "c2f5076ac40d4f3b9e09386ebd16cb5a",
      "1e143509f92e4dc69458fd886ca7f79a",
      "e116cdcfa7034ac3b7d2b288822fdd4e",
      "7dd1b620988c45ce876ad28c78b37969",
      "d928463d3c714dc68053f2eedb1fae13",
      "b4fe3eabcaa64a39bcd5a5f95d2437b5",
      "e31ed91892134488b5952258aba79d9f",
      "4b3600304c5945b19eaa11349b66693f",
      "eef6dfb54c394c428be0e5e8bfc55bf1",
      "5b7dac555576435b873641d309b8ee7f",
      "a7945fe409d24b39abe64c6e8ef50eb9",
      "240db7e596be477795a365c19354b97f",
      "da78fde25a85487c8cbcee4db98addbc",
      "0e361eb0eb4a4233ad35af3a18bd94b4",
      "cf34eda7c0054691aa35edd24f192fff",
      "ea32691ae4e449caa22fdaef407d6aab",
      "127c33440a01418fb7a974c4d3d8b99c",
      "2560de25e2ac4162ae7ae58320333b46",
      "ddc5eff6752a4943b8be7aa264460280",
      "5537f6e5c46c44fc8184302cdeb41d8a",
      "c11bd0c32959411492fc310e3f2e2d49",
      "92503fc6c4364184a441a11db3e70503",
      "c3bbdf297920447f9b2caaf43825db31",
      "cfce890b185d4c1a911f833935a74180",
      "9f4728cf15a84fda9a49484856b9fb21",
      "087bb32476404b61b195e9fa15ffc76b",
      "c3f8c55875f5410baddc07c393ce3731",
      "c8f0f966a1674c5dafcf8e89f5609002",
      "665b422319464ff8935a69399a0735d7",
      "b9c704c775f34d4fa9dff1b68c2a31aa"
     ]
    },
    "id": "VOJx4vIYJvnW"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "#Interpretability Study (XAI): Prediction Score Distribution Analysis"
   ],
   "metadata": {
    "id": "b60Epzhw8hSK"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The following cell creates simple **explainability-style score histograms** using prediction results that were already generated earlier. It does not run any new model inference. The cell reads the saved `predictions.csv` files for the Base D7 model and for three enhanced versions of D7 that were trained with 10 percent of D2 data using different speaker selections (**trainEnh1**, **trainEnh2**, and **trainEnh3**). For each model, predictions from the three random seeds are combined at the clip level by averaging the predicted Parkinson’s probability for the same audio file. This produces one pooled score per clip, which makes it easier to see how clearly the model separates Parkinson’s speech from healthy speech.\n",
    "\n",
    "Using these pooled scores, the cell generates histogram plots showing how prediction scores are distributed. Two types of plots are created for each model. The first compares score distributions for Parkinson’s versus healthy clips overall. The second breaks the same comparison down by sex, with separate plots for male and female speakers. Clips with unknown or non-standard sex labels are excluded from the sex-specific plots so the comparisons remain clean and easy to interpret. Along with the plots, the pooled per-clip prediction values are saved as CSV files so the underlying data can be reviewed or reused later.\n",
    "\n",
    "To make loading files reliable across different run folder layouts, the cell first checks for `predictions.csv` files directly inside the expected run directory. If they are not found there, it follows pointer files such as `tag_run_pointer.json` or `last_run_pointer.json` to locate the actual run folder that contains the seed-specific prediction outputs. Each predictions file is validated to ensure required columns are present, that labels match across seeds for the same clip, and that any missing seed results are reported with warnings instead of being silently ignored.\n",
    "\n",
    "All histogram plots are written to an `xai_histograms/` subfolder inside each model’s run directory. Plots for the Base D7 model are clearly labeled as baseline results without any target data adaptation. Plots for the enhanced models are labeled to show that the model was trained with 10 percent of D2 data, with each enhanced draw shown separately. This layout keeps comparisons clear while maintaining a clean and traceable record of how score distributions change from the base model to each enhanced version."
   ],
   "metadata": {
    "id": "E6U1CYz_rNK-"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# =========================\n",
    "# XAI HISTOGRAMS (NO INFERENCE) — Base vs D7 + 10% D2 (3 draws)\n",
    "# - Reads existing predictions.csv (no rerun)\n",
    "# - Pools 3 seeds by ENSEMBLE MEAN per clip_path\n",
    "# - Plots simple score histograms:\n",
    "#     (A) Overall: PD vs HC\n",
    "#     (B) By sex: PD M/F and HC M/F\n",
    "# - Saves SEPARATE PNGs per histogram under:\n",
    "#     <MODEL_TAG_ROOT>/xai_histograms/\n",
    "#\n",
    "# IMPORTANT FIX vs your error:\n",
    "# Enhanced runs store predictions.csv under RUN_ROOT (tag+stamp), not TAG_ROOT.\n",
    "# So we:\n",
    "#   1) Try to find predictions.csv under TAG_ROOT\n",
    "#   2) If none found, read TAG_ROOT/tag_run_pointer.json -> run_root, then search there\n",
    "#\n",
    "# IMPORTANT CLARIFICATION (per your note):\n",
    "# - \"Base D7\" does NOT include any 10% D2 training.\n",
    "# - Therefore, we DO NOT label anything in the Base folder as \"D7 + 10% D2\".\n",
    "#   In the Base folder we write BASE-only histograms.\n",
    "#   In each Enhanced folder, we write Base vs that Enhanced draw.\n",
    "# =========================\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# -------------------------\n",
    "# 0) Inputs (from your message)\n",
    "# -------------------------\n",
    "D7_OUT_ROOT = \"/content/drive/MyDrive/AI_PD_Project/Datasets/D7-Multilingual (D1_D4_D5v2_D6)/preprocessed_v1\"\n",
    "\n",
    "BASE_TAG_ROOT = \"/content/drive/MyDrive/AI_PD_Project/Datasets/D7-Multilingual (D1_D4_D5v2_D6)/preprocessed_v1/multilingual_test_runs/run_exp_frozen_LNDO_base_initBaseline_20251229_060853\"\n",
    "\n",
    "ENH_TAG_ROOTS = {\n",
    "    \"trainEnh1\": \"/content/drive/MyDrive/AI_PD_Project/Datasets/D7-Multilingual (D1_D4_D5v2_D6)/preprocessed_v1/monolingual_test_runs/run_exp_frozen_LNDO_trainEnh1_initBaseline_20251227_224654\",\n",
    "    \"trainEnh2\": \"/content/drive/MyDrive/AI_PD_Project/Datasets/D7-Multilingual (D1_D4_D5v2_D6)/preprocessed_v1/monolingual_test_runs/run_exp_frozen_LNDO_trainEnh2_initBaseline_20251227_184205\",\n",
    "    \"trainEnh3\": \"/content/drive/MyDrive/AI_PD_Project/Datasets/D7-Multilingual (D1_D4_D5v2_D6)/preprocessed_v1/monolingual_test_runs/run_exp_frozen_LNDO_trainEnh3_initBaseline_20251227_203719\",\n",
    "}\n",
    "\n",
    "SEEDS = [1337, 2024, 7777]\n",
    "\n",
    "# Recommended, defensible value: 50 bins for probabilities in [0,1] (bin width = 0.02)\n",
    "BINS = 50\n",
    "\n",
    "XAI_DIRNAME = \"xai_histograms\"\n",
    "\n",
    "# -------------------------\n",
    "# 1) Helpers\n",
    "# -------------------------\n",
    "def _read_json(p: Path) -> dict:\n",
    "    with open(p, \"r\", encoding=\"utf-8\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def _resolve_run_root_from_tag(tag_root: Path) -> Path:\n",
    "    \"\"\"\n",
    "    Returns a path where predictions.csv files actually live.\n",
    "    - First try tag_root itself (Base typically stores preds in subfolders under tag_root)\n",
    "    - If not found, try tag_root/tag_run_pointer.json -> run_root (Enhanced)\n",
    "    - If still not found, try TEST_ROOT/last_run_pointer.json as fallback\n",
    "    \"\"\"\n",
    "    tag_root = Path(tag_root)\n",
    "\n",
    "    # Quick success: predictions exist under tag_root\n",
    "    if list(tag_root.glob(\"**/predictions.csv\")):\n",
    "        return tag_root\n",
    "\n",
    "    # Enhanced normal case: tag pointer exists under TAG_ROOT\n",
    "    tag_ptr = tag_root / \"tag_run_pointer.json\"\n",
    "    if tag_ptr.exists():\n",
    "        obj = _read_json(tag_ptr)\n",
    "        # Enhanced code may store either run_root or stamp_run_dir\n",
    "        run_root = obj.get(\"run_root\", None) or obj.get(\"stamp_run_dir\", None)\n",
    "        if run_root:\n",
    "            return Path(run_root)\n",
    "\n",
    "    # Fallback: last_run_pointer.json in the parent TEST_ROOT\n",
    "    test_root = tag_root.parent\n",
    "    last_ptr = test_root / \"last_run_pointer.json\"\n",
    "    if last_ptr.exists():\n",
    "        obj = _read_json(last_ptr)\n",
    "        run_root = obj.get(\"run_root\", None) or obj.get(\"stamp_run_dir\", None)\n",
    "        if run_root:\n",
    "            return Path(run_root)\n",
    "\n",
    "    raise FileNotFoundError(\n",
    "        \"Could not locate any predictions.csv under TAG_ROOT, and could not resolve a run_root via pointer files.\\n\"\n",
    "        f\"TAG_ROOT checked: {str(tag_root)}\\n\"\n",
    "        f\"Expected pointer file: {str(tag_root / 'tag_run_pointer.json')}\\n\"\n",
    "        f\"Fallback pointer file: {str(tag_root.parent / 'last_run_pointer.json')}\"\n",
    "    )\n",
    "\n",
    "def _find_predictions_csvs(run_root: Path, seeds) -> list[tuple[int, Path]]:\n",
    "    \"\"\"\n",
    "    Find predictions.csv for each seed under run_root, using either:\n",
    "    - folder/file path containing 'seed{seed}', OR\n",
    "    - reading the 'seed' column inside predictions.csv\n",
    "    \"\"\"\n",
    "    run_root = Path(run_root)\n",
    "    all_preds = sorted(run_root.glob(\"**/predictions.csv\"))\n",
    "    if not all_preds:\n",
    "        raise FileNotFoundError(f\"No predictions.csv found under resolved RUN_ROOT: {str(run_root)}\")\n",
    "\n",
    "    chosen = []\n",
    "\n",
    "    # First pass: pick files by seed hint in path\n",
    "    for s in seeds:\n",
    "        matches = [p for p in all_preds if f\"seed{s}\" in str(p)]\n",
    "        if len(matches) == 1:\n",
    "            chosen.append((s, matches[0]))\n",
    "        elif len(matches) > 1:\n",
    "            # pick the deepest (typically per-seed run folder) to avoid accidental duplicates\n",
    "            matches = sorted(matches, key=lambda p: len(str(p).split(\"/\")), reverse=True)\n",
    "            chosen.append((s, matches[0]))\n",
    "\n",
    "    # If we got all seeds, done\n",
    "    if len(chosen) == len(seeds):\n",
    "        return chosen\n",
    "\n",
    "    # Second pass: load each predictions.csv and filter by seed column if present\n",
    "    chosen = []\n",
    "    for s in seeds:\n",
    "        found = None\n",
    "        for p in all_preds:\n",
    "            try:\n",
    "                df = pd.read_csv(p)\n",
    "            except Exception:\n",
    "                continue\n",
    "            if \"seed\" in df.columns and (df[\"seed\"] == s).any():\n",
    "                found = p\n",
    "                break\n",
    "        if found is None:\n",
    "            raise FileNotFoundError(\n",
    "                f\"Could not find predictions.csv for seed={s} under {str(run_root)}.\\n\"\n",
    "                \"Searched by path hint 'seed####' and by reading 'seed' column.\"\n",
    "            )\n",
    "        chosen.append((s, found))\n",
    "\n",
    "    return chosen\n",
    "\n",
    "def load_and_pool_from_tag_root(tag_root: Path, model_name: str, seeds) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Reads predictions.csv for seeds and returns one pooled row per clip_path:\n",
    "      y_score_mean = mean(y_score across seeds)\n",
    "    Keeps y_true and sex_norm (assumed consistent per clip_path).\n",
    "    \"\"\"\n",
    "    tag_root = Path(tag_root)\n",
    "    run_root = _resolve_run_root_from_tag(tag_root)\n",
    "    print(f\"\\n[{model_name}] TAG_ROOT: {str(tag_root)}\")\n",
    "    print(f\"[{model_name}] RESOLVED RUN_ROOT for predictions: {str(run_root)}\")\n",
    "\n",
    "    pred_files = _find_predictions_csvs(run_root, seeds)\n",
    "    print(f\"[{model_name}] predictions.csv files:\")\n",
    "    for s, p in pred_files:\n",
    "        print(f\"  seed={s}: {str(p)}\")\n",
    "\n",
    "    dfs = []\n",
    "    for s, p in pred_files:\n",
    "        df = pd.read_csv(p)\n",
    "\n",
    "        required = [\"clip_path\", \"y_true\", \"y_score\", \"sex_norm\"]\n",
    "        missing = [c for c in required if c not in df.columns]\n",
    "        if missing:\n",
    "            raise ValueError(\n",
    "                f\"[{model_name}] Missing columns in {str(p)}: {missing}\\n\"\n",
    "                f\"Columns found: {list(df.columns)}\"\n",
    "            )\n",
    "\n",
    "        df = df[[\"clip_path\", \"y_true\", \"y_score\", \"sex_norm\"]].copy()\n",
    "        df[\"clip_path\"] = df[\"clip_path\"].astype(str)\n",
    "        df[\"y_true\"] = df[\"y_true\"].astype(int)\n",
    "        df[\"y_score\"] = df[\"y_score\"].astype(float)\n",
    "        df[\"sex_norm\"] = df[\"sex_norm\"].astype(str).str.upper().str.strip()\n",
    "\n",
    "        df[\"seed\"] = int(s)\n",
    "        dfs.append(df)\n",
    "\n",
    "    all_df = pd.concat(dfs, axis=0, ignore_index=True)\n",
    "\n",
    "    # Sanity: y_true should not disagree across seeds for the same clip_path\n",
    "    chk = all_df.groupby(\"clip_path\")[\"y_true\"].nunique()\n",
    "    bad = chk[chk > 1]\n",
    "    if len(bad) > 0:\n",
    "        ex = bad.index[:10].tolist()\n",
    "        raise RuntimeError(\n",
    "            f\"[{model_name}] y_true mismatch across seeds for some clip_path. Examples: {ex}\"\n",
    "        )\n",
    "\n",
    "    # Pool: mean score per clip_path across seeds\n",
    "    pooled = (\n",
    "        all_df.groupby(\"clip_path\", as_index=False)\n",
    "        .agg(\n",
    "            y_true=(\"y_true\", \"first\"),\n",
    "            sex_norm=(\"sex_norm\", \"first\"),\n",
    "            y_score_mean=(\"y_score\", \"mean\"),\n",
    "            n_seeds=(\"seed\", \"nunique\"),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Safety check: ensure each clip got all seeds\n",
    "    missing_seeds = pooled[pooled[\"n_seeds\"] != len(seeds)]\n",
    "    if len(missing_seeds) > 0:\n",
    "        print(f\"[{model_name}] WARNING: {len(missing_seeds)} clips did not have all {len(seeds)} seeds.\")\n",
    "        print(\"Showing first 10 missing-seed clips:\")\n",
    "        display(missing_seeds.head(10))\n",
    "\n",
    "    return pooled\n",
    "\n",
    "def _save_hist(values: np.ndarray, out_png: Path, title: str, bins: int):\n",
    "    \"\"\"\n",
    "    Save a simple histogram as a separate PNG.\n",
    "    Guard: if values is empty, skip and print a note (prevents matplotlib errors and misleading blank plots).\n",
    "    \"\"\"\n",
    "    out_png.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    values = np.asarray(values, dtype=np.float64)\n",
    "    if values.size == 0:\n",
    "        print(\"NOTE: No data for plot, skipping:\", out_png.name, \"|\", title)\n",
    "        return\n",
    "\n",
    "    plt.figure()\n",
    "    plt.hist(values, bins=bins)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Predicted PD probability (y_score)\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_png, dpi=200)\n",
    "    plt.close()\n",
    "\n",
    "def _prep_slices(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Standardize label and sex for plotting.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    df[\"label\"] = np.where(df[\"y_true\"] == 1, \"PD\", \"HC\")\n",
    "    df[\"sex_norm\"] = df[\"sex_norm\"].astype(str).str.upper().str.strip()\n",
    "    df.loc[~df[\"sex_norm\"].isin([\"M\", \"F\"]), \"sex_norm\"] = \"UNK\"\n",
    "    return df\n",
    "\n",
    "def write_base_only_histograms(base_tag_root: Path, base_pooled: pd.DataFrame, bins: int):\n",
    "    \"\"\"\n",
    "    Writes BASE-only plots under:\n",
    "      <BASE_TAG_ROOT>/xai_histograms/\n",
    "    This avoids any misleading \"10% D2\" wording in Base folder titles.\n",
    "    \"\"\"\n",
    "    base_tag_root = Path(base_tag_root)\n",
    "    out_dir = base_tag_root / XAI_DIRNAME\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    b = _prep_slices(base_pooled)\n",
    "\n",
    "    # Overall PD/HC\n",
    "    for label in [\"PD\", \"HC\"]:\n",
    "        _save_hist(\n",
    "            b.loc[b[\"label\"] == label, \"y_score_mean\"].to_numpy(),\n",
    "            out_dir / f\"Base__{label}__hist.png\",\n",
    "            title=f\"Base D7 → D2 TEST | {label} | pooled across 3 seeds\",\n",
    "            bins=bins,\n",
    "        )\n",
    "\n",
    "    # By sex (PD and HC)\n",
    "    for label in [\"PD\", \"HC\"]:\n",
    "        for sex in [\"M\", \"F\"]:\n",
    "            _save_hist(\n",
    "                b.loc[(b[\"label\"] == label) & (b[\"sex_norm\"] == sex), \"y_score_mean\"].to_numpy(),\n",
    "                out_dir / f\"Base__{label}__SEX_{sex}__hist.png\",\n",
    "                title=f\"Base D7 → D2 TEST | {label} | sex={sex} | pooled across 3 seeds\",\n",
    "                bins=bins,\n",
    "            )\n",
    "\n",
    "    # Save pooled CSV (audit trail)\n",
    "    pooled_out = out_dir / \"pooled_predictions__Base_D7.csv\"\n",
    "    base_pooled.to_csv(pooled_out, index=False)\n",
    "\n",
    "    print(f\"\\nWROTE BASE-only XAI histograms to: {str(out_dir)}\")\n",
    "\n",
    "def write_base_vs_enh_histograms(enh_tag_root: Path, base_pooled: pd.DataFrame, enh_pooled: pd.DataFrame,\n",
    "                                draw_name: str, bins: int):\n",
    "    \"\"\"\n",
    "    Writes Base vs Enhanced plots under:\n",
    "      <ENH_TAG_ROOT>/xai_histograms/\n",
    "    Output PNGs:\n",
    "      - Base PD, Base HC\n",
    "      - Enhanced PD, Enhanced HC\n",
    "      - Base PD M/F + HC M/F\n",
    "      - Enhanced PD M/F + HC M/F\n",
    "\n",
    "    Title wording is explicit:\n",
    "      - Base D7 (no target adaptation)\n",
    "      - D7 + 10% D2 (this draw)\n",
    "    \"\"\"\n",
    "    enh_tag_root = Path(enh_tag_root)\n",
    "    out_dir = enh_tag_root / XAI_DIRNAME\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    b = _prep_slices(base_pooled)\n",
    "    e = _prep_slices(enh_pooled)\n",
    "\n",
    "    # Overall PD/HC\n",
    "    for label in [\"PD\", \"HC\"]:\n",
    "        _save_hist(\n",
    "            b.loc[b[\"label\"] == label, \"y_score_mean\"].to_numpy(),\n",
    "            out_dir / f\"Base__{label}__hist.png\",\n",
    "            title=f\"Base D7 (no target adaptation) → D2 TEST | {label} | pooled across 3 seeds\",\n",
    "            bins=bins,\n",
    "        )\n",
    "        _save_hist(\n",
    "            e.loc[e[\"label\"] == label, \"y_score_mean\"].to_numpy(),\n",
    "            out_dir / f\"{draw_name}__{label}__hist.png\",\n",
    "            title=f\"D7 + 10% D2 ({draw_name}) → D2 TEST | {label} | pooled across 3 seeds\",\n",
    "            bins=bins,\n",
    "        )\n",
    "\n",
    "    # By sex (PD and HC)\n",
    "    for label in [\"PD\", \"HC\"]:\n",
    "        for sex in [\"M\", \"F\"]:\n",
    "            _save_hist(\n",
    "                b.loc[(b[\"label\"] == label) & (b[\"sex_norm\"] == sex), \"y_score_mean\"].to_numpy(),\n",
    "                out_dir / f\"Base__{label}__SEX_{sex}__hist.png\",\n",
    "                title=f\"Base D7 (no target adaptation) → D2 TEST | {label} | sex={sex} | pooled across 3 seeds\",\n",
    "                bins=bins,\n",
    "            )\n",
    "            _save_hist(\n",
    "                e.loc[(e[\"label\"] == label) & (e[\"sex_norm\"] == sex), \"y_score_mean\"].to_numpy(),\n",
    "                out_dir / f\"{draw_name}__{label}__SEX_{sex}__hist.png\",\n",
    "                title=f\"D7 + 10% D2 ({draw_name}) → D2 TEST | {label} | sex={sex} | pooled across 3 seeds\",\n",
    "                bins=bins,\n",
    "            )\n",
    "\n",
    "    # Save pooled CSVs (audit trail)\n",
    "    base_out = out_dir / \"pooled_predictions__Base_D7.csv\"\n",
    "    enh_out  = out_dir / f\"pooled_predictions__{draw_name}_D7plus10pctD2.csv\"\n",
    "    base_pooled.to_csv(base_out, index=False)\n",
    "    enh_pooled.to_csv(enh_out, index=False)\n",
    "\n",
    "    print(f\"\\nWROTE Base vs {draw_name} XAI histograms to: {str(out_dir)}\")\n",
    "\n",
    "# -------------------------\n",
    "# 2) Load + pool BASE once\n",
    "# -------------------------\n",
    "base_tag_root = Path(BASE_TAG_ROOT)\n",
    "base_pooled = load_and_pool_from_tag_root(base_tag_root, model_name=\"BASE\", seeds=SEEDS)\n",
    "\n",
    "# Write BASE-only histograms under the BASE folder itself (titles are now correct and unambiguous)\n",
    "write_base_only_histograms(base_tag_root=base_tag_root, base_pooled=base_pooled, bins=BINS)\n",
    "\n",
    "# -------------------------\n",
    "# 3) For each draw: load + pool enhanced, then write Base vs Enhanced into that draw folder\n",
    "# -------------------------\n",
    "for draw_name, tag_root in ENH_TAG_ROOTS.items():\n",
    "    enh_tag_root = Path(tag_root)\n",
    "    enh_pooled = load_and_pool_from_tag_root(enh_tag_root, model_name=draw_name, seeds=SEEDS)\n",
    "\n",
    "    write_base_vs_enh_histograms(\n",
    "        enh_tag_root=enh_tag_root,\n",
    "        base_pooled=base_pooled,\n",
    "        enh_pooled=enh_pooled,\n",
    "        draw_name=draw_name,\n",
    "        bins=BINS,\n",
    "    )\n",
    "\n",
    "print(\"\\nDONE.\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104,
     "referenced_widgets": [
      "1ca0ae5368a24d6cb5460ae3ca587ecc",
      "6b935686afe34fed954b1942c568221e",
      "838e08ddbec04a5cbf028b3591e20628",
      "457e9350f939417bb359422eed160ef2",
      "723441f805b64350a396daf89b7312ee",
      "b9ec79eb71644d1db246cb95645bb7fa",
      "eecbbd08b2a44085bc155f6388b5027d",
      "22111dea9eb1416aa0b75102acc4bc2a",
      "7ea3a6aa970f47d6b5506f541091b803",
      "534e06abe66a48699c90e0e79c81503b",
      "dbb83da5956f4ccf8abbc9bdd371e394",
      "a85ef561cedc4026980ed6207b2dda24",
      "78b7462a988c4c06872d5c8e1d81307e",
      "736431c35d9347c582180847366ca146",
      "5148a9793dd946feb474da25f661a144",
      "043597ac86354bc38a712a5dcb31b2a3",
      "7b10deb5923547d19ff112b5650ef700",
      "e3995c1cf6f7472aa6c384eccaee1a53",
      "08396f2623d440c8896725c841f4d1c1",
      "6a4920389f5b4e12988ff133469342a9",
      "fbb8b0bc016a4e00ad9741c428cd7470",
      "d2f4c4bb6e894572bf516198f39183d3"
     ]
    },
    "id": "x_tWWcIXwn4H"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "The following cell creates a single ROC curve that summarizes TrainEnh2 performance on the D2 test set across multiple random seeds and highlights two practical decision thresholds on that curve. It automatically locates the most recent TrainEnh2 test run by reading the test history index, finds each seed’s `predictions.csv`, and computes an ROC curve and AUROC for every seed. It also reads the validation-selected Youden J threshold that was saved in the prediction files, then separately finds the latest threshold sweep run for TrainEnh2 and loads the recommended fairness-constrained threshold from `sweep_summary.json`. If needed, it looks up the matching sensitivity and specificity from `sweep_table.csv`.\n",
    "\n",
    "After loading the ROC data for all seeds, the cell builds a mean ROC curve by interpolating each seed’s ROC onto a shared false positive rate grid, then averaging the true positive rates. It also computes a simple spread band using the standard deviation to show how results vary from seed to seed. The Youden J operating point is computed using the pooled D2 test predictions across all seeds so that the plotted sensitivity and specificity reflect actual test-set performance. For the fairness-constrained operating point, the cell uses the sensitivity and specificity reported by the sweep and converts specificity to false positive rate for plotting.\n",
    "\n",
    "Finally, the cell generates and saves `mean_roc_curve_with_operating_points.png` in the same TrainEnh2 test run folder. The plot shows faint ROC curves for individual seeds, the mean ROC curve with a shaded variation band, a dashed chance line, and two clearly labeled markers. One marker represents the Youden J (validation-optimal) threshold, and the other represents the recommended fairness-constrained threshold. Each marker is annotated with the threshold value and the corresponding sensitivity and specificity."
   ],
   "metadata": {
    "id": "T8pydQtcjkiY"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#Mean ROC Curve for TrainEnh2 to D2 Test"
   ],
   "metadata": {
    "id": "IpEBoET0a24s"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Mean ROC + Two Operating Points (TrainEnh2 → D2 Test)\n",
    "# Inputs:\n",
    "#   - TrainEnh2 test predictions (per seed)\n",
    "#   - TrainEnh2 threshold sweep summary/table (fairness-constrained threshold)\n",
    "# Outputs:\n",
    "#   - mean_roc_curve_with_operating_points.png saved in the TrainEnh2 test run folder\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "\n",
    "# -------------------------\n",
    "# 0) Roots and tags\n",
    "# -------------------------\n",
    "D7_OUT_ROOT = \"/content/drive/MyDrive/AI_PD_Project/Datasets/D7-Multilingual (D1_D4_D5v2_D6)/preprocessed_v1\"\n",
    "TARGET_ENH_TAG = \"trainEnh2\"\n",
    "\n",
    "# -------------------------\n",
    "# 1) Locate latest TrainEnh2 TEST run (predictions)\n",
    "# -------------------------\n",
    "test_root = Path(D7_OUT_ROOT) / \"monolingual_test_runs\"\n",
    "history_path = test_root / \"history_index.jsonl\"\n",
    "if not history_path.exists():\n",
    "    raise FileNotFoundError(\"Missing history_index.jsonl under monolingual_test_runs.\")\n",
    "\n",
    "latest_test = None\n",
    "with open(history_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        obj = json.loads(line)\n",
    "        if str(obj.get(\"enh_tag\", \"\")).lower() == TARGET_ENH_TAG.lower():\n",
    "            latest_test = obj  # last match wins\n",
    "\n",
    "if latest_test is None:\n",
    "    raise FileNotFoundError(\"No TrainEnh2 entries found in test history.\")\n",
    "\n",
    "run_dirs = latest_test.get(\"run_dirs\", [])\n",
    "if not run_dirs:\n",
    "    raise FileNotFoundError(\"TrainEnh2 test entry missing run_dirs.\")\n",
    "\n",
    "run_root = Path(run_dirs[0]).parent\n",
    "if not run_root.exists():\n",
    "    raise FileNotFoundError(\"TrainEnh2 test run folder does not exist.\")\n",
    "\n",
    "print(\"Test run folder:\", run_root)\n",
    "\n",
    "# -------------------------\n",
    "# 2) Load per-seed predictions and read Youden J (VAL-optimal) threshold\n",
    "# -------------------------\n",
    "pred_paths = sorted(run_root.glob(\"run_*_seed*/predictions.csv\"))\n",
    "if not pred_paths:\n",
    "    raise FileNotFoundError(\"No predictions.csv files found in test run.\")\n",
    "\n",
    "seed_rocs = []\n",
    "youden_thresholds = []\n",
    "\n",
    "for p in pred_paths:\n",
    "    df = pd.read_csv(p)\n",
    "\n",
    "    if not {\"y_true\", \"y_score\"}.issubset(df.columns):\n",
    "        raise ValueError(f\"Missing y_true/y_score in {p}\")\n",
    "\n",
    "    y_true_seed = df[\"y_true\"].to_numpy(int)\n",
    "    y_score_seed = df[\"y_score\"].to_numpy(float)\n",
    "\n",
    "    if len(np.unique(y_true_seed)) < 2:\n",
    "        raise ValueError(f\"Only one class present in {p}\")\n",
    "\n",
    "    fpr, tpr, _ = roc_curve(y_true_seed, y_score_seed)\n",
    "    auc = float(roc_auc_score(y_true_seed, y_score_seed))\n",
    "    seed = int(p.parent.name.split(\"seed\")[-1])\n",
    "\n",
    "    seed_rocs.append({\"seed\": seed, \"fpr\": fpr, \"tpr\": tpr, \"auc\": auc})\n",
    "\n",
    "    if \"threshold_used_global\" in df.columns:\n",
    "        youden_thresholds.append(float(df[\"threshold_used_global\"].iloc[0]))\n",
    "\n",
    "if not youden_thresholds:\n",
    "    raise ValueError(\"VAL-optimal threshold (threshold_used_global) not found in predictions.csv.\")\n",
    "\n",
    "youden_thr = float(youden_thresholds[0])\n",
    "print(f\"Youden J threshold (VAL-optimal): {youden_thr:.6f}\")\n",
    "\n",
    "# -------------------------\n",
    "# 3) Locate latest TrainEnh2 threshold sweep (fairness threshold)\n",
    "# -------------------------\n",
    "sweep_root = Path(D7_OUT_ROOT) / \"threshold_sweeps\"\n",
    "sweep_dirs = sorted(\n",
    "    [d for d in sweep_root.glob(\"run_D7_trainEnh2_on_D2test_*\") if d.is_dir()],\n",
    "    key=lambda d: d.stat().st_mtime,\n",
    "    reverse=True,\n",
    ")\n",
    "if not sweep_dirs:\n",
    "    raise FileNotFoundError(\"No TrainEnh2 threshold_sweeps found under threshold_sweeps/.\")\n",
    "\n",
    "sweep_dir = sweep_dirs[0]\n",
    "summary_path = sweep_dir / \"sweep_summary.json\"\n",
    "table_path = sweep_dir / \"sweep_table.csv\"\n",
    "\n",
    "if not summary_path.exists():\n",
    "    raise FileNotFoundError(\"Missing sweep_summary.json in the latest threshold sweep folder.\")\n",
    "\n",
    "with open(summary_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    sweep_summary = json.load(f)\n",
    "\n",
    "# Fairness-constrained recommendation (as written by the  sweep code)\n",
    "if \"chosen_threshold\" not in sweep_summary:\n",
    "    raise ValueError(\n",
    "        \"sweep_summary.json must contain 'chosen_threshold'. \"\n",
    "        f\"Found keys: {list(sweep_summary.keys())}\"\n",
    "    )\n",
    "\n",
    "fair_thr = float(sweep_summary[\"chosen_threshold\"])\n",
    "print(f\"Fairness-constrained threshold (chosen_threshold): {fair_thr:.6f}\")\n",
    "print(\"Threshold sweep folder:\", sweep_dir)\n",
    "\n",
    "# -------------------------\n",
    "# 3.5) Extract fairness operating-point sens/spec (best effort)\n",
    "# -------------------------\n",
    "def _find_metric(d: dict, candidates: list[str]):\n",
    "    for k in candidates:\n",
    "        if k in d and d[k] is not None:\n",
    "            try:\n",
    "                return float(d[k])\n",
    "            except Exception:\n",
    "                pass\n",
    "    return None\n",
    "\n",
    "fair_sens = None\n",
    "fair_spec = None\n",
    "\n",
    "chosen_metrics = sweep_summary.get(\"chosen_metrics\", {}) or {}\n",
    "\n",
    "# Try common names first\n",
    "fair_sens = _find_metric(chosen_metrics, [\"sensitivity\", \"mean_sensitivity\", \"tpr\", \"TPR\"])\n",
    "fair_spec = _find_metric(chosen_metrics, [\"specificity\", \"mean_specificity\", \"tnr\", \"TNR\"])\n",
    "\n",
    "# If not found, fall back to sweep_table.csv by nearest threshold\n",
    "if (fair_sens is None) or (fair_spec is None):\n",
    "    if not table_path.exists():\n",
    "        raise ValueError(\n",
    "            \"chosen_metrics did not contain sensitivity/specificity, and sweep_table.csv is missing. \"\n",
    "            \"Cannot label fairness point with sens/spec.\"\n",
    "        )\n",
    "\n",
    "    tbl = pd.read_csv(table_path)\n",
    "\n",
    "    # Find the threshold column\n",
    "    thr_col = None\n",
    "    for c in [\"threshold\", \"thr\", \"thresh\"]:\n",
    "        if c in tbl.columns:\n",
    "            thr_col = c\n",
    "            break\n",
    "    if thr_col is None:\n",
    "        raise ValueError(f\"sweep_table.csv missing a threshold column. Found: {list(tbl.columns)}\")\n",
    "\n",
    "    # Find sensitivity/specificity columns\n",
    "    sens_col = None\n",
    "    spec_col = None\n",
    "    sens_candidates = [\"mean_sensitivity\", \"sensitivity\", \"tpr\", \"TPR\"]\n",
    "    spec_candidates = [\"mean_specificity\", \"specificity\", \"tnr\", \"TNR\"]\n",
    "\n",
    "    for c in sens_candidates:\n",
    "        if c in tbl.columns:\n",
    "            sens_col = c\n",
    "            break\n",
    "    for c in spec_candidates:\n",
    "        if c in tbl.columns:\n",
    "            spec_col = c\n",
    "            break\n",
    "\n",
    "    if sens_col is None or spec_col is None:\n",
    "        raise ValueError(\n",
    "            \"sweep_table.csv does not have recognizable sensitivity/specificity columns. \"\n",
    "            f\"Found: {list(tbl.columns)}\"\n",
    "        )\n",
    "\n",
    "    # Pick nearest threshold row\n",
    "    thr_vals = pd.to_numeric(tbl[thr_col], errors=\"coerce\").to_numpy(dtype=float)\n",
    "    if np.all(np.isnan(thr_vals)):\n",
    "        raise ValueError(\"Threshold column in sweep_table.csv could not be parsed as numbers.\")\n",
    "\n",
    "    idx = int(np.nanargmin(np.abs(thr_vals - fair_thr)))\n",
    "    fair_thr = float(thr_vals[idx])  # snap to actual grid value used in sweep\n",
    "    fair_sens = float(tbl.iloc[idx][sens_col])\n",
    "    fair_spec = float(tbl.iloc[idx][spec_col])\n",
    "\n",
    "# -------------------------\n",
    "# 4) Build mean ROC across seeds\n",
    "# -------------------------\n",
    "fpr_grid = np.linspace(0.0, 1.0, 501)\n",
    "tprs = []\n",
    "\n",
    "for r in seed_rocs:\n",
    "    fpr = np.asarray(r[\"fpr\"], dtype=float)\n",
    "    tpr = np.asarray(r[\"tpr\"], dtype=float)\n",
    "\n",
    "    if fpr[0] > 0.0:\n",
    "        fpr = np.concatenate([[0.0], fpr])\n",
    "        tpr = np.concatenate([[0.0], tpr])\n",
    "    if fpr[-1] < 1.0:\n",
    "        fpr = np.concatenate([fpr, [1.0]])\n",
    "        tpr = np.concatenate([tpr, [1.0]])\n",
    "\n",
    "    tprs.append(np.interp(fpr_grid, fpr, tpr))\n",
    "\n",
    "tprs = np.vstack(tprs)\n",
    "mean_tpr = tprs.mean(axis=0)\n",
    "std_tpr = tprs.std(axis=0, ddof=1) if tprs.shape[0] > 1 else np.zeros_like(mean_tpr)\n",
    "mean_auc = float(np.mean([r[\"auc\"] for r in seed_rocs]))\n",
    "\n",
    "# -------------------------\n",
    "# 5) Compute Youden J operating point on D2 TEST (pooled predictions)\n",
    "# -------------------------\n",
    "all_true, all_score = [], []\n",
    "for p in pred_paths:\n",
    "    df = pd.read_csv(p)\n",
    "    all_true.append(df[\"y_true\"].to_numpy(int))\n",
    "    all_score.append(df[\"y_score\"].to_numpy(float))\n",
    "\n",
    "y_true = np.concatenate(all_true)\n",
    "y_score = np.concatenate(all_score)\n",
    "\n",
    "def operating_point(thr: float):\n",
    "    y_pred = (y_score >= float(thr)).astype(int)\n",
    "\n",
    "    tp = int(((y_pred == 1) & (y_true == 1)).sum())\n",
    "    fn = int(((y_pred == 0) & (y_true == 1)).sum())\n",
    "    tn = int(((y_pred == 0) & (y_true == 0)).sum())\n",
    "    fp = int(((y_pred == 1) & (y_true == 0)).sum())\n",
    "\n",
    "    eps = 1e-12\n",
    "    sens = tp / (tp + fn + eps)\n",
    "    spec = tn / (tn + fp + eps)\n",
    "    fpr = 1.0 - spec\n",
    "    return {\"thr\": float(thr), \"sens\": float(sens), \"spec\": float(spec), \"fpr\": float(fpr)}\n",
    "\n",
    "pt_you = operating_point(youden_thr)\n",
    "\n",
    "# Fairness point: use sweep-provided sens/spec, compute FPR from specificity\n",
    "pt_fair = {\n",
    "    \"thr\": float(fair_thr),\n",
    "    \"sens\": float(fair_sens),\n",
    "    \"spec\": float(fair_spec),\n",
    "    \"fpr\": float(1.0 - float(fair_spec)),\n",
    "}\n",
    "\n",
    "print(\"\\nOperating points:\")\n",
    "print(f\"  Youden J (VAL-optimal): thr={pt_you['thr']:.6f}, sens={pt_you['sens']:.4f}, spec={pt_you['spec']:.4f}\")\n",
    "print(f\"  Fairness-constrained:   thr={pt_fair['thr']:.6f}, sens={pt_fair['sens']:.4f}, spec={pt_fair['spec']:.4f}\")\n",
    "\n",
    "# -------------------------\n",
    "# 6) Plot and save\n",
    "# -------------------------\n",
    "plt.figure()\n",
    "\n",
    "# Seed curves (light context)\n",
    "for r in seed_rocs:\n",
    "    plt.plot(r[\"fpr\"], r[\"tpr\"], alpha=0.35)\n",
    "\n",
    "# Mean ROC + spread band\n",
    "plt.plot(fpr_grid, mean_tpr)\n",
    "plt.fill_between(\n",
    "    fpr_grid,\n",
    "    np.clip(mean_tpr - std_tpr, 0.0, 1.0),\n",
    "    np.clip(mean_tpr + std_tpr, 0.0, 1.0),\n",
    "    alpha=0.15,\n",
    ")\n",
    "\n",
    "# Fairness-constrained point with sens/spec\n",
    "plt.scatter(\n",
    "    [pt_fair[\"fpr\"]],\n",
    "    [pt_fair[\"sens\"]],\n",
    "    s=80,\n",
    "    zorder=5,\n",
    "    label=(\n",
    "        \"Recomm. Fairness-Constrained Threshold\\n\"\n",
    "        f\"thr={pt_fair['thr']:.3f}, \"\n",
    "        f\"Sens={pt_fair['sens']:.2f}, Spec={pt_fair['spec']:.2f}\"\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Youden J point (VAL-optimal) with sens/spec\n",
    "plt.scatter(\n",
    "    [pt_you[\"fpr\"]],\n",
    "    [pt_you[\"sens\"]],\n",
    "    s=80,\n",
    "    zorder=5,\n",
    "    label=(\n",
    "        \"Youden J Threshold (VAL-optimal)\\n\"\n",
    "        f\"thr={pt_you['thr']:.3f}, \"\n",
    "        f\"Sens={pt_you['sens']:.2f}, Spec={pt_you['spec']:.2f}\"\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Chance line\n",
    "plt.plot([0, 1], [0, 1], linestyle=\"--\")\n",
    "\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(f\"Mean ROC (TrainEnh2 → D2 Test) | mean AUROC={mean_auc:.3f} | seeds={len(seed_rocs)}\")\n",
    "plt.legend(loc=\"lower right\", frameon=True)\n",
    "plt.tight_layout()\n",
    "\n",
    "out_png = run_root / \"mean_roc_curve_with_operating_points.png\"\n",
    "plt.savefig(out_png, dpi=150)\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "print(\"\\nSaved plot:\", str(out_png))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 681
    },
    "id": "X-rU5vWVd3MG"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "The following cell runs a complete “target data ablation” experiment to study how adding small, speaker-balanced amounts of D2 training data into the multilingual D7 training set affects performance on the D2 test split. It loads the base D7 manifest along with three prebuilt D7 training enhancement manifests (TrainEnh1, TrainEnh2, and TrainEnh3). These enhancement manifests act as separate, non-overlapping pools of D2 speakers that can be added in controlled steps. Multiple training conditions are created, starting with 0 percent added D2 speakers (base only) and increasing through finer increments (5, 10, 15, 20, 25, and 29 percent) by combining half or full portions of the enhancement pools. For each condition, the cell builds fresh training and validation manifests, optionally stages audio clips to local storage for faster access, trains a frozen Wav2Vec2 model with two small task-specific heads across three random seeds, evaluates each trained model on the D2 test split, and records AUROC. At the end, it writes a summary table and a curve plot showing mean AUROC versus the percentage of D2 speakers added, with error bars across seeds.\n",
    "\n",
    "The setup section defines the D7 and D2 dataset locations on Drive, creates an output folder for the ablation run, and fixes all key training settings. These include random seeds, number of epochs, early stopping patience, learning rate, effective batch size through gradient accumulation, dropout, device selection, and mixed precision when running on GPU. An option is also provided to copy audio clips to local SSD for faster reading. Several helper functions handle repeated tasks such as checking file existence, reading and writing CSV files with proper missing values, setting random seeds, normalizing clip paths into a consistent format, printing split counts, and running quick checks for missing or corrupted audio files before training starts.\n",
    "\n",
    "The model used in this cell keeps the Wav2Vec2 backbone frozen and trains only two small classifier heads, one intended for vowel clips and one for all other clip types. Frame-level features are pooled into a single vector per clip using the attention mask, and simple routing logic sends each clip to the correct head based on its task label. The dataset and collate code load waveforms from disk, build attention masks, pad batches to a common length, and keep clip paths in the batch so that slow or failed reads can be traced to specific files. For vowel clips, the attention mask ignores trailing near-silence, while other clips use the full waveform.\n",
    "\n",
    "A key part of the cell constructs the gradual D2 exposure steps in a controlled way. It reads each TrainEnh manifest, keeps only rows that truly come from D2 using a strict `source_dataset == \"D2\"` filter, and samples speakers rather than individual clips so all clips from a chosen speaker stay together. For the “half” steps, it selects half of the available Parkinson’s speakers and half of the available Healthy speakers from that pool, using a fixed random seed so the same speakers are chosen each time. It also checks that the enhancement pools do not share speakers, since the logic assumes the three draws are non-overlapping.\n",
    "\n",
    "For each condition, the cell creates a condition-specific folder on Drive and writes a new `manifest_all.csv` that contains only training and validation splits. Training data consist of the base D7 training set plus the selected D2 rows for that condition, while validation data always come from the base D7 validation split. Duplicate clip paths are removed so the same audio file cannot be counted twice. A small JSON file is also written for each condition, recording how many speakers and clips were added and which enhancement parts were used, so each condition can be traced later.\n",
    "\n",
    "Training is then run for each condition and each seed. Before training starts, the cell checks that all referenced audio files exist and performs a quick audio header scan to catch corrupted clips early. Training uses early stopping based on validation AUROC and saves the best model weights, which include only the two heads and related normalization and dropout blocks. A per-epoch history file is also written. Watchdog timers print warnings if data loading or training becomes unusually slow and include example clip paths to help identify problematic files. After training finishes for a condition, the saved best weights are reloaded and the model is evaluated on the D2 test split, writing per-seed predictions and metrics to disk.\n",
    "\n",
    "At the end of the run, results from all conditions are combined into a single table containing per-seed AUROCs, the mean AUROC, and the standard deviation. This table is saved as a CSV in the ablation output folder. The cell then creates a plot of mean AUROC versus the percentage of D2 training speakers included, adds error bars to show variation across seeds, fits a quadratic trend line to show the overall pattern, and saves the figure as a PNG. Finally, it prints the total runtime and attempts to stop the Colab runtime automatically to avoid leaving a GPU session running."
   ],
   "metadata": {
    "id": "l-utY08Cjgva"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#Ablation 4: Target Data Ablation"
   ],
   "metadata": {
    "id": "QeGZcFxFawQk"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# ============================================================\n",
    "# Target-Data Ablation: Finer Cumulative D2 Exposure (Two-Head Wav2Vec2)\n",
    "# ============================================================\n",
    "# Inputs:\n",
    "# - D7 base manifest (train, val) and D7 TrainEnh1/2/3 manifests\n",
    "# - D7 clips folder\n",
    "# - D2 manifest (test split) and D2 clips folder\n",
    "# Outputs:\n",
    "# - Per-condition manifest on Drive (train + val)\n",
    "# - Per-condition TrainVal runs saved on Drive\n",
    "# - Per-condition D2 test runs saved on Drive\n",
    "# - Summary CSV and curve plot saved under target_data_ablation_finer/\n",
    "\n",
    "import os, json, time, shutil\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from transformers import Wav2Vec2Model\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "START_TIME = time.time()\n",
    "\n",
    "# -------------------------\n",
    "# Drive mount (Colab)\n",
    "# -------------------------\n",
    "try:\n",
    "    from google.colab import drive  # type: ignore\n",
    "    if not os.path.exists(\"/content/drive\"):\n",
    "        drive.mount(\"/content/drive\")\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Run setup\n",
    "# ============================================================\n",
    "\n",
    "D7_ROOT = Path(\"/content/drive/MyDrive/AI_PD_Project/Datasets/D7-Multilingual (D1_D4_D5v2_D6)/preprocessed_v1\")\n",
    "D2_ROOT = Path(\"/content/drive/MyDrive/AI_PD_Project/Datasets/D2-Slovak (EWA-DB)/EWA-DB/preprocessed_v1\")\n",
    "\n",
    "D7_MAN_DIR   = D7_ROOT / \"manifests\"\n",
    "D7_CLIP_ROOT = D7_ROOT  # clip paths are stored like \"clips/train/..wav\"\n",
    "\n",
    "ABL_ROOT = D7_ROOT / \"target_data_ablation_finer\"\n",
    "ABL_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "SEEDS = [1337, 2024, 7777]\n",
    "\n",
    "MAX_EPOCHS = 10\n",
    "PATIENCE   = 2\n",
    "LR         = 1e-3\n",
    "\n",
    "EFFECTIVE_BS  = 64\n",
    "PER_DEVICE_BS = 16\n",
    "GRAD_ACCUM    = max(1, EFFECTIVE_BS // PER_DEVICE_BS)\n",
    "\n",
    "BACKBONE_CKPT = \"facebook/wav2vec2-base\"\n",
    "DROPOUT_P     = 0.2\n",
    "\n",
    "# Task routing\n",
    "VOWEL_TASK_VALUE = \"vowl\"\n",
    "TINY_THRESH = 1e-4\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "USE_AMP = (DEVICE.type == \"cuda\")\n",
    "\n",
    "# Progress + hang debugging\n",
    "PRINT_EVERY_N_BATCHES = 25\n",
    "FETCH_WATCHDOG_SEC    = 90\n",
    "STEP_WATCHDOG_SEC     = 180\n",
    "INFO_SCAN_MAX_PRINT   = 20\n",
    "\n",
    "# Optional: stage clips to local SSD (recommended)\n",
    "STAGE_TO_LOCAL   = True\n",
    "LOCAL_CACHE_ROOT = Path(\"/content/abl_cache\")\n",
    "LOCAL_CACHE_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# D2 filter rule: must use \"source_dataset\"\n",
    "D2_SOURCE_KEY = \"D2\"  # strict on purpose; avoids silently pulling other datasets\n",
    "\n",
    "# Finer points use fractions of each non-overlapping draw\n",
    "DRAW_FRAC = {\n",
    "    \"A_half\": 0.5,  # half of TrainEnh1\n",
    "    \"B_half\": 0.5,  # half of TrainEnh2\n",
    "    \"C_half\": 0.5,  # half of TrainEnh3\n",
    "}\n",
    "\n",
    "print(\"Target-Data Ablation: Finer Cumulative D2 Exposure\")\n",
    "print(\"--------------------------------------------------\")\n",
    "print(f\"[Init] Output folder: {ABL_ROOT}\")\n",
    "print(f\"[Init] Device: {DEVICE} | AMP: {USE_AMP}\")\n",
    "print(f\"[Init] Stage to local SSD: {STAGE_TO_LOCAL}\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Helpers\n",
    "# ============================================================\n",
    "\n",
    "def _ensure_exists(p: Path, what: str) -> None:\n",
    "    if not p.exists():\n",
    "        raise FileNotFoundError(f\"Missing {what} at: {str(p)}\")\n",
    "\n",
    "def _read_csv(p: Path) -> pd.DataFrame:\n",
    "    return pd.read_csv(p)\n",
    "\n",
    "def _write_csv(df: pd.DataFrame, p: Path) -> None:\n",
    "    p.parent.mkdir(parents=True, exist_ok=True)\n",
    "    df.to_csv(p, index=False, na_rep=\"\")\n",
    "\n",
    "def _set_seed(seed: int) -> None:\n",
    "    import random\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.amp.autocast(enabled=USE_AMP):\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def _new_run_stamp() -> str:\n",
    "    return time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "def _safe_unique_count(df: pd.DataFrame, col: str) -> int | None:\n",
    "    if col not in df.columns:\n",
    "        return None\n",
    "    return int(df[col].nunique(dropna=True))\n",
    "\n",
    "def _choose_train_split_name(df: pd.DataFrame) -> str:\n",
    "    splits = df[\"split\"].astype(str)\n",
    "    if (splits == \"train\").any():\n",
    "        return \"train\"\n",
    "    candidates = splits[~splits.isin([\"val\", \"test\"])].value_counts()\n",
    "    if len(candidates) == 0:\n",
    "        raise ValueError(\"No usable training split found (excluding val/test).\")\n",
    "    return str(candidates.index[0])\n",
    "\n",
    "def _norm_clip_path(raw: str) -> str:\n",
    "    \"\"\"\n",
    "    Normalize to relative form:\n",
    "      clips/<split_folder>/...wav\n",
    "    \"\"\"\n",
    "    s = str(raw).replace(\"\\\\\", \"/\").strip()\n",
    "    if \"://\" in s:\n",
    "        s = s.split(\"://\", 1)[-1]\n",
    "    if \"/clips/\" in s:\n",
    "        s = \"clips/\" + s.split(\"/clips/\", 1)[-1]\n",
    "    elif s.startswith(\"clips/\"):\n",
    "        pass\n",
    "    else:\n",
    "        known = [\"train\", \"val\", \"test\", \"train_enh1\", \"train_enh2\", \"train_enh3\"]\n",
    "        for k in known:\n",
    "            if s.startswith(k + \"/\"):\n",
    "                s = \"clips/\" + s\n",
    "                break\n",
    "    while \"//\" in s:\n",
    "        s = s.replace(\"//\", \"/\")\n",
    "    return s.lstrip(\"/\")\n",
    "\n",
    "def _print_split_counts(manifest_path: Path, name: str) -> None:\n",
    "    df = _read_csv(manifest_path)\n",
    "    for c in [\"split\", \"clip_path\"]:\n",
    "        if c not in df.columns:\n",
    "            raise ValueError(f\"{name}: missing required column '{c}'\")\n",
    "\n",
    "    spk_col = \"speaker_id\" if \"speaker_id\" in df.columns else None\n",
    "\n",
    "    print(f\"\\n[Counts] {name}\")\n",
    "    for split in [\"train\", \"val\", \"test\"]:\n",
    "        sub = df[df[\"split\"].astype(str) == split]\n",
    "        if len(sub) == 0:\n",
    "            continue\n",
    "        spk = int(sub[spk_col].nunique(dropna=True)) if spk_col else None\n",
    "        msg = f\"  - {split}: clips={len(sub)}\"\n",
    "        if spk is not None:\n",
    "            msg += f\" | speakers={spk}\"\n",
    "        print(msg)\n",
    "\n",
    "def _scan_exists(clip_root: Path, df: pd.DataFrame, label: str) -> None:\n",
    "    missing = 0\n",
    "    for cp in df[\"clip_path\"].astype(str).tolist():\n",
    "        if not (clip_root / cp).exists():\n",
    "            missing += 1\n",
    "    print(f\"[Check] {label}: total={len(df)} missing={missing}\")\n",
    "\n",
    "def _scan_soundfile_info(clip_root: Path, df: pd.DataFrame, label: str) -> None:\n",
    "    \"\"\"\n",
    "    Quick file header scan (no full decode).\n",
    "    \"\"\"\n",
    "    import soundfile as sf\n",
    "    bad = []\n",
    "    t0 = time.time()\n",
    "    for cp in df[\"clip_path\"].astype(str).tolist():\n",
    "        p = clip_root / cp\n",
    "        try:\n",
    "            _ = sf.info(str(p))\n",
    "        except Exception as e:\n",
    "            bad.append((str(p), repr(e)))\n",
    "            if len(bad) >= INFO_SCAN_MAX_PRINT:\n",
    "                break\n",
    "    dt = time.time() - t0\n",
    "    if len(bad) == 0:\n",
    "        print(f\"[Info] {label}: header scan ok ({len(df)} files) | {dt:.1f}s\")\n",
    "    else:\n",
    "        print(f\"[Info] {label}: header scan found issues (showing up to {len(bad)}) | {dt:.1f}s\")\n",
    "        for p, e in bad:\n",
    "            print(\"  -\", p)\n",
    "            print(\"    \", e)\n",
    "        raise RuntimeError(\"Header scan failed. Fix or remove the listed files before training.\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Local staging (fast reads during training)\n",
    "# ============================================================\n",
    "\n",
    "def _copy_file_if_needed(src: Path, dst: Path) -> None:\n",
    "    dst.parent.mkdir(parents=True, exist_ok=True)\n",
    "    if dst.exists():\n",
    "        if src.stat().st_size == dst.stat().st_size:\n",
    "            return\n",
    "        raise RuntimeError(f\"Destination exists with different size:\\n  src={src}\\n  dst={dst}\")\n",
    "    shutil.copy2(str(src), str(dst))\n",
    "\n",
    "def _stage_manifest_clips_to_local(cond_name: str, src_clip_root: Path, manifest_all: pd.DataFrame) -> Path:\n",
    "    \"\"\"\n",
    "    Local staging for one condition.\n",
    "\n",
    "    Input:\n",
    "    - src_clip_root: root folder that contains \"clips/...\"\n",
    "    - manifest_all: rows with clip_path values like \"clips/train/...wav\"\n",
    "\n",
    "    Output:\n",
    "    - local_root: /content/abl_cache/<cond_name> with:\n",
    "        - clips/...\n",
    "        - manifests/manifest_all.csv\n",
    "    \"\"\"\n",
    "    # Option A: reset this condition folder to avoid stale clips\n",
    "    local_root = LOCAL_CACHE_ROOT / cond_name\n",
    "    if local_root.exists():\n",
    "        shutil.rmtree(local_root)\n",
    "\n",
    "    local_mans = local_root / \"manifests\"\n",
    "    local_root.mkdir(parents=True, exist_ok=True)\n",
    "    local_mans.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    _write_csv(manifest_all, local_mans / \"manifest_all.csv\")\n",
    "\n",
    "    paths = manifest_all[\"clip_path\"].astype(str).tolist()\n",
    "    unique_paths = list(dict.fromkeys(paths))\n",
    "\n",
    "    t0 = time.time()\n",
    "    copied = 0\n",
    "    for cp in tqdm(unique_paths, desc=f\"Stage to local ({cond_name})\", leave=False):\n",
    "        src = src_clip_root / cp\n",
    "        dst = local_root / cp\n",
    "        if not src.exists():\n",
    "            raise FileNotFoundError(f\"Missing source file for staging: {src}\")\n",
    "        _copy_file_if_needed(src, dst)\n",
    "        copied += 1\n",
    "    dt = time.time() - t0\n",
    "    print(f\"[Stage] {cond_name}: copied_files={copied} | {dt/60.0:.1f} min\")\n",
    "    return local_root\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Model (frozen backbone + two task heads)\n",
    "# ============================================================\n",
    "\n",
    "class FrozenW2VTwoHead(nn.Module):\n",
    "    \"\"\"\n",
    "    - Wav2Vec2 backbone is frozen\n",
    "    - Two small heads are trained:\n",
    "      * vowel head for task == VOWEL_TASK_VALUE\n",
    "      * other head for all other tasks\n",
    "    \"\"\"\n",
    "    def __init__(self, backbone_ckpt: str, dropout_p: float):\n",
    "        super().__init__()\n",
    "        self.backbone = Wav2Vec2Model.from_pretrained(backbone_ckpt)\n",
    "        for p in self.backbone.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "        H = int(self.backbone.config.hidden_size)\n",
    "        self.pre_vowel = nn.Sequential(nn.LayerNorm(H), nn.Dropout(float(dropout_p)))\n",
    "        self.pre_other = nn.Sequential(nn.LayerNorm(H), nn.Dropout(float(dropout_p)))\n",
    "        self.head_vowel = nn.Linear(H, 2)\n",
    "        self.head_other = nn.Linear(H, 2)\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    def masked_mean_pool(self, last_hidden, attn_mask_samples):\n",
    "        feat_mask = self.backbone._get_feature_vector_attention_mask(last_hidden.shape[1], attn_mask_samples)\n",
    "        feat_mask = feat_mask.to(last_hidden.device).unsqueeze(-1).type_as(last_hidden)\n",
    "        summed = (last_hidden * feat_mask).sum(dim=1)\n",
    "        denom = feat_mask.sum(dim=1).clamp(min=1.0)\n",
    "        return summed / denom\n",
    "\n",
    "    def forward(self, input_values, attention_mask, task_group):\n",
    "        out = self.backbone(input_values=input_values, attention_mask=attention_mask)\n",
    "        pooled = self.masked_mean_pool(out.last_hidden_state, attention_mask)\n",
    "\n",
    "        logits = []\n",
    "        for i, tg in enumerate(task_group):\n",
    "            if tg == VOWEL_TASK_VALUE:\n",
    "                z = self.pre_vowel(pooled[i])\n",
    "                logits.append(self.head_vowel(z))\n",
    "            else:\n",
    "                z = self.pre_other(pooled[i])\n",
    "                logits.append(self.head_other(z))\n",
    "        return torch.stack(logits, dim=0)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Dataset (adds clip_path in each sample for debug prints)\n",
    "# ============================================================\n",
    "\n",
    "class AudioManifestDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Loads audio from clip_path under clip_root.\n",
    "    Builds an attention mask:\n",
    "    - vowel clips: trim trailing near-silence\n",
    "    - other clips: use full clip\n",
    "    \"\"\"\n",
    "    def __init__(self, df: pd.DataFrame, clip_root: Path):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.clip_root = clip_root\n",
    "        for c in [\"clip_path\", \"label_num\", \"task\"]:\n",
    "            if c not in self.df.columns:\n",
    "                raise ValueError(f\"Manifest missing column '{c}'\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        row = self.df.iloc[idx]\n",
    "        rel = str(row[\"clip_path\"])\n",
    "        label = int(row[\"label_num\"])\n",
    "        task_group = str(row[\"task\"])\n",
    "\n",
    "        wav_path = self.clip_root / rel\n",
    "\n",
    "        import soundfile as sf\n",
    "        y, _sr = sf.read(str(wav_path))\n",
    "\n",
    "        if isinstance(y, np.ndarray) and y.ndim > 1:\n",
    "            y = y.mean(axis=1)\n",
    "\n",
    "        y = np.asarray(y, dtype=np.float32)\n",
    "        attn = np.zeros_like(y, dtype=np.int64)\n",
    "\n",
    "        if task_group == VOWEL_TASK_VALUE:\n",
    "            k = -1\n",
    "            for j in range(len(y) - 1, -1, -1):\n",
    "                if abs(float(y[j])) > TINY_THRESH:\n",
    "                    k = j\n",
    "                    break\n",
    "            attn[:] = 1 if k < 0 else 0\n",
    "            if k >= 0:\n",
    "                attn[:k+1] = 1\n",
    "        else:\n",
    "            attn[:] = 1\n",
    "\n",
    "        return {\n",
    "            \"clip_path\": rel,\n",
    "            \"input_values\": torch.from_numpy(y),\n",
    "            \"attention_mask\": torch.from_numpy(attn),\n",
    "            \"labels\": torch.tensor(label, dtype=torch.long),\n",
    "            \"task_group\": task_group,\n",
    "        }\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Pads waveforms and masks to max length in the batch.\n",
    "    Keeps clip_path for debug.\n",
    "    \"\"\"\n",
    "    max_len = int(max(b[\"input_values\"].numel() for b in batch))\n",
    "    xs, ms, ys, tgs, cps = [], [], [], [], []\n",
    "    for b in batch:\n",
    "        x = b[\"input_values\"]\n",
    "        m = b[\"attention_mask\"]\n",
    "        pad = max_len - x.numel()\n",
    "        if pad > 0:\n",
    "            x = torch.cat([x, torch.zeros(pad, dtype=x.dtype)], dim=0)\n",
    "            m = torch.cat([m, torch.zeros(pad, dtype=m.dtype)], dim=0)\n",
    "        xs.append(x)\n",
    "        ms.append(m)\n",
    "        ys.append(b[\"labels\"])\n",
    "        tgs.append(b[\"task_group\"])\n",
    "        cps.append(b[\"clip_path\"])\n",
    "    return {\n",
    "        \"clip_path\": cps,\n",
    "        \"input_values\": torch.stack(xs, dim=0),\n",
    "        \"attention_mask\": torch.stack(ms, dim=0),\n",
    "        \"labels\": torch.stack(ys, dim=0),\n",
    "        \"task_group\": tgs,\n",
    "    }\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Metrics\n",
    "# ============================================================\n",
    "\n",
    "def _eval_auc(model: nn.Module, loader: DataLoader) -> float:\n",
    "    model.eval()\n",
    "    ys, ps = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            x = batch[\"input_values\"].to(DEVICE)\n",
    "            m = batch[\"attention_mask\"].to(DEVICE)\n",
    "            y = batch[\"labels\"].to(DEVICE)\n",
    "            tg = batch[\"task_group\"]\n",
    "\n",
    "            with torch.cuda.amp.autocast(enabled=USE_AMP):\n",
    "                logits = model(x, m, tg)\n",
    "                prob_pd = torch.softmax(logits, dim=1)[:, 1]\n",
    "\n",
    "            ys.append(y.detach().cpu().numpy())\n",
    "            ps.append(prob_pd.detach().cpu().numpy())\n",
    "\n",
    "    y_all = np.concatenate(ys)\n",
    "    p_all = np.concatenate(ps)\n",
    "    if len(np.unique(y_all)) < 2:\n",
    "        return float(\"nan\")\n",
    "    return float(roc_auc_score(y_all, p_all))\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Build speaker-balanced subsets from TrainEnh draws\n",
    "# ============================================================\n",
    "\n",
    "def _select_d2_rows(enh_train: pd.DataFrame, manifest_name: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Select only D2 rows using source_dataset.\n",
    "    \"\"\"\n",
    "    if \"source_dataset\" not in enh_train.columns:\n",
    "        raise ValueError(\n",
    "            f\"{manifest_name} missing 'source_dataset'. \"\n",
    "            \"D2 filtering requires this column.\"\n",
    "        )\n",
    "\n",
    "    src = enh_train[\"source_dataset\"].astype(str).str.strip()\n",
    "    has_d2 = (src == D2_SOURCE_KEY).any()\n",
    "    if not has_d2:\n",
    "        top = src.value_counts().head(10).to_dict()\n",
    "        raise ValueError(\n",
    "            f\"{manifest_name}: no rows where source_dataset == '{D2_SOURCE_KEY}'. \"\n",
    "            f\"Top source_dataset values: {top}\"\n",
    "        )\n",
    "    return enh_train[src == D2_SOURCE_KEY].copy()\n",
    "\n",
    "def _pick_balanced_speakers(\n",
    "    df_d2: pd.DataFrame,\n",
    "    frac: float,\n",
    "    seed: int,\n",
    "    manifest_name: str\n",
    ") -> tuple[set[str], dict]:\n",
    "    \"\"\"\n",
    "    Speaker-level sampling with PD/HC balance.\n",
    "\n",
    "    Input:\n",
    "    - df_d2: D2-only rows from one TrainEnh draw (one speaker may have multiple clips)\n",
    "    - frac: fraction of speakers to include within PD and HC separately\n",
    "\n",
    "    Output:\n",
    "    - chosen_speakers: speaker_id set\n",
    "    - meta: counts for logging\n",
    "    \"\"\"\n",
    "    for c in [\"speaker_id\", \"label_num\"]:\n",
    "        if c not in df_d2.columns:\n",
    "            raise ValueError(f\"{manifest_name}: missing '{c}' needed for speaker sampling\")\n",
    "\n",
    "    spk = df_d2[[\"speaker_id\", \"label_num\"]].drop_duplicates().copy()\n",
    "    spk[\"speaker_id\"] = spk[\"speaker_id\"].astype(str)\n",
    "\n",
    "    spk_pd = spk[spk[\"label_num\"].astype(int) == 1][\"speaker_id\"].tolist()\n",
    "    spk_hc = spk[spk[\"label_num\"].astype(int) == 0][\"speaker_id\"].tolist()\n",
    "\n",
    "    rng = np.random.RandomState(int(seed))\n",
    "    rng.shuffle(spk_pd)\n",
    "    rng.shuffle(spk_hc)\n",
    "\n",
    "    n_pd = len(spk_pd)\n",
    "    n_hc = len(spk_hc)\n",
    "\n",
    "    k_pd = int(np.floor(frac * n_pd))\n",
    "    k_hc = int(np.floor(frac * n_hc))\n",
    "\n",
    "    chosen = set(spk_pd[:k_pd]) | set(spk_hc[:k_hc])\n",
    "\n",
    "    meta = {\n",
    "        \"manifest\": manifest_name,\n",
    "        \"frac\": float(frac),\n",
    "        \"pd_speakers_total\": int(n_pd),\n",
    "        \"hc_speakers_total\": int(n_hc),\n",
    "        \"pd_speakers_kept\": int(k_pd),\n",
    "        \"hc_speakers_kept\": int(k_hc),\n",
    "        \"speakers_kept_total\": int(len(chosen)),\n",
    "    }\n",
    "    return chosen, meta\n",
    "\n",
    "def _subset_rows_by_speakers(df_d2: pd.DataFrame, speakers: set[str]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Keep all clips for included speakers.\n",
    "    \"\"\"\n",
    "    s = df_d2[\"speaker_id\"].astype(str)\n",
    "    return df_d2[s.isin(speakers)].copy()\n",
    "\n",
    "def _load_draw_d2_rows(mpath: Path) -> tuple[pd.DataFrame, str]:\n",
    "    \"\"\"\n",
    "    Read one TrainEnh manifest and return its D2-only training rows.\n",
    "    \"\"\"\n",
    "    enh = _read_csv(mpath)\n",
    "    for c in [\"clip_path\", \"split\", \"label_num\", \"task\", \"source_dataset\", \"speaker_id\"]:\n",
    "        if c not in enh.columns:\n",
    "            raise ValueError(f\"{mpath.name} missing '{c}'\")\n",
    "\n",
    "    enh[\"clip_path\"] = enh[\"clip_path\"].map(_norm_clip_path)\n",
    "    split_name = _choose_train_split_name(enh)\n",
    "    enh_train = enh[enh[\"split\"].astype(str) == split_name].copy()\n",
    "    if len(enh_train) == 0:\n",
    "        raise ValueError(f\"{mpath.name}: split '{split_name}' is empty\")\n",
    "\n",
    "    d2_rows = _select_d2_rows(enh_train, mpath.name)\n",
    "    return d2_rows, split_name\n",
    "\n",
    "def _build_draw_buckets(\n",
    "    m_enh1: Path,\n",
    "    m_enh2: Path,\n",
    "    m_enh3: Path,\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Pre-compute speaker-balanced subsets for finer points.\n",
    "\n",
    "    Outputs:\n",
    "    - dict with full and half subsets:\n",
    "      A_full, A_half, B_full, B_half, C_full, C_half\n",
    "    \"\"\"\n",
    "    d2_a, split_a = _load_draw_d2_rows(m_enh1)\n",
    "    d2_b, split_b = _load_draw_d2_rows(m_enh2)\n",
    "    d2_c, split_c = _load_draw_d2_rows(m_enh3)\n",
    "\n",
    "    # Use a fixed seed for deterministic speaker picks (not tied to training seed)\n",
    "    pick_seed = 424242\n",
    "\n",
    "    spk_a_half, meta_a = _pick_balanced_speakers(d2_a, DRAW_FRAC[\"A_half\"], pick_seed, m_enh1.name)\n",
    "    spk_b_half, meta_b = _pick_balanced_speakers(d2_b, DRAW_FRAC[\"B_half\"], pick_seed, m_enh2.name)\n",
    "    spk_c_half, meta_c = _pick_balanced_speakers(d2_c, DRAW_FRAC[\"C_half\"], pick_seed, m_enh3.name)\n",
    "\n",
    "    buckets = {\n",
    "        \"A_full\": d2_a.copy(),\n",
    "        \"A_half\": _subset_rows_by_speakers(d2_a, spk_a_half),\n",
    "        \"B_full\": d2_b.copy(),\n",
    "        \"B_half\": _subset_rows_by_speakers(d2_b, spk_b_half),\n",
    "        \"C_full\": d2_c.copy(),\n",
    "        \"C_half\": _subset_rows_by_speakers(d2_c, spk_c_half),\n",
    "        \"meta\": {\n",
    "            \"split_used\": {\"A\": split_a, \"B\": split_b, \"C\": split_c},\n",
    "            \"half_pick_seed\": int(pick_seed),\n",
    "            \"half_pick_details\": [meta_a, meta_b, meta_c],\n",
    "        },\n",
    "    }\n",
    "\n",
    "    # Quick sanity: draws should be non-overlapping speakers (given the current process)\n",
    "    spkA = set(d2_a[\"speaker_id\"].astype(str).unique().tolist())\n",
    "    spkB = set(d2_b[\"speaker_id\"].astype(str).unique().tolist())\n",
    "    spkC = set(d2_c[\"speaker_id\"].astype(str).unique().tolist())\n",
    "    overlaps = {\n",
    "        \"A∩B\": int(len(spkA & spkB)),\n",
    "        \"A∩C\": int(len(spkA & spkC)),\n",
    "        \"B∩C\": int(len(spkB & spkC)),\n",
    "    }\n",
    "    print(\"\\n[Build] Draw speaker overlaps (expected ~0):\", overlaps)\n",
    "\n",
    "    return buckets\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Build condition manifest (base + selected draw buckets)\n",
    "# ============================================================\n",
    "\n",
    "def _prepare_condition_manifest_from_rows(\n",
    "    cond_name: str,\n",
    "    pct: int,\n",
    "    extra_rows: pd.DataFrame,\n",
    "    sources_meta: list[dict],\n",
    ") -> Path:\n",
    "    \"\"\"\n",
    "    Writes:\n",
    "      <cond>/manifests/manifest_all.csv   (train + val)\n",
    "      <cond>/logs/condition_meta.json\n",
    "    \"\"\"\n",
    "    dst = ABL_ROOT / cond_name\n",
    "    dst_mans = dst / \"manifests\"\n",
    "    dst_logs = dst / \"logs\"\n",
    "    dst.mkdir(parents=True, exist_ok=True)\n",
    "    dst_mans.mkdir(parents=True, exist_ok=True)\n",
    "    dst_logs.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    print(f\"\\n[Build] {cond_name} ({pct}%)\")\n",
    "    print(f\"[Build] Writing to: {dst}\")\n",
    "\n",
    "    base_all = _read_csv(D7_MAN_DIR / \"manifest_all.csv\")\n",
    "    for c in [\"clip_path\", \"split\", \"label_num\", \"task\"]:\n",
    "        if c not in base_all.columns:\n",
    "            raise ValueError(f\"Base manifest missing '{c}'\")\n",
    "\n",
    "    base_all[\"clip_path\"] = base_all[\"clip_path\"].map(_norm_clip_path)\n",
    "\n",
    "    base_train = base_all[base_all[\"split\"].astype(str) == \"train\"].copy()\n",
    "    base_val   = base_all[base_all[\"split\"].astype(str) == \"val\"].copy()\n",
    "    if len(base_train) == 0:\n",
    "        raise ValueError(f\"[Build] {cond_name}: base train is empty\")\n",
    "    if len(base_val) == 0:\n",
    "        raise ValueError(f\"[Build] {cond_name}: base val is empty\")\n",
    "\n",
    "    base_train_paths = set(base_train[\"clip_path\"].astype(str).tolist())\n",
    "\n",
    "    extra = extra_rows.copy() if len(extra_rows) > 0 else base_train.iloc[0:0].copy()\n",
    "    extra[\"clip_path\"] = extra[\"clip_path\"].map(_norm_clip_path)\n",
    "\n",
    "    # Dedup against base, and within extra\n",
    "    cp = extra[\"clip_path\"].astype(str)\n",
    "    extra = extra[~cp.isin(base_train_paths)].copy()\n",
    "    extra = extra.drop_duplicates(subset=[\"clip_path\"]).copy()\n",
    "\n",
    "    train_df = pd.concat([base_train, extra], axis=0, ignore_index=True)\n",
    "    train_df[\"split\"] = \"train\"\n",
    "    val_df = base_val.copy()\n",
    "    val_df[\"split\"] = \"val\"\n",
    "    out_all = pd.concat([train_df, val_df], axis=0, ignore_index=True)\n",
    "\n",
    "    _write_csv(out_all, dst_mans / \"manifest_all.csv\")\n",
    "\n",
    "    added_raw = int(len(extra_rows))\n",
    "    added_dedup = int(len(extra))\n",
    "\n",
    "    spk_added = _safe_unique_count(extra, \"speaker_id\")\n",
    "    print(f\"[Counts] Base train={len(base_train)} | Added D2 (raw)={added_raw} | Added D2 (dedup)={added_dedup}\")\n",
    "    print(f\"[Counts] Final train={len(train_df)} | Final val={len(val_df)}\")\n",
    "    if spk_added is not None:\n",
    "        print(f\"[Counts] Added D2 speakers (if available)={spk_added}\")\n",
    "\n",
    "    meta = {\n",
    "        \"cond_name\": cond_name,\n",
    "        \"pct_target_speakers\": pct,\n",
    "        \"base_train_rows\": int(len(base_train)),\n",
    "        \"base_val_rows\": int(len(base_val)),\n",
    "        \"added_d2_raw_total\": int(added_raw),\n",
    "        \"added_d2_dedup_total\": int(added_dedup),\n",
    "        \"train_rows_written\": int(len(train_df)),\n",
    "        \"val_rows_written\": int(len(val_df)),\n",
    "        \"added_d2_unique_speakers_if_available\": spk_added,\n",
    "        \"sources\": sources_meta,\n",
    "        \"created_at\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "    }\n",
    "    with open(dst_logs / \"condition_meta.json\", \"w\") as f:\n",
    "        json.dump(meta, f, indent=2)\n",
    "\n",
    "    return dst\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Train + validation (runs saved on Drive, clips read from clip_root)\n",
    "# ============================================================\n",
    "\n",
    "def _run_trainval(manifest_path: Path, clip_root: Path, out_root: Path, tag: str) -> Path:\n",
    "    run_stamp = _new_run_stamp()\n",
    "    exp_root = out_root / \"trainval_runs\" / f\"exp_{tag}_{run_stamp}\"\n",
    "    exp_root.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    print(f\"\\n[TrainVal] {tag}\")\n",
    "    print(f\"[TrainVal] Writing to: {exp_root}\")\n",
    "\n",
    "    manifest = _read_csv(manifest_path)\n",
    "    manifest[\"clip_path\"] = manifest[\"clip_path\"].map(_norm_clip_path)\n",
    "\n",
    "    df_train = manifest[manifest[\"split\"].astype(str) == \"train\"].copy()\n",
    "    df_val   = manifest[manifest[\"split\"].astype(str) == \"val\"].copy()\n",
    "\n",
    "    if len(df_train) == 0:\n",
    "        raise ValueError(f\"[TrainVal] {tag}: train split is empty\")\n",
    "    if len(df_val) == 0:\n",
    "        raise ValueError(f\"[TrainVal] {tag}: val split is empty\")\n",
    "\n",
    "    print(f\"[TrainVal] train clips={len(df_train)} | val clips={len(df_val)}\")\n",
    "    _scan_exists(clip_root, df_train, f\"{tag} train\")\n",
    "    _scan_exists(clip_root, df_val,   f\"{tag} val\")\n",
    "    _scan_soundfile_info(clip_root, df_train.head(500), f\"{tag} train (first 500)\")\n",
    "    _scan_soundfile_info(clip_root, df_val, f\"{tag} val\")\n",
    "\n",
    "    train_ds = AudioManifestDataset(df_train, clip_root)\n",
    "    val_ds   = AudioManifestDataset(df_val, clip_root)\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=PER_DEVICE_BS, shuffle=True,  num_workers=0, collate_fn=collate_fn)\n",
    "    val_loader   = DataLoader(val_ds,   batch_size=PER_DEVICE_BS, shuffle=False, num_workers=0, collate_fn=collate_fn)\n",
    "\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=USE_AMP)\n",
    "\n",
    "    for seed in SEEDS:\n",
    "        _set_seed(seed)\n",
    "        seed_dir = exp_root / f\"run_D7_seed{seed}\"\n",
    "        seed_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        model = FrozenW2VTwoHead(BACKBONE_CKPT, DROPOUT_P).to(DEVICE)\n",
    "        opt = torch.optim.AdamW([p for p in model.parameters() if p.requires_grad], lr=LR)\n",
    "\n",
    "        print(f\"[TrainVal] Seed {seed}: warmup fetch...\")\n",
    "        t_fetch = time.time()\n",
    "        it = iter(train_loader)\n",
    "        batch0 = next(it)\n",
    "        print(f\"[TrainVal] Seed {seed}: warmup fetch ok | {time.time()-t_fetch:.2f}s | sample: {batch0['clip_path'][0]}\")\n",
    "\n",
    "        best_auc = -1.0\n",
    "        best_epoch = -1\n",
    "        best_state = None\n",
    "        bad = 0\n",
    "        history = []\n",
    "\n",
    "        pbar = tqdm(range(1, MAX_EPOCHS + 1), desc=f\"Seed {seed} epochs\", leave=False)\n",
    "        for epoch in pbar:\n",
    "            model.train()\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "\n",
    "            total_loss = 0.0\n",
    "            steps = 0\n",
    "            last_i = 0\n",
    "\n",
    "            loader_iter = iter(train_loader)\n",
    "\n",
    "            for i in range(1, len(train_loader) + 1):\n",
    "                last_i = i\n",
    "\n",
    "                t0_fetch = time.time()\n",
    "                batch = next(loader_iter)\n",
    "                dt_fetch = time.time() - t0_fetch\n",
    "                if dt_fetch > FETCH_WATCHDOG_SEC:\n",
    "                    print(f\"\\n[Hang] Slow batch fetch: epoch={epoch} i={i} fetch_sec={dt_fetch:.1f}\")\n",
    "                    print(\"[Hang] Sample clip paths:\")\n",
    "                    for cp in batch[\"clip_path\"][:8]:\n",
    "                        print(\"  -\", cp)\n",
    "\n",
    "                t0_step = time.time()\n",
    "\n",
    "                x = batch[\"input_values\"].to(DEVICE)\n",
    "                m = batch[\"attention_mask\"].to(DEVICE)\n",
    "                y = batch[\"labels\"].to(DEVICE)\n",
    "                tg = batch[\"task_group\"]\n",
    "\n",
    "                with torch.cuda.amp.autocast(enabled=USE_AMP):\n",
    "                    logits = model(x, m, tg)\n",
    "                    loss = model.loss_fn(logits, y)\n",
    "\n",
    "                scaler.scale(loss / GRAD_ACCUM).backward()\n",
    "\n",
    "                if i % GRAD_ACCUM == 0:\n",
    "                    scaler.step(opt)\n",
    "                    scaler.update()\n",
    "                    opt.zero_grad(set_to_none=True)\n",
    "\n",
    "                total_loss += float(loss.item())\n",
    "                steps += 1\n",
    "\n",
    "                dt_step = time.time() - t0_step\n",
    "                if dt_step > STEP_WATCHDOG_SEC:\n",
    "                    print(f\"\\n[Hang] Slow step: epoch={epoch} i={i} step_sec={dt_step:.1f}\")\n",
    "                    print(\"[Hang] Sample clip paths:\")\n",
    "                    for cp in batch[\"clip_path\"][:8]:\n",
    "                        print(\"  -\", cp)\n",
    "\n",
    "                if i % PRINT_EVERY_N_BATCHES == 0:\n",
    "                    print(f\"[TrainVal] epoch={epoch} i={i}/{len(train_loader)} loss={float(loss.item()):.4f} fetch={dt_fetch:.2f}s step={dt_step:.2f}s\")\n",
    "\n",
    "            if last_i % GRAD_ACCUM != 0:\n",
    "                scaler.step(opt)\n",
    "                scaler.update()\n",
    "                opt.zero_grad(set_to_none=True)\n",
    "\n",
    "            val_auc = _eval_auc(model, val_loader)\n",
    "            history.append({\"epoch\": epoch, \"train_loss\": total_loss / max(1, steps), \"val_auc\": val_auc})\n",
    "            pbar.set_postfix({\"val_auc\": f\"{val_auc:.4f}\"})\n",
    "\n",
    "            if val_auc > best_auc + 1e-6:\n",
    "                best_auc = val_auc\n",
    "                best_epoch = epoch\n",
    "                bad = 0\n",
    "                best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
    "            else:\n",
    "                bad += 1\n",
    "                if bad >= PATIENCE:\n",
    "                    break\n",
    "\n",
    "        if best_state is None:\n",
    "            raise RuntimeError(\"No best_state captured; training failed.\")\n",
    "\n",
    "        torch.save(best_state, seed_dir / \"best_heads.pt\")\n",
    "        with open(seed_dir / \"history.json\", \"w\") as f:\n",
    "            json.dump(history, f, indent=2)\n",
    "        with open(seed_dir / \"summary_trainval_seed.json\", \"w\") as f:\n",
    "            json.dump({\"seed\": seed, \"best_val_auc\": best_auc, \"best_epoch\": best_epoch}, f, indent=2)\n",
    "\n",
    "    with open(exp_root / \"summary_trainval.json\", \"w\") as f:\n",
    "        json.dump({\"tag\": tag, \"seeds\": SEEDS}, f, indent=2)\n",
    "\n",
    "    return exp_root\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# D2 test (reads from D2, writes into condition folder)\n",
    "# ============================================================\n",
    "\n",
    "def _run_d2_test(exp_root: Path, tag: str, out_root: Path) -> Path:\n",
    "    run_stamp = _new_run_stamp()\n",
    "    run_root = out_root / \"multilingual_test_runs\" / f\"{tag}_{run_stamp}\"\n",
    "    run_root.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    d2_manifest_path = D2_ROOT / \"manifests\" / \"manifest_all.csv\"\n",
    "    _ensure_exists(d2_manifest_path, \"D2 manifests/manifest_all.csv\")\n",
    "\n",
    "    d2_all = _read_csv(d2_manifest_path)\n",
    "    d2_all[\"clip_path\"] = d2_all[\"clip_path\"].map(_norm_clip_path)\n",
    "\n",
    "    df_test = d2_all[d2_all[\"split\"].astype(str) == \"test\"].copy()\n",
    "    for c in [\"clip_path\", \"label_num\", \"task\"]:\n",
    "        if c not in df_test.columns:\n",
    "            raise ValueError(f\"D2 manifest missing required column '{c}'\")\n",
    "\n",
    "    d2_ds = AudioManifestDataset(df_test, D2_ROOT)\n",
    "    d2_loader = DataLoader(d2_ds, batch_size=PER_DEVICE_BS, shuffle=False, num_workers=0, collate_fn=collate_fn)\n",
    "\n",
    "    print(f\"[D2 Test] {tag} | D2 test clips={len(d2_ds)}\")\n",
    "    print(f\"[D2 Test] Writing to: {run_root}\")\n",
    "\n",
    "    aurocs = []\n",
    "\n",
    "    for seed in SEEDS:\n",
    "        seed_dir = run_root / f\"run_D7_on_D2test_seed{seed}\"\n",
    "        seed_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        best_heads = exp_root / f\"run_D7_seed{seed}\" / \"best_heads.pt\"\n",
    "        _ensure_exists(best_heads, f\"best_heads.pt for seed {seed}\")\n",
    "\n",
    "        model = FrozenW2VTwoHead(BACKBONE_CKPT, DROPOUT_P).to(DEVICE)\n",
    "        state = torch.load(best_heads, map_location=\"cpu\")\n",
    "        model.load_state_dict(state, strict=True)\n",
    "        model.eval()\n",
    "\n",
    "        ys, ps = [], []\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(d2_loader, desc=f\"Infer seed {seed}\", leave=False):\n",
    "                x = batch[\"input_values\"].to(DEVICE)\n",
    "                m = batch[\"attention_mask\"].to(DEVICE)\n",
    "                y = batch[\"labels\"].to(DEVICE)\n",
    "                tg = batch[\"task_group\"]\n",
    "\n",
    "                with torch.cuda.amp.autocast(enabled=USE_AMP):\n",
    "                    logits = model(x, m, tg)\n",
    "                    prob_pd = torch.softmax(logits, dim=1)[:, 1]\n",
    "\n",
    "                ys.append(y.detach().cpu().numpy())\n",
    "                ps.append(prob_pd.detach().cpu().numpy())\n",
    "\n",
    "        y_all = np.concatenate(ys)\n",
    "        p_all = np.concatenate(ps)\n",
    "\n",
    "        auc = float(\"nan\") if len(np.unique(y_all)) < 2 else float(roc_auc_score(y_all, p_all))\n",
    "        aurocs.append(auc)\n",
    "\n",
    "        with open(seed_dir / \"metrics.json\", \"w\") as f:\n",
    "            json.dump({\"seed\": seed, \"d2_test_auroc\": auc}, f, indent=2)\n",
    "\n",
    "        pd.DataFrame({\"y_true\": y_all.astype(int), \"y_score\": p_all.astype(float)}).to_csv(\n",
    "            seed_dir / \"predictions.csv\", index=False\n",
    "        )\n",
    "\n",
    "    with open(run_root / \"summary_test.json\", \"w\") as f:\n",
    "        json.dump({\"tag\": tag, \"seeds\": SEEDS, \"aurocs\": aurocs}, f, indent=2)\n",
    "\n",
    "    return run_root\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Main run (finer points, then summary plot)\n",
    "# ============================================================\n",
    "\n",
    "_ensure_exists(D7_MAN_DIR / \"manifest_all.csv\", \"D7 base manifest_all.csv\")\n",
    "_ensure_exists(D2_ROOT / \"manifests\" / \"manifest_all.csv\", \"D2 manifest_all.csv\")\n",
    "\n",
    "M_ENH1 = D7_MAN_DIR / \"manifest_train_enh1.csv\"\n",
    "M_ENH2 = D7_MAN_DIR / \"manifest_train_enh2.csv\"\n",
    "M_ENH3 = D7_MAN_DIR / \"manifest_train_enh3.csv\"\n",
    "_ensure_exists(M_ENH1, \"D7 manifest_train_enh1.csv\")\n",
    "_ensure_exists(M_ENH2, \"D7 manifest_train_enh2.csv\")\n",
    "_ensure_exists(M_ENH3, \"D7 manifest_train_enh3.csv\")\n",
    "\n",
    "_print_split_counts(D2_ROOT / \"manifests\" / \"manifest_all.csv\", \"D2 splits\")\n",
    "\n",
    "print(\"\\n[Build] Pre-computing speaker-balanced half draws...\")\n",
    "draws = _build_draw_buckets(M_ENH1, M_ENH2, M_ENH3)\n",
    "\n",
    "# Conditions:\n",
    "# 0%: base\n",
    "# 5%: base + A_half\n",
    "# 10%: base + A_full\n",
    "# 15%: base + A_full + B_half\n",
    "# 20%: base + A_full + B_full\n",
    "# 25%: base + A_full + B_full + C_half\n",
    "# 29%: base + A_full + B_full + C_full\n",
    "conditions = [\n",
    "    (\"base_0pct\",    0,  []),\n",
    "    (\"enhA_5pct\",    5,  [\"A_half\"]),\n",
    "    (\"enhA_10pct\",   10, [\"A_full\"]),\n",
    "    (\"enhAB_15pct\",  15, [\"A_full\", \"B_half\"]),\n",
    "    (\"enhAB_20pct\",  20, [\"A_full\", \"B_full\"]),\n",
    "    (\"enhABC_25pct\", 25, [\"A_full\", \"B_full\", \"C_half\"]),\n",
    "    (\"enhABC_29pct\", 29, [\"A_full\", \"B_full\", \"C_full\"]),\n",
    "]\n",
    "\n",
    "results = []\n",
    "\n",
    "for cond_name, pct, parts in conditions:\n",
    "    # 1) Extra rows for this condition\n",
    "    extra_rows_list = []\n",
    "    sources_meta = []\n",
    "\n",
    "    for key in parts:\n",
    "        df_part = draws[key].copy()\n",
    "        extra_rows_list.append(df_part)\n",
    "\n",
    "        spk_ct = _safe_unique_count(df_part, \"speaker_id\")\n",
    "        sources_meta.append({\n",
    "            \"source_part\": key,\n",
    "            \"d2_rows\": int(len(df_part)),\n",
    "            \"d2_speakers_if_available\": spk_ct,\n",
    "        })\n",
    "\n",
    "    # Add draw metadata once for traceability\n",
    "    if len(parts) > 0:\n",
    "        sources_meta.append({\"draw_meta\": draws[\"meta\"]})\n",
    "\n",
    "    extra_rows = (\n",
    "        pd.concat(extra_rows_list, axis=0, ignore_index=True)\n",
    "        if len(extra_rows_list) > 0\n",
    "        else pd.DataFrame()\n",
    "    )\n",
    "\n",
    "    # 2) Build condition manifest on Drive\n",
    "    dx_drive = _prepare_condition_manifest_from_rows(\n",
    "        cond_name=cond_name,\n",
    "        pct=pct,\n",
    "        extra_rows=extra_rows,\n",
    "        sources_meta=sources_meta,\n",
    "    )\n",
    "\n",
    "    manifest_drive_path = dx_drive / \"manifests\" / \"manifest_all.csv\"\n",
    "    manifest_drive = _read_csv(manifest_drive_path)\n",
    "    manifest_drive[\"clip_path\"] = manifest_drive[\"clip_path\"].map(_norm_clip_path)\n",
    "\n",
    "    df_train = manifest_drive[manifest_drive[\"split\"].astype(str) == \"train\"].copy()\n",
    "    df_val   = manifest_drive[manifest_drive[\"split\"].astype(str) == \"val\"].copy()\n",
    "    print(f\"[Counts] {cond_name}: train={len(df_train)} | val={len(df_val)}\")\n",
    "\n",
    "    # 3) Choose where clips are read from during training\n",
    "    clip_root = D7_CLIP_ROOT\n",
    "    manifest_for_train = manifest_drive_path\n",
    "\n",
    "    if STAGE_TO_LOCAL:\n",
    "        local_root = _stage_manifest_clips_to_local(cond_name, D7_CLIP_ROOT, manifest_drive)\n",
    "        clip_root = local_root\n",
    "        manifest_for_train = local_root / \"manifests\" / \"manifest_all.csv\"\n",
    "\n",
    "        _scan_exists(clip_root, df_train, f\"{cond_name} train (local)\")\n",
    "        _scan_exists(clip_root, df_val,   f\"{cond_name} val (local)\")\n",
    "\n",
    "    tag = f\"target_ablation_finer_{cond_name}\"\n",
    "\n",
    "    # 4) Train (writes outputs to Drive condition folder)\n",
    "    exp_root = _run_trainval(\n",
    "        manifest_path=manifest_for_train,\n",
    "        clip_root=clip_root,\n",
    "        out_root=dx_drive,\n",
    "        tag=tag\n",
    "    )\n",
    "\n",
    "    # 5) Test on D2 (writes outputs to Drive condition folder)\n",
    "    test_run = _run_d2_test(exp_root=exp_root, tag=tag, out_root=dx_drive)\n",
    "\n",
    "    # 6) Collect AUROCs\n",
    "    aurocs = []\n",
    "    for seed in SEEDS:\n",
    "        mpath = test_run / f\"run_D7_on_D2test_seed{seed}\" / \"metrics.json\"\n",
    "        _ensure_exists(mpath, f\"metrics.json for {cond_name} seed {seed}\")\n",
    "        with open(mpath, \"r\") as f:\n",
    "            aurocs.append(json.load(f)[\"d2_test_auroc\"])\n",
    "\n",
    "    mean_auc = float(np.nanmean(aurocs))\n",
    "    sd_auc   = float(np.nanstd(aurocs, ddof=1)) if np.sum(~np.isnan(aurocs)) >= 2 else float(\"nan\")\n",
    "\n",
    "    results.append({\n",
    "        \"condition\": cond_name,\n",
    "        \"pct_target_speakers\": pct,\n",
    "        \"aurocs_by_seed\": aurocs,\n",
    "        \"auroc_mean\": mean_auc,\n",
    "        \"auroc_sd\": sd_auc,\n",
    "        \"drive_condition_root\": str(dx_drive),\n",
    "        \"clip_root_used_for_train\": str(clip_root),\n",
    "    })\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Summary + curve plot\n",
    "# ============================================================\n",
    "\n",
    "summary_df = pd.DataFrame(results).sort_values(\"pct_target_speakers\").reset_index(drop=True)\n",
    "summary_csv = ABL_ROOT / \"target_data_ablation_finer_summary.csv\"\n",
    "summary_df.to_csv(summary_csv, index=False)\n",
    "\n",
    "print(\"\\n[Done] Summary CSV:\", summary_csv)\n",
    "display(summary_df)\n",
    "\n",
    "x = summary_df[\"pct_target_speakers\"].to_numpy(dtype=float)\n",
    "y = summary_df[\"auroc_mean\"].to_numpy(dtype=float)\n",
    "yerr = summary_df[\"auroc_sd\"].to_numpy(dtype=float)\n",
    "\n",
    "coeffs = np.polyfit(x, y, deg=2)\n",
    "poly_fn = np.poly1d(coeffs)\n",
    "\n",
    "x_fit = np.linspace(float(np.min(x)), float(np.max(x)), 200)\n",
    "y_fit = poly_fn(x_fit)\n",
    "\n",
    "plt.figure()\n",
    "plt.errorbar(x, y, yerr=yerr, fmt=\"o\", capsize=4, label=\"Mean ± SD (3 seeds)\")\n",
    "plt.plot(x_fit, y_fit, \"--\", linewidth=2, color=\"orange\", label=\"Trend Line\")\n",
    "\n",
    "plt.xticks(x, [f\"{int(v)}%\" for v in x])\n",
    "plt.xlabel(\"Percent of D2 train speakers included in training\")\n",
    "plt.ylabel(\"D2 test AUROC\")\n",
    "plt.title(\"Target-Data Ablation: Finer Cumulative D2 Exposure vs D2-Test AUROC\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "\n",
    "plot_path = ABL_ROOT / \"target_data_ablation_finer_curve_quadratic.png\"\n",
    "plt.savefig(plot_path, dpi=200, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "print(\"[Done] Plot saved:\", plot_path)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# End\n",
    "# ============================================================\n",
    "\n",
    "elapsed_hr = (time.time() - START_TIME) / 3600.0\n",
    "print(f\"\\nTotal wall time: {elapsed_hr:.2f} hours\")\n",
    "\n",
    "print(\"\\nAll done. Unassigning the runtime to stop the L4 instance...\")\n",
    "try:\n",
    "    from google.colab import runtime  # type: ignore\n",
    "    print(\"Calling runtime.unassign() now.\")\n",
    "    runtime.unassign()\n",
    "except Exception as e:\n",
    "    print(\"Could not unassign runtime automatically. The runtime can be stopped manually in Colab.\")\n",
    "    print(\"Reason:\", repr(e))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "b1d16b720e71446bae409b0ffe35c55e",
      "a6b71cc851d44f82a9fe6d39ff5920c6",
      "d76443af1287438b8a6cbeba0b3eee19",
      "5c184b18236e4e9cafae57c6f88966de",
      "12deb1344d9a49cc883463b835565507",
      "f7bb58aa87d74e248e20fd2e16253f01",
      "29cddd722251401981c07b58095e7778",
      "41e641879960472d9997b18fc0e80c01",
      "20055ef845c0419db82164b69e4f8d7c",
      "e3454cfca7df4db992a686af719a30b0",
      "c6a08eed1aba47ad9f3b59e1a900b157",
      "e97b9aa6e8dd4e85a5d0a83c20ed524d",
      "488f9417e0ca4b058fcbcf81811e1bb8",
      "b0b6de1174214a12badf5e852f57cfd4",
      "0d93db039c74481482313437a90da80f",
      "1f13d3dacb2641628f31ad8062a92b4c",
      "b962f0c6557a4b05a89ef1e0276edc1d",
      "03c519c2b0d04ed4b836c6a16fdc737d",
      "758768e1f8f5431187f6443c38c66880",
      "a47ecccbde6a4d84aa5fb72b70d72f40",
      "57a8bc2d82e0402fb573d1c68d5a34d7",
      "5e7a34751ffb428fb7b49358a7cb70e9",
      "a62065d83c5c47a4a12b44e317a927f5",
      "2c3b0aefcded43c6ba90e7dcee3d00bd",
      "30e618e663ba410ebac601972d03bbe4",
      "b9f360882ac349718fa4d3a3cb383dcf",
      "0a4d9c8fbcbd4eeaab25dcef6667579a",
      "3b5a1ae768654bf8901b423f2c551652",
      "283ebc380cbc4a56a2607fec0f3ccce9",
      "0e663602336a46528d5280b791c65930",
      "ebe3e9237b9b484e930c2784be2f680b",
      "91dd9e8dc6e54d9981b863e677c3f429",
      "fb25892cb9d24f999893320589b7feab",
      "14f9680195754023b0a72daa0d06f5de",
      "270d0a381f13493da09eedd4f21314d7",
      "169d0a044c7848658953ba664d6985df",
      "d8be2474178645bc806c1e5177f23c50",
      "7ba781264ebc4a5bbd6151ba91054e81",
      "2414418c24e04733acf77cb8895b8eb5",
      "614a2d1e823c4b91ad93b411ba02fbcd",
      "3ad6d83f3c704c7da4d397285b04d117",
      "f7da6d54acd9432a915982daf827f94d",
      "381cd39949ae45b9a213060151b83486",
      "366badb0456e468eb4d692926c195cc3",
      "07bfca3c4bd04b9585a8672a0bbad01d",
      "b7da166b0a7c45ebb370db33904df7cb",
      "c78520ded27e4a67baf61c6864bfd1b0",
      "fbf15a15cafa4307898be7e5da190673",
      "3a80347fa1654126826cbe1fd5248823",
      "605931b4dac74208988ff859b1596cc5",
      "6d8e9cfc984f44ef85897fafa5c47d57",
      "7a755a65adaa449890105339ccb3f158",
      "76cd133036f94af19910b8eb43b8f56a",
      "52359235cb944481a17189c09ef94fd2",
      "65b05c401d5f4bacb4a5bdd7c1851283",
      "f1b08c7d465c4f2893b4146dbe1d7e79",
      "cdfa674cecca401e8bfed7831f452fcf",
      "75b84fd789ec434aade2b3663ce27f8b",
      "ff026947b8f440fab391ddd0a6e4a9be",
      "169258447244463e9989e6bbc6e27734",
      "397d5c73c7b24ffd9ead4dc8b58e84dc",
      "77a97e80730c485fa5d44fd28fcef529",
      "3cb2ed397740432b8a170f44af19bf42",
      "1b8ebc82652a49d5abacf894aa870e11",
      "6c1a17deef36453087646c831618c35d",
      "3189fc16f23e4663b5cecd20e1709e33",
      "e9b416bd4fa346d8b38437dc2f0cdfa3",
      "e765a01342364eb890bdc76138490513",
      "9471a87bf8994e32abb5aec89cbb5e85",
      "a7fa742bc1ba4d21b638b086fb958906",
      "625b0299d5304010b38d111de27fde1a",
      "495302c0fc4b4116bf523a18ccbd0e19",
      "8df8894615234ddd8718f977dae667e4",
      "e69e58c0c9a148abb5fdd39e63693254",
      "b414c668b58349d49f94bfe5b2a60f8c",
      "adb2d1fb14e447dd97ba93abfcb8fa28",
      "2e1204f08bf3410187b3b88a774b8edf",
      "6be99dace8ec40738e2ff8e856bd0059",
      "eca3fece795844c1a071caa5fadda2fa",
      "0c0481b7960046a683361b6bcdf6c431",
      "f7d74321f91540e98d3f56af060eaee2",
      "f315dcd4d3da44909e34775923a0d5b8",
      "12e19d3f39ac434ba62cc22ef40d3558",
      "a06e4aad770149a99baad0c816432fb9",
      "2ce69d925504459097c6fa2d6a01804a",
      "4acfc17b76d343048fd7518e48b91b3c",
      "6e8c36da7ef54d0196cdfaeaa0f79213",
      "6e0ef2af34a047cb93079ff3e353b1c1",
      "5436bb5fc5eb47c4a99a1982d81d2570",
      "4460eb354b454818b3ae67f73c80a19f",
      "0ca88c53ab04408b8a7c88ce1bd17f7d",
      "731c9c3d95f5447e9ab6acd2ee8605cf",
      "8460e50a68d74cbe9b0c430ec164099d",
      "904e13af9cc64052a1f58f661ed81836",
      "da56c69ed41c4f968611f21fa9cc0857",
      "c636eb3d58a94771a79fd8ccc77fff20",
      "8632b3cadb5740d78ad46c6696231168",
      "b3ffe22eb4604542ba92a9cc230fb43b",
      "ac353dc130524b87903e97367eea26f0",
      "4da72aedfcf84fe3aa7d4c51d94100fc",
      "c7689c9f6d884652b44cfedc0f42399a",
      "3739a4c4656b43d2b229197ad0fc7f34",
      "832285cf9ce241f7b1afa95496c69b64",
      "a3caaa5c01f240388784fafa2d9b4a0b",
      "f5aafcb42e094f3690ef16bae68305fc",
      "fb73d8e6359b477fa2544f277784d808",
      "5dcf0df12a184103867098b67e05bb58",
      "c56c502f75f14aacb74db2cd00493899",
      "f72f2ce1ed024f63b6479ce5680aad55",
      "918349b24f0b43b9ac8fb79b8a7a6ba7",
      "083b88cf476d48f492a21d7377dd3275",
      "853f5ebfb60149158dcbcef1e25ec545",
      "e3a91990011b469198c27167c7e4a6e7",
      "209293b19a7648a2b7e360585191d706",
      "6f4c186b3af449468db2cb9accf312c4",
      "c25be1d3afee407bb81e9222c1483671",
      "11e61a8e91ab4551a12cca3a49baceab",
      "22b2467684fc495586fbed09c4d91b21",
      "6e6852e2fb2c4249b4fd8def30516dd3",
      "714f64280b18474bada5ac4e98ce1c08",
      "c455d35062ac4440954afa45d1df1b22",
      "fbae3c85dbbe4850a2d4dad185d5cb5a",
      "6c6cdad80d51420e8c7f0e05607d0096",
      "763d4f5106c9433da4f9ad5de4360ffb",
      "68329cae762c4eb4af6722326415abd1",
      "384cc326782443b895ac80bfbb8e24c0",
      "346e995b2d744e7296e821058503e2d0",
      "53486a1d11e74262a4126b5c82c72fab",
      "48c44fc9c15140248b44546d6a4b7153",
      "f0e7aa64fb9a4010aa4d5e80a04c6348",
      "86e4a296864f443ca76fccecd77658a1",
      "8fe8c9da6e53440fab15cf54dd27c9e4",
      "ac420b7bf41b4ab686bc966a77f885a0",
      "1eaddee8d53146ce9c5f7b472b436826",
      "b6ea6664ea3446b8a4dc56e5ca8243dd",
      "cb96ba58b90a4e40b2d770eb479c7f78",
      "105ded3855614c889020e1d49ae3db0e",
      "cfa3e26109a040edb8fa9ec50baf723b",
      "aa01ab98d27648afb69908f80aeab3ce",
      "83965385ff6f493ebe40a4c8a25ee73d",
      "d5172a6983664cdcaf075b07952ff8b8",
      "2d43347aeef74a80bab3db71577bdf34",
      "b7ce6f97c5ec42e7a33755448958c439",
      "87cb954121844978b1a133a19141d0ba",
      "09a61738ae7f494d825ef7888f0af063",
      "e30e36bc66d44897b11d06048747e3a5",
      "a4a44959d4cb40659504817284b487ee",
      "17f291a6ffe2415fb6b9bd5c6c9bb2e6",
      "1aa1ed9ab584458894c083c0bdc09070",
      "8520b9ccd91e4f43a59d4e237e8303ff",
      "d01c997e48a84da5bfad06e24115ed7e",
      "6b9d0c71198442a5a42c7dcd881d6f6f",
      "e3a6b94e9ed94eb3ae53ead8a5037da3",
      "0f2453a4d60c411dae8b2ea4b7e518ec",
      "f9aeae8fce93477c8363e7124734911f",
      "1458799155be4921837c0ebbdc70aa8d",
      "7cfb373c3cec411ab6bde9aafa0bf67a",
      "61976235780c4b12bfec17603afd9929",
      "ade9ca3b98ff4e25a4ce2f107a576f4f",
      "46b05f52e0954483b5967f24adb4cd05",
      "42d81e456f714220b84b8e24aba970a3",
      "67b1b737c35d458fba5c58d3cdb0e74e",
      "bc206b114c0740a893d30a58c26705a6",
      "86d44a9e1c224cb983c4109d863a1d17",
      "a22c6abf32ba49fe9405a73fefc32b1c",
      "12979b4ffb4d427280d9f2bb22ea3264",
      "ad01f607fc9149e9bd11f4d0569c7efc",
      "3f2faf31109b4e8798bca6100cb1ac0e",
      "d361bdf774ec423b86cd53aaca01a174",
      "62ec5a057ab34e4fa5575e652f857fd8",
      "39adc1a5c78a4cb0a55b4e6da77d94f8",
      "09a0dc8797db4907acef558a16f03899",
      "0d40cec35b2c423e8e27a772c32d1b1d",
      "ba8529343565432ba5773c26dc134a10",
      "da674628ca5c442bb34b0e9ed72a0c11",
      "e6f7d9da15fb4811a8a4c27fbd651f7f",
      "6215503a9edc471891962410754869fd",
      "21c3b3de82aa41cea765d6fed2c91edc",
      "d3476a0efadc4f4fa05e95001989fa28",
      "2dc26e722d924aadb5abb50cb10986e8",
      "9e1e8fd220664bcf8962ba2990ae42b0",
      "58942a1cc4934a269b6988c81abea294",
      "ec5250fe33784d33a4ac7049e15524cd",
      "95aa704556a148ac930417e4ad523bb2",
      "918e7afa731740368f718c8ea4b66727",
      "6c4c1a77406d4aaeae884c427c989bc2",
      "de616aaf6dec48fb893db8ed3f49445f",
      "c90d426e30f14fdbb4fcfd86ada74c97",
      "ec7c975b29594c689f9ab707d4c5a605",
      "904a3051ad394ddfadcdfe930632b3b7",
      "16b5cd25b3194b378976c13e308a0899",
      "dc49b8f2384d4ee894766eab483869d4",
      "33a425af8fae49ce9a6ba86e51cd4ff1",
      "efd8f4e895c14910b8f38d4093067f73",
      "01fdb11cb61e43e5adeac002a73b14bd",
      "904221b6353f4e68a3209f85ed17cf00",
      "fd6742eb083640b6ac4e016716055610",
      "889535a722f0455697261273c13d6ae7",
      "82b6525b32434a02b1ee92e44a0e1f64",
      "41ea234c9e9b4aa99aeba074900a93df",
      "f546dddad846437e8e4a31812e2cb339",
      "5e888409d18449208f334c9eed0f0f7f",
      "8b165d1cab0e4339907eb4a4553d31f9",
      "f0cd485484a9474786701914e658347c",
      "47b7ff8c0627431fb1602ce78cd5d62c",
      "3de66fc3e25c4c88bce462d120938f63",
      "d4b60fe4139444edb4b7f4cab5b2ee7a",
      "9b7ced11794a43888cd8c73934fc3d7e",
      "5a952a8c08d94199ab520c1c2fc11b79",
      "cb8f1e34d74c4080824f67eeffbbbe80",
      "5ef0abc1e16f4ba3a8671c35d348432b",
      "91d12890c9484ab2a19f75553440f4fb",
      "2ccfebe17fce42a885a7dfef44b9494f",
      "8571669f9cd64e57bdd02a2106db1ff4",
      "8738adb17e5c49feafd310d305c6185f",
      "c830ac432b514552bf99cd7f30b55cb6",
      "e4a7f4530f7c420592544b5b1741c3b0",
      "c7de590ff7e3466096eeafd5f8f47421",
      "66ebc4d141184f41a3177fa2e6582f76",
      "4963cd93000d45d9bcc853fe411a8d9c",
      "a5f4281d77e143db9ba8a2697be99151",
      "7e44f48dcea9453b9223e6326b4f5bcc",
      "fb9830b69a3e470ba8b6b5ac11faf981",
      "7859ca37ec874a9c890c4428b87a13c1",
      "7c0226f5525444389589fb25b878694e",
      "9aa3b2a5d4374bfa8a64a14a2ca03a05",
      "fb2c2e4f6b9f41719d3269f001209ab5",
      "bbbf14b85bd544ae8986040875dafac1",
      "6fa26cf400064f199e973161c161c73e",
      "4af7aff91dfd451bb496659da52de8bb",
      "f304cf60b6c349558409a053b7fd4470",
      "376814b785554b4f9e8737b97dd84d82",
      "d8d1112a30f744df81fdfa1e5d83c51d",
      "0626d1a1841d49b193e3e4e547c6273d",
      "6617c5abbc7341beafe404a45dab0cff",
      "8c387d7faba94e668b3ae8ce80b08d34",
      "c0a15e7254ca469e819140d4580ec40d",
      "2483d1f277844e42ad53492d2fce8fd5",
      "d85af6b0d37942f084c781508435d2fa",
      "3d225c56fb3e48b6a449d153e0ccb057",
      "6dc66c1c94f747e08c85132524c9d0e2",
      "810a4427aae241eabd95e2cbb9a5146e",
      "96a023e3433849bcab059e7137ec35e8",
      "2b381be4d68148df940e5529ea2d1f48",
      "5f869893bfc04bcbb6bf6414e7ca7aae",
      "8f04707544ee43b0880faa534f881faf",
      "fba108a6c43e421cbc04e4400dda9457",
      "d021fb2861a44c57963b1f5e4e0614e9",
      "4e74ea7870a34a62848ccf4de3652e0e",
      "30c8b978d30d42da822d730e919ab40f",
      "7ec8a6a72b144c53a5277ee53a990261",
      "2b89185df26a44eabe94c92dd4a050f7",
      "4e8bb3c792224eac9c7c3dc159445793",
      "e0547beed6d545de9aeb21f26e16f173",
      "b6868628517543158d896862fb4267b5",
      "004247f2c6d6477f9938e823242edd39",
      "d6a27779d7d24563accd7fb97224bd30",
      "19b49555a81649e9b22dcbe2226e166f",
      "4681a54f78cb4c579364c82490c9f97d",
      "e6fbd36fb20f4789b3e9ecaa4b262361",
      "80274b26924f4c649b5c1c8d050e3c44",
      "5077f4ed530f4998935eef547607f630",
      "66eae8bb8afd470c90abff1c06a0a547",
      "8d9fb6331bab4f0598fd9f3adcb6dac6",
      "6612816009e84c22b62a7373f1a3b72c",
      "e0783f7568474bccb22b75c2aad704ea",
      "54529517aa6046a0a643eb079ebb3342",
      "ddef41d19d3b45439088480250490fd3",
      "37ef5ef3078a4523834995731f10bd0e",
      "da57139997f34fd4bf2d42b6af5d7020",
      "8779660b1a3d49d7b30e94204927ab06",
      "f14943ab00cd4120a0a47a0eb3e1f055",
      "b2d996cc9be44f9d965e9383afc0966b",
      "7493486aa670486599f0c0a3e21aad34",
      "b821b68f9b8e4057856bcca90e333d33",
      "1b71618953a249fc97aedab57813a9a7",
      "830c2840d528491e983a3a9d16f35aac",
      "95546d41e5a34c189227c7e59e7333c7",
      "81d1bdfc70434ed08ec274720b287ac9",
      "49b54b89b08a4091848053bd72df4637",
      "8df2763a169a454aaf6bd6490043164c",
      "d0bd6e03946a4fc392b796d3c1c279e3",
      "fef15333c6a145a3bb1b97d5501b16bf",
      "5f03c066340741478ca22f4902f4c6ff",
      "b93d7eaeab3f43a3b0b22105e2d94011",
      "39a5e043937b444b9dee0b9b5c66e2a3",
      "19fde6415f6e49e3a1163b1716aad604",
      "3b94991a090742139c0e31db75c63c0b",
      "d847a461f10c47b7a5a81f81a898b67f",
      "bead535c4b514c9a9a84e5e76b835bca",
      "34e9fe4c99c54725a5a8858a08731239",
      "893820fda88f4420b98072c3ab65b013",
      "0d44b9015b3649339297b22c27edd936",
      "f5803048474b4628a58920c125adf684",
      "3d00e592f19641688b04f8d39ff07433",
      "9c57c91af3da467cbb329509248ea934",
      "afcceb8b5be94d99a2bb5cc08c9ea58b",
      "a1a9ed2ed32d4940a7cb46e05428615f",
      "00e2270aecfe4ffd9c465330905b38e7",
      "3e6cc6dd0eba46c895879d390fcd85c1",
      "11061c48ddbd40a6a88775aec8ef27dc",
      "f1b3b527ae7d4148beea78c6c9fad315",
      "394fd73aad9f4147a2c70077a51ae5ac",
      "04df2940c0bd443ebdcf8d9b2c70fde8",
      "c7a53d0732e3419aa16eec48b288df8e",
      "0daaa80f59724e69b107880daefb1b70",
      "b251710f29e648a7a3375ca68725c0af",
      "f751217337bc44eb833010bdba9ccfea",
      "c6e1a329545a4853bb96888234da3010",
      "87118cf04d90462eaf92a9d1abb4dae9",
      "1437efbeeb924ea3b542088471ee2b8c",
      "2685c9b2a349405ab0f1af0f2043f7ad",
      "a4e85ff154c8490f9949de882a8b468a",
      "bf5de04a6d5d420da4070d4229000aa4",
      "265b9608a9134487bded8d5ba24dbec3",
      "5da5a17a938c4c0cbc6a43dbc2445789",
      "da4f6ad736ba4ccd92a94a9237a5ae72",
      "a599b5822e1145ba8791427591cbdf40",
      "4b04711e1982467096c71ba8b1826450",
      "b0d6cd89ef414ec989a43af315f3bdb0",
      "e1b52a5324384ca5845aed56c0dd312a",
      "b1cd1d6763b64c6685968946172e95b3",
      "55c0fd763e8f4ae4a8256644cff66039",
      "1c4426e68f6d44a0bf049cf8679f0ad2",
      "2ddb2635cded46a6a9e025eddb816b90",
      "7e3af965e63840ccb5e318bebb5e7dda",
      "706d4bb9aaa04069af4bec3981a0ce04",
      "f82eaa8a82fb4b7a90258622ffa31ed5",
      "82d709561aaa42fba55bd4988f372c54",
      "48166992937f4762a97c5945d0b9819a",
      "b805413a1d49464592ebae6ed5edc4e2",
      "59931236991041ada8719f72f8053299",
      "d58a60028b6d4b3e9dd6e343c21caba1",
      "f4a5ca9dd54341859545da89040a6f77",
      "f0b73fbbb8404a67b7820efe2358fe03",
      "9aa2a750ea2e4d80a97ff37693c07130",
      "69a8d96e65d4454392d3d8f2aaf6322a",
      "9911cd59c88140e28c44e3a9f7679ec2",
      "b0f75ca5671c40afadca46cf72802ce4",
      "c5d4d07f960a4e5e93617de5bae143b7",
      "89ba21370a0442d98d18496cd22c1a76",
      "8484844a13c64ba38687c0d541dd9ee3",
      "96db5c0787d74ef38f54773c5a36bbb6",
      "7fcc8c9dc0ca44ab9213ba337bbaade4",
      "1d9af967bc0c482ab7808bd5c5e853c4",
      "268323c62ea54d2d8e862725440d8663",
      "2cfcc056eafd43d197688acd26d1a982",
      "eb22a245081f4f179cb664ba6d595d30",
      "9dc7ec2ef65d405bbc17ccc5fbf0e12e",
      "6f84aeb798664f679e11e383cd279c2d",
      "c443527996eb4931b09a979a01d5a707",
      "f80a4e0f269048e4bcd69524abf26417",
      "0332a8d11e62442391b27c79b9ad335c",
      "b4f48b5f32ad4fe5bb90e3ea2a0c395d",
      "5fbafb53c8e54370af80403d5013a417",
      "c9d7f21ba56a4b54a6fc3fc01a2b941f",
      "c493a6c1d62e4185b99294e1d50b2a99",
      "313ebbc0fea04c2495c2993e233da2cb",
      "300aed7b29fc43989afa59ca8cb9446a",
      "01f17720cadb4f4698c771b8fc7a6c6a",
      "1bc3e3baf5a04d17a99dfc21472641a8",
      "c0712b5c57c3429391611445395a04a5",
      "9139ba4ddc34482bb035d27cd95d178a",
      "6d77a23f769745eaac54861ae4faf880",
      "31c304f22cd24dae86972b37b19b6c64",
      "e0d55df7939944bc8975acbd65b01737",
      "f67f3c4d1a6f4d8bba243cb67301c3b3",
      "2c2b6c52c379414f8a44916235cfff19",
      "5a5fa1822c2f47eb961738361df9f1c7",
      "25726b47d2124442942e477112796a77",
      "a8cf3e9fa6a647678c600923bd38bf88",
      "b4a4ca966a964f03aa8c0574d9d5c099",
      "54026e43eb9e46e4aa3b32920ddc8277",
      "89589ddfa71a4afc99770c3b3e067d15",
      "3fa68f280b94433c9c0d91181044b7fa",
      "ec4f8357803d498b8b356f808db65a34",
      "f01b0acf31074800afdbb70d3f78ae98",
      "636a3b70c4a4407586b9ed6ff6fab0d9",
      "6337be6b88494f03bdc12e571aa5ef09",
      "543084962f6e480880457a902982a675",
      "987edcbb294a4c8a88263319d0fef926",
      "ab098e45bd284f759a4e815d66876f2b",
      "d6658de0f2d447339e4e3c397b34b9b4",
      "b0d9251588f84ecdbcca1f14582ce02f",
      "f38d79d2e7ed4843b92571d967bffec6",
      "40205011a37c4bbfabb84c1128767049",
      "5c4a0918f32843ee9191f5a0e84e6a27",
      "95764ca49a864dd2a244ad688ae67a40",
      "95b10320826d40f7b9020d5facf70dd1",
      "455906182c7e46acb83cb6fc4cf0ac3f",
      "3f5e76497cf94bc1a80517e82740fe74",
      "29c5bdd600284c6cbfef18483fed4689",
      "d353dc4e6f294787b99c5fa517a58bc5",
      "f5ed1c3ba4864864bf54c3d418e95989",
      "ffc529e096be42df95f7ecf0a3b8e73c",
      "ccb55f70f2f14379b098b8ea7155a264",
      "4d9e10052200458ebfcf573b7636f1d6",
      "6dc9569a17dc4a6a8e5ca705ccb17040",
      "04124d18fcce43f89e4833942906fc29",
      "140e5250b3db43959a7d6a0386cdf9da",
      "6131ddb4f0a44b349768c70ab2499292",
      "bed37416c0f2428085d31225f1869765",
      "5b60375778a945d3b1d4eca055ebbf2f",
      "779c79ebbb034c30bb0847f1e0b9139a",
      "ea6f9111ffae49a8b79ae9a2b6b5998f",
      "6e89cf4a6a39408eacafbe29c54243a6",
      "0e0453e6b36b44aebc381397f9d73586",
      "2b327b5b36c342c6b20468474917bfc1",
      "5468366e8ac34aef9d809762cdde4b1d",
      "c7131760ff07415890a9c68ccd915c79",
      "74fecfcf0e2748cfb0fa4eed1d1d6145",
      "ea5c23fc4a3f4742b3189e3258eab2ac",
      "f9871593dd104faaa27e9d586a2166cc",
      "fd3ffeed17a34a239a944ac7e38ead15",
      "2ddcbac5b6c246d2b464691289381880",
      "89e084fe1aee40d1918366c7e6f41bc7",
      "8de7b04805fc4135a14692f79117f635",
      "e993d0f3550d48208a419aed72f94dbd",
      "af8adf225dcb4a229fa676a7fb22bbf8",
      "903775651fb04211999f50d14ef15d0b",
      "1f06f4dc04604e34baf21d011c593b3d",
      "31ca03a4333544efa7642760cf8aa76c",
      "6590f5ad18d1420894c2db859fec4d89",
      "00bc407b34c84837a1553dacfa2059f8",
      "a6e483f62f344e169eab868c139c97de",
      "9b419b43c3a5418195e299f5d7a617d2",
      "14377fef54cd4f69a6b064cd1d27ef05",
      "9b5ed5f2814e4b7aa8270f6a68521ee2",
      "a322508b9ab145c68fe13045664b4ce3",
      "0b501e43293f4f9eac73b96ed3bd0b73",
      "09f09dd0e1694e12a22966a906aace7f",
      "e74e94da89b24e8cbf8ccc415ecdee80",
      "37b528740213494cb5834eb9a9b932a9",
      "1fb81bd4ebc14092aa819a441fb84351",
      "c25fda3e3f5e42b8a074c365d92b290b",
      "c30600e31aa047308b7c7a27bcf0ccaf",
      "8f6d3ecdcb5c4330ae023567bcc5b05f",
      "0e787950fd81479c9b1181039521b5c1",
      "72229f3e83724ac0ba83bb7dd327d57c",
      "12e2f6365fe448a0982d4f71b85f9f27",
      "0ef18cc049324a99afb2126ec7d0c317",
      "3f0c18f01688441d8e39738694b0472d",
      "72b674ba0ad84901a0416c981d38b4e2",
      "e6837f24d7e44a5bb7e4ec4ecd80cd6e",
      "36e1f2582d664ef8b4c568ed1e30a69f",
      "cb714475e8664f3494e49bd4621dd36a",
      "0e99c50fb49b41eda16032a07d07a462",
      "4ae9be15ed3a4d489c46132751191015",
      "230d53535b664a5e9139ff38a7903d0b",
      "062613ad5cb440fb9fbc927b1e1390ba",
      "b015e4178db64f78807068a68f096926",
      "a968ca7442e34455ae54da41073a32cd",
      "74713d4b266940859de6706fdb188454",
      "c70269c8b2e647c0b04b01fc17b381f0",
      "bef80c626aea478d9fef0d3234baa955",
      "8af0772657d2463b9d270d525610a64b",
      "3a0c43f0d8284a83b56b781fb580da0c",
      "504a5c4ac5d947029d9eacb2e966c7dd",
      "8fb144293f924708b3d8928ac831690a",
      "41c787f2a7c64ee08cb89ba22335ead1",
      "b75be34ccd1d481aa083859537a30a72",
      "75fb3c20613a45799639a0313cbaeaa4",
      "69380876d71647c480193f1262561880",
      "4365608d53894d0ea0f68cc5a77ed575",
      "edf5e7873fc0496bb6f4f3bb0d7041d7",
      "dfa2a5f4879f4956b074a3f9b9fc09f9",
      "850e33e74a0943fda1c8cdd487e1cfe4",
      "7f87b8f4f3124c08bc39e77a960ce5ec",
      "5d8ebe269aca4f0baf3d8be10a161a38",
      "5c18880ebbf746219bf2b94cb29ebfdd",
      "f707b0cad4de49299d7e237b1ec7a585",
      "0242b8234eda4f71b68fecd3583a8b5b",
      "10f17a299ae840f392a8a8604dc990b3",
      "5d3fd4e05e584928a84234f4b3496460",
      "01310fac99a847c6ac28b53899b9d988",
      "312570295a7347f896a0f354bb1085b3",
      "acdb8e591c9943488a88388171ad1928",
      "cda429ceb37647e49af556206edf5e48",
      "6c1d6541e97249b39a62f0213d740161",
      "35496bca8b944119818394a753bc821d",
      "094b72c8b54a446ba87495c1f8b753fa",
      "07946af39e0943ecbcd798736b583a0b",
      "e1d74afe1f37493b9ca3d5c978086065",
      "9b508f5a793945e79acc03f20cc37342",
      "e0c8a04022d74144add237986c8df37c",
      "7ab087244fd548abb72feb93047ef23c",
      "e5218440a2dc423ab41a0a21184f82f6",
      "827e0087af7349289b7a8e8b6b4f68e0",
      "8bd032c5c83e498bae90b94c14c8708b",
      "81103ebf2d494e908297fc4033f67f2b",
      "c0ce870965634c0eb3269d11e00bdaab",
      "e951be8c0a264b9da44d510d152024ee",
      "ece765580f784af5add5ff770f770b46",
      "7a1e06ca0c154d8c86b6e1b39249fe0b",
      "4f0924a32bd04841bcfdea0519706eb1",
      "0d5d4a09894f4d9ab9e26a306d7d6c1b",
      "7e7c84095b8f40bdbd5ee86f49b47391",
      "a78001911f9146f99056afea12ddae45",
      "4d898d871a944d78a63d69225791652d",
      "72e6e26179664d47ae08b7a09fba30f8",
      "50cab9a19ed24616956957f5a83ae777",
      "7a462f46352741bebbf9412be11b135f",
      "d1de5d0dcca847d7bb7856b743a5bbe2",
      "993b2fbdb6744566aae0574095133d97",
      "2c2becdf5b2f421987c44e853bed987e",
      "63151a4dccb74ddc81b224c7b70e5f4c",
      "2d4d4370240d4959a3b74e47d386e0d1",
      "c824390a4d51499394b25e56177af2c9",
      "267e4ba1bac74e289eef1f3e09f1e394",
      "aac80af5800a4a779bbe601a29c4293e",
      "c65807af79404d4d9d421c840192e3ff",
      "39109eec5e19431da129e8873a4be393",
      "db0f9f5b426c4de8bf859bf2613b401c",
      "6c5a7abcb26f4e1d9ede7223c2a34cb7",
      "23e76c5be1634c10ab561b8ec013d21a",
      "8f81430d895640e28fa7b7c94c2f2410",
      "e247e46eb2244ebba1c1d7f8f1200df6",
      "ac0159278b0a49c29e664a1e79b11666",
      "97f1ea41e4dc4ca6b2fb4891f9fe0554",
      "f9e72ec7c8b74183a9065d9227db51b1",
      "a2fa3a542c134011aeac4bf4f03b7157",
      "678be128ecb94cd8ba985084a6a2114c",
      "5604aa075c344787b40759440d9debf0",
      "ca86712e10fb41e8ad0fa2ccf1c73e94",
      "53801739cbf44e44acf21951fcfbc89f",
      "948e26acb6404fbe9fab1add8c7ae918",
      "3549d1086b024e54a6615a7ce7a13f25",
      "86b16d984cf64f378be18285a14a8674",
      "41cca66e10f44a32a33056077801cbc3",
      "72f6bc8829de419aba99b347e8e49942",
      "641c0edb96c84e6dbe95a39976ff309e",
      "9f49298d8dfd495bbc6513a35cdf0577",
      "9a31dff4a54b40629d2cae6c2ec3a5de",
      "5593795c95064a878e55d65a8b66f55d",
      "a86fec5623b74c60859b5ab6249cdf4b",
      "9642adb49c4b4dffbf512eadf8d936fd",
      "a1179ec111e244a1a8dbf7d4829500f8",
      "bde3ae709d094e5297d0c29b45796df8",
      "693632470223488e835a2b115974f194",
      "56971378f6c24aafa2ecf9b052e30448",
      "2e5c0f9230074ea688ff99f1a7669421",
      "5e7e6d62d58442adbe5c36a2eb07f621",
      "4e68df8da3a04f9ea5008e7a7c5c5014",
      "a50b954e60794467a2d84e8404b96b1d",
      "3103450519354b7cb8f0ff3caeef034b",
      "875573745d844ab0911b8fa1b545d2ce",
      "329d418f6939487b98d7a5b5c7931a5a",
      "5ba18733648142b88d84a28cc52102d2",
      "ce1a3e82de2847c5859a73094fa0f57c",
      "e0a63ad76e3741de8713c2d9dfa1303c",
      "030e2a1ebb4e4167890d6d28fe52e779",
      "b9abd4badfbe44e48708123dab08bc4a",
      "f9182f96bdc1438691529d15b524383b",
      "08210557628c4385bb280de0670ac22e",
      "8d82864e776440bf8ea82ba286442035",
      "655fdebfc62f4b71a2301fc417caa6a8",
      "2bbd3af0bd474c97b7b41e681082c095",
      "c8481d192b3242ee945f5897c0da9267",
      "ba826fc4c5c248dbacec8e493cd957f7",
      "49a70e3e9f3e4f98b3b67d1c14d8df6f",
      "3ac241077bc74f99bca13058182646ed",
      "1e16a70fb5134f4bba2fc730dbf026ef",
      "74ec558a08634560aed3fd71d2067901",
      "66a00133f3324687bd8299ab6994dd3a",
      "027f8f8b970041dab13667d50e387a8a",
      "677a0b7d64c9435998629b810205b523",
      "25a20dd726454a24a03906ae805adb38",
      "820f4fba2b0a428aafc7903df17aaf3d",
      "308086b17d7d476688801a03a11c3b99",
      "ec6fa778ffc44bd6a9a7e3b1d2d03d2a",
      "2380c321ee1645c8aa7a41250384f356",
      "6837d7923177493e80d10e43ac87cfd7"
     ]
    },
    "id": "DIan4c3ztJZE"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}